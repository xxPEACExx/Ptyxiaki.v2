<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2058720-B1" country="EP" doc-number="2058720" kind="B1" date="20140108" family-id="40430148" file-reference-id="299959" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146588356" ucid="EP-2058720-B1"><document-id><country>EP</country><doc-number>2058720</doc-number><kind>B1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-08168372-A" is-representative="YES"><document-id mxw-id="PAPP154850548" load-source="docdb" format="epo"><country>EP</country><doc-number>08168372</doc-number><kind>A</kind><date>20081105</date><lang>EN</lang></document-id><document-id mxw-id="PAPP176640827" load-source="docdb" format="original"><country>EP</country><doc-number>08168372.4</doc-number><date>20081105</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140550007" ucid="KR-20070114322-A" load-source="docdb"><document-id format="epo"><country>KR</country><doc-number>20070114322</doc-number><kind>A</kind><date>20071109</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130710</date></intention-to-grant-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989325373" load-source="docdb">G05D   1/02        20060101AFI20121115BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1894825247" load-source="docdb" scheme="CPC">G05D   1/024       20130101 LI20150625BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2044391112" load-source="docdb" scheme="CPC">G05D   1/027       20130101 LA20150625BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2044391172" load-source="docdb" scheme="CPC">G05D   1/0272      20130101 LA20150625BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2044396752" load-source="docdb" scheme="CPC">G05D   1/0274      20130101 FI20150625BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132369701" lang="DE" load-source="patent-office">Vorrichtung und Verfahren zur Erstellung einer dreidimensionalen Karte unter Anwendung von strukturiertem Licht</invention-title><invention-title mxw-id="PT132369702" lang="EN" load-source="patent-office">Apparatus and method for generating three-dimensional map using structured light</invention-title><invention-title mxw-id="PT132369703" lang="FR" load-source="patent-office">Appareil et procédé de génération de carte tridimensionnelle en utilisant de la lumière structurée</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR919503971" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SAMSUNG ELECTRONICS CO LTD</last-name><address><country>KR</country></address></addressbook></applicant><applicant mxw-id="PPAR919522724" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SAMSUNG ELECTRONICS CO., LTD.</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919528145" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>PARK DONG-RYEOL</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919508361" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>PARK, DONG-RYEOL</last-name></addressbook></inventor><inventor mxw-id="PPAR919022733" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>PARK, DONG-RYEOL</last-name><address><street>c/o Samsung Advanced Institute of Technology, Mt. 14-1, Nongseo-dong, Giheung-gu</street><city>Yongin-Si Gyeonggi-Do 449-712</city><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919522403" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>KIM DONG-JO</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919507187" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>KIM, DONG-JO</last-name></addressbook></inventor><inventor mxw-id="PPAR919022735" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>KIM, DONG-JO</last-name><address><street>c/o Samsung Advanced Institute of Technology, Mt. 14-1, Nongseo-dong, Giheung-gu</street><city>Yongin-Si Gyeonggi-Do 449-712</city><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919541780" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>BANG SEOK-WON</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919518654" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>BANG, SEOK-WON</last-name></addressbook></inventor><inventor mxw-id="PPAR919022732" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>BANG, SEOK-WON</last-name><address><street>c/o Samsung Advanced Institute of Technology, Mt. 14-1, Nongseo-dong, Giheung-gu</street><city>Yongin-Si Gyeonggi-Do 449-712</city><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919537788" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>LEE HYOUNG-KI</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919505964" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>LEE, HYOUNG-KI</last-name></addressbook></inventor><inventor mxw-id="PPAR919022734" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>LEE, HYOUNG-KI</last-name><address><street>c/o Samsung Advanced Institute of Technology, Mt. 14-1, Nongseo-dong, Giheung-gu</street><city>Yongin-Si Gyeonggi-Do 449-712</city><country>KR</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR919022737" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Samsung Electronics Co., Ltd.</last-name><iid>101328413</iid><address><street>129, Samsung-ro Yeongtong-gu</street><city>SUWON-SI, GYEONGGI-DO, 443-742</city><country>KR</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR919022736" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Grünecker, Kinkeldey, Stockmair &amp; Schwanhäusser Anwaltssozietät</last-name><iid>100060488</iid><address><street>Leopoldstrasse 4</street><city>80802 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS549923672" load-source="docdb">AT</country><country mxw-id="DS549892051" load-source="docdb">BE</country><country mxw-id="DS549869367" load-source="docdb">BG</country><country mxw-id="DS549872719" load-source="docdb">CH</country><country mxw-id="DS549802180" load-source="docdb">CY</country><country mxw-id="DS549923673" load-source="docdb">CZ</country><country mxw-id="DS549892052" load-source="docdb">DE</country><country mxw-id="DS549802181" load-source="docdb">DK</country><country mxw-id="DS549802182" load-source="docdb">EE</country><country mxw-id="DS549784788" load-source="docdb">ES</country><country mxw-id="DS549869368" load-source="docdb">FI</country><country mxw-id="DS549869369" load-source="docdb">FR</country><country mxw-id="DS549892057" load-source="docdb">GB</country><country mxw-id="DS549802183" load-source="docdb">GR</country><country mxw-id="DS549892058" load-source="docdb">HR</country><country mxw-id="DS549923674" load-source="docdb">HU</country><country mxw-id="DS549872720" load-source="docdb">IE</country><country mxw-id="DS549892059" load-source="docdb">IS</country><country mxw-id="DS549869374" load-source="docdb">IT</country><country mxw-id="DS549802184" load-source="docdb">LI</country><country mxw-id="DS549878351" load-source="docdb">LT</country><country mxw-id="DS549923675" load-source="docdb">LU</country><country mxw-id="DS549878352" load-source="docdb">LV</country><country mxw-id="DS549878353" load-source="docdb">MC</country><country mxw-id="DS549784113" load-source="docdb">MT</country><country mxw-id="DS549923676" load-source="docdb">NL</country><country mxw-id="DS549892060" load-source="docdb">NO</country><country mxw-id="DS549802185" load-source="docdb">PL</country><country mxw-id="DS549878358" load-source="docdb">PT</country><country mxw-id="DS549923677" load-source="docdb">RO</country><country mxw-id="DS549802186" load-source="docdb">SE</country><country mxw-id="DS549878359" load-source="docdb">SI</country><country mxw-id="DS549892065" load-source="docdb">SK</country><country mxw-id="DS549784114" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><description mxw-id="PDES63960779" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p0001" num="0001">This application claims priority from Korean Patent Application No.<patcit id="pcit0001" dnum="KR1020070114322"><text> 10-2007-0114322 filed on November 9, 2007</text></patcit> in the Korean Intellectual Property Office.</p><heading id="h0002">BACKGROUND</heading><p id="p0002" num="0002">1. Field</p><p id="p0003" num="0003">One or more embodiments of the present invention relate to an apparatus and method for generating a three-dimensional map, and, more particularly, to an apparatus and method for generating a three-dimensional map using structured light that is emitted upward to a ceiling.</p><p id="p0004" num="0004">2. Description of the Related Art</p><p id="p0005" num="0005">In general, robots have been developed to improve factory automation. In recent years, robots available for home and office use as well as industrial robots have been put to practical use. Examples of the robots available for home and office use include cleaning robots, guide robots, and security robots.</p><p id="p0006" num="0006">In mobile robots, such as cleaning robots, it is generally indispensable to create a map that is recognized by the robots in order to designate a traveling path of the robot or a working area of the robot. In order to create the map, various methods, such as methods of using a vision sensor, an ultrasonic sensor, and a touch sensor, have been used to control the robot to create a map while being autonomously navigated. However, a method of using structured light and a camera is very effective since it needs a small amount of computation and can be used in a place where there is a large variation in brightness.</p><p id="p0007" num="0007">In this method, as shown in <figref idrefs="f0001">FIG. 1</figref>, an active light source 10, such as a laser, is used to emit a predetermined laser beam to an obstacle 30, and a sensor 20, such as a camera, captures an image formed by the laser beam reflected from the obstacle. Then, it is possible to calculate the distance between the laser emission point and the obstacle 30, which is a reflection point, by using a triangular method, on the basis of coordinates of the image captured by the camera 20, the radiation angle of light, and the distance between the camera 20 and the laser emission point.<!-- EPO <DP n="2"> --></p><p id="p0008" num="0008">In the related art, since light is emitted in the forward direction in which the mobile robot moves, the mobile robot can generate only a two-dimensional map due to limited information.</p><p id="p0009" num="0009">A three dimensional map-generating apparatus is known from <patcit id="pcit0002" dnum="WO2007051972A"><text>WO 2007/051972</text></patcit>.</p><p id="p0010" num="0010">In <figref idrefs="f0001">FIG. 1</figref>, a distance d between the light source 10 and the camera sensor 20 is referred to as a baseline. The longer the distance becomes, the higher the range resolution becomes. When robots having a limited height, such as cleaning robots, emit light forward, in many cases, the baseline is short. In this case, the range resolution at a long distance is lowered, which makes it difficult to generate a map.</p><heading id="h0003">SUMMARY</heading><p id="p0011" num="0011">An aspect of the present invention is to provide an apparatus and method for generating a three-dimensional map by radiating structured light upward and accumulating pose information of a mobile robot and distance data obtained at each pose.</p><p id="p0012" num="0012">Additional aspects and/or advantages will be set forth in part in the description which follows and, in part, will be apparent from the description, or may be learned by practice of the invention.</p><p id="p0013" num="0013">According to an aspect of the present invention, there is provided a three-dimensional map-generating apparatus. The apparatus includes an odometer detecting a pose of a mobile robot, a distance-measuring sensor, including a light source module disposed on an upper surface of the mobile robot to emit light upward, relative to a movement direction of the mobile robot, and a camera module disposed on an upper surface of the mobile robot to capture an image formed by the light as reflected from an obstacle, measuring a distance to the obstacle for the mobile robot using the captured image, and a map-generating unit generating a three-dimensional map using the distance measured by the distance-measuring sensor while changing the pose of the mobile robot.</p><p id="p0014" num="0014">According to another aspect of the present invention, there is provided a three-dimensional map-generating method. The method includes detecting a pose of a mobile robot, measuring a distance to an obstacle for the mobile robot using a distance-measuring sensor that includes a light source module emitting light upward, relative to a movement direction of the mobile robot, and a camera module capturing an image formed by the light as reflected from the obstacle, and measuring the distance to the obstacle using the captured image, and generating a three-dimensional map by measuring the distance while changing the pose of the mobile robot.<!-- EPO <DP n="3"> --></p><heading id="h0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0015" num="0015">The above and other features and advantages of the present invention will become apparent by describing in detail preferred embodiments thereof with reference to the attached drawings in which:
<ul><li><figref idrefs="f0001">FIG. 1</figref> is a diagram illustrating an apparatus for measuring a distance using structured light;</li><li><figref idrefs="f0002">FIG. 2</figref> is a diagram illustrating the emission of light to an obstacle in order to capture a structured light image;</li><li><figref idrefs="f0003">FIG. 3</figref> is a diagram illustrating a camera image captured by a camera sensor shown in <figref idrefs="f0002">FIG. 2</figref>;</li><li><figref idrefs="f0004">FIG. 4</figref> is a block diagram illustrating a three-dimensional map-generating apparatus using structured light, according to an embodiment of the invention;</li><li><figref idrefs="f0005">FIG. 5</figref> is a diagram illustrating the structure of a distance-measuring sensor of the three-dimensional map-generating apparatus using structured light, according to an embodiment of the invention;</li><li><figref idrefs="f0006">FIG. 6</figref> is a diagram illustrating a process of radiating a laser beam onto the ceiling and wall to obtain distance data using the three-dimensional map-generating apparatus using structured light, according to an embodiment of the invention;</li><li><figref idrefs="f0007">FIGS. 7</figref> and <figref idrefs="f0008">8</figref> are diagrams illustrating a process of obtaining the distance data for generating a three-dimensional map while moving and rotating the three-dimensional map-generating apparatus using structured light, according to an embodiment of the invention to change the pose thereof;</li><li><figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref> are diagrams illustrating three-dimensional maps obtained by the three-dimensional map-generating apparatus using structured light according to the embodiment of the invention, as viewed from the ceiling and the wall, respectively;</li><li><figref idrefs="f0011">FIG. 10</figref> is a flowchart illustrating a method of generating a three-dimensional map using structured light, according to an embodiment of the invention;</li><li><figref idrefs="f0012">FIG. 11</figref> is a flowchart illustrating a process of detecting the position of the mobile robot using the three-dimensional map generated, according to an embodiment of the invention;</li><li><figref idrefs="f0013">FIG. 12</figref> is a flowchart illustrating a method of moving the mobile robot in parallel to the wall; and</li><li><figref idrefs="f0014">FIG. 13</figref> is a diagram illustrating a process of positioning the mobile robot in parallel to the wall.</li></ul><!-- EPO <DP n="4"> --></p><heading id="h0005">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p0016" num="0016">Reference will now be made in detail to embodiments, examples of which are illustrated in the accompanying drawings, wherein like reference numerals refer to like elements throughout.</p><p id="p0017" num="0017">Exemplary embodiments of the present invention will hereinafter be described with reference to block diagrams or flowcharts. It will be understood that each block of the flowchart illustrations, and combinations of blocks in the flowchart illustrations, can be implemented by computer readable code/instructions. This computer readable code can be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create elements for implementing the operations specified in the flowchart block or blocks, for example.</p><p id="p0018" num="0018">Thus, embodiments of the present invention may be implemented through such computer readable code/instructions in/on a medium, e.g., a computer readable medium, to control at least one processing element to implement any above described embodiment. The medium can correspond to any medium/media permitting the storing and/or transmission of the computer readable code.</p><p id="p0019" num="0019">The computer readable code can be recorded/transferred on a medium in a variety of ways, with examples of the medium including recording media, such as magnetic storage media (e.g., ROM, floppy disks, hard disks, etc.) and optical recording media (e.g., CD-ROMs, or DVDs), and transmission media such as media carrying or controlling carrier waves as well as elements of the Internet, for example. Thus, the medium may be such a defined and measurable structure carrying or controlling a signal or information, such as a device carrying a bitstream, for example, according to embodiments of the present invention. The media may also be a distributed network, so that the computer readable code is stored/transferred and executed in a distributed fashion. Still further, as only an example, the processing element could include a processor or a computer processor, and processing elements may be distributed and/or included in a single device.<!-- EPO <DP n="5"> --></p><p id="p0020" num="0020">Below, first will be discussed a principle of measuring a distance using structured light according to an embodiment of the invention.</p><p id="p0021" num="0021"><figref idrefs="f0001">FIG. 1</figref> is a side view illustrating a distance measuring apparatus using structured light, <figref idrefs="f0002">FIG. 2</figref> is a diagram illustrating light emitted to an obstacle in order to obtain a structured light image, <figref idrefs="f0003">FIG. 3</figref> is a diagram illustrating an image captured by the camera sensor shown in <figref idrefs="f0002">FIG. 2</figref>. Herein, the term apparatus should be considered synonymous with the term system, and not limited to a single enclosure or all described elements embodied in single respective enclosures in all embodiments, but rather, depending on embodiment, is open to being embodied together or separately in differing enclosures and/or locations through differing elements, e.g., a respective apparatus/system could be a single processing element or implemented through a distributed network, noting that additional and alternative embodiments are equally available.</p><p id="p0022" num="0022">An active light source 10, such as a laser, is used to emit light to an obstacle 30, and a camera sensor 20 acquires image information reflected from the obstacle 30. In this case, the camera sensor 20 is positioned above the light source 10 at a constant distance d from the light source 10, and acquires the image information. The light source 10 may be a near infrared line laser. The near infrared laser beam makes it possible to acquire image information in a dark place in which there is no light.</p><p id="p0023" num="0023">Referring to <figref idrefs="f0002">FIG. 2</figref>, the light source 10 emits laser beams to the obstacle 30 at a constant viewing angel α in a plan view. <figref idrefs="f0003">FIG. 3</figref> shows a camera image 40 in the form of a line profile that is captured by the camera module 20. Light components reflected from points a and b of the obstacle 30 shown in <figref idrefs="f0002">FIG. 2</figref> are represented as points a and b of the camera image 40 shown in <figref idrefs="f0003">FIG. 3</figref>, and values in a Y-axis direction are proportional to the distance between the camera sensor 20 and the obstacle 30.</p><p id="p0024" num="0024">Distance data between the light source 10 and the obstacle 30 is calculated by a triangular method on the basis of the distance between the camera module 20 and the obstacle 30 obtained from coordinates on the camera image 40, the angle θ of the camera sensor 20 with respect to the obstacle 30, and a distance d between the camera module 20 and the light source 10. Since the triangular method has been known, a detailed description thereof will be omitted.</p><p id="p0025" num="0025">Next, a three-dimensional map-generating apparatus using structured light according to an embodiment of the invention will be described.<!-- EPO <DP n="6"> --></p><p id="p0026" num="0026"><figref idrefs="f0004">FIG. 4</figref> is a block diagram illustrating the three-dimensional map-generating apparatus using structured light according to the embodiment of the invention. <figref idrefs="f0005">FIG. 5</figref> is a diagram illustrating the structure of a distance-measuring sensor of the three-dimensional map-generating apparatus using structured light according to the embodiment of the invention. <figref idrefs="f0006">FIG. 6</figref> is a diagram illustrating the measurement of distance data by radiating a laser beam on the ceiling and wall using the three-dimensional map-generating apparatus using structured light according to the embodiment of the invention.</p><p id="p0027" num="0027">The three-dimensional map-generating apparatus using structured light according to the embodiment of the invention may include an odometer 110 and a distance-measuring sensor 120 having a light source module 122 and a camera module 124. The three-dimensional map-generating apparatus may further include a plane-extracting unit 210, a feature-map-generating unit 220, and a position-detecting unit 230.</p><p id="p0028" num="0028">The odometer 110 is provided in a mobile robot to detect the relative pose of the mobile robot. The term "pose" means the position and direction angle of the mobile robot. When the mobile robot moves from pose A to pose B, the odometer 110 can detect a variation in the pose of the mobile robot with respect to pose A. For example, an encoder or a gyrosensor may be used as the odometer 110. The encoder integrates the distance and direction of the mobile robot in order to know the current pose of the mobile robot.</p><p id="p0029" num="0029">The distance-measuring sensor 120 includes a light source module 122 that emits light and a camera module 124 that captures the image of an obstacle using light reflected therefrom, and measures the distance from the obstacle. In the present embodiment, a line laser is used as the light source. When the line laser emits light at a predetermined angle in the horizontal direction, the camera module captures an image formed by light reflected from the obstacle within the emission range of light, thereby obtaining distance data. As described above, the distance data may be calculated by the triangular method.</p><p id="p0030" num="0030">As shown in <figref idrefs="f0005">FIG. 5</figref>, the distance-measuring sensor 120 according to the embodiment of the invention is provided at an upper part of the mobile robot, emits light to the ceiling, that is, in the upward direction, and measures the distance from an obstacle on the ceiling.</p><p id="p0031" num="0031">In particular, a cleaning robot is preferably designed to be thin. However, as described above, in the related art, when a mobile robot emits light forward, a base line is shortened, and resolution at a long distance is lowered, which results in difficulties in generating a map. However, In the present embodiment, as shown in <figref idrefs="f0005">FIG. 5</figref>, the light source module 122 and the<!-- EPO <DP n="7"> --> camera module 124 are provided at an upper part of the mobile robot to lengthen a base line d, which makes it possible to solve the problem of the resolution being lowered.</p><p id="p0032" num="0032">In addition, in the present embodiment, since a laser beam is used, it is possible to perform the generation of a map and localization in dark environments.</p><p id="p0033" num="0033"><figref idrefs="f0006">FIG. 6</figref> shows a process of obtaining distance data by radiating laser beams upward in the actual environment in which there is a ceiling and walls. As shown in <figref idrefs="f0006">FIG. 6</figref>, it is possible to obtain distance data between the mobile robot and the ceiling or the wall within the radiation range of the laser beam. As the laser beam is radiated at a larger angle, the distance data can be obtained in a wider range. Alternatively, two pairs of the light source module and the camera module having a predetermined angle range may be provided to obtain distance data in the entire range on the upper surface of the mobile robot, that is, at an angle of 180° on the upper surface of the mobile robot.</p><p id="p0034" num="0034">The three-dimensional map-generating apparatus using structured light according to the embodiment of the invention generates a three-dimensional map using pose information and distance data information that are respectively obtained from the odometer 110 and the distance-measuring sensor 120 while changing the pose of the mobile robot.</p><p id="p0035" num="0035">Next, the principle of generating a three-dimensional map using pose information and distance data information will be described with reference to <figref idrefs="f0007">FIGS. 7</figref> and <figref idrefs="f0008">8</figref>.</p><p id="p0036" num="0036"><figref idrefs="f0007">FIGS. 7</figref> and <figref idrefs="f0008">8</figref> are diagrams illustrating a process of obtaining distance data for generating a three-dimensional map by moving and rotating the three-dimensional map-generating apparatus using structured light according to the embodiment of the invention to change the pose thereof.</p><p id="p0037" num="0037"><figref idrefs="f0007">FIG. 7</figref> is a diagram illustrating the operation of the mobile robot measuring the distance using the distance-measuring sensor 120 while moving in a straight line. When the mobile robot is disposed at a position corresponding to a solid line, the solid line drawn on the ceiling and the wall indicates distance information acquired by the line laser. Then, the pose of the mobile robot is changed to measure the distance. That is, in <figref idrefs="f0007">FIG. 7</figref>, the position of the mobile robot is changed to measure the distance. In <figref idrefs="f0007">FIG. 7</figref>, assuming that the mobile robot moves in parallel to both walls to a position corresponding to a dotted line to measure the distance, the distance data obtained by the distance-measuring sensor 120 at the position corresponding to the solid line is identical with the distance data obtained by the distance-measuring sensor 120 at the position corresponding to the dotted line. That is, only two-dimensional distance data is obtained<!-- EPO <DP n="8"> --> at the positions corresponding to the solid line and the dotted line. However, it is possible to obtain three-dimensional distance data by acquiring from the odometer 110 information indicating that the mobile robot is moved in a straight line from the position corresponding to the solid line by a distance of d, and reflecting the acquired information to the distance data obtained at the position corresponding to the dotted line.</p><p id="p0038" num="0038"><figref idrefs="f0008">FIG. 8</figref> is a diagram illustrating the operation of the mobile robot measuring the distance using the distance-measuring sensor 120 while rotating at a fixed position in the counterclockwise direction.</p><p id="p0039" num="0039">When the mobile robot is disposed at a position corresponding to a solid line, the solid line drawn on the ceiling and the side walls indicates distance information acquired by the line laser, similar to <figref idrefs="f0007">FIG. 7</figref>. Then, the pose of the mobile robot is changed to measure the distance. That is, in <figref idrefs="f0008">FIG. 8</figref>, the mobile robot is rotated at a fixed position in the counterclockwise direction to measure the distance. As described above, distance data obtained by the distance-measuring sensor 120 at the positions is two-dimensional data. However, it is possible to obtain three-dimensional distance data by acquiring from the odometer 110 information indicating that the mobile robot is rotated in the counterclockwise direction from the position corresponding to the solid line by an angle of Θ and reflecting the acquired information to the distance data obtained at the position corresponding to the dotted line.</p><p id="p0040" num="0040">That is, as described above, it is possible to obtain three-dimensional distance data at the current pose by reflecting relative pose information obtained from the odometer 110 to two-dimensional distance data obtained from the distance-measuring sensor 120. In addition, it is possible to generate a three-dimensional map by accumulating three-dimensional distance data obtained by changing the pose of the mobile robot.</p><p id="p0041" num="0041"><figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref> are diagrams illustrating three-dimensional maps obtained by the three-dimensional map-generating apparatus using structured light according to the present embodiment of the invention, as viewed from the ceiling and the wall, respectively.</p><p id="p0042" num="0042"><figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref> show the three-dimensional maps that are generated on the basis of the distance data obtained by the rotation of the mobile robot at a fixed position. In <figref idrefs="f0009">FIG. 9A</figref>, a hollow 310 without distance information is formed at the center of the map, and the mobile robot is positioned at the center of the hollow 310. The hollow 310 without distance information is formed within a predetermined distance range from the position of the mobile robot since the<!-- EPO <DP n="9"> --> light source module 122 is provided at the edge of the mobile robot, not the center, as shown in <figref idrefs="f0005">FIG. 5</figref>.</p><p id="p0043" num="0043">As shown in <figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref>, it is possible to generate a three-dimensional map by accumulating the three-dimensional distance data. It is possible to find edges between the ceiling and the wall and an apex of the ceiling formed by intersection of two edges from the three-dimensional map, and it is also possible to acquire information on the ceiling, a ventilating hole formed in the ceiling, and a fluorescent lamp provided on the ceiling from the three-dimensional map.</p><p id="p0044" num="0044">The plane-extracting unit 210 extracts a plane from the three-dimensional map. As shown in <figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref>, it is possible to extract edges between the ceiling and the wall and an apex formed by intersection between two edges, and thus the plane-extracting unit 210 can extract a plane, such as the ceiling, from the extracted edges and apexes.</p><p id="p0045" num="0045">The feature-map-generating unit 220 extracts feature points, such as apexes, from the extracted plane, and generates a feature map on the basis of the feature points.</p><p id="p0046" num="0046">The position-detecting unit 230 compares the feature points extracted from the map at the current position with feature points of the feature map stored beforehand to detect the position of the mobile robot. A process of detecting the position of the mobile robot will be described below with reference to <figref idrefs="f0012">FIG. 11</figref>.</p><p id="p0047" num="0047"><figref idrefs="f0011">FIG. 10</figref> is a flowchart illustrating a method of generating a three-dimensional map using structured light according to an embodiment of the invention.</p><p id="p0048" num="0048">First, the pose of the mobile robot is changed (S410). Then, the odometer 110 detects relative pose information (S420). The term "relative pose information" means a variation in the position and direction angle of the mobile robot between the previous pose and the current pose.</p><p id="p0049" num="0049">Then, a line laser is used to acquire range information at the current position (S430). The term "range information" means distance data that is obtained by radiating a laser beam from the line laser within a predetermined angle range. The pose information is reflected to the acquired range information to generate three-dimensional distance data (S440), thereby updating the three-dimensional map. In this case, these processes are repeated until the number of range information items is larger than a threshold value (S450) to update the three-dimensional map. The threshold value may be a predetermined number of times the pose of the mobile robot is changed, or an arbitrary number required to acquire specific information. The<!-- EPO <DP n="10"> --> arbitrary number required to acquire specific information may be the number of range information items required to measure the overall area of the ceiling. That is, these processes are repeated until information on the overall area of the ceiling is acquired. When the information on the overall area of the ceiling is acquired, the generation of the three-dimensional map may end.</p><p id="p0050" num="0050">After generating the three-dimensional map, the plane-extracting unit 210 may extract a plane from the three-dimensional map (S460). As shown in <figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref>, it is possible to extract edges between the ceiling and the wall and an apex formed by intersection between two edges, and thus the plane-extracting unit 210 can extract a plane, such as the ceiling, from the extracted edges and apexes.</p><p id="p0051" num="0051">Then, the feature-map-generating unit 220 may extract feature points from the extracted plane and generate a feature map (S470). The generated feature map may be used to detect the position of the mobile robot, which will be described below.</p><p id="p0052" num="0052">It is possible to detect the current position of the mobile robot using the three-dimensional map generated according to an embodiment of the invention.</p><p id="p0053" num="0053"><figref idrefs="f0012">FIG. 11</figref> is a flowchart illustrating a process of detecting the position of a mobile robot using the three-dimensional map generated according to an embodiment of the invention.</p><p id="p0054" num="0054">First, a three-dimensional map is generated at the current position of the mobile robot while the mobile robot is rotated (S510). The generation of the three-dimensional map by the rotation of the mobile robot at the fixed position has been described with reference to <figref idrefs="f0009">FIGS. 9A</figref> and <figref idrefs="f0010">9B</figref>. Then, feature points, such as apexes, are extracted from the three-dimensional map generated at the current position (S520). The extracted feature points are compared with the feature points of the map stored beforehand to calculate a robot position probability on the entire map (S530). When the three-dimensional map is generated, regions may be divided due to, for example, the limit of the radiation angle of the line laser and the wall. In this case, when the three-dimensional map is generated by the rotation of the mobile robot at the fixed position as shown in <figref idrefs="f0009 f0010">FIG. 9</figref>, it is difficult to generate a three-dimensional map including the entire region of the ceiling or all the regions of the map, but a three-dimensional map for only a local area is formed. Therefore, the three-dimensional map generated at the current position may be a portion of the entire three-dimensional map. The feature points extracted from the three-dimensional map generated at the current position may also be some of all the feature points forming the entire feature map. Therefore, it is possible to calculate the robot position probability<!-- EPO <DP n="11"> --> on the entire map by comparing the distribution of the feature points that are extracted from the three-dimensional map generated at the current position with the distribution of the feature points forming the entire feature map. For example, a Monte Carlo method may be used to calculate the robot position probability. The Monte Carlo method is used to calculate the probability distribution of a desired numerical value from repeated statistical sampling experiments, and a detailed description thereof will be omitted. When the calculated robot position probability is smaller than a predetermined reference value, the detection of the robot fails, and the robot is disposed at another position (S550). The above-mentioned processes are repeated to detect the position of the mobile robot again. In contrast, when the robot position probability is larger than the predetermined reference value, the position having the probability value may be determined as the current position of the mobile robot.</p><p id="p0055" num="0055">As an application of the three-dimensional map-generating apparatus and method according to the embodiments of the invention, the mobile robot may travel in parallel to the wall.</p><p id="p0056" num="0056"><figref idrefs="f0013">FIG. 12</figref> is a flowchart illustrating a method of moving the mobile robot in parallel to the wall, and <figref idrefs="f0014">FIG. 13</figref> is a diagram illustrating a process of positioning the mobile robot in parallel to the wall.</p><p id="p0057" num="0057">The mobile robot radiates a laser beam at the current position to measure a distance d1 to the wall (S610). The distance to the wall means the shortest distance between the surface of the mobile robot from which the laser beam is emitted and the wall, not the shortest distance between the mobile robot and the wall. When the distance to the wall is known at a specific radiation angle, it is possible to calculate the shortest distance between the mobile robot and the wall in the horizontal direction by using the triangular method using the angle and the distance. As shown in <figref idrefs="f0014">FIG. 13</figref>, when the laser beam is radiated upward in a direction vertical to the front of the mobile robot, the distance between the mobile robot and the wall is d1.</p><p id="p0058" num="0058">Then, the mobile robot is rotated in a specific direction (S620). As can be seen from <figref idrefs="f0014">FIG. 13</figref>, the mobile robot is rotated in the clockwise direction. After the rotation in the clockwise direction, the distance between the front of the mobile robot and the wall is represented by a dotted line.</p><p id="p0059" num="0059">The mobile robot is rotated by a predetermined angle in the clockwise direction as shown in <figref idrefs="f0014">FIG. 13</figref>, and then a distance d2 between the mobile robot and the wall is measured by the above-mentioned method (S630).<!-- EPO <DP n="12"> --></p><p id="p0060" num="0060">Then, the distance d1 is compared with the distance d2 (640). When the distance d2 is smaller than the distance d1, the distance between the mobile robot and the wall decreases due to the rotation of the mobile robot, which means that the mobile robot is parallel to the wall with the rotation of the mobile robot. This is because the distance between the mobile robot and the wall is the smallest when the mobile robot is parallel to the wall. Therefore, when d1 &gt; d2 is satisfied, d2 is set to d1 (S650), and the mobile robot is rotated in the same direction as described above by a predetermined angle (S620). After the rotation, the distance d2 between the mobile robot and the wall is measured, and the distance d2 is compared with the distance d1. When the distance d2 is larger than the distance d1, the mobile robot is rotated from a position that is parallel to the wall to a position that is not parallel to the wall. Therefore, the mobile robot is rotated to a position corresponding to the distance d1 (S660). In this case, it is assumed that the mobile robot is disposed at a position parallel to the wall. The mobile robot is disposed in parallel to the wall and then moves forward to encounter the front wall (S670), thereby moving the mobile robot in parallel to the wall.</p><p id="p0061" num="0061">In particular, the cleaning efficiency of a cleaning robot greatly depends on the traveling path of the robot. Therefore, as described above, when the cleaning robot is moved in parallel to the wall, it is possible to improve the cleaning efficiency of the cleaning robot, as compared to the existing random method.</p></description><claims mxw-id="PCLM56985074" lang="DE" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-de-01-0001" num="0001"><claim-text>Eine Vorrichtung zum Erzeugen einer drei-dimensionalen Karte, die umfasst:
<claim-text>ein Hodometer (110), das eine Pose eines mobilen Roboters detektiert;</claim-text>
<claim-text>einen entfernungsmessenden Sensor (120), der ein Lichtquellenmodul (122) enthält, das an einer oberen Oberfläche des mobilen Roboters angeordnet ist, um, bezogen auf eine Bewegungsrichtung des mobilen Roboters, Licht nach oben auszustrahlen, und der ein Kameramodul (124) enthält, das an der oberen Oberfläche des mobilen Roboters angeordnet ist, um ein Bild zu erfassen, das durch das von einem Hindernis zurückgestrahlte Licht geformt wird, und der außerdem eine Entfernung zu dem Hindernis für den mobilen Roboter unter Nutzung des erfassten Bildes misst; und</claim-text>
<claim-text>eine kartenerzeugende Einheit (130), die eine drei-dimensionale Karte unter Nutzung der von dem entfernungsmessenden Sensor beim Ändern der Pose des mobilen Roboters gemessenen Entfernung erzeugt.</claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 1, wobei das Lichtquellenmodul ein Laser ist.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 1, wobei die kartenerzeugende Einheit drei-dimensionale Daten sammelt, die durch Abbilden der Pose des mobilen Roboters auf, beim Ändern der Pose des mobilen Roboters gemessene, zwei-dimensionale Entfernungsdaten erhalten werden, und dabei die drei-dimensionale Karte erzeugt.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 1, wobei die Pose des mobilen Roboters eine Position und einen Richtungswinkel des mobilen Roboters enthält.<!-- EPO <DP n="17"> --></claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 1, wobei das Hodometer ein Codierer oder ein Kreisel ist.</claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 1, die des Weiteren umfasst:
<claim-text>eine flächenextrahierende Einheit (210), die eine Fläche aus der erzeugten drei-dimensionalen Karte extrahiert.</claim-text></claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 6, die des Weiteren umfasst:
<claim-text>eine Einheit (220) zum Erzeugen einer Merkmalskarte, die Merkmalpunkte von der extrahierten Fläche extrahiert, um eine Merkmalskarte zu erzeugen.</claim-text></claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Die Vorrichtung zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 7, die des Weiteren umfasst:
<claim-text>eine Einheit (230) zum Detektieren einer Position, die die Merkmalpunkte, die an einer gegenwärtigen Position aus der Karte des mobilen Roboters extrahiert wurden, mit zuvor gespeicherten Merkmalpunkten auf der Merkmalskarte vergleicht, um die Position des mobilen Roboters zu detektieren.</claim-text></claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Ein Verfahren zum Erzeugen einer drei-dimensionalen Karte, wobei das Verfahren umfasst:
<claim-text>Detektieren einer Pose eines mobilen Roboters;</claim-text>
<claim-text>Messen einer Entfernung zu einem Hindernis für den mobilen Roboter unter Nutzung eines entfernungsmessenden Sensors, der ein Lichtquellenmodul (122) enthält, das an<!-- EPO <DP n="18"> --> einer oberen Oberfläche des mobilen Roboters angeordnet ist und das, bezogen auf eine Bewegungsrichtung des mobilen Roboters, Licht nach oben ausstrahlt, und der entfernungsmessende Sensor ein Kameramodul (124) enthält, das an der oberen Oberfläche des mobilen Roboters angeordnet ist, um ein Bild, das durch das von dem Hindernis reflektierte Licht geformt wird, zu erfassen, und der eine Entfernung zu dem Hindernis unter Nutzung des erfassten Bildes misst; und</claim-text>
<claim-text>Erzeugen einer drei-dimensionalen Karte durch Messen der Entfernung beim Ändern der Pose des mobilen Roboters.</claim-text></claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 9, wobei das Lichtquellenmodul ein Laser ist.</claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 9, wobei die drei-dimensionale Karte durch Sammeln drei-dimensionaler Daten erzeugt wird, die durch Abbilden der Pose des mobilen Roboters auf, beim Ändern der Pose des mobilen Roboters gemessene, zwei-dimensionale Entfernungsdaten erhalten werden.</claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 9, wobei die Pose des mobilen Roboters eine Position und einen Richtungswinkel des mobilen Roboters enthält.</claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 9, wobei das Hodometer ein Codierer oder ein Kreisel ist.</claim-text></claim><claim id="c-de-01-0014" num="0014"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 9, das des Weiteren umfasst:
<claim-text>Extrahieren einer Fläche aus der erzeugten drei-dimensionalen Karte.</claim-text><!-- EPO <DP n="19"> --></claim-text></claim><claim id="c-de-01-0015" num="0015"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 14, das des Weiteren umfasst:
<claim-text>Extrahieren von Merkmalpunkten aus der extrahierten Fläche, um eine Merkmalskarte zu erzeugen.</claim-text></claim-text></claim><claim id="c-de-01-0016" num="0016"><claim-text>Das Verfahren zum Erzeugen einer drei-dimensionalen Karte nach Patentanspruch 15, das des Weiteren umfasst:
<claim-text>Vergleichen von Merkmalpunkten, die aus der Karte des mobilen Roboters an einer gegenwärtigen Position extrahiert wurden, mit zuvor gespeicherten Merkmalpunkten auf der Merkmalskarte, um die Position des mobilen Roboters zu detektieren.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56985075" lang="EN" load-source="patent-office"><!-- EPO <DP n="13"> --><claim id="c-en-01-0001" num="0001"><claim-text>A three-dimensional map-generating apparatus, comprising:
<claim-text>an odometer (110) detecting a pose of a mobile robot;</claim-text>
<claim-text>a distance-measuring sensor (120), including a light source module (122) disposed on an upper surface of the mobile robot to emit light upward relative to a movement direction of the mobile robot and a camera module (124), disposed on the upper surface of the mobile robot, to capture an image formed by the light as reflected from an obstacle, measuring a distance to the obstacle for the mobile robot using the captured image; and</claim-text>
<claim-text>a map-generating unit (130) generating a three-dimensional map using the distance measured by the distance-measuring sensor while changing the pose of the mobile robot.</claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>The three-dimensional map-generating apparatus of claim 1, wherein the light source module is a laser.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>The three-dimensional map-generating apparatus of claim 1, wherein the map-generating unit accumulates three-dimensional data obtained by reflecting the pose of the mobile robot to measured two-dimensional distance data while changing the pose of the mobile robot, thereby generating the three-dimensional map.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>The three-dimensional map-generating apparatus of claim 1, wherein the pose of the mobile robot includes position and direction angle of the mobile robot.</claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>The three-dimensional map-generating apparatus of claim 1, wherein the odometer is an<!-- EPO <DP n="14"> --> encoder or a gyro.</claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>The three-dimensional map-generating apparatus of claim 1, further comprising:
<claim-text>a plane-extracting unit (210) extracting a plane from the generated three-dimensional map.</claim-text></claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>The three-dimensional map-generating apparatus of claim 6, further comprising:
<claim-text>a feature-map-generating unit (220) extracting feature points from the extracted plane to generate a feature map.</claim-text></claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>The three-dimensional map-generating apparatus of claim 7, further comprising:
<claim-text>a position-detecting unit (230) comparing the feature points extracted from the map of the mobile robot at a current position with previously stored feature points on the feature map to detect the position of the mobile robot.</claim-text></claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>A three-dimensional map-generating method, comprising:
<claim-text>detecting a pose of a mobile robot;</claim-text>
<claim-text>measuring a distance to an obstacle for the mobile robot using a distance-measuring sensor that includes a light source module (122) disposed on an upper surface of the mobile robot emitting light upward relative to a movement direction of the mobile robot and a camera module (124), disposed on the upper surface of the mobile robot, capturing an image formed by the light as reflected from the obstacle, and measuring the distance to the obstacle using the captured image; and</claim-text>
<claim-text>generating a three-dimensional map by measuring the distance while changing the pose of the mobile robot.</claim-text><!-- EPO <DP n="15"> --></claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>The three-dimensional map-generating method of claim 9, wherein the light source module is a laser.</claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>The three-dimensional map-generating method of claim 9, wherein the three-dimensional map is generated by accumulating three-dimensional data obtained by reflecting the pose of the mobile robot to measured two-dimensional distance data while changing the pose of the mobile robot.</claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>The three-dimensional map-generating method of claim 9, wherein the pose of the mobile robot includes a position and direction angle of the mobile robot.</claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>The three-dimensional map-generating method of claim 9, wherein the odometer is an encoder or a gyro.</claim-text></claim><claim id="c-en-01-0014" num="0014"><claim-text>The three-dimensional map-generating method of claim 9, further comprising:
<claim-text>extracting a plane from the generated three-dimensional map.</claim-text></claim-text></claim><claim id="c-en-01-0015" num="0015"><claim-text>The three-dimensional map-generating method of claim 14, further comprising:
<claim-text>extracting feature points from the extracted plane to generate a feature map.</claim-text></claim-text></claim><claim id="c-en-01-0016" num="0016"><claim-text>The three-dimensional map-generating method of claim 15, further comprising:
<claim-text>comparing feature points extracted from the map of the mobile robot at a current position with previously stored feature points on the feature map to detect the position of the mobile robot.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56985076" lang="FR" load-source="patent-office"><!-- EPO <DP n="20"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Appareil de génération de carte en trois dimensions comprenant :
<claim-text>un compteur kilométrique (110) détectant la pose (position et orientation) d'un robot mobile,</claim-text>
<claim-text>un détecteur de mesure de distance (120) incluant un module de source lumineuse (122) disposé sur la surface supérieure du robot mobile afin d'émettre de la lumière vers le haut par rapport à la direction d'un mouvement du robot mobile, ainsi qu'un module d'appareil de prise de vues (124) disposé sur la surface supérieure du robot mobile afin de capturer une image formée par la lumière telle qu'elle est réfléchie depuis un obstacle mesurant une distance à l'obstacle, le robot mobile utilisant l'image capturée, et</claim-text>
<claim-text>une unité de génération de carte (130) générant une carte en trois dimensions en utilisant la distance mesurée par le détecteur de mesure de distance tout en changeant la pose du robot mobile.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 1, dans lequel le module de source lumineuse est un laser.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 1, dans lequel l'unité de génération<!-- EPO <DP n="21"> --> de carte accumule des données en trois dimensions obtenues par réflexion de la pose du robot mobile en données de distance en deux dimensions mesurée tout en modifiant la pose du robot mobile, ce qui génère ainsi la carte en trois dimensions.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 1, dans lequel la pose du robot mobile inclut une position et un angle de direction du robot mobile.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 1, dans lequel le compteur kilométrique est un codeur ou un gyroscope.</claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 1, comprenant en outre :
<claim-text>une unité d'extraction de plan (210) qui extrait un plan de la carte en trois dimensions générée.</claim-text></claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 6, comprenant en outre :
<claim-text>une unité de génération de carte de caractéristiques (220) qui extrait des points caractéristiques du plan extrait afin de générer une carte de caractéristiques.</claim-text></claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Appareil de génération de carte en trois dimensions selon la revendication 7, comprenant en outre :
<claim-text>une unité de détection de position (230) qui compare les points caractéristiques extraits de la carte du robot mobile à la position actuelle avec des points caractéristiques mémorisés précédemment sur la carte de caractéristiques afin de détecter la position du robot mobile.</claim-text><!-- EPO <DP n="22"> --></claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Procédé de génération de carte en trois dimensions comprenant :
<claim-text>la détection de la pose d'un robot mobile,</claim-text>
<claim-text>une mesure de distance à un obstacle, le robot mobile utilisant un détecteur de mesure de distance qui inclut un module de source lumineuse (122) disposé sur la surface supérieure du robot mobile et émettant de la lumière vers le haut par rapport à la direction d'un mouvement du robot mobile, ainsi qu'un module d'appareil de prise de vues (124) disposé sur la surface supérieure du robot mobile, capturant une image formée par la lumière telle qu'elle est réfléchie depuis un obstacle et qui mesure la distance à l'obstacle en utilisant l'image capturée, et</claim-text>
<claim-text>la génération d'une carte en trois dimensions en mesurant la distance tout en changeant la pose du robot mobile.</claim-text></claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 9, dans lequel le module de source lumineuse est un laser.</claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 9, dans lequel la carte en trois dimensions est générée en accumulant des données en trois dimensions obtenues par la réflexion de la pose du robot mobile en données de distance en deux dimensions mesurée tout en changeant la pose du robot mobile.</claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 9, dans lequel la pose du robot mobile inclut une position et un angle de direction du robot mobile.<!-- EPO <DP n="23"> --></claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 9, dans lequel le compteur kilométrique est un codeur ou un gyroscope.</claim-text></claim><claim id="c-fr-01-0014" num="0014"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 9, comprenant en outre :
<claim-text>l'extraction d'un plan à partir de la carte en trois dimensions générée.</claim-text></claim-text></claim><claim id="c-fr-01-0015" num="0015"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 14, comprenant en outre :
<claim-text>l'extraction de points caractéristiques du plan extrait afin de générer une carte de caractéristiques</claim-text></claim-text></claim><claim id="c-fr-01-0016" num="0016"><claim-text>Procédé de génération de carte en trois dimensions selon la revendication 15, comprenant en outre :
<claim-text>la comparaison des points caractéristiques extraits de la carte du robot mobile à la position actuelle avec des points caractéristiques mémorisés précédemment sur la carte de caractéristiques afin de détecter la position du robot mobile.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW16672050" load-source="patent-office"><!-- EPO <DP n="24"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="158" he="102" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="25"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="158" he="105" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="26"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="96" he="94" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="27"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="165" he="136" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="28"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="165" he="100" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="29"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="162" he="125" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="30"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="164" he="124" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="31"> --><figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="163" he="122" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> --><figure id="f0009" num="9A"><img id="if0009" file="imgf0009.tif" wi="163" he="129" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> --><figure id="f0010" num="9B"><img id="if0010" file="imgf0010.tif" wi="120" he="88" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> --><figure id="f0011" num="10"><img id="if0011" file="imgf0011.tif" wi="165" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> --><figure id="f0012" num="11"><img id="if0012" file="imgf0012.tif" wi="165" he="126" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> --><figure id="f0013" num="12"><img id="if0013" file="imgf0013.tif" wi="165" he="167" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0014" num="13"><img id="if0014" file="imgf0014.tif" wi="165" he="69" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
