<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2337345-B1" country="EP" doc-number="2337345" kind="B1" date="20140108" family-id="42355797" file-reference-id="315045" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146588592" ucid="EP-2337345-B1"><document-id><country>EP</country><doc-number>2337345</doc-number><kind>B1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-10733347-A" is-representative="YES"><document-id mxw-id="PAPP154850784" load-source="docdb" format="epo"><country>EP</country><doc-number>10733347</doc-number><kind>A</kind><date>20100120</date><lang>JA</lang></document-id><document-id mxw-id="PAPP170517481" load-source="docdb" format="original"><country>EP</country><doc-number>10733347.8</doc-number><date>20100120</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140548381" ucid="JP-2009012815-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2009012815</doc-number><kind>A</kind><date>20090123</date></document-id></priority-claim><priority-claim mxw-id="PPC140549043" ucid="JP-2010000278-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2010000278</doc-number><kind>W</kind><date>20100120</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130716</date></intention-to-grant-date><search-report-dispatch-date><date>20110803</date></search-report-dispatch-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-2063010586" load-source="docdb">G06T   7/00        20060101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL-2063010646" load-source="docdb">H04N   5/91        20060101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL-2063014122" load-source="docdb">H04N   5/76        20060101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL-2063015779" load-source="docdb">H04N   7/173       20110101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL-2063017509" load-source="docdb">H04N  21/435       20110101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL-2063018268" load-source="docdb">H04N  21/432       20110101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL-2063019716" load-source="docdb">H04N  21/235       20110101ALI20150402RHJP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326070" load-source="docdb">G06F  17/30        20060101AFI20130614BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326071" load-source="docdb">G06K   9/00        20060101ALI20130614BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326072" load-source="docdb">G06K   9/46        20060101ALI20130614BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326073" load-source="docdb">G06T   7/20        20060101ALI20130614BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326074" load-source="docdb">H04N   5/14        20060101ALI20130614BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326076" load-source="docdb">H04N  21/44        20110101ALI20130614BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989326077" load-source="docdb">H04N  21/8352      20110101ALI20130614BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861411777" load-source="docdb" scheme="CPC">G06T   7/254       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068548837" load-source="docdb" scheme="CPC">H04N  21/44008     20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068549389" load-source="docdb" scheme="CPC">G06T2207/10016     20130101 LA20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068549503" load-source="docdb" scheme="CPC">H04N   5/14        20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068551339" load-source="docdb" scheme="CPC">G06F  17/30811     20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068552090" load-source="docdb" scheme="CPC">G06K   9/00744     20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068554078" load-source="docdb" scheme="CPC">G06F  17/30799     20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068556674" load-source="docdb" scheme="CPC">G06F  17/30256     20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2068556781" load-source="docdb" scheme="CPC">H04N  21/8352      20130101 LI20150421BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989624193" load-source="docdb" scheme="CPC">G06K   9/46        20130101 LI20130425BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989628636" load-source="docdb" scheme="CPC">H04N  19/00        20130101 FI20130425BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132370409" lang="DE" load-source="patent-office">VORRICHTUNG ZUR EXTRAKTION VON VIDEOIDENTIFIKATOREN</invention-title><invention-title mxw-id="PT132370410" lang="EN" load-source="patent-office">VIDEO IDENTIFIER EXTRACTING DEVICE</invention-title><invention-title mxw-id="PT132370411" lang="FR" load-source="patent-office">DISPOSITIF D'EXTRACTION D'IDENTIFIANT DE VIDÉO</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR919537794" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>NEC CORP</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR919514873" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>NEC CORPORATION</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919504995" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>OAMI RYOMA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919505514" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>OAMI, RYOMA</last-name></addressbook></inventor><inventor mxw-id="PPAR919023823" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>OAMI, RYOMA</last-name><address><street>c/o NEC Corporation 7-1, Shiba 5-chome Minato-ku</street><city>Tokyo 108-8001</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919533490" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>IWAMOTO KOTA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919535780" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>IWAMOTO, KOTA</last-name></addressbook></inventor><inventor mxw-id="PPAR919023824" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>IWAMOTO, KOTA</last-name><address><street>c/o NEC Corporation 7-1, Shiba 5-chome Minato-ku</street><city>Tokyo 108-8001</city><country>JP</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR919023826" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Nec Corporation</last-name><iid>101193625</iid><address><street>7-1, Shiba 5-chome Minato-ku</street><city>Tokyo 108-8001</city><country>JP</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR919023825" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Betten &amp; Resch</last-name><iid>100060687</iid><address><street>Theatinerstrasse 8</street><city>80333 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="JP-2010000278-W"><document-id><country>JP</country><doc-number>2010000278</doc-number><kind>W</kind><date>20100120</date><lang>JA</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2010084739-A1"><document-id><country>WO</country><doc-number>2010084739</doc-number><kind>A1</kind><date>20100729</date><lang>JA</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549924704" load-source="docdb">AT</country><country mxw-id="DS549894510" load-source="docdb">BE</country><country mxw-id="DS549871824" load-source="docdb">BG</country><country mxw-id="DS549877594" load-source="docdb">CH</country><country mxw-id="DS549894511" load-source="docdb">CY</country><country mxw-id="DS549804133" load-source="docdb">CZ</country><country mxw-id="DS549881772" load-source="docdb">DE</country><country mxw-id="DS549894512" load-source="docdb">DK</country><country mxw-id="DS549894517" load-source="docdb">EE</country><country mxw-id="DS549787018" load-source="docdb">ES</country><country mxw-id="DS549871825" load-source="docdb">FI</country><country mxw-id="DS549871830" load-source="docdb">FR</country><country mxw-id="DS549881773" load-source="docdb">GB</country><country mxw-id="DS549894518" load-source="docdb">GR</country><country mxw-id="DS549881774" load-source="docdb">HR</country><country mxw-id="DS549804134" load-source="docdb">HU</country><country mxw-id="DS549877595" load-source="docdb">IE</country><country mxw-id="DS549894519" load-source="docdb">IS</country><country mxw-id="DS549871831" load-source="docdb">IT</country><country mxw-id="DS549894520" load-source="docdb">LI</country><country mxw-id="DS549871832" load-source="docdb">LT</country><country mxw-id="DS549924705" load-source="docdb">LU</country><country mxw-id="DS549871833" load-source="docdb">LV</country><country mxw-id="DS549871838" load-source="docdb">MC</country><country mxw-id="DS549924706" load-source="docdb">MK</country><country mxw-id="DS549924707" load-source="docdb">MT</country><country mxw-id="DS549924708" load-source="docdb">NL</country><country mxw-id="DS549804135" load-source="docdb">NO</country><country mxw-id="DS549924709" load-source="docdb">PL</country><country mxw-id="DS549894525" load-source="docdb">PT</country><country mxw-id="DS549788001" load-source="docdb">RO</country><country mxw-id="DS549924710" load-source="docdb">SE</country><country mxw-id="DS549881775" load-source="docdb">SI</country><country mxw-id="DS549804140" load-source="docdb">SK</country><country mxw-id="DS549804141" load-source="docdb">SM</country><country mxw-id="DS549787019" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><description mxw-id="PDES63961012" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">TECHNICAL FIELD</heading><p id="p0001" num="0001">The present invention relates to video signature extraction devices, video signature extraction methods, and video signature extraction programs, for retrieving videos, which are capable of detecting similar or identical moving image segments among a plurality of moving images.</p><heading id="h0002">BACKGROUND ART</heading><p id="p0002" num="0002">An exemplary device for extracting and matching features of moving images is described in Non-Patent Document 1. <figref idrefs="f0014">Fig. 14</figref> is a block diagram showing the device described in Non-Patent Document 1.</p><p id="p0003" num="0003">A block unit feature extraction unit 1000 extracts features in block units from a first video to be input, and outputs a first feature to a matching unit 1030, Another block unit feature extraction unit 1010 extracts features in block units from a second video to be input, and outputs a second feature to the matching unit 1030. A weighting coefficient calculation unit 1020 calculates a weighting value of each of the blocks based on a learning video to be input, and outputs a weighting coefficient to the matching unit 1030. The matching unit 1030<!-- EPO <DP n="2"> --> compares the first feature output from the block unit feature extraction unit 1000 with the second feature output from the block unit feature extraction unit 1010 using the weighting coefficient output from the weighting coefficient calculation unit 1020, and outputs a matching result.</p><p id="p0004" num="0004">Next, operation of the device shown in <figref idrefs="f0014">Fig. 14</figref> will be described.</p><p id="p0005" num="0005">The block unit feature extraction unit 1000 divides each of the frames of the input first video into blocks, and calculates a feature for identifying the video from each block. Specifically, the block unit feature extraction unit 1000 determines the type of the edge for each block, and calculates the type as a feature of each block. Then, for each of the frames, the block unit feature extraction unit 1000 forms a feature vector configured of the edge types of the respective blocks. Then, the block unit feature extraction unit 1000 calculates the feature vector of each of the frames, and outputs the acquired feature to the matching unit 1030 as the first feature.</p><p id="p0006" num="0006">Operation of the block unit feature extraction unit 1010 is similar to that of the block unit feature extraction unit 1000. The block unit feature extraction unit 1010 calculates the second feature from the input second video, and outputs the acquired second feature to the matching unit 1030.</p><p id="p0007" num="0007">On the other hand, the weighting coefficient calculation unit 1020 calculates probability<!-- EPO <DP n="3"> --> that a caption is inserted in each block of a frame beforehand, using a learning video. Then, based on the calculated probability, the weighting coefficient calculation unit 1020 calculates a weighting coefficient of each block. Specifically, a weighting coefficient is calculated such that weighting becomes high as the probability of a caption being superposed is low, in order to improve robustness to caption superposition, The acquired weighting coefficient is output to the matching unit 1030.</p><p id="p0008" num="0008">The matching unit 1030 compares the first feature output from the block unit feature extraction unit 1000 with the second feature output from the block unit feature extraction unit 1010, using the weighting coefficient output from the weighting coefficient calculation unit 1020. Specifically, the matching unit 1030 compares the features of the blocks at the same position in the two frames, and calculates a score of the block unit such that the score is 1 if they are the same, and the score is 0 if they are not the same. The matching unit 1030 sums the acquired scores of the block units by weighting them with use of the weighting coefficients, and calculates a matching score (similarity of a frame unit) of the frame. The matching unit 1030 performs these processes on the respective frames to thereby acquire a matching result between the first video and the second video,</p><p id="p0009" num="0009">Through these processes, it is possible to perform matching of moving images while reducing influences of caption superposition in portions where the influences may be large, and to achieve high matching accuracy even with caption superposition.</p><p id="p0010" num="0010"><!-- EPO <DP n="4"> --> Patent Document 1 describes a device for retrieving moving images, using features of images such as mean values in block units or DCT coefficients and motion vector information obtained between previous and next frames. In the moving image retrieval device of Patent Document 1, first, at least one of values of physical moving image feature information including luminance, color difference information, and color information of each frame, a mean value thereof, the sum of the values, or a difference value thereof, is extracted from the input image with respect to each frame, Then, the extracted values are aligned on a time axis, and all values in the alignment or values extracted from the alignment in certain intervals or irregular intervals are extracted as moving image feature information. Alternatively, it is also possible to extract a DCT coefficient and motion compensation information of a frame from compressed moving image data, and obtain a mean value of DCT coefficients, a sum value thereof, or a difference value of the values, and from the motion compensation information, obtain at least one of a motion vector, an average motion vector between previous and next frames, a sum motion vector, a difference vector, a motion vector of the frame as a whole, and the like. Then, the obtained values are aligned on a time axis, and all values in the alignment or values extracted from the alignment in certain intervals or irregular intervals are extracted as moving image feature information.</p><heading id="h0003">PRIOR ART DOCUMENTS</heading><heading id="h0004">PATENT DOCUMENT</heading><p id="p0011" num="0011">Patent Document 1: Japanese Unexamined Patent Publication No. <patcit id="pcit0001" dnum="JP2000194727A"><text>2000-194727</text></patcit></p><heading id="h0005">NON-PATENT DOCUMENTS</heading><!-- EPO <DP n="5"> --><p id="p0012" num="0012"><ul><li>Non-Patent Document 1: <nplcit id="ncit0001" npl-type="s"><text>Kota Iwamoto, Eiji Kasutani, Akio Yamada, "Image Signature Robust to Caption Superimposition for Video Sequence Identification", Proceedings of International Conference on Image Processing (ICIP2006), 2006</text></nplcit></li><li>Non-Patent Document 2: <nplcit id="ncit0002" npl-type="s"><text>Eiji Kasutani, Ryoma Oami, Akio Yamada, Takami Sato, and Kyoji Hirata, "Video Material Archive System for Efficient Video Editing Based on Media Identification", Proceedings of International Conference on Multimedia and Expo (ICME2004), pp.727-730, 2004</text></nplcit></li></ul></p><p id="p0013" num="0013"><patcit id="pcit0002" dnum="US20070253594A1"><text>US 2007/0253594 A1</text></patcit> discloses a method and system for generating a fingerprint for a video object. The method includes obtaining a plurality of frames associated with a video object. Additionally, the method includes, for each of the plurality of frames, processing information associated with the plurality of frames, determining a plurality of spatial signatures for the each of the plurality of frames based on at least information associated with the each of the plurality of frames, and determining a plurality of temporal signatures for the each of the plurality of frames based on at least information associated with the plurality of frames. The plurality of spatial signatures corresponds to a plurality of resolutions respectively, and the plurality of temporal signatures corresponding to a plurality of frame rates respectively.</p><heading id="h0006">SUMMARY OF THE INVENTION</heading><heading id="h0007">PROBLEMS TO BE SOLVED BY THE INVENTION</heading><p id="p0014" num="0014">A problem involved in the above art is that it is difficult to improve the discrimination accuracy in a time direction in scenes having less temporal changes. In the case of Non-Patent Document 1, as the weighting at the time of matching is determined by the probability of caption superposition, control is not focused on matching of scenes having less temporal changes. In scenes having less temporal changes, it is often the case that the screen image seldom moves, and that changes in the image such as motion and brightness changes are caused only in a local area. In order to improve the discrimination accuracy in that case, although it is only necessary to extract features in more detail, including extracting features in block units, this causes a problem of an increase in the feature size. Even in the case of Patent Document 1, although motion information is used and so motion is taken into account in features, features obtained from motion information and features obtained from luminance values and DCT coefficients are<!-- EPO <DP n="6"> --> used independent from each other. As such, if extraction is performed to a more detailed level, a problem of an increase in the feature size, which is the same as that involved in Non-Patent Document 1, will also be caused.</p><heading id="h0008">[Object of the Invention]</heading><p id="p0015" num="0015">An object of the present invention is to provide a video signature extraction device capable of solving a problem of low discrimination capability of video signatures generated from moving images having less temporal changes.</p><heading id="h0009">MEANS FOR SOLVING THE PROBLEMS</heading><p id="p0016" num="0016">The present invention is defined in the independent claims. The dependent claims define embodiments of the invention.<br/>
A video signature extraction device, according to an embodiment includes an each-picture feature extraction unit which extracts a feature of each picture, which is a frame or a field, as an each-picture visual feature from an input video; a time axial direction change region extraction unit which analyzes an image change in a time direction with respect to predetermined regions in a picture from the video, obtains a region having a large image change, and generates change region information which is information designating the region; an each-region feature extraction unit which extracts a feature of the region corresponding to the change region information as an each-region visual feature from the video; and a multiplexing unit which multiplexes the each-picture visual feature, the each-region visual feature, and the change region information, and generates a video signature.</p><heading id="h0010">EFFECTS OF THE INVENTION</heading><p id="p0017" num="0017">As the present invention is configured as described above, the present invention is able to achieve an advantageous effect of improving the discrimination accuracy in a time direction even in scenes having less temporal changes.</p><heading id="h0011">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0018" num="0018"><ul><li><figref idrefs="f0001">Fig, 1</figref> is a region diagram showing a first embodiment of an image signature extraction device according to the present invention.</li><li><figref idrefs="f0002">Fig. 2</figref> is a block diagram showing an exemplary configuration of a time axial direction change region extraction unit 100.</li><li><figref idrefs="f0003">Fig. 3</figref> is an illustration for explaining an exemplary process performed by a change<!-- EPO <DP n="7"> --> region extraction unit 410.</li><li><figref idrefs="f0004">Fig. 4</figref> is an illustration showing a case where a plurality of predetermined regions in a picture are blocks,</li><li><figref idrefs="f0005">Fig. 5</figref> is an illustration showing a case where a plurality of predetermined regions in a picture are local regions in different shapes,</li><li><figref idrefs="f0006">Fig. 6</figref> is a block diagram showing another exemplary configuration of the time axial direction change region extraction unit 100.</li><li><figref idrefs="f0007">Fig. 7</figref> is an illustration for explaining an exemplary process performed by a change region extraction unit 510.</li><li><figref idrefs="f0008">Fig. 8</figref> is a block diagram showing an exemplary configuration of a video signature matching device for matching video signatures generated by the video signature extraction device of the first embodiment.<!-- EPO <DP n="8"> --></li><li><figref idrefs="f0009">Fig. 9</figref> is an illustration for explaining a matching process of two videos.</li><li><figref idrefs="f0010">Fig. 10</figref> is an illustration for explaining a process performed by a region matching unit 230.</li><li><figref idrefs="f0011">Fig. 11</figref> is a block diagram showing a second embodiment of a video signature extraction device according to the present invention.</li><li><figref idrefs="f0012">Fig. 12</figref> is a block diagram showing an exemplary configuration, of a video signature matching device for matching video signatures generated by the video signature extraction device of the second embodiment.</li><li><figref idrefs="f0001">Fig. 1</figref> is an illustration showing an example of a feature.</li><li><figref idrefs="f0014">Fig. 14</figref> is a block diagram for explaining related art of the present invention.</li></ul></p><heading id="h0012">DESCRIPTION OF EMBODIMENTS</heading><p id="p0019" num="0019">Next, best modes for carrying out the invention will be described in detail with reference to the drawings.</p><p id="p0020" num="0020">Referring to <figref idrefs="f0001">Fig. 1</figref> showing an image signature extraction device according to a first embodiment of the present invention, the image signature extraction device includes a time axial direction change region extraction unit 100, an each-region feature extraction unit 110, an each-picture feature extraction unit 130, and a multiplexing unit 120.</p><p id="p0021" num="0021">The each-picture feature extraction unit 130 extracts an each-picture feature from an input video, and outputs it to the multiplexing unit 120. The time axial direction change region<!-- EPO <DP n="9"> --> extraction, unit 100 obtains change region information from the video, and outputs it to the each-region feature extraction unit 110 and to the multiplexing unit 120. The each-region feature extraction unit 110 extracts an each-region visual feature from the video based on the change region information output from the time axial direction change region extraction unit 100, and outputs it to the multiplexing unit 120. The multiplexing unit 120 multiplexes the each-picture visual feature output from the each-picture feature extraction unit 130, the change region information output from the time axial direction change region extraction unit 100, and the each-region visual feature output from the each-region feature extraction unit 110, and generates and outputs a video signature.</p><p id="p0022" num="0022">It should be noted that the video signature extraction device of the present embodiment can be realised by a computer in the following manner, for example, A disk or a semiconductor memory, storing programs for allowing a computer to function as the video signature extraction device is prepared, and the computer is caused to read the program. The computer controls the operation of itself according to the readout program to thereby realize the time axial direction change region extraction unit 100, the each-region feature extraction unit 110, the multiplexing unit 120, and the each-picture feature extraction unit 130 on the self computer.</p><p id="p0023" num="0023">Next, operation of the first embodiment shown in <figref idrefs="f0001">Fig. 1</figref> will be described in detail.</p><p id="p0024" num="0024">First, a video is input to the each-picture feature extraction unit 130. If the original video is coded, the video is first decoded by a decoder, and then the data is input in picture units<!-- EPO <DP n="10"> --> composed of frames or fields.</p><p id="p0025" num="0025">The each-picture feature extraction unit 130 calculates a feature vector of each picture. The each-picture feature extraction unit 130 considers a picture as one still image, and extracts a vector of a visual feature indicating features such as colors, patterns, shapes, and the like of this picture. At the feature, it is possible to use a feature vector which is obtained by calculating a difference between features of regions with respect to a pair of local regions corresponding to each dimension of the feature vector (for example, calculating a mean value of pixel, value within a region with respect to each region of a pair of regions and obtaining a difference in mean values between regions), and using a quantized value obtained by quantizing the difference as a value of each dimension. The feature vector, calculated for each picture, is output to the multiplexing unit 120 as an each-picture visual feature.</p><p id="p0026" num="0026">Further, the video is also input to the time axial direction change region extraction unit 100. In the time axial direction change region extraction unit 100, an amount of change of the image in a time direction is calculated. An amount of change in each of the predetermined regions in the picture is calculated using a current target picture and the previous and next pictures. Then, a region where the amount of change is relatively large in the screen image is obtained. Regions for obtaining the amounts of change are formed by dividing a picture. The regions may be a plurality of blocks as shown in <figref idrefs="f0004">Fig. 4</figref>, or a plurality of local regions having different shapes as shown in <figref idrefs="f0005">Fig. 5</figref>. Further, the shape of the blocks is not limited to rectangle. As a region having a larger change in a time direction has a larger possibility of contributing to<!-- EPO <DP n="11"> --> discrimination of a video, a plurality of regions are selected in order in which a region having a largest amount of change is the first Selection may be performed by selecting a certain number of regions in descending order, or selecting regions in which the amount of change is a threshold or larger. The details of calculating the amount of change will be described below. Information for specifying the selected regions such as index information of the selected regions is output as change region information. For example, in a scene where an anchor person speaks in a news program, there is a case where no motion is generated in areas other than an area around the face of the anchor person. In that case, as a change in a time direction in the region corresponding to the face of the anchor person becomes relatively larger than changes in other regions in the screen image, information designating the region corresponding to the face is output as change region information.</p><p id="p0027" num="0027">It should be noted that the change region information may be calculated for each picture, or calculated for several pictures in a lump, and output. For example, if a portion with motion within a shot is limited to a particular region, it is possible to calculate and output change region information which is common to the entire shot. More specifically, it is possible that change region information, obtained for one picture within a shot, is also used for another picture in the shot. It is also possible to calculate time axial direction changes for all or a plurality of pictures within a shot and, with use of a representative value thereof (mean, median, or the like), obtain and describe change region information for the entire shot and use it for all pictures within the shot.</p><p id="p0028" num="0028"><!-- EPO <DP n="12"> --> However, units for outputting change region information are not limited to shots, and change region information may be output in fixed time intervals such as every several pictures. It is also possible to calculate a time segment, to which the same change region information is applicable, from the amount of change in a time direction, and calculate and output the change region information in a lump with respect to the pictures included in the time segment. In that case, as the number of pictures put together varies each time, the number of pictures is also described together. A time segment to which the same change region information is applicable is able to be calculated by applying threshold processing on variation of the amount of change in the time direction between pictures. As such, an amount of change in the time axial direction in the head picture in a time segment and an amount of change in the time axial direction in the current picture are compared, and if the degree of change exceeds a threshold, a segment up to the previous picture is considered as one group, whereby change region information with respect to the segment is calculated. The change region information with respect to that segment may be used as change region information of any picture in the segment or a representative value of change region information of the pictures in the segment. Through these processes, regardless of a processing target video, the amount of information of the change region information can be reduced while keeping high descrimination accuracy in the time direction.</p><p id="p0029" num="0029">The change region information calculated as described above is output to the each-region feature extraction unit 110 and to the multiplexing unit 120.</p><p id="p0030" num="0030"><!-- EPO <DP n="13"> --> The each-region feature extraction unit 110 extracts a feature in a region unit with respect to a region specified by the change region information output from the time axial direction change region extraction unit 100. In this process, the feature in a region unit may be the same as, or different from, the feature of the entire screen image calculated by the each-picture feature extraction unit 130. For example, it is possible to use a feature in which, with respect to the above-described pair of local regions corresponding to each dimension of the feature vector, a feature difference between the regions is calculated and used as each dimensional value of the feature vector. The feature of the region designated by the change region information is output to the multiplexing unit 120 as an each-region visual feature,</p><p id="p0031" num="0031">The multiplexing unit 120 multiplexes the each-picture visual feature output from the each-picture feature extraction unit 130, the each-region visual feature output from the each-picture feature extraction unit 110, and the change region information output from the time axial direction change region extraction unit 100, and generates and outputs a video signature. In this embodiment, the multiplexing unit 120 generates a video signature by multiplexing them in such a manner that these pieces of information can be separated at the time of matching. As multiplexing methods, it is possible to multiplex three pieces of information for each picture by interleaving them, or separately put together each of the each-picture visual feature, the each-region visual feature, and the change region information and finally connect them to thereby multiplex them, or multiplex the each-picture visual feature, the each-region visual feature, and the change region information for each predetermined segment (for example, by a time segment unit for calculating change region information).<!-- EPO <DP n="14"> --></p><p id="p0032" num="0032">Next, an embodiment of the time axial direction change region extraction unit 100 will be described with reference to <figref idrefs="f0002">Fig. 2</figref>.</p><p id="p0033" num="0033">Referring to <figref idrefs="f0002">Fig. 2</figref> showing an embodiment of the time axial direction change region extraction unit 100, the time axial direction change region extraction unit 100 includes an inter-picture difference calculation unit 400 and a change region extraction unit 410.</p><p id="p0034" num="0034">The inter-picture difference calculation unit 400 calculates inter-picture difference information from the input video, and outputs it to the change region extraction unit 410. The change region extraction unit 410 calculates change region information using the inter-picture difference information output from the inter-picture difference calculation unit 400 and a feature extraction parameter (information describing each dimension of the feature and the extraction target region), and outputs it.</p><p id="p0035" num="0035">Next, operation of the time axial direction change region extraction unit 100 shown in <figref idrefs="f0002">FIG 2</figref> will be described.</p><p id="p0036" num="0036">First, a video is input to the inter-picture difference calculation unit 400. The inter-picture difference calculation unit 400 calculates a difference in pixel value between pictures. Calculation of a difference may be performed for each pixel unit or performed for a region for which calculation for some pixels can be made at once (for example, a block). For<!-- EPO <DP n="15"> --> example, a method in which a representative value (mean, median, etc.) with respect to each region is obtained beforehand, and then, a difference with a representative value of a region at the same location is obtained between pictures. Further, a difference between pixel values may be a difference between luminance values. It is also possible to use color components of R, G, and B as pixel values, and calculate a difference of at least one of them to use as a difference of the pixel value. Of course, a color space may be any color space such as HSV or L*a*b*, rather than RGB. Further, as a difference, it is possible to obtain an absolute value of a difference by performing absolute value computation, rather than simply subtracting a pixel value. The calculated difference data between the pictures is output to the change region extraction unit 410 as inter-picture difference information.</p><p id="p0037" num="0037">The change region extraction unit 410 calculates difference information of each region from the inter-picture difference information. In order to do so, first, in the processing target picture, a value to be incremented in a region corresponding to a moving object is calculated. This is achieved by obtaining a product of a difference value between the processing target picture and the previous picture, and a difference value between the processing target picture and the next picture.</p><p id="p0038" num="0038">This is shown in <figref idrefs="f0003">Fig. 3</figref>. In <figref idrefs="f0003">Fig. 3</figref>, a T picture represents a processing target picture, a T-1 picture represents the previous picture, and a T+1 picture represents the next picture. In these pictures, it is assumed that a rectangle shaded object remains stationary, and only a round black object moves. In this case, the inter-picture difference calculation unit 400 has calculated<!-- EPO <DP n="16"> --> a difference between the processing target T picture and the previous T-1 picture. In this case, a difference is only generated by the movement of the round object, as shown in <figref idrefs="f0003">Fig. 3</figref>. However, the difference value itself tends to become larger at both location of the round object in the T picture and location of the object in the T-1 picture. Similarly, a difference between the next T+1 picture and the T picture becomes larger at both location of the round object in the T picture and location of the round object in the T+1 picture. Then, a product of both difference images is calculated. As it is only the position of the round object in the T picture where the difference value becomes larger in both difference images, it is possible to increase only the difference in the moving object region in the T picture. Although a method of calculation using the previous and next pictures of the processing target picture has been described in this embodiment, calculation can also be performed in the same manner using pictures of a few pictures before and a few pictures after. As such, it is possible to increase only the difference in the moving object region in the same manner using a T-m picture and a T+n picture. By collecting the results obtained in this way by each region, the amount of change in the region is calculated.</p><p id="p0039" num="0039">More specifically, the amount of change in a region is calculated according to the following Expression 1.<maths id="math0001" num="[Expression 1]"><math display="block"><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>=</mo><mstyle displaystyle="false"><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>R</mi><mfenced><mi>i</mi></mfenced></mrow></munder></mstyle><mfenced open="|" close="|" separators=""><msub><mi>f</mi><mrow><mi>T</mi><mo>-</mo><mn>1</mn></mrow></msub><mfenced><mi>x</mi></mfenced><mo>-</mo><msub><mi>f</mi><mi>T</mi></msub><mfenced><mi>x</mi></mfenced></mfenced><mo>⁢</mo><mfenced open="|" close="|" separators=""><msub><mi>f</mi><mrow><mi>T</mi><mo>+</mo><mn>1</mn></mrow></msub><mfenced><mi>x</mi></mfenced><mo>-</mo><msub><mi>f</mi><mi>T</mi></msub><mfenced><mi>x</mi></mfenced></mfenced></mstyle></math><img id="ib0001" file="imgb0001.tif" wi="95" he="25" img-content="math" img-format="tif"/></maths></p><p id="p0040" num="0040"><!-- EPO <DP n="17"> --> In Expression 1, f<sub>T</sub>(x) represents a pixel value at a position x of the T picture, R(i) represents the i<sup>th</sup> region (a group of pixels), and w(i) represents the amount of change in the i<sup>th</sup> region. Although simple addition in a region is used in this embodiment, it is also possible to calculate the amount of change in a region by obtaining an average within a region, using a square for addition, or using another statistic such as a median or a maximum value. Further, it is also possible to calculate the amount of change by not using the values of all pixels in a region. For example, the amount of change can be calculated by using every other pixel.</p><p id="p0041" num="0041">Based on the amounts of change with respect to respective regions calculated in this manner, a region having a large amount of change is obtained. Specifically, it is possible to calculate regions in which the amount of change exceeds a certain threshold, or select a certain number of regions in descending order of amount of change. Then, information describing the selected regions (e.g., indexes of regions) is output as change region information. For example, in the case where the regions defined on the picture are blocks obtained by dividing the screen image into sixteen pieces as shown in <figref idrefs="f0004">Fig. 4</figref> and the amount of change increases in the shaded blocks, the indexes 6, 10, and 12, of the blocks are output as change region information. Further, in the case where the regions defined on the picture are a plurality of local regions in random shapes as shown in <figref idrefs="f0005">Fig. 5</figref> and the amount of change increases in the shaded local region, the index 2 of the local region is output as change region information.</p><p id="p0042" num="0042">Further, the change region information is not necessary calculated for all pictures, and may be calculated for every other picture. In that case, it is possible to sum the amounts of<!-- EPO <DP n="18"> --> change with respect to the regions calculated in a plurality of pictures to obtain change region information corresponding to the pictures.</p><p id="p0043" num="0043">If the feature in the entire image largely changes temporality, as it is possible to perform matching without features in region units, it is not necessary to calculate a feature of each region (block or local region) with respect to such a video or a video segment. For example, if the number of regions having small amount of change in a time axial direction is not more than a certain threshold, a feature is not calculated for each block or local region. Specifically, nothing is output as change region information, or change region information includes information indicating that there is no feature extraction target region.</p><p id="p0044" num="0044">Thereby, it is possible to avoid calculating unnecessary region features so as to prevent the size of video features from being increased to an unnecessary level, whereby features can be calculated only from necessary portions.</p><p id="p0045" num="0045">As the time axial direction change region extraction unit 100 shown in <figref idrefs="f0002">Fig. 2</figref> is only necessary to obtain a difference between pictures basically, a processing load can be suppressed.</p><p id="p0046" num="0046">Next, another embodiment of the time axial direction change region extraction unit 100 will be described with reference to <figref idrefs="f0006">Fig. 6</figref>.</p><p id="p0047" num="0047">Referring to <figref idrefs="f0006">Fig. 6</figref> showing another embodiment of the time axial direction change<!-- EPO <DP n="19"> --> region extraction unit 100, the time axial direction change region extraction unit 100 includes a motion information calculation unit 500 and a change region extraction unit 510.</p><p id="p0048" num="0048">The motion information calculation unit 500 receives a video, calculates a motion vector, and outputs motion vector information to the change region extraction unit 510. The change region extraction unit 510 calculates change region information using the motion vector information output from the motion information calculation unit 500 and a feature extraction parameter, and outputs it.</p><p id="p0049" num="0049">Next, operation of the time axial direction change region extraction unit 100 shown in <figref idrefs="f0006">Fig. 6</figref> will be described.</p><p id="p0050" num="0050">First, a video is input to the motion information calculation unit 500. The motion information calculation unit 500 performs motion estimation between the current target picture and the previous (or next) picture to calculate a motion vector. As a method of calculation a motion vector, any vector estimation methods including a method based on a conventional gradient method and a method based on a block matching method may be used. Further, motion may be calculated in pixel units, or it is also possible to divide an image into a plurality of small regions and motion may be calculated for the small region units. Information describing the location of the motion vector calculated in this manner is output as motion vector information. The motion vector information may be information directly describing each motion vector calculated within the picture, or information describing motion only in a region<!-- EPO <DP n="20"> --> where a motion vector other than 0 is calculated, together with information specifying the region. The calculated motion vector describing information is output to the change region extraction unit 510.</p><p id="p0051" num="0051">The change region extraction unit 510 collects the calculated motion vectors for each region, and calculates the amount of motion within the region.</p><p id="p0052" num="0052">This is shown in <figref idrefs="f0007">Fig. 7. Fig. 7</figref> shows the states of the T picture and the T-1 picture. By performing motion estimation processing on these pictures, a motion vector is calculated in a portion corresponding to the motion of the round object. Although the case of using an immediately previous picture has been described in this example, it is possible to perform motion estimation processing using a picture of some pictures ago or some pictures after. Further it is also possible to perform motion estimation processing using a several number of pictures, rather than using only two pictures. Even in that case, a motion picture is also calculated in a portion with motion. By using this motion vector, the amount of motion within each region is calculated. For example, the sum of the lengths of the motion vectors is calculated within a region, which is represented by Expression 2.<maths id="math0002" num="[Expression 2]"><math display="block"><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>=</mo><mstyle displaystyle="false"><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>R</mi><mfenced><mi>i</mi></mfenced></mrow></munder></mstyle><mfenced open="|" close="|" separators=""><mi>v</mi><mfenced><mi>x</mi></mfenced></mfenced></mstyle></math><img id="ib0002" file="imgb0002.tif" wi="50" he="24" img-content="math" img-format="tif"/></maths></p><p id="p0053" num="0053">In Expression 2, v(x) represents a motion vector at x. The other signs are the same as<!-- EPO <DP n="21"> --> those used in Expression 1. Although simple addition in a region is used in this embodiment, it is also possible to obtain a representative amount of motion in a region by obtaining an average within a region, using a square for addition, or using another statistic such as a median or a maximum value. Further, it is also possible to calculate the amount of motion by not using the all motion vectors in a region. For example, the amount of motion can be calculated by thinning appropriately.</p><p id="p0054" num="0054">As the time axial direction change region extraction unit 100 shown in <figref idrefs="f0006">Fig. 6</figref> calculates motion, the amount of processing increases in general, compared with the case shown in <figref idrefs="f0002">Fig. 2</figref>. However, as actual motion is calculated, a region having motion in a time direction can be obtained with higher accuracy, compared with the case shown in <figref idrefs="f0002">Fig. 2</figref>.</p><p id="p0055" num="0055">Next, an embodiment of the video signature matching device of the present invention will be described.</p><p id="p0056" num="0056">Referring to <figref idrefs="f0008">Fig. 8</figref> showing an embodiment of the video signature matching device, the video signature matching device includes a demultiplexing unit 200, another demultiplexing unit 210, a picture matching unit 220, a region matching unit 230, and a matching result determination unit 240. It should be noted that this embodiment of the video signature matching device can be realized by a computer which is controllable by programs.</p><p id="p0057" num="0057">The demultiplexing unit 200 demultiplexes an input first video signature, and outputs a<!-- EPO <DP n="22"> --> first each-picture visual feature to the picture matching unit 220 and outputs a first each-region visual feature and first change region information to the region matching unit 230. Similarly, the demultiplexing unit 210 also demultiplexes an input second video signature, and outputs a second each-picture visual feature to the picture matching unit 220 and outputs a second each-region visual feature and second change region information to the region matching unit 230. The picture matching unit 220 compares the first each-picture visual feature output from the demultiplexing unit 220 with the second each-picture visual feature output from the demultiplexing unit 210, and outputs a picture matching result to the matching result determination unit 240, and also outputs region matching execution information to the region matching unit 230. Based on the region matching execution information output from the picture matching unit 220, the first change region information output from the demultiplexing unit 200, and the second change region information output from the demultiplexing unit 210, the region matching unit 230 compares the first each-region visual feature output from the demultiplexing unit 200 with the second each-region visual feature output from the demultiplexing unit 210, and outputs a region matching result to the matching result determination unit 240. The matching result determination unit 240 calculates a matching result from the picture matching result output from the picture matching unit 220 and the region matching result output from the region matching unit 230, and outputs it.</p><p id="p0058" num="0058">Next, operation of the embodiment of the video signature matching device according to the present invention shown in <figref idrefs="f0008">Fig. 8</figref> will be described.</p><p id="p0059" num="0059"><!-- EPO <DP n="23"> --> The first video signature is input to the demultiplexing unit 200. The demultiplexing unit 200 separates the first each-picture visual feature, the first each-region visual feature, and the first change region information, from the first video signature. In this process, separation is performed by means of a separation method corresponding to the method used for multiplexing by the multiplexing unit 120. The first each-picture visual feature generated by separation is output to the picture matching unit 220, and the first each-region feature and the first change region information are output to the region matching unit 230.</p><p id="p0060" num="0060">The second video signature is input to the demultiplexing unit 210. Operation of the demultiplexing unit 210 is the same as that of the demultiplexing unit 200, and the second each-picture visual feature generated by separation is output to the picture matching unit 220, and the second each-region feature and the second change region information are output to the region matching unit 230.</p><p id="p0061" num="0061">The picture matching unit 220 compares the first each-picture visual feature output from the demultiplexing unit 200 with the second each-picture visual feature output from the demultiplexing unit 210. They may be compared using the degree of similarity indicating similarity of both features, or using a distance indicating the level of difference between both features. In the case of comparing them using a distance, comparison will be performed according to Expression 3.</p><p id="p0062" num="0062"><!-- EPO <DP n="24"> --><maths id="math0003" num="[Expression 3]"><math display="block"><mi>d</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mfenced open="|" close="|" separators=""><msub><mi>v</mi><mn>1</mn></msub><mfenced><mi>i</mi></mfenced><mo>-</mo><msub><mi>v</mi><mn>2</mn></msub><mfenced><mi>i</mi></mfenced></mfenced></math><img id="ib0003" file="imgb0003.tif" wi="52" he="22" img-content="math" img-format="tif"/></maths></p><p id="p0063" num="0063">It should be noted that N represents the number of dimensions of the feature, and v<sub>1</sub>(i) and v<sub>2</sub>(i) respectively represent values of the i<sup>th</sup> dimension of the first and second each-picture visual features. By performing comparison in picture units and summing, specific segments of the first video and the second video are compared. For example, a number of pairs of pictures having distance values not more that a threshold is obtained in a comparison in picture units, and if the value is sufficiently large relative to the number of pictures included in the segment, the both videos are determined to be of the same segment, while if not, they are determined not to be of the same segment. By performing this process on combinations in arbitrary segments of the first video and the second video, all of the same segments of random length included in these videos can be determined. Instead of performing threshold processing on distances in picture units, it is also possible to perform determination according to whether or not a value obtained by summing the distances within a segment is smaller than a predetermined threshold. Of course, a mean value may be obtained, rather than a total value. Alternatively, comparison may be performed in a segment while eliminating outlier. Such comparison may be performed using a median or a result of M assumption, instead of a mean value, for example.</p><p id="p0064" num="0064">As a method of comparing segments of any length, the matching method described in Non-Patent Document 2 can also be used. As shown in <figref idrefs="f0009">Fig. 9</figref>, for matching between videos, a matching window having a length of L pictures is provided, and the window is caused to slide on the first video and the second video respectively, and they are compared with each other. If the<!-- EPO <DP n="25"> --> segments within the matching windows are determined to be identical, the matching window is extended by a length of p pictures so as to continue the matching process. As long as both segments are determined to be identical, the process of extending the matching window by p pictures is repeated so as to obtain the identical segments with the maximum length. Thereby, the identical segments with the maximum length, in the compared segments, can be acquired effectively.</p><p id="p0065" num="0065">It should be noted that although the case of using a distance as a measure has been described above, comparison can also be performed using the degree of similarity. In that case, comparison is specifically performed using the degree of similarity S calculated by Expression 4.<maths id="math0004" num="[Expression 4]"><math display="block"><mi>S</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mi mathvariant="italic">Sim</mi><mfenced separators=""><msub><mi>v</mi><mn>1</mn></msub><mfenced><mi>i</mi></mfenced><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mfenced><mi>i</mi></mfenced></mfenced></math><img id="ib0004" file="imgb0004.tif" wi="63" he="27" img-content="math" img-format="tif"/></maths></p><p id="p0066" num="0066">Sim(x, y) is a function showing similarity between x and y, and the value becomes larger as the values of x and y are more similar. For example, if the distance between x and y is d(x, y), a function shown as Expression 5 can be used. <maths id="math0005" num="[Expression 5]"><math display="block"><mi mathvariant="italic">Sim</mi><mfenced><mi>x</mi><mi>y</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mfenced><mi>x</mi><mi>y</mi></mfenced></mrow></mfrac></math><img id="ib0005" file="imgb0005.tif" wi="65" he="25" img-content="math" img-format="tif"/></maths></p><p id="p0067" num="0067">Alternatively, Sim(x, y) may be a function that returns 1 when x and y match, and returns 0 otherwise, as Kronecker delta. Alternatively, if an angle (cosine value) between<!-- EPO <DP n="26"> --> feature vectors is used as a degree of similarity, comparison is performed based on the degree of similarity S calculated by Expression 6.<maths id="math0006" num="[Expression 6]"><math display="block"><mi>S</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>v</mi><mn>1</mn></msub><mfenced><mi>i</mi></mfenced><mo>⁢</mo><msub><mi>v</mi><mn>2</mn></msub><mfenced><mi>i</mi></mfenced></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>v</mi><mn>1</mn></msub><mo>⁢</mo><msup><mfenced><mi>i</mi></mfenced><mn>2</mn></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><msub><mi>v</mi><mn>2</mn></msub><mo>⁢</mo><msup><mfenced><mi>i</mi></mfenced><mn>2</mn></msup></mrow></mfrac></math><img id="ib0006" file="imgb0006.tif" wi="63" he="40" img-content="math" img-format="tif"/></maths></p><p id="p0068" num="0068">By using the degree of similarity acquired as described above, matching can be performed in a similar manner to that of the case of distance.</p><p id="p0069" num="0069">Then, a matching result is output to the matching result determination unit 240. A matching result includes information specifying identical segments, such as picture numbers and time information of the identical segments. On the other hand, if there is no identical segment, information indicating such a fact is included. It is also possible that a case where nothing is included in a matching result corresponds to the case where no identical segment is present.</p><p id="p0070" num="0070">When matching is performed in this manner, there is a case where a video segment having less motion in a time direction corresponds to not only one segment, but to a plurality of segments of another video (including a case of matching any partial section in a series of segments). Even if a segment corresponds to one segment, there is a case where a plurality of matching candidate segments exist substantially, because there is not a large difference in distance value or degree of similarity with other candidate segments. In that case, as sufficient<!-- EPO <DP n="27"> --> matching was not able to be performed in the each-picture matching, region matching execution information notifying execution of region matching is output to the region matching unit 230. In contrast, if there is no segment which can be determined to match, or if there is a large difference between the distance value or the degree of similarity of the segments which were determined to match and the distance value or the degree of similarity of other candidate segments, it is determined that matching for each region is not necessary, so that region matching execution information will not be output.</p><p id="p0071" num="0071">The region matching unit 230 compares the first each-region visual feature output from the demultiplexing unit 200 with the second each-region visual feature output from the demultiplexing unit 210, based on the first change region information output from the demultiplexing unit 200 and the second change region information output from the demultiplexing unit 210. However, this matching is performed according to the region matching execution information output from the picture matching unit 220. This means that if region matching execution information is not output, matching is not performed, and a region matching result is not output, When region matching execution information is output, region matching is performed. The region matching execution information also includes information specifying target video segments, that is, information regarding target segments which were not able to be narrowed down in the picture matching, and region matching is performed on those segments.</p><p id="p0072" num="0072">When performing matching, pieces of change region information are compared to check<!-- EPO <DP n="28"> --> whether there are regions located at the same position. If there are no regions located at the same position, region matching is not performed. On the other hand, if there is at least one region located as the same position, each-region matching is performed on such a region. A matching method in region units is the same as the case of performing matching on the entire screen. As such, a distance between features is calculated, and if the distance is not larger than a certain threshold, the regions are determined to match each other. Alternatively, it is possible to use a degree of similarity between features, instead of a distance, and if the degree of similarity is larger than a certain threshold, the regions are determined to match each other. If there are a plurality of regions at the same position, matching is respectively performed on all of the regions. For example, in the case where change region information of one video designates a block in <figref idrefs="f0004">Fig. 4</figref> and change region information of another video designates a block in <figref idrefs="f0010">Fig. 10</figref>, the positions of the blocks having indexes 6 and 10 are the same in both cases. As such, matching is performed on the blocks 6 and 10 to determine whether they match each other.</p><p id="p0073" num="0073">Similar to the case of matching between pictures, the above-described matching process is performed on regions with respect to a picture included in certain segments, and checks whether the segments match each other. In this process, although there may be a picture which cannot be compared due to inconsistency in the change region information, matching between regions is performed by eliminating such a picture from evaluation. For example, among a T picture, a T+1 picture, and a T+2 picture, if matching can be performed between the T picture and the T+2 picture but cannot be performed on the T+1 picture, only a result obtained from the T picture and the T+2 is used for determination.<!-- EPO <DP n="29"> --></p><p id="p0074" num="0074">Then, in the region matching process, a matching result including segments determined to match is output to the matching result determination unit 240 as a region matching result.</p><p id="p0075" num="0075">The matching result determination unit 240 determines and outputs a final matching result, based on the picture matching result output from the picture matching unit 220 and the region matching result output from the region matching unit 230. If there is only a picture matching result, determination is made from the picture matching result. If there are both picture matching result and region matching result, the both results are compared, and a matched segment included in both the picture matching result and the region matching result is output. If the region matching result does not include any matching segment because matching was not performed due to inconsistency in change region information, the picture matching result is directly output.</p><p id="p0076" num="0076">The description of the embodiment of the video signature matching device according to the present invention ends.</p><heading id="h0013">[Effects of First Embodiment]</heading><p id="p0077" num="0077">According to the present embodiment, even in the case where a change in a time direction is small in the screen image as a whole and it is difficult to accurately perform position matching in the time direction, matching accuracy in the time direction can be improved by obtaining local change information and describing it compactly so as to reduce the size of the<!-- EPO <DP n="30"> --> video signature. As such, even in a scene having a poor temporal change, as a feature is calculated independently for a region where changes in the image such as motion or luminous changes occur, it is possible to perform matching with high reliability using regions with motion.</p><p id="p0078" num="0078">Further, in the present embodiment, as an inter-picture pixel value difference is calculated between a picture for which change region information is calculated and previous and next pictures thereof, and the change region information is calculated based on the inter-picture pixel value difference, a processing load to calculate the change region information can be reduced.</p><p id="p0079" num="0079">Further, in the present embodiment, as motion estimation processing is performed between a picture for which change region information is calculated and previous and next pictures thereof, and the change region information is calculated based on the estimated degree of the magnitude of motion, it is possible to obtain a region including motion in a time direction with high accuracy.</p><p id="p0080" num="0080">Next, a second embodiment of the present invention will be described with reference to the drawings.</p><p id="p0081" num="0081">Referring to <figref idrefs="f0011">Fig. 11</figref> showing a video signature extraction device according to the second embodiment of the present invention, the video signature extraction device includes the time axial direction change region extraction unit 100, the each-region feature extraction unit 110, an<!-- EPO <DP n="31"> --> each-picture feature extraction unit 630, a multiplexing unit 620, and a matching weight information extraction unit 610.</p><p id="p0082" num="0082">The connection relationship of the time axial direction change region extraction unit 100 and the each-region feature extraction unit 110 is the same as that shown in <figref idrefs="f0001">FIG. 1</figref>. The each-picture feature extraction unit 630 receives a video and a feature extraction parameter, and outputs an each-picture visual feature to the multiplexing unit 620. The matching weight information extraction unit 610 receives a video and a feature extraction parameter, and outputs matching weight information to the multiplexing unit 620. The multiplexing unit 620 receives change region information output from the time axial direction change region extraction unit 100, an each-feature visual feature output from the each-region feature extraction unit 110, an each-picture visual feature output from the each-picture feature extraction unit 130, and matching weight information output from the matching weight information extraction unit 610, and outputs a multiplexed result as a video signature. It should be noted that the video signature extraction device of the present embodiment can be realized by a computer which is controllable by programs.</p><p id="p0083" num="0083">Next, operation of the second embodiment shown in <figref idrefs="f0011">Fig. 11</figref> will be described in detail.</p><p id="p0084" num="0084">Operation of the time axial direction change region extraction unit 100 and operation of the each-region feature extraction unit 110 are the same as those in the case shown in <figref idrefs="f0001">Fig. 1</figref>.</p><p id="p0085" num="0085"><!-- EPO <DP n="32"> --> Operation of the each-picture feature extraction unit 630 is also similar to that of the each-picture feature extraction unit 130, except for extracting a feature of each picture in accordance with a feature extraction parameter. However, a visual feature is not a feature obtained by converting the entire image on the screen but a feature calculated from a partial region within the screen image. As such, it is assumed that each dimension of a feature vector corresponds to a particular region within the screen image by a feature extraction parameter and that a value of a feature extracted from the region is stored. For example, each dimension of a feature vector is assumed to be a feature extracted from each block formed by dividing the screen image into blocks. A value of each dimension of a feature may be obtained from a predetermined region in any shape. Information describing a region which is an extraction target with respect to each dimension of a feature is called a feature parameter. Specifically, if each dimension of a feature vector is a feature extracted from a particular block within the screen image, information describing the particular block for extracting the feature (coordinate value of the block, index number of the block, and the like) serves as a feature parameter. In another case, if a local region in any of a variety of shapes corresponds to each dimension of a feature vector, information describing the local region (information indicating location, size, and shape of the local region) serves as a feature parameter.</p><p id="p0086" num="0086">The matching weight information extraction unit 610 calculates an amount of change of the image in a time direction in a region corresponding to each dimension of the feature by the feature extraction parameter, determines a weighting coefficient of each dimension to be used for matching in accordance with the amount of change, and outputs information describing the<!-- EPO <DP n="33"> --> weighting coefficient as matching weight information.</p><p id="p0087" num="0087">This means that an amount of change is first calculated for each region using the current target picture and previous and next pictures. The amount of change may be an amount of change in a time direction of the image calculated by means of the method shown in <figref idrefs="f0003">Fig. 3</figref>, or an amount of motion calculated by means of the method shown in <figref idrefs="f0007">Fig. 7</figref>.</p><p id="p0088" num="0088">Next, according to the amount of change in a time direction calculated with respect to each dimension, information describing the degree of weighting for each dimension of the feature to be used for matching is determined. As a region having a larger change in a time direction has a higher possibility of contributing to discrimination of a video, weighting is performed such that a larger change is determined to be more important. For example, a degree of weighting may be determined by a function which monotonically increases with respect to an amount of change in a time direction. Matching weight information may be a coefficient itself which determines the degree of weighting, or may be information of index designating a class among classes formed by dividing the degrees of weighting from low to high. In a scene where an anchor person speaks in a news program, for example, there is a case where no motion is found in areas other than an area around the face of the anchor person. In that case, as a change in a time direction in the dimension of the region corresponding to the face of the anchor person becomes relatively larger than changes in other regions in the screen image, matching weight information, in which weight of the dimension of the feature corresponding to the face region (particularly, a region corresponding to the mouth and eyes) is high, is calculated.<!-- EPO <DP n="34"> --></p><p id="p0089" num="0089">It should be noted that the matching weight information may be calculated for each picture, or calculated for several pictures in a lump, and output. For example, if a portion with motion within a shot is limited to a particular region, it is possible to calculate and output matching weight information with respect to the entire shot. More specifically, it is possible that matching weight information, obtained for one picture in a shot, is also used for other pictures in the shot. Thereby, the amount of calculation of obtaining the matching weight information can be reduced, and also, the amount of information of the image signature can be reduced. Alternatively, it is possible to calculate matching weight information for all or a plurality of pictures in a shot and, with use of a representative value thereof (mean, median, or the like), describe matching weight of the entire shot and use it for all pictures in the shot. Thereby, the amount of information of the video signature can be reduced.</p><p id="p0090" num="0090">However, units for outputting matching weight information are not limited to shots, and may be fixed time intervals such as every several pictures. It is also possible to calculate time segments to which the same matching weight information is applicable from time direction variation information, and calculate and output matching weight information in a lump with respect to the pictures included in the time segments. In that case, as the number of pictures put together varies each time, the number of pictures is also described together. Time segments to which the same matching weight information is applicable are able to be calculated by applying threshold processing on changes in the time direction variation information between pictures. As such, time direction variation information in the head picture in a time segment and time<!-- EPO <DP n="35"> --> direction variation information of the current picture are compared, and if the degree of change exceeds a threshold, a segment up to the previous picture is considered as one unit, whereby matching weight information with respect to such segment is calculated. The matching weight information with respect to such segment may be used as matching weight information of an arbitrary picture in the segment or a representative value of matching weight information of the pictures in the segment. Through these processes, regardless of a processing target video, the amount of information of the matching weight information can be reduced while keeping high discrimination accuracy in a time direction.</p><p id="p0091" num="0091">Further, if a plurality of dimensions of a feature vector correspond to the same region, they may be shown in a lump as one weight information. For example in the case of Edge Histogram set in ISO/IEC 1 5938-3, every five bins correspond to the same region. In that case, weight information may be described in a lump every five bins.</p><p id="p0092" num="0092">The multiplexing unit 620 multiplexes the change region information output from the time axial direction change region extraction unit 100, the each-region visual feature output from the each-region feature extraction unit 110, the each-picture visual feature output from the each-picture feature extraction unit 130, and the matching weight information output from the matching weight information extraction unit 610, and generates and outputs a video signature. The operation of the multiplexing unit 620 is similar to that of the multiplexing unit 120 shown in <figref idrefs="f0001">FIG. 1</figref>, except for multiplexing the matching weight information output from the matching weight information extraction unit 610.<!-- EPO <DP n="36"> --></p><p id="p0093" num="0093">Next, a matching device according to the second embodiment of the present invention will be described.</p><p id="p0094" num="0094">Referring to <figref idrefs="f0012">FIG. 12</figref> showing a matching device for matching a video signature generated according to the second embodiment of the present invention, the matching device includes a demultiplexing unit 700, another demultiplexing unit 710, a picture matching unit 720, a weighting coefficient calculation unit 730, a region matching unit 230, and a matching result determination unit 240.</p><p id="p0095" num="0095">The demultiplexing unit 700 demultiplexes an input first video signature, outputs a first each-picture visual feature to the picture matching unit 720, outputs a first each-region visual feature and first change region information to the region matching unit 230, and outputs first matching weight information to the weighting coefficient calculation unit 730. Similarly, the demultiplexing unit 710 demultiplexes an input second video signature, outputs a second each-picture visual feature to the picture matching unit 720, outputs a second each-region visual feature and second change region information to the region matching unit 230, and outputs second matching weight information to the weighting coefficient calculation unit 730. The weighting coefficient calculation unit 730 calculates a weighting coefficient from the first matching weight information output from the demultiplexing unit 700 and the second matching weight information output from the demultiplexing unit 710, and outputs the weighting coefficient to the picture matching unit 720. The picture matching unit 720 uses the weighting<!-- EPO <DP n="37"> --> coefficient output from the weighting coefficient calculation unit 730 to compare the first each-picture visual feature output from the demultiplexing unit 700 with the second each-picture visual feature output from the demultiplexing unit 710, and outputs a picture matching result to the matching result determination unit 240, and outputs region matching execution information to the region matching unit 230. Based on the region matching execution information output from the picture matching unit 720, the first change region information output from the demultiplexing unit 700, and the second change region information output from the demultiplexing unit 710, the region matching unit 230 compares the first each-region visual feature output from the demultiplexing unit 700 with the second each-region visual feature output from the demultiplexing unit 710 and outputs a region matching result to the matching result determination unit 240. The matching result determination unit 240 calculates a matching result from the picture matching result output from the picture matching unit 720 and the region matching result output from the region matching unit 230, and outputs the matching result. It should be noted that the matching device of the present embodiment can be realized by a computer which is controllable by programs.</p><p id="p0096" num="0096">Next, operation of the matching device shown in <figref idrefs="f0012">Fig. 12</figref> will be described.</p><p id="p0097" num="0097">Operation of the demultiplexing unit 700 is almost similar to that of the demultiplexing unit 200 shown in <figref idrefs="f0008">Fig. 8</figref>, but also separates first matching weight information from the first video signature. Similarly, operation of the demultiplexing unit 700 is almost similar to that of the demultiplexing unit 210 shown in <figref idrefs="f0008">Fig. 8</figref>, but also separates second matching weight<!-- EPO <DP n="38"> --> information from the second video signature. The separated first matching weight information and the second matching weight, information are input to the weighting coefficient calculation unit 730.</p><p id="p0098" num="0098">The weighting coefficient calculation unit 730 calculates a weighting coefficient with respect to each dimension of the feature, from the first matching weight information and the second matching weight information. A plurality of methods may be used for calculating a weighting coefficient from the first matching weight information and the second matching weight, information, if the calculated weighting coefficient satisfies conditions such that it becomes smaller when both pieces of matching weight information correspond to a smaller weight value and it increases when at least one of weight values corresponding to the matching weight information increases. For example, if respective weights calculated from the first matching weight information and the second matching weight information are w<sub>1</sub>(i) and w<sub>2</sub>(i), a weighting coefficient w(i) is calculated from the following Expression 7.<maths id="math0007" num="[Expression 7]"><math display="block"><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>=</mo><mi>max</mi><mfenced separators=""><msub><mi>w</mi><mn>1</mn></msub><mfenced><mi>i</mi></mfenced><mo>,</mo><msub><mi>w</mi><mn>2</mn></msub><mfenced><mi>i</mi></mfenced></mfenced></math><img id="ib0007" file="imgb0007.tif" wi="64" he="24" img-content="math" img-format="tif"/></maths></p><p id="p0099" num="0099">More generally, the following Expression 8 may be used.<maths id="math0008" num="[Expression 8]"><math display="block"><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>=</mo><msup><mfenced open="|" close="|" separators=""><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msup><mfenced><mi>i</mi></mfenced><mi>p</mi></msup><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msup><mfenced><mi>i</mi></mfenced><mi>p</mi></msup></mfenced><mrow><mn>3</mn><mo>/</mo><mi>p</mi></mrow></msup></math><img id="ib0008" file="imgb0008.tif" wi="70" he="29" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="39"> --></p><p id="p0100" num="0100">In Expression 8, p represents any natural number, and when p is infinite, the expression results in Expression 7.</p><p id="p0101" num="0101">The weight coefficient is calculated for each dimension of the feature, and is output to the picture matching unit 720.</p><p id="p0102" num="0102">While the operation of the picture matching unit 720 is basically similar to that of the picture matching unit 220 shown in <figref idrefs="f0008">Fig. 8</figref>, except for an aspect of using a weight coefficient calculated as described above when performing matching between feature vectors.</p><p id="p0103" num="0103">In that case, the features may be compared using the degree of similarity showing similarity between them, or using a distance showing the degree of difference between them. In the case of using a distance, comparison is made using a distance d calculated according to Expression 9, rather than Expression 3.<maths id="math0009" num="[Expression 9]"><math display="block"><mi>d</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>⁢</mo><mfenced open="|" close="|" separators=""><msub><mi>v</mi><mn>1</mn></msub><mfenced><mi>i</mi></mfenced><mo>-</mo><msub><mi>v</mi><mn>2</mn></msub><mfenced><mi>i</mi></mfenced></mfenced></math><img id="ib0009" file="imgb0009.tif" wi="65" he="26" img-content="math" img-format="tif"/></maths></p><p id="p0104" num="0104">In Expression 9, w(i) represents a weight coefficient corresponding to the i<sup>th</sup> dimension. Similarly, in the case of using a degree of similarity, Expression 10 and Expression 11 are used, rather than Expression 4 and Expression 6.<!-- EPO <DP n="40"> --> <maths id="math0010" num="[Expression 10]"><math display="block"><mi>S</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>⁢</mo><mi mathvariant="italic">Sim</mi><mfenced separators=""><msub><mi>v</mi><mn>1</mn></msub><mfenced><mi>i</mi></mfenced><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mfenced><mi>i</mi></mfenced></mfenced></math><img id="ib0010" file="imgb0010.tif" wi="69" he="26" img-content="math" img-format="tif"/></maths><maths id="math0011" num="[Expression 11]"><math display="block"><mrow><mi>S</mi><mo>=</mo><mfrac><mrow><mrow><mstyle displaystyle="true"><mrow><munderover><mrow><mo>∑</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></munderover></mrow></mstyle><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>⁢</mo><msub><mrow><mi>v</mi></mrow><mrow><mn>1</mn></mrow></msub><mfenced><mi>i</mi></mfenced><mo>⁢</mo><msub><mrow><mi>v</mi></mrow><mrow><mn>2</mn></mrow></msub><mfenced><mi>i</mi></mfenced></mrow></mrow><mrow><mrow><msqrt><mrow><mfenced separators=""><mstyle displaystyle="true"><mrow><munderover><mrow><mo>∑</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></munderover></mrow></mstyle><mi>w</mi><mfenced><mi>i</mi></mfenced><mrow><msub><mrow><mi>v</mi></mrow><mrow><mn>1</mn></mrow></msub></mrow><msup><mfenced><mi>i</mi></mfenced><mrow><mn>2</mn></mrow></msup></mfenced><mo>⁢</mo><mfenced separators=""><mstyle displaystyle="true"><mrow><munderover><mrow><mo>∑</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></munderover></mrow></mstyle><mi>w</mi><mfenced><mi>i</mi></mfenced><mo>⁢</mo><msub><mrow><mi>v</mi></mrow><mrow><mn>2</mn></mrow></msub><mo>⁢</mo><msup><mfenced><mi>i</mi></mfenced><mrow><mn>2</mn></mrow></msup></mfenced></mrow></msqrt></mrow></mrow></mfrac></mrow></math><img id="ib0011" file="imgb0011.tif" wi="97" he="39" img-content="math" img-format="tif"/></maths></p><p id="p0105" num="0105">Operation of the region matching unit 230 and operation of the matching result determination unit 240 are similar to those of the case shown in <figref idrefs="f0008">Fig. 8</figref>.</p><heading id="h0014">[Effects of Second Embodiment]</heading><p id="p0106" num="0106">According to the present embodiment, matching accuracy in a time direction can be improved, compared with the first embodiment. This is because by increasing the weight of the feature corresponding to a region where a change such as motion or a luminance change is caused in the image, matching is less affected by a feature change due to coding distortion or the like. For example, it is assumed that a scene in which an anchor person reads the news in a studio consists of two pictures A and B, and that a difference between the pictures A and B is only the mouth of the anchor person and the others are completely same. When a picture C, which is completely the same as the picture A is given and it is mechanically determined to which of the pictures A and B the picture C is similar, if there is no coding distortion, a distance<!-- EPO <DP n="41"> --> between the picture C and the picture A is zero. On the other hand, regarding a distance between the picture C and the picture B, as a weight of a mouth portion with motion is large, the distance is sufficiently large. Now, considering that coding distortion exists in the background part of the picture A, for example, although the distance between the picture C and the picture A becomes large due to the coding distortion, as the weight of the background part with no motion is small, the distance between the picture C and the picture A will never be larger than the distance between the picture C and the picture B.</p><p id="p0107" num="0107">Next, a third embodiment of the present invention will be described.</p><p id="p0108" num="0108"><figref idrefs="f0013">Fig. 13</figref> shows an exemplary method of extracting features from a picture. In this method, pairs of any two regions within a picture are set beforehand, and a difference between the features of the two regions of a pair is obtained as a feature vector. In this embodiment, respective pairs of regions are indicated as P1, P2, P3, ···, and a feature determined from the n<sup>th</sup> pair is indicated as Vn. Pairs of regions may take various combinations of shapes and positions of regions, as shown in <figref idrefs="f0013">Fig. 13</figref>. Also, various methods can be used for calculating a feature Vn from the pair Pn. For example, there is a method in which a mean value of luminance is calculated in each of a shaded region and a reticulated region of a pair, and a value of the feature Vn is determined from the magnitude relation thereof. Specifically, a mean luminance value obtained within a reticulated region is subtracted from a mean luminance value obtained within a shaded region to calculate a difference, and when the difference is positive, Vn = 1, while when the difference is negative, Vn = -1. It is also possible that if the absolute value of the difference<!-- EPO <DP n="42"> --> is smaller than a threshold, Vn is zero, so that the feature Vn is indicated by three values. It should be noted that another representative value can be used, instead of the mean luminance value. For example, a median value within a region or a value corresponding to the top a% of the luminance values in descending order may be used, or an amount showing the edge feature may be used as a representative value. For example, it is possible to apply a filter for detecting an edge to a region, and perform statistical processing such as averaging from the result to obtain a representative value.</p><p id="p0109" num="0109">The time axial direction change region extraction unit 100 calculates a change in the screen image in a time direction, with respect to each region formed by dividing it into M*N pieces (M and N represent natural numbers). For this calculation, Expressions 1 and 2 can be used. A region having a large amount of change in a time direction is selected, and an index of the region is output as change region information. As methods for selecting such a region, selecting a region when an amount of change in a time direction is not less than a given threshold, or selecting a given number of regions from the top when the regions are arranged in descending order of the amount of change, may be used.</p><p id="p0110" num="0110">It should be noted that if there are a large number of regions having large amount of change, discrimination can often be made only using the entire picture. In that case, it is possible not to calculate a feature in a region unit. For example, if the number of regions having small amount of change is not more than a certain threshold, a feature in a region unit is not calculated. As such, nothing is output as change region information, or change region<!-- EPO <DP n="43"> --> information may include information showing there is no feature extraction target region.</p><p id="p0111" num="0111">The obtained change region information is output to the each-region feature extraction unit 110. The each-region feature extraction unit 110 extracts a feature of each region with respect to a region designated by the change region information output from the time axial direction change region extraction unit. As this feature, one similar to that calculated with respect to the entire picture can be used. As such, as shown in <figref idrefs="f0013">Fig. 13</figref>, any two regions within a picture are set as a pair, and a difference between the features of the pair of two regions is obtained as a feature vector. A method of setting a pair in this process and a method of calculating a representative value in a region may be the same as those used for the entire picture, or different Further, a method of calculating a feature may be changed for each region.</p><p id="p0112" num="0112">As described above, even in a scene having less temporal change, it is possible to construct features with which video segments can be discriminated in a time axial direction with high accuracy.</p><p id="p0113" num="0113">While the embodiments of the present invention have been described above, the present invention is not limited to these examples. It will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the scope of the present invention.</p><p id="p0114" num="0114">This application is based upon and claims the benefit of priority from Japanese patent<!-- EPO <DP n="44"> --> application No. <patcit id="pcit0003" dnum="JP2009012815A"><text>2009-12815, filed on January 23, 2009</text></patcit>, the disclosure of which is incorporated herein in its entirety by reference.</p><heading id="h0015">INDUSTRIAL APPLICABILITY</heading><p id="p0115" num="0115">The present invention is applicable to retrieval of similar or identical videos from various videos with high accuracy. In particular, regarding retrieval of the same segments of videos, the present invention is applicable to identification of illegally copied moving images distributed on the networks and identification of commercials distributed on actual airwaves.</p><heading id="h0016">REFERENCE NUMERALS</heading><p id="p0116" num="0116"><dl id="dl0001" compact="compact"><dt>100</dt><dd>time axial direction change region extraction unit</dd><dt>110</dt><dd>each-region feature extraction unit</dd><dt>120, 620</dt><dd>multiplexing unit</dd><dt>130, 630</dt><dd>each-picture feature extraction unit</dd><dt>200, 210, 700, 710</dt><dd>demultiplexing unit</dd><dt>220, 720</dt><dd>picture matching unit</dd><dt>230</dt><dd>region matching unit</dd><dt>240</dt><dd>matching result determination unit</dd><dt>400</dt><dd>inter-picture difference calculation unit</dd><dt>410</dt><dd>change region extraction unit</dd><dt>500</dt><dd>motion information calculation unit</dd><dt>510</dt><dd>change region extraction unit<!-- EPO <DP n="45"> --></dd><dt>610</dt><dd>matching weight information extraction unit</dd><dt>730</dt><dd>weighting coefficient calculation unit</dd></dl></p></description><claims mxw-id="PCLM56985776" lang="DE" load-source="patent-office"><!-- EPO <DP n="59"> --><claim id="c-de-01-0001" num="0001"><claim-text>Eine Videosignatur-Extraktionsvorrichtung, aufweisend:
<claim-text>Eine Merkmalsextraktionseinheit (130) für jedes Bild, die von einem Eingangsvideo einen Vektor eines visuellen Merkmals jedes Bildes als ein visuelles Merkmal für jedes Bild extrahiert, wobei das Bild ein Videoframe oder -feld ist:
<claim-text>Eine Bereichsextraktionseinheit (100) für eine Zeitachsenrichtungsveränderung, die eine Bildveränderung in einer Zeitrichtung bezüglich vorbestimmten Bereichen in einem Bild des Videos berechnet, einen Bereich mit einem großen Betrag von Bildveränderung in einer Zeitrichtung unter den vorbestimmten Bereichen des Bildes auswählt, und zwar durch Auswählen von Bereichen, in denen der Betrag von Bildänderung gleich oder größer ist als ein Schwellwert oder durch Auswählen einer vorbestimmten Anzahl von Bereichen in absteigender Reihenfolge des Betrags von Bildänderung, und eine Veränderungsbereichsinformation erzeugt, die den ausgewählten Bereich bezeichnet;</claim-text>
<claim-text>eine Merkmalsextraktionseinheit (110) für jeden Bereich, die von dem Video ein visuelles Merkmal des Bereichs, der durch die Veränderungsbereichsinformation bestimmt wird, als ein visuelles Merkmal für jeden Bereich extrahiert; und</claim-text>
<claim-text>eine Multiplexeinheit (120), die das visuelle Merkmal für jedes Bild, das visuelle Merkmal für jeden Bereich, und die Veränderungsbereichsinformation multiplext, um eine Videosignatur zu erzeugen.</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach Anspruch 1, wobei die Bereichsextraktionseinheit (110) für Zeitachsenveränderung eine Inter-Bild-Pixelwert-Differenz zwischen dem Bild, für das die Veränderungsbereichsinformation berechnet wird und einem vorhergehenden oder nachfolgenden Bild berechnet und die Veränderungsbereichsinformation basierend auf der Inter-Bild-Pixelwert-Differenz berechnet.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach Anspruch 1, wobei<br/>
die Bereichsextraktionseinheit (100) für Zeitachsenveränderung eine Bewegungsabschätzungsverarbeitung durchführt, und zwar zwischen dem Bild, für<!-- EPO <DP n="60"> --> das die Veränderungsbereichsinformation berechnet werden und einem vorhergehenden oder nachfolgenden Bild, und die Veränderungsbereichsinformation basierend auf einem Grad der Größe einer abgeschätzten Bewegung berechnet.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach einem der Ansprüche 1 bis 3, wobei<br/>
die Merkmalsextraktionseinheit (130) für jedes Bild Merkmale von einer Region extrahiert, die jeder von Dimensionen eines Merkmals basierend auf dem Video entspricht,<br/>
die Videosignatur-Extraktionsvorrichtung ferner eine Extraktionseinheit (610) für Übereinstimmungsgewichtsinformationen einschließt, die analysiert, bezüglich jedes der Bilder des Videos, eine Bildveränderung in einer Zeitrichtung in der Region, die jeder der Dimensionen des Merkmals entspricht, und Übereinstimmungsgewichtsinformationen ausgibt, die Informationen sind, welche ein Gewicht entsprechend einem Grad der Bildveränderung beschreiben, und<br/>
die Multiplexeinheit (120) ferner die Übereinstimmungsgewichtsinformationen ebenfalls multiplext und eine Videosignatur erzeugt.</claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach Anspruch 4, wobei<br/>
die Übereinstimmungsgewichtsinformationen ein Gewicht beschreiben, das einen größeren Wert für eine Dimension mit einem größeren Betrag von Bildveränderung aufweist.</claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach Anspruch 4 oder 5, wobei<br/>
die Übereinstimmungsgewichtsinformations-Extraktionseinheit (610) eine Inter-Bild-Pixelwert-Differenz berechnet, und zwar zwischen dem Bild, für das die Übereinstimmungsgewichtsinformationen berechnet werden und einem vorhergehenden oder nächsten Bild, und ferner die Übereinstimmungsgewichtsinformationen entsprechend jedem der Dimensionen des Merkmals basierend auf der Inter-Bild-Pixelwert-Differenz berechnet.</claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach Anspruch 4 oder 5, wobei<br/>
die Extraktionseinheit (610) für Übereinstimmungsgewichtsinformationen eine Bewegungsabschätzungsverarbeitung durchführt, und zwar zwischen dem Bild, für<!-- EPO <DP n="61"> --> das die Übereinstimmungsgewichtsinformationen berechnet werden und einem vorhergehenden oder nachfolgenden Bild, und ferner die Übereinstimmungsgewichtsinformationen entsprechend jedem der Dimensionen des Merkmals basierend auf einem Grad der Größe einer abgeschätzten Bewegung berechnet.</claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach einem der Ansprüche 1 bis 7,k wobei<br/>
die Bereichsextraktionseinheit (100) für Zeitachsenveränderung eine bestimmte Anzahl von Bereichen auswählt, die ausgewählt werden in Reihenfolge mit einer Region, welche den größten Betrag an Bildveränderung aufweist als erste.</claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach einem der Ansprüche 1 bis 7, wobei<br/>
die Bereichsextraktionseinheit (100) für Zeitachsenveränderung eine Region auswählt, in der eine Bildveränderung nicht weniger als ein Schwellwert ist.</claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach einem der Ansprüche 1 bis 9, wobei<br/>
der Bereich, der durch die Bereichsregionsinformation bestimmt wird, ein Block ist, der durch Teilen des Bildes gebildet wird.</claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>Die Videosignatur-Extraktionsvorrichtung nach einem der Ansprüche 1 bis 10, wobei<br/>
jede der Dimensionen von dem visuellen Merkmal jeden Bildes ein Wert ist, der einer Differenz zwischen Merkmalen von zwei beliebigen Regionen in unterschiedlichen Formen innerhalb des Bildes sind, wobei die Regionen im Voraus bezüglich der Dimension festgelegt wurden.</claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>Eine Videosignatur-Matching-Vorrichtung, aufweisend:
<claim-text>Eine erste Demultiplex-Einheit (200), die von einer ersten Videosignatur, die von einem Video durch eine Videosignatur-Extraktionsvorrichtung nach Anspruch 1 extrahiert wurde, ein visuelles Merkmal für ein erstes Bild, eine erste<!-- EPO <DP n="62"> --> Veränderungsbereichsinformation und ein erstes visuelles Merkmal für jeden Bereich separiert;</claim-text>
<claim-text>eine zweite Demultiplexeinheit (210), die von einer zweiten Videosignatur, welche von einem anderen Video durch eine Videosignatur-Extraktionsvorrichtung nach Anspruch 1 extrahiert wurde, ein zweites visuelles Merkmal für jedes Bild, eine zweite Veränderungsbereichsinformation und ein zweites visuelles Merkmal für jeden Bereich separiert;</claim-text>
<claim-text>eine Bild-Matching-Einheit (220), die das erste visuelle Merkmal für jedes Bild mit dem zweiten visuellen Merkmal für jedes Bild vergleicht, ein Bild-Matching-Ergebnis erzeugt und, wenn von dem Bild-Matching-Ergebnis ermittelt wird, dass ein Videosegment einer Mehrzahl von Matching-Kandidatensegmenten eines anderen Videos entspricht, Bereichs-Matching-Ausführungsinformationen ausgibt;</claim-text>
<claim-text>eine Bereichs-Matching-Einheit (230), die, wenn die Bereichs-Matching-Ausführungsinformationen ausgegeben werden, die erste Veränderungsbereichsinformation mit der zweiten Veränderungsbereichsinformation vergleicht und, wenn Informationen, die Bereiche bestimmen, welche an derselben Position auf einem Bildschirm angeordnet sind, enthalten sind, visuelle Merkmale für jeden Bereich entsprechend den Bereichen des ersten visuellen Merkmals für jeden Bereich und des zweiten visuellen Merkmals für jeden Bereich vergleicht und ein Bereichs-Matching-Ergebnis für jeden Bereich erzeugt, wobei der Bereich, der durch die ersten Veränderungsbereichsinformationen bestimmt wird, derselbe ist wie der Bereich, der durch die zweiten Veränderungsbereichsinformationen bestimmt wird; und</claim-text>
<claim-text>eine Matching-Ergebnis-Ermittlungseinheit (240), die identische Videosegmente von dem Bild-Matching-Ergebnis identifiziert und, wenn die Bereichs-Matching-Ausführungsinformationen ausgegeben werden, von dem Bereichs-Matching-Ergebnis.</claim-text></claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>Die Videosignatur-Matching-Vorrichtung nach Anspruch 12, wobei<br/>
die erste Videosignatur ferner erste Matching-Gewichtsinformationen einschließt, die Matching-Gewichtsinformationen für jede von Dimensionen des ersten visuellen Merkmals für jedes Bild sind, und ein Gewicht beschreiben, das einer Bildveränderung in einem Bereich innerhalb des Bildes entsprechend der Dimension entspricht,<br/>
<!-- EPO <DP n="63"> -->die zweite Videosignatur ferner zweite Matching-Gewichtsinformationen einschließt, welche Matching-Gewichtsinformationen für jede von Dimensionen des zweiten visuellen Merkmals für jedes Bild sind und ein Gewicht beschreiben, das einer Bildveränderung in einem Bereich entspricht innerhalb des Bilds entsprechend der Dimension,<br/>
die erste Demultiplexeinheit (200) ferner die ersten Matching-Gewichtsinformationen separiert,<br/>
die zweite Demultiplexeinheit (210) ferner die zweiten Matching-Gewichtsinformationen separiert,<br/>
die Videosignatur-Matching-Vorrichtung ferner eine Gewichtskoeffizient-Berechnungeinheit (730) einschließt, die einen Gewichtskoeffizienten für jeden der Dimensionen des Merkmals berechnet, aus der ersten Matching-Gewichtsinformation und der zweiten Matching-Gewichtsinformation, und<br/>
die Bild-Matching-Einheit (220) eine Gewichtung für jede der Dimensionen unter Verwendung des Gewichtskoeffizienten durchführt und das erste visuelle Merkmal für jedes Bild mit dem zweiten visuellen Merkmal für jedes Bild vergleicht.</claim-text></claim><claim id="c-de-01-0014" num="0014"><claim-text>Eine Videosignatur-Extraktionsverfahren aufweisend:
<claim-text>Extrahieren von einem Eingangsvideo einen Vektor eines visuellen Merkmals jedes Bildes als ein visuelles Merkmal für jedes Bild, wobei das Bild ein Videoframe oder -feld ist;</claim-text>
<claim-text>Berechnen einer Bildveränderung in einer Zeitrichtung bezüglich vorbestimmter Bereiche in einem Bild des Videos, Auswählen eines Bereichs aufweisend einen großen Betrag von Bildveränderung in einer Zeitrichtung unter den vorbestimmten Bereichen des Bildes durch Auswählen von Bereichen, in denen der Betrag von Bildveränderung gleich ist oder größer wie ein Schwellwert, oder durch Auswählen einer vorbestimmten Anzahl von Bereichen in absteigender Reihenfolge des Betrags von Bildveränderung, und Erzeugen von Veränderungsbereichsinformationen, die den ausgewählten Bereich bestimmen;</claim-text>
<claim-text>Extrahieren eines visuellen Merkmals des Bereichs, der bestimmt wird durch die Veränderungsbereichsinformationen als ein visuelles Merkmal für jeden Bereich; und</claim-text>
<claim-text>Multiplexen des visuellen Merkmals für jedes Bild, des visuellen Merkmals für jeden Bereich und der Veränderungsbereichsinformationen, um eine Videosignatur zu erzeugen.</claim-text><!-- EPO <DP n="64"> --></claim-text></claim><claim id="c-de-01-0015" num="0015"><claim-text>Das Videosignatur-Extraktionsverfahren nach Anspruch 14, wobei<br/>
das Erzeugen der Veränderungsbereichsinformationen einschließt das Berechnen einer Inter-Bild-Pixelwert-Differenz zwischen dem Bild, für das die Veränderungsbereichsinformationen berechnet werden und einem vorhergehenden oder nächsten Bild, und Berechnen der Veränderungsbereichsinformationen basierend auf der Inter-Bild-Pixelwert-Differenz.</claim-text></claim><claim id="c-de-01-0016" num="0016"><claim-text>Das Videosignatur-Extraktionsverfahren nach Anspruch 14, wobei<br/>
das Erzeugen der Veränderungsbereichsinformationen einschließt das Durchführen von Bewegungsabschätzungsverarbeitung zwischen dem Bild, für das die Veränderungsbereichsinformationen berechnet werden und einem vorhergehenden oder nachfolgenden Bild, und Berechnen der Veränderungsbereichsinformationen basierend auf dem Grad der Größe einer abgeschätzten Bewegung.</claim-text></claim><claim id="c-de-01-0017" num="0017"><claim-text>Das Videosignatur-Extraktionsverfahren nach einem der Ansprüche 14 bis 16, wobei<br/>
das Extrahieren des visuellen Merkmals für jedes Bild einschließt das Extrahieren von Merkmalen von einem Bereich entsprechend jeder Dimension eines Merkmals basierend auf dem Video,<br/>
das Videosignatur-Extraktionsverfahren ferner einschließt das Analysieren, bezüglich jedes Bildes des Videos, einer Bildveränderung in einer Zeitrichtung in dem Bereich entsprechend jeder der Dimensionen des Merkmals, und Berechnen von Matching-Gewichtsinformationen, die Informationen sind, welche ein Gewicht beschreiben, das dem Grad einer Bildveränderung entspricht, und<br/>
das Erzeugen der Videosignatur auch das Multiplexen der Matching-Gewichtsinformationen einschließt und eine Videosignatur erzeugt.</claim-text></claim><claim id="c-de-01-0018" num="0018"><claim-text>Das Videosignatur-Extraktionsverfahren nach Anspruch 17, wobei<br/>
die Matching-Gewichtsinformationen ein Gewicht beschreiben, das einen größeren Wert für eine Dimension mit einer größeren Bildveränderung aufweist.</claim-text></claim><claim id="c-de-01-0019" num="0019"><claim-text>Das Videosignatur-Extraktionsverfahren nach Anspruch 17 oder 18, wobei<br/>
<!-- EPO <DP n="65"> -->das Berechnen der Matching-Gewichtsinformationen einschließt das Berechnen einer Inter-Bild-Pixelwert-Differenz zwischen dem Bild, für das die Matching-Gewichtsinformationen berechnet werden und einem vorhergehenden oder nachfolgenden Bild, und das Berechnen der Matching-Gewichtsinformationen entspricht jeder der Dimensionen des Merkmals basierend auf der Inter-Bild-Pixelwert-Differenz.</claim-text></claim><claim id="c-de-01-0020" num="0020"><claim-text>Das Videosignatur-Extraktionsverfahren nach Anspruch 17 oder 18, wobei<br/>
das Berechnen der Matching-Gewichtsinformationen einschließt das Durchführen von Bewegungsabschätzungsverarbeitung zwischen dem Bild, für das die Matching-Gewichtsinformationen berechnet werden und einem vorhergehenden oder nachfolgenden Bild, und die Matching-Gewichtsinformationen berechnet entsprechend jeder der Dimensionen des Merkmals basierend auf einem Grad der Größe einer abgeschätzten Bewegung.</claim-text></claim><claim id="c-de-01-0021" num="0021"><claim-text>Das Videosignatur-Extraktionsverfahren nach einem der Ansprüche 14 bis 20, wobei<br/>
die Erzeugung der Veränderungsbereichsinformationen einschließt das Auswählen einer bestimmten Anzahl von Bereichen, die ausgewählt werden in Reihenfolge mit einem Bereich aufweisend den größten Betrag an Bildveränderung als dem ersten.</claim-text></claim><claim id="c-de-01-0022" num="0022"><claim-text>Das Videosignatur-Extraktionsverfahren nach einem der Ansprüche 14 bis 20, wobei<br/>
das Erzeugen der Veränderungsbereichsinformationen einschließt das Auswählen einer Region, bei der eine Bildveränderung nicht kleiner ist als ein Schwellwert.</claim-text></claim><claim id="c-de-01-0023" num="0023"><claim-text>Das Videosignatur-Extraktionsverfahren nach einem der Ansprüche 14 bis 22, wobei<br/>
der Bereich, der bestimmt wird durch die Veränderungsbereichsinformationen ein Block ist, der durch Teilen des Bildes gebildet wird.<!-- EPO <DP n="66"> --></claim-text></claim><claim id="c-de-01-0024" num="0024"><claim-text>Das Videosignatur-Extraktionsverfahren nach einem der Ansprüche 14 bis 23, wobei<br/>
jede der Dimensionen des visuellen Merkmals für jedes Bild ein Wert ist, der entspricht einer Differenz zwischen Merkmalen von zwei beliebigen Regionen in unterschiedlichen Formen innerhalb des Bildes, wobei die Regionen im Voraus bezüglich der Dimensionen festgelegt wurden.</claim-text></claim><claim id="c-de-01-0025" num="0025"><claim-text>Ein Videosignatur-Matching-Verfahren, aufweisend:
<claim-text>Separieren eines ersten visuellen Merkmals für jedes Bild, einer ersten Veränderungsbereichsinformation und eines ersten visuellen Merkmals für jeden Bereich aus einer ersten Videosignatur, die extrahiert wurde von einem Video durch eine Videosignatur-Extraktionsvorrichtung nach Anspruch 1;</claim-text>
<claim-text>Separieren eines zweiten visuellen Merkmals für jedes Bild, einer zweiten Veränderungsbereichsinformation und eines zweiten visuellen Merkmals für jeden Bereich aus einer zweiten Videosignatur, die extrahiert wurde von einem anderen Video durch eine Videosignatur-Extraktionsvorrichtung nach Anspruch 1;</claim-text>
<claim-text>Vergleichen des ersten visuellen Merkmals für jedes Bild mit dem zweiten visuellen Merkmal für jedes Bild, Erzeugen eines Bild-Matching-Ergebnisses und, wenn ermittelt wird von dem Bild-Matching-Ergebnis, dass ein Videosegment einer Mehrzahl von Matching-Kandidatensegmenten eines anderen Videos entspricht, Ausgeben von Bereichs-Matching-Ausführungsinformationen;</claim-text>
<claim-text>wenn die Bereichs-Matching-Ausführungsinformationen ausgegeben werden, Vergleichen der ersten Veränderungsbereichsinformationen mit den zweiten Veränderungsbereichsinformationen und, wenn die Informationen, die Bereiche bestimmen, welche an derselben Position auf einem Bildschirm angeordnet sind, enthalten sind, Vergleichen von visuellen Merkmalen für jeden Bereich entsprechend den Bereichen des ersten visuellen Merkmals für jeden Bereich und des zweiten visuellen Merkmals für jeden Bereich und, Erzeugen eines Bereichs-Matching-Ergebnisses für jeden Bereich, wobei der Bereich, der bestimmt wird durch die erste Bereichsveränderungsinformation derselbe ist wie der Bereich, der bestimmt wird durch die zweite Bereichsveränderungsinformation; und</claim-text>
<claim-text>Identifizieren identischer Videosegmente von dem Bild-Matching-Ergebnis und, wenn die Bereichs-Matching-Information ausgegeben wird, von dem Bereichs-Matching-Ergebnis.</claim-text><!-- EPO <DP n="67"> --></claim-text></claim><claim id="c-de-01-0026" num="0026"><claim-text>Das Videosignatur-Matching-Verfahren nach Anspruch 25, wobei<br/>
die erste Videosignatur ferner einschließt erste Matching-Gewichtsinformationen, die Matching-Gewichtsinformationen für jede von Dimensionen des ersten visuellen Merkmals für jedes Bild sind und ein Gewicht beschreiben, das einer Bildveränderung in einem Bereich innerhalb des Bildes entsprechend der Dimension entspricht,<br/>
die zweite Videosignatur ferner einschließt zweite Matching-Gewichtsinformationen, die Matching-Gewichtsinformationen für jede von Dimensionen des zweiten visuellen Merkmals für jedes Bild sind und ein Gewicht beschreiben, das entspricht einer Bildveränderung in einem Bereich innerhalb des Bildes entsprechend der Dimension,<br/>
das Separieren von der ersten Videosignatur einschließt das Separieren erster Matching-Gewichtsinformationen,<br/>
das Separieren von der zweiten Videosignatur einschließt das Separieren zweiter Matching-Gewichtsinformationen,<br/>
das Video-Matching-Verfahren ferner einschließt das Berechnen eines Gewichtskoeffizienten für jede von Dimensionen des Merkmals, von der ersten Matching-Gewichtsinformation und der zweiten Matching-Gewichtsinformation, und<br/>
das Erzeugen des Bild-Matching-Ergebnisses einschließt das Durchführen des Gewichtens für jede der Dimensionen unter Verwendung des Gewichtskoeffizienten und das Vergleichen des ersten visuellen Merkmals für jedes Bild mit dem zweiten visuellen Merkmal für jedes Bild.</claim-text></claim><claim id="c-de-01-0027" num="0027"><claim-text>Ein Programm, das einen Computer veranlasst, zu operieren als:
<claim-text>Eine Merkmalsextraktionseinheit für jedes Bild, die von einem Eingangsvideo einen Vektor eines visuellen Merkmals jedes Bildes als ein visuelles Merkmal für jedes Bild extrahiert, wobei das Bild ein Videoframe oder -feld ist;</claim-text>
<claim-text>eine Veränderungsbereichsextraktionseinheit für eine Zeitachsenrichtung, die eine Bildveränderung in einer Zeitrichtung bezüglich vorbestimmter Bereiche in einem Bild des Videos berechnet, einen Bereich mit einem großen Betrag von Bildveränderung in einer Zeitrichtung unter den vorbestimmten Bereichen des Bildes auswählt, und zwar durch Auswählen von Bereichen, in denen der Betrag von Bildveränderung gleich oder größer ist als eine Schwellwert oder durch Auswählen einer vorbestimmten Anzahl von Bereichen in absteigender Reihenfolge des Betrags<!-- EPO <DP n="68"> --> von Bildveränderung, und Veränderungsbereichsinformationen, die den ausgewählten Bereich bestimmen, erzeugt;</claim-text>
<claim-text>eine Merkmalsextraktionseinheit für jeden Bereich, der aus dem Video ein visuelles Merkmal des Bereichs extrahiert, der bestimmt wird durch die Veränderungsbereichsinformationen als ein visuelles Merkmal für jeden Bereich; und</claim-text>
<claim-text>eine Multiplexeinheit, die das visuelle Merkmal für jedes Bild, das visuelle Merkmal für jeden Bereich und die Veränderungsbereichsinformationen multiplext, um eine Videosignatur zu erzeugen.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56985777" lang="EN" load-source="patent-office"><!-- EPO <DP n="46"> --><claim id="c-en-01-0001" num="0001"><claim-text>A video signature extraction device, comprising:
<claim-text>an each-picture feature extraction unit (130) that extracts from an input video a vector of a visual feature of each picture as an each-picture visual feature, the picture being a video frame or field;</claim-text>
<claim-text>a time axial direction change region extraction unit (100) that calculates an image change in a time direction with respect to predetermined regions in a picture of the video, selects a region having a large amount of image change in a time direction among the predetermined regions of the picture by selecting regions in which the amount of image change equals or exceeds a threshold or by selecting a predetermined number of regions in descending order of the amount of image change, and generates change region information designating the selected region;</claim-text>
<claim-text>an each-region feature extraction unit (110) that extracts from the video a visual feature of the region designated by the change region information as an each-region visual feature, and</claim-text>
<claim-text>a multiplexing unit (120) that multiplexes the each-picture visual feature, the each-region visual feature, and the change region information to generate a video signature.</claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>The video signature extraction device, according to Claim 1, wherein<br/>
the time axial direction change region extraction unit (110) calculates an<!-- EPO <DP n="47"> --> inter-picture pixel value difference between the picture for which the change region information is calculated and a previous or next picture, and calculates the change region information based on the inter-picture pixel value difference.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>The video signature extraction device, according to Claim 1, wherein<br/>
the time axial direction change region extraction unit (100) performs motion estimation processing between the picture for which the change region information is calculated and a previous or next picture, and calculates the change region information based on a degree of magnitude of an estimated motion.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>The video signature extraction device, according to any of Claims 1 to 3, wherein<br/>
the each-picture feature extraction unit (130) extracts features from a region corresponding to each of dimensions of a feature based on the video,<br/>
the video signature extraction device further includes a matching weight information extraction unit (610) that analyzes, with respect to each of pictures of the video, an image change in a time direction in the region corresponding to each of the dimensions of the feature, and outputs matching weight information which is information describing a weight corresponding to a degree of the image change, and<br/>
the multiplexing unit (120) further multiplexes the matching weight information as well, and generates a video signature.<!-- EPO <DP n="48"> --></claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>The video signature extraction device, according to Claim 4, wherein<br/>
the matching weight information describes a weight which takes a larger value for a dimension having a larger amount of image change.</claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>The video signature extraction device, according to Claim 4 or 5, wherein<br/>
the matching weight information extraction unit (610) calculates an inter-picture pixel value difference between the picture for which the matching weight information is calculated and a previous or next picture, and calculates the matching weight information corresponding to each of the dimensions of the feature based on the inter-picture pixel value difference.</claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>The video signature extraction device, according to Claim 4 or 5, wherein<br/>
the matching weight information extraction unit (610) performs motion estimation processing between the picture for which the matching weight information is calculated and a previous or next picture, and calculates the matching weight information corresponding to each of the dimensions of the feature based on a degree of magnitude of an estimated motion.</claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>The video signature extraction device, according to any of Claims 1 to 7, wherein<br/>
the time axial direction change region extraction unit (100) selects a certain number of regions which are selected in order with a region having a largest amount<!-- EPO <DP n="49"> --> of image change being the first.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>The video signature extraction device, according to any of Claims 1 to 7, wherein<br/>
the time axial direction change region extraction unit (100) selects a region in which an image change is not less than a threshold.</claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>The video signature extraction device, according to any of Claims 1 to 9, wherein<br/>
the region designated by the change region information is a block formed by dividing the picture</claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>The video signature extraction device, according to any of Claims 1 to 10, wherein<br/>
each of the dimensions of the each-picture visual feature is a value corresponding to a difference between features of any two regions in different shapes within the picture, the regions having been set beforehand with respect to the dimension.</claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>A video signature matching device, comprising:
<claim-text>a first demultiplexing unit (200) that, from a first video signature extracted from a video by a video signature extraction device according to claim 1, separates a<!-- EPO <DP n="50"> --> first each-picture visual feature, a first change region information, and a first each-region visual feature,</claim-text>
<claim-text>a second demultiplexing unit (210) that, from a second video signature extracted from another video by a video signature extraction device according to claim 1, separates a second each-picture visual feature, a second change region information, and a second each-region visual feature;</claim-text>
<claim-text>a picture matching unit (220) that compares the first each-picture visual feature with the second each-picture visual feature, generates a picture matching result, and when it is determined from the picture matching result that a video segment corresponds to a plurality of matching candidate segments of another video, outputs region matching execution information;</claim-text>
<claim-text>a region matching unit (230) that, when the region matching execution information is output, compares the first change region information with the second change region information, and if information designating regions which are located at the same position on a screen is included, compares each-region visual features corresponding to the regions of the first each-region visual feature and the second each-region visual feature, and generates a region matching result for each region wherein the region designated by the first change region information is the same as the region designated by the second change region information; and</claim-text>
<claim-text>a matching result determination unit (240) that identifies identical video segments from the picture matching result and, if the region matching execution information is output, from the region matching, result.</claim-text><!-- EPO <DP n="51"> --></claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>The video signature matching device, according to Claim 12, wherein<br/>
the first video signature further includes first matching weight information which is matching weight information for each of dimensions of the first each-picture visual feature and describes a weight corresponding to an image change in a region within the picture corresponding to the dimension,<br/>
the second video signature further includes second matching weight information which is matching weight information for each of dimensions of the second each-picture visual feature and describes a weight corresponding to an image change in a region within the picture corresponding to the dimension,<br/>
the first demultiplexing unit (200) further separates the first matching weight information,<br/>
the second demultiplexing unit (210) further separates the second matching weight information,<br/>
the video signature matching device further includes a weighting coefficient calculation unit (730) that calculates a weighting coefficient for each of the dimensions of the feature, from the first matching weight information and the second matching weight information, and<br/>
the picture matching unit (220) performs weighting for each of the dimension using the weighting coefficient, and compares the first each-picture visual feature with the second each-picture visual feature.<!-- EPO <DP n="52"> --></claim-text></claim><claim id="c-en-01-0014" num="0014"><claim-text>A video signature extraction method, comprising:
<claim-text>extracting from an input video a vector of a visual feature of each picture as an each-picture visual feature, the picture being a video frame or field;</claim-text>
<claim-text>calculating an image change in a time direction with respect to predetermined regions in a picture of the video, selecting a region having a large amount of image change in a time direction among the predetermined regions of the picture by selecting regions in which the amount of image change equals or exceeds a threshold or by selecting a predetermined number of regions in descending order of the amount of image change, and generating change region information designating the selected region;</claim-text>
<claim-text>extracting from the video a visual feature of the region designated by the change region information as an each-region visual feature; and</claim-text>
<claim-text>multiplexing the each-picture visual feature, the each-region visual feature, and the change region information to generate a video signature.</claim-text></claim-text></claim><claim id="c-en-01-0015" num="0015"><claim-text>The video signature extraction method, according to Claim 14, wherein the generating the change region information includes calculating an inter-picture pixel value difference between the picture for which the change region information is calculated and a previous or next picture, and calculating the change region information based on the inter-picture pixel value difference.</claim-text></claim><claim id="c-en-01-0016" num="0016"><claim-text>The video signature extraction method, according to Claim 14, wherein<br/>
<!-- EPO <DP n="53"> -->the generating the change region information includes performing motion estimation processing between the picture for which the change region information is calculated and a previous or next picture, and calculating the change region information based on a degree of magnitude of an estimated motion.</claim-text></claim><claim id="c-en-01-0017" num="0017"><claim-text>The video signature extraction method, according to any of Claims 14 to 16, wherein<br/>
the extracting the each-picture visual feature includes extracting features from a region corresponding to each of dimensions of a feature based on the video,<br/>
the video signature extraction method further includes analyzing, with respect to each of pictures of the video, an image change in a time direction in the region corresponding to each of the dimensions of the feature, and calculating matching weight information which is information describing a weight corresponding to a degree of the image change, and<br/>
the generating the video signature includes multiplexing the matching weight information as well, and generates a video signature.</claim-text></claim><claim id="c-en-01-0018" num="0018"><claim-text>The video signature extraction method, according to Claim 17, wherein<br/>
the matching weight information describes a weight which takes a larger value for a dimension having a larger image change.</claim-text></claim><claim id="c-en-01-0019" num="0019"><claim-text>The video signature extraction method, according to Claim 17 or 18,<!-- EPO <DP n="54"> --> wherein<br/>
the calculating the matching weight information includes calculating an inter-picture pixel value difference between the picture for which the matching weight information is calculated and a previous or next picture, and calculating the matching weight information corresponding to each of the dimensions of the feature based on the inter-picture pixel value difference.</claim-text></claim><claim id="c-en-01-0020" num="0020"><claim-text>The video signature extraction method, according to Claim 17 or 18, wherein<br/>
the calculating the matching weight information includes performing motion estimation processing between the picture for which the matching weight information is calculated and a previous or next picture, and calculates the matching weight information corresponding to each of the dimensions of the feature based on a degree of magnitude of an estimated motion.</claim-text></claim><claim id="c-en-01-0021" num="0021"><claim-text>The video signature extraction method, according to any of Claims 14 to 20, wherein<br/>
the generation of the change region information includes selecting a certain number of regions which are selected in order with a region having a largest amount of image change being the first</claim-text></claim><claim id="c-en-01-0022" num="0022"><claim-text>The video signature extraction method, according to any of Claims 14 to<!-- EPO <DP n="55"> --> 20, wherein<br/>
the generation of the change region information includes selecting a region in which an image change is not less than a threshold.</claim-text></claim><claim id="c-en-01-0023" num="0023"><claim-text>The video signature extraction method, according to any of Claims 14 to 22, wherein<br/>
the region designated by the change region information is a block formed by dividing the picture.</claim-text></claim><claim id="c-en-01-0024" num="0024"><claim-text>The video signature extraction method, according to any of Claims 14 to 23, wherein<br/>
each of the dimensions of the each-picture visual feature is a value corresponding to a difference between features of any two regions in different shapes within the picture, the regions having been set beforehand with respect to the dimension.</claim-text></claim><claim id="c-en-01-0025" num="0025"><claim-text>A video signature matching method, comprising:
<claim-text>from a first video signature extracted from a video by a video signature extraction device according to claim 1, separating a first each-picture visual feature, a first change region information, and a first each-region visual feature;</claim-text>
<claim-text>from a second video signature extracted from another video by a video signature extraction device according to claim 1, separating a second each-picture<!-- EPO <DP n="56"> --> visual feature, a second change region information, and a second each-region visual feature;</claim-text>
<claim-text>comparing the first each-picture visual feature with the second each-picture visual feature, generating a picture matching result, and when it is determined from the picture matching result that a video segment corresponds to a plurality of matching candidate segments of another video, outputs region matching execution information;</claim-text>
<claim-text>when the region matching execution information is output, compares the first change region information with the second change region information, and if information designating regions which are located at the same position on a screen is included, comparing each-region visual features corresponding to the regions of the first each-region visual feature and the second each-region visual feature, and generating a region matching result for each region wherein the region designated by the first region change information is the same as the region designated by the second region change information; and</claim-text>
<claim-text>identifying identical video segments from the picture matching result and, if the region matching execution information is output, from the region matching result.</claim-text></claim-text></claim><claim id="c-en-01-0026" num="0026"><claim-text>The video signature matching method, according to Claim 25, wherein the first video signature further includes first matching weight information which is matching weight information for each of dimensions of the first each-picture<!-- EPO <DP n="57"> --> visual feature and describes a weight corresponding to an image change in a region within the picture corresponding to the dimension,<br/>
the second video signature further includes second matching weight information which is matching weight information for each of dimensions of the second each-picture visual feature and describes a weight corresponding to an image change in a region within the picture corresponding to the dimension,<br/>
the separating from the first video signature includes separating the first matching weight information,<br/>
the separating from the second video signature includes separating the second matching weight information,<br/>
the video signature matching method further includes calculating a weighting coefficient for each of the dimensions of the feature, from the first matching weight information and the second matching weight information, and<br/>
the generating the picture matching result includes performing weighting for each of the dimension using the weighting coefficient, and comparing the first each-picture visual feature with the second each-picture visual feature.</claim-text></claim><claim id="c-en-01-0027" num="0027"><claim-text>A program causing a computer to function as:
<claim-text>an each-picture feature extraction unit that extracts from an input video a vector of a visual feature of each picture as an each-picture visual feature, the picture being a video frame or field,</claim-text>
<claim-text>a time axial direction change region extraction unit that calculates an image<!-- EPO <DP n="58"> --> change in a time direction with respect to predetermined regions in a picture of the video, selects a region having a large amount of image change in a time direction among the predetermined regions of the picture by selecting regions in which the amount of image change equals or exceeds a threshold or by selecting a predetermined number of regions in descending order of the amount of image change, and generates change region information designating the selected region;</claim-text>
<claim-text>an each-region feature extraction unit that extracts from the video a visual feature of the region designated by the change region information as an each-region visual feature; and</claim-text>
<claim-text>a multiplexing unit that multiplexes the each-picture visual feature, the each-region visual feature, and the change region information to generate a video signature.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56985778" lang="FR" load-source="patent-office"><!-- EPO <DP n="69"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Dispositif d'extraction de signature vidéo, comprenant :
<claim-text>une unité d'extraction de caractéristiques de chaque cliché (130) qui extrait depuis une vidéo d'entrée un vecteur d'une caractéristique visuelle de chaque cliché en tant que caractéristique visuelle de chaque cliché, le cliché étant une trame ou un champ vidéo ;</claim-text>
<claim-text>une unité d'extraction de région de changement de direction axiale temporelle (100) qui calcule un changement d'image dans une direction temporelle par rapport à des régions prédéterminées dans un cliché de la vidéo, sélectionne une région ayant une grande quantité de changement d'image dans une direction temporelle parmi les régions prédéterminées du cliché en sélectionnant des régions dans lesquelles la quantité de changement d'image est égale ou supérieure à un seuil ou en sélectionnant un nombre prédéterminé de régions dans l'ordre décroissant de la quantité de changement d'image, et génère une information de région de changement désignant la région sélectionnée ;</claim-text>
<claim-text>une unité d'extraction de caractéristiques de chaque région (110) qui extrait depuis la vidéo une caractéristique visuelle de la région désignée par l'information de région de changement en tant que caractéristique visuelle de chaque région ; et</claim-text>
<claim-text>une unité de multiplexage (120) qui multiplexe la caractéristique visuelle de chaque cliché, la caractéristique visuelle de chaque région, et<!-- EPO <DP n="70"> --> l'information de région de changement pour générer une signature vidéo.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Dispositif d'extraction de signature vidéo selon la revendication 1, dans lequel<br/>
l'unité d'extraction de région de changement de direction axiale temporelle (100) calcule une différence de valeur de pixel intercliché entre le cliché pour lequel l'information de région de changement est calculée et un cliché précédent ou suivant, et calcule l'information de région de changement en se basant sur la différence de valeur de pixel intercliché.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Dispositif d'extraction de signature vidéo selon la revendication 1, dans lequel<br/>
l'unité d'extraction de région de changement de direction axiale temporelle (100) réalise un traitement d'estimation de mouvement entre le cliché pour lequel l'information de région de changement est calculée et un cliché précédent ou suivant, et calcule l'information de région de changement en se basant sur un degré de grandeur d'un mouvement estimé.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Dispositif d'extraction de signature vidéo selon l'une quelconque des revendications 1 à 3, dans lequel<br/>
l'unité d'extraction de caractéristiques de chaque cliché (130) extrait des caractéristiques depuis une région correspondant à chacune des dimensions d'une caractéristique en se basant sur la vidéo,<br/>
<!-- EPO <DP n="71"> -->le dispositif d'extraction de signature vidéo comprend en outre une unité d'extraction de l'information de poids concordant (610) qui analyse, par rapport à chacun des clichés de la vidéo, un changement d'image dans une direction temporelle dans la région correspondant à chacune des dimensions de la caractéristique, et produit en sortie une information de poids concordant qui est une information décrivant un poids correspondant à un degré du changement d'image, et<br/>
l'unité de multiplexage (120) multiplexe en outre également l'information de poids concordant, et génère une signature vidéo.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Dispositif d'extraction de signature vidéo selon la revendication 4, dans lequel<br/>
l'information de poids concordant décrit un poids qui prend une valeur plus grande pour une dimension ayant une plus grande quantité de changement d'image.</claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Dispositif d'extraction de signature vidéo selon la revendication 4 ou 5, dans lequel<br/>
l'unité d'extraction de l'information de poids concordant (610) calcule une différence de valeur de pixel intercliché entre le cliché pour lequel l'information de poids concordant est calculée et un cliché précédent ou suivant, et calcule l'information de poids concordant correspondant à chacune des dimensions de la caractéristique en se basant sur la différence de valeur de pixel intercliché.<!-- EPO <DP n="72"> --></claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Dispositif d'extraction de signature vidéo selon la revendication 4 ou 5, dans lequel<br/>
l'unité d'extraction de l'information de poids concordant (610) réalise un traitement d'estimation de mouvement entre le cliché pour lequel l'information de poids concordant est calculée et un cliché précédent ou suivant, et calcule l'information de poids concordant correspondant à chacune des dimensions de la caractéristique en se basant sur un degré de grandeur d'un mouvement estimé.</claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Dispositif d'extraction de signature vidéo selon l'une quelconque des revendications 1 à 7, dans lequel<br/>
l'unité d'extraction de région de changement de direction axiale temporelle (100) sélectionne un certain nombre de régions qui sont sélectionnées dans l'ordre, une région ayant une quantité la plus grande de changement d'image étant la première.</claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Dispositif d'extraction de signature vidéo selon l'une quelconque des revendications 1 à 7, dans lequel<br/>
l'unité d'extraction de région de changement de direction axiale temporelle (100) sélectionne une région dans laquelle un changement d'image n'est pas inférieur à un seuil.</claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Dispositif d'extraction de signature vidéo selon l'une quelconque des revendications 1 à 9, dans lequel<br/>
<!-- EPO <DP n="73"> -->la région désignée par l'information de région de changement est un bloc formé en divisant le cliché.</claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Dispositif d'extraction de signature vidéo selon l'une quelconque des revendications 1 à 10, dans lequel<br/>
chacune des dimensions de la caractéristique visuelle de chaque cliché est une valeur correspondant à une différence entre des caractéristiques de deux régions quelconques dans différentes formes au sein du cliché, les régions étant établies au préalable par rapport à la dimension.</claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Dispositif de concordance de signature vidéo, comprenant :
<claim-text>une première unité de démultiplexage (200) qui, à partir d'une première signature vidéo extraite depuis une vidéo par un dispositif d'extraction de signature vidéo selon la revendication 1, sépare une première caractéristique visuelle de chaque cliché, une première information de région de changement et une première caractéristique visuelle de chaque région ;</claim-text>
<claim-text>une seconde unité de démultiplexage (210) qui, à partir d'une seconde signature vidéo extraite depuis une autre vidéo par un dispositif d'extraction de signature vidéo selon la revendication 1, sépare une seconde caractéristique visuelle de chaque cliché, une seconde information de région de changement et une seconde caractéristique visuelle de chaque région ;</claim-text>
<claim-text>une unité de concordance de cliché (220) qui compare la première caractéristique visuelle de chaque<!-- EPO <DP n="74"> --> cliché avec la seconde caractéristique visuelle de chaque cliché, et génère un résultat de concordance de cliché, et lorsqu'il est déterminé à partir du résultat de concordance de cliché qu'un segment vidéo correspond à une pluralité de segments candidats concordants d'une autre vidéo, produit en sortie une information d'exécution de concordance de région ;</claim-text>
<claim-text>une unité de concordance de région (230) qui, lorsque l'information d'exécution de concordance de région est produite en sortie, compare la première information de région de changement avec la seconde information de région de changement, et si une information désignant des régions qui sont situées à la même position sur un écran est incluse, compare des caractéristiques visuelles de chaque région correspondant aux régions de la première caractéristique visuelle de chaque région et de la seconde caractéristique visuelle de chaque région, et génère un résultat de concordance de région pour chaque région où la région désignée par la première information de région de changement est la même que la région désignée par la seconde information de région de changement ; et</claim-text>
<claim-text>une unité de détermination de résultat de concordance (240) qui identifie des segments vidéo identiques à partir du résultat de concordance de cliché et, si l'information d'exécution de concordance de région est produite en sortie, à partir du résultat de concordance de région.</claim-text><!-- EPO <DP n="75"> --></claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Dispositif de concordance de signature vidéo selon la revendication 12, dans lequel<br/>
la première signature vidéo comprend en outre une première information de poids concordant qui est une information de poids concordant pour chacune des dimensions de la première caractéristique visuelle de chaque cliché et décrit un poids correspondant à un changement d'image dans une région au sein du cliché correspondant à la dimension,<br/>
la seconde signature vidéo comprend en outre une seconde information de poids concordant qui est une information de poids concordant pour chacune des dimensions de la seconde caractéristique visuelle de chaque cliché et décrit un poids correspondant à un changement d'image dans une région au sein du cliché correspondant à la dimension,<br/>
la première unité de démultiplexage (200) sépare en outre la première information de poids concordant,<br/>
la seconde unité de démultiplexage (210) sépare en outre la seconde information de poids concordant,<br/>
le dispositif de concordance de signature vidéo comprend en outre une unité de calcul de coefficient de pondération (730) qui calcule un coefficient de pondération pour chacune des dimensions de la caractéristique, à partir de la première information de poids concordant et de la seconde information de poids concordant, et<br/>
l'unité de concordance de cliché (220) réalise une pondération pour chacune des dimensions en utilisant le coefficient de pondération, et compare la première<!-- EPO <DP n="76"> --> caractéristique visuelle de chaque cliché avec la seconde caractéristique visuelle de chaque cliché.</claim-text></claim><claim id="c-fr-01-0014" num="0014"><claim-text>Procédé d'extraction de signature vidéo, comprenant :
<claim-text>l'extraction depuis une vidéo d'entrée d'un vecteur d'une caractéristique visuelle de chaque cliché en tant que caractéristique visuelle de chaque cliché, le cliché étant une trame ou un champ vidéo ;</claim-text>
<claim-text>le calcul d'un changement d'image dans une direction temporelle par rapport à des régions prédéterminées dans un cliché de la vidéo, la sélection d'une région ayant une grande quantité de changement d'image dans une direction temporelle parmi les régions prédéterminées du cliché en sélectionnant des régions dans lesquelles la quantité de changement d'image est égale ou supérieure à un seuil ou en sélectionnant un nombre prédéterminé de régions dans l'ordre décroissant de la quantité de changement d'image, et la génération d'une information de région de changement désignant la région sélectionnée ;</claim-text>
<claim-text>l'extraction depuis la vidéo d'une caractéristique visuelle de la région désignée par l'information de région de changement en tant que caractéristique visuelle de chaque région ; et</claim-text>
<claim-text>le multiplexage de la caractéristique visuelle de chaque cliché, la caractéristique visuelle de chaque région, et l'information de région de changement pour générer une signature vidéo.</claim-text><!-- EPO <DP n="77"> --></claim-text></claim><claim id="c-fr-01-0015" num="0015"><claim-text>Procédé d'extraction de signature vidéo selon la revendication 14, dans lequel<br/>
la génération de l'information de région de changement comprend le calcul d'une différence de valeur de pixel intercliché entre le cliché pour lequel l'information de région de changement est calculée et un cliché précédent ou suivant, et le calcul de l'information de région de changement en se basant sur la différence de valeur de pixel intercliché.</claim-text></claim><claim id="c-fr-01-0016" num="0016"><claim-text>Procédé d'extraction de signature vidéo selon la revendication 14, dans lequel<br/>
la génération de l'information de région de changement comprend la réalisation d'un traitement d'estimation de mouvement entre le cliché pour lequel l'information de région de changement est calculée et un cliché précédent ou suivant, et le calcul de l'information de région de changement en se basant sur un degré de grandeur d'un mouvement estimé.</claim-text></claim><claim id="c-fr-01-0017" num="0017"><claim-text>Procédé d'extraction de signature vidéo selon l'une quelconque des revendications 14 à 16, dans lequel<br/>
l'extraction de la caractéristique visuelle de chaque cliché comprend l'extraction des caractéristiques depuis une région correspondant à chacune des dimensions d'une caractéristique en se basant sur la vidéo,<br/>
le procédé d'extraction de signature vidéo comprend en outre l'analyse, par rapport à chacun des clichés de la vidéo, d'un changement d'image dans une<!-- EPO <DP n="78"> --> direction temporelle dans la région correspondant à chacune des dimensions de la caractéristique, et le calcul d'une information de poids concordant qui est une information décrivant un poids correspondant à un degré du changement d'image, et<br/>
la génération de la signature vidéo comprend également le multiplexage de l'information de poids concordant, et génère une signature vidéo.</claim-text></claim><claim id="c-fr-01-0018" num="0018"><claim-text>Procédé d'extraction de signature vidéo selon la revendication 17, dans lequel<br/>
l'information de poids concordant décrit un poids qui prend une valeur plus grande pour une dimension ayant un changement d'image plus grand.</claim-text></claim><claim id="c-fr-01-0019" num="0019"><claim-text>Procédé d'extraction de signature vidéo selon la revendication 17 ou 18, dans lequel<br/>
le calcul de l'information de poids concordant comprend le calcul d'une différence de valeur de pixel intercliché entre le cliché pour lequel l'information de poids concordant est calculée et un cliché précédent ou suivant, et le calcul de l'information de poids concordant correspondant à chacune des dimensions de la caractéristique en se basant sur la différence de valeur de pixel intercliché.</claim-text></claim><claim id="c-fr-01-0020" num="0020"><claim-text>Procédé d'extraction de signature vidéo selon la revendication 17 ou 18, dans lequel<br/>
le calcul de l'information de poids concordant comprend la réalisation d'un traitement d'estimation de mouvement entre le cliché pour lequel l'information de<br/>
<!-- EPO <DP n="79"> -->poids concordant est calculée et un cliché précédent ou suivant, et le calcul de l'information de poids concordant correspondant à chacune des dimensions de la caractéristique en se basant sur un degré de grandeur d'un mouvement estimé.</claim-text></claim><claim id="c-fr-01-0021" num="0021"><claim-text>Procédé d'extraction de signature vidéo selon l'une quelconque des revendications 14 à 20, dans lequel<br/>
la génération de l'information de région de changement comprend la sélection d'un certain nombre de régions qui sont sélectionnées dans l'ordre, une région ayant une quantité la plus grande de changement d'image étant la première.</claim-text></claim><claim id="c-fr-01-0022" num="0022"><claim-text>Procédé d'extraction de signature vidéo selon l'une quelconque des revendications 14 à 20, dans lequel<br/>
la génération de l'information de région de changement comprend la sélection d'une région dans laquelle un changement d'image n'est pas inférieur à un seuil.</claim-text></claim><claim id="c-fr-01-0023" num="0023"><claim-text>Procédé d'extraction de signature vidéo selon l'une quelconque des revendications 14 à 22, dans lequel<br/>
la région désignée par l'information de région de changement est un bloc formé en divisant le cliché.<!-- EPO <DP n="80"> --></claim-text></claim><claim id="c-fr-01-0024" num="0024"><claim-text>Procédé d'extraction de signature vidéo selon l'une quelconque des revendications 14 à 23, dans lequel<br/>
chacune des dimensions de la caractéristique visuelle de chaque cliché est une valeur correspondant à une différence entre des caractéristiques de deux régions quelconques dans différentes formes au sein du cliché, les régions étant établies au préalable par rapport à la dimension.</claim-text></claim><claim id="c-fr-01-0025" num="0025"><claim-text>Procédé de concordance de signature vidéo, comprenant :
<claim-text>à partir d'une première signature vidéo extraite depuis une vidéo par un dispositif d'extraction de signature vidéo selon la revendication 1, la séparation d'une première caractéristique visuelle de chaque cliché, d'une première information de région de changement et d'une première caractéristique visuelle de chaque région ;</claim-text>
<claim-text>à partir d'une seconde signature vidéo extraite depuis une autre vidéo par un dispositif d'extraction de signature vidéo selon la revendication 1, la séparation d'une seconde caractéristique visuelle de chaque cliché, d'une seconde information de région de changement et d'une seconde caractéristique visuelle de chaque région ;</claim-text>
<claim-text>la comparaison de la première caractéristique visuelle de chaque cliché avec la seconde caractéristique visuelle de chaque cliché, la génération d'un résultat de concordance de cliché, et lorsqu'il est déterminé à partir du résultat de<!-- EPO <DP n="81"> --> concordance de cliché qu'un segment vidéo correspond à une pluralité de segments candidats concordants d'une autre vidéo, la production en sortie d'une information d'exécution de concordance de région ;</claim-text>
<claim-text>lorsque l'information d'exécution de concordance de région est produite en sortie, la comparaison de la première information de région de changement avec la seconde information de région de changement, et si une information désignant des régions qui sont situées à la même position sur un écran est incluse, la comparaison des caractéristiques visuelles de chaque région correspondant aux régions de la première caractéristique visuelle de chaque région et de la seconde caractéristique visuelle de chaque région, et la génération d'un résultat de concordance de région pour chaque région où la région désignée par la première information de région de changement est la même que la région désignée par la seconde information de région de changement ; et</claim-text>
<claim-text>l'identification des segments vidéo identiques à partir du résultat de concordance de cliché et, si l'information d'exécution de concordance de région est produite en sortie, à partir du résultat de concordance de région.</claim-text></claim-text></claim><claim id="c-fr-01-0026" num="0026"><claim-text>Procédé de concordance de signature vidéo selon la revendication 25, dans lequel<br/>
la première signature vidéo comprend en outre une première information de poids concordant qui est une information de poids concordant pour chacune des dimensions de la première caractéristique visuelle de<!-- EPO <DP n="82"> --> chaque cliché et décrit un poids correspondant à un changement d'image dans une région au sein du cliché correspondant à la dimension,<br/>
la seconde signature vidéo comprend en outre une seconde information de poids concordant qui est une information de poids concordant pour chacune des dimensions de la seconde caractéristique visuelle de chaque cliché et décrit un poids correspondant à un changement d'image dans une région au sein du cliché correspondant à la dimension,<br/>
la séparation à partir de la première signature vidéo comprend la séparation de la première information de poids concordant,<br/>
la séparation à partir de la seconde signature vidéo comprend la séparation de la seconde information de poids concordant,<br/>
le procédé de concordance de signature vidéo comprend en outre le calcul d'un coefficient de pondération pour chacune des dimensions de la caractéristique, à partir de la première information de poids concordant et de la seconde information de poids concordant, et<br/>
la génération du résultat de concordance de cliché comprend la réalisation d'une pondération pour chacune des dimensions en utilisant le coefficient de pondération, et la comparaison de la première caractéristique visuelle de chaque cliché avec la seconde caractéristique visuelle de chaque cliché.</claim-text></claim><claim id="c-fr-01-0027" num="0027"><claim-text>Programme amenant un ordinateur à fonctionner comme :<!-- EPO <DP n="83"> -->
<claim-text>une unité d'extraction de caractéristiques de chaque cliché qui extrait depuis une vidéo d'entrée un vecteur d'une caractéristique visuelle de chaque cliché en tant que caractéristique visuelle de chaque cliché, le cliché étant une trame ou un champ vidéo ;</claim-text>
<claim-text>une unité d'extraction de région de changement de direction axiale temporelle qui calcule un changement d'image dans une direction temporelle par rapport à des régions prédéterminées dans un cliché de la vidéo, sélectionne une région ayant une grande quantité de changement d'image dans une direction temporelle parmi les régions prédéterminées du cliché en sélectionnant des régions dans lesquelles la quantité de changement d'image est égale ou supérieure à un seuil ou en sélectionnant un nombre prédéterminé de régions dans l'ordre décroissant de la quantité de changement d'image, et génère une information de région de changement désignant la région sélectionnée ;</claim-text>
<claim-text>une unité d'extraction de caractéristiques de chaque région qui extrait depuis la vidéo une caractéristique visuelle de la région désignée par l'information de région de changement en tant que caractéristique visuelle de chaque région ; et</claim-text>
<claim-text>une unité de multiplexage qui multiplexe la caractéristique visuelle de chaque cliché, la caractéristique visuelle de chaque région, et l'information de région de changement pour générer une signature vidéo.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW16672257" load-source="patent-office"><!-- EPO <DP n="84"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="154" he="222" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="85"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="111" he="217" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="86"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="165" he="190" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="87"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="133" he="132" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="88"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="145" he="132" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="89"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="110" he="215" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="90"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="165" he="120" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="91"> --><figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="139" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="92"> --><figure id="f0009" num="9"><img id="if0009" file="imgf0009.tif" wi="107" he="218" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="93"> --><figure id="f0010" num="10"><img id="if0010" file="imgf0010.tif" wi="130" he="130" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="94"> --><figure id="f0011" num="11"><img id="if0011" file="imgf0011.tif" wi="165" he="229" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="95"> --><figure id="f0012" num="12"><img id="if0012" file="imgf0012.tif" wi="157" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="96"> --><figure id="f0013" num="13"><img id="if0013" file="imgf0013.tif" wi="160" he="167" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="97"> --><figure id="f0014" num="14"><img id="if0014" file="imgf0014.tif" wi="137" he="204" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
