<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681717-A2" country="EP" doc-number="2681717" kind="A2" date="20140108" family-id="46798729" file-reference-id="252638" date-produced="20180822" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586089" ucid="EP-2681717-A2"><document-id><country>EP</country><doc-number>2681717</doc-number><kind>A2</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12755538-A" is-representative="NO"><document-id mxw-id="PAPP154848281" load-source="docdb" format="epo"><country>EP</country><doc-number>12755538</doc-number><kind>A</kind><date>20120304</date><lang>EN</lang></document-id><document-id mxw-id="PAPP168045780" load-source="docdb" format="original"><country>EP</country><doc-number>12755538.1</doc-number><date>20120304</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140547867" ucid="US-201113076445-A" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201113076445</doc-number><kind>A</kind><date>20110331</date></document-id></priority-claim><priority-claim mxw-id="PPC140549871" ucid="US-201161449373-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201161449373</doc-number><kind>P</kind><date>20110304</date></document-id></priority-claim><priority-claim mxw-id="PPC140546994" ucid="US-2012027643-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2012027643</doc-number><kind>W</kind><date>20120304</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL2114898609" load-source="docdb">G06T   7/20        20060101AFI20140916BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL2114899074" load-source="docdb">G06K   9/46        20060101ALI20140916BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1989634264" load-source="docdb" scheme="CPC">G06T2207/30241     20130101 LA20130712BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989642272" load-source="docdb" scheme="CPC">G06T   7/20        20130101 FI20130712BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989650084" load-source="docdb" scheme="CPC">G06T2207/30201     20130101 LA20130712BHEP        </classification-cpc><classification-cpc mxw-id="PCL2112538746" load-source="docdb" scheme="CPC">G06T2207/10016     20130101 LA20140912BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132362900" lang="DE" load-source="patent-office">AGGREGIERTE GESICHTSVERFOLGUNG IN VIDEOINHALTEN</invention-title><invention-title mxw-id="PT132362901" lang="EN" load-source="patent-office">AGGREGATED FACIAL TRACKING IN VIDEO</invention-title><invention-title mxw-id="PT132362902" lang="FR" load-source="patent-office">DÉTECTION FACIALE AGRÉGÉE DANS UNE VIDÉO</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR919537992" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>MICROSOFT CORP</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR919526142" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>MICROSOFT CORPORATION</last-name></addressbook></applicant><applicant mxw-id="PPAR919008646" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Microsoft Corporation</last-name><iid>101214593</iid><address><street>One Microsoft Way</street><city>Redmond, Washington 98052-6399</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919544692" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>LEICHTER IDO</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919542008" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>LEICHTER, IDO</last-name></addressbook></inventor><inventor mxw-id="PPAR919008071" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>LEICHTER, IDO</last-name><address><street>c/o Microsoft Corporation LCA - International Patents One Microsoft Way</street><city>Redmond, Washington 98052-6399</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919533392" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>KRUPKA EYAL</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919515169" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>KRUPKA, EYAL</last-name></addressbook></inventor><inventor mxw-id="PPAR919018038" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>KRUPKA, EYAL</last-name><address><street>c/o Microsoft Corporation LCA - International Patents One Microsoft Way</street><city>Redmond, Washington 98052-6399</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919512781" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>ABRAMOVSKI IGOR</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919508120" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>ABRAMOVSKI, IGOR</last-name></addressbook></inventor><inventor mxw-id="PPAR919016784" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>ABRAMOVSKI, IGOR</last-name><address><street>c/o Microsoft Corporation LCA - International Patents One Microsoft Way</street><city>Redmond, Washington 98052-6399</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919526292" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>KVIATKOVSKY IGOR</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919520089" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>KVIATKOVSKY, IGOR</last-name></addressbook></inventor><inventor mxw-id="PPAR919014143" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>KVIATKOVSKY, IGOR</last-name><address><street>c/o Microsoft Corporation LCA - International Patents One Microsoft Way</street><city>Redmond, Washington 98052-6399</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919019392" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Fischer, Michael Maria</last-name><iid>101371614</iid><address><street>Olswang Germany LLP IP Rosental 4</street><city>80331 Munich</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2012027643-W"><document-id><country>US</country><doc-number>2012027643</doc-number><kind>W</kind><date>20120304</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012122069-A2"><document-id><country>WO</country><doc-number>2012122069</doc-number><kind>A2</kind><date>20120913</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549756153" load-source="docdb">AL</country><country mxw-id="DS549832828" load-source="docdb">AT</country><country mxw-id="DS549756154" load-source="docdb">BE</country><country mxw-id="DS549763874" load-source="docdb">BG</country><country mxw-id="DS549913228" load-source="docdb">CH</country><country mxw-id="DS549756155" load-source="docdb">CY</country><country mxw-id="DS549831074" load-source="docdb">CZ</country><country mxw-id="DS549756792" load-source="docdb">DE</country><country mxw-id="DS549756156" load-source="docdb">DK</country><country mxw-id="DS549756169" load-source="docdb">EE</country><country mxw-id="DS549834004" load-source="docdb">ES</country><country mxw-id="DS549763875" load-source="docdb">FI</country><country mxw-id="DS549763876" load-source="docdb">FR</country><country mxw-id="DS549756797" load-source="docdb">GB</country><country mxw-id="DS549756170" load-source="docdb">GR</country><country mxw-id="DS549756798" load-source="docdb">HR</country><country mxw-id="DS549831075" load-source="docdb">HU</country><country mxw-id="DS549913229" load-source="docdb">IE</country><country mxw-id="DS549756171" load-source="docdb">IS</country><country mxw-id="DS549763889" load-source="docdb">IT</country><country mxw-id="DS549756172" load-source="docdb">LI</country><country mxw-id="DS549756799" load-source="docdb">LT</country><country mxw-id="DS549832829" load-source="docdb">LU</country><country mxw-id="DS549763890" load-source="docdb">LV</country><country mxw-id="DS549763891" load-source="docdb">MC</country><country mxw-id="DS549832834" load-source="docdb">MK</country><country mxw-id="DS549832835" load-source="docdb">MT</country><country mxw-id="DS549834005" load-source="docdb">NL</country><country mxw-id="DS549844705" load-source="docdb">NO</country><country mxw-id="DS549834010" load-source="docdb">PL</country><country mxw-id="DS549763892" load-source="docdb">PT</country><country mxw-id="DS549831076" load-source="docdb">RO</country><country mxw-id="DS549763909" load-source="docdb">RS</country><country mxw-id="DS549834011" load-source="docdb">SE</country><country mxw-id="DS549913230" load-source="docdb">SI</country><country mxw-id="DS549844718" load-source="docdb">SK</country><country mxw-id="DS549844719" load-source="docdb">SM</country><country mxw-id="DS549832836" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA100065956" ref-ucid="WO-2012122069-A2" lang="EN" load-source="patent-office"><p num="0000">A facial detecting system may analyze a video by traversing the video forwards and backwards to create tracks of a person within the video. After separating the video into shots, the frames of each shot may be analyzed using a face detector algorithm to produce some analyzed information for each frame. A facial track may be generated by grouping the faces detected and by traversing the sequence of frames forwards and backwards. Facial tracks may be joined together within a shot to generate a single track for a person's face within the shot, even when the tracks are discontinuous.</p></abstract><abstract mxw-id="PA100335064" ref-ucid="WO-2012122069-A2" lang="EN" source="national office" load-source="docdb"><p>A facial detecting system may analyze a video by traversing the video forwards and backwards to create tracks of a person within the video. After separating the video into shots, the frames of each shot may be analyzed using a face detector algorithm to produce some analyzed information for each frame. A facial track may be generated by grouping the faces detected and by traversing the sequence of frames forwards and backwards. Facial tracks may be joined together within a shot to generate a single track for a person's face within the shot, even when the tracks are discontinuous.</p></abstract><abstract mxw-id="PA100065957" ref-ucid="WO-2012122069-A2" lang="FR" load-source="patent-office"><p num="0000">Un système de détection faciale peut analyser une vidéo en parcourant la vidéo d'avant en arrière pour créer des pistes d'une personne à l'intérieur de la vidéo. Après avoir séparé la vidéo en vues, les trames de chaque vue peuvent être analysées à l'aide d'un algorithme de détection faciale afin de produire des informations analysées pour chaque trame. Une piste faciale peut être générée en groupant les visages détectés et en parcourant la séquence de trames d'avant en arrière. Des pistes faciales peuvent être assemblées à l'intérieur d'une vue afin de générer une seule piste pour le visage d'une personne à l'intérieur de la vue, même lorsque les pistes sont discontinues.</p></abstract><abstract mxw-id="PA100335065" ref-ucid="WO-2012122069-A2" lang="FR" source="national office" load-source="docdb"><p>Un système de détection faciale peut analyser une vidéo en parcourant la vidéo d'avant en arrière pour créer des pistes d'une personne à l'intérieur de la vidéo. Après avoir séparé la vidéo en vues, les trames de chaque vue peuvent être analysées à l'aide d'un algorithme de détection faciale afin de produire des informations analysées pour chaque trame. Une piste faciale peut être générée en groupant les visages détectés et en parcourant la séquence de trames d'avant en arrière. Des pistes faciales peuvent être assemblées à l'intérieur d'une vue afin de générer une seule piste pour le visage d'une personne à l'intérieur de la vue, même lorsque les pistes sont discontinues.</p></abstract><description mxw-id="PDES51372634" ref-ucid="WO-2012122069-A2" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> Aggregated Facial Tracking in Video </p><p id="p0002" num="0002"> Background </p><p id="p0003" num="0003"> [0001] Face tracking in video can be difficult. Many face detector algorithms may detects a face when a person is facing a camera, but may be less accurate when the person is viewed in profile. As the person turns away from the camera, the face detector algorithms may fail to detect a face at all. </p><p id="p0004" num="0004"> Summary </p><p id="p0005" num="0005"> [0002] A facial detecting system may analyze a video by traversing the video forwards and backwards to create tracks of a person's face within the video. After separating the video into shots, the frames of each shot may be analyzed using a face detector algorithm to produce some analyzed information for each frame. A facial track may be generated by grouping the faces detected and by traversing the sequence of frames forwards and backwards. Facial tracks may be joined together within a shot to generate a single track for a person's face within the shot, even when the tracks are discontinuous. </p><p id="p0006" num="0006"> [0003] This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This </p><p id="p0007" num="0007">Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. </p><p id="p0008" num="0008"> Brief Description of the Drawings </p><p id="p0009" num="0009"> [0004] FIGURE 1 is a diagram of an embodiment showing a network environment with a device that analyzes video. </p><p id="p0010" num="0010"> [0005] FIGURE 2 is a flowchart of an embodiment showing a method for analyzing video. </p><p id="p0011" num="0011"> [0006] FIGURE 3 is a flowchart of an embodiment showing a method for determining shots in a video. </p><p id="p0012" num="0012"> [0007] FIGURE 4 is a flowchart of an embodiment showing a method for facial tracking in video. </p><p id="p0013" num="0013"> [0008] FIGURE 5 is a flowchart of an embodiment showing a method for linking analysis of existing facial tracks. </p><p id="p0014" num="0014"> [0009] FIGURE 6 is an example diagram of an embodiment showing a sequence of video frames with a resulting facial track. 
<!-- EPO <DP n="4"/>-->
 Detailed Description </p><p id="p0015" num="0015"> [0010] A facial detecting system may detect faces within a video by analyzing both forward and backward through a video's sequence of frames. The faces may be initially detected by a face detection algorithm on a frame by frame basis, and then processed using a facial track analyzer to create a sequence of frames containing the same face. </p><p id="p0016" num="0016"> [0011] The facial track analyzer may operate by traversing the sequence of frames in a forward and/or backward manner to detecting matching faces. Once a set of sequences of faces are detected, a facial track may be generated by connecting the face objects from successive frames in the video. In many cases, multiple facial tracks may be generated in a video shot for a person's face because some frames may not have the face detected. In such cases, the separate facial tracks may be joined together into a single facial track by comparing the tracks in various manners. </p><p id="p0017" num="0017"> [0012] The facial tracks may be generated by comparing merely the position and size of facial objects in some embodiments. In such embodiments, the trajectory of a facial object may be determined from two, three, or more frames and a new frame may be analyzed to determine if the new frame contains a face object that matches the trajectory. </p><p id="p0018" num="0018"> [0013] In some embodiments, the facial tracks may be generated by comparing information derived from the image, such as color histograms, facial structure, or other data. In such embodiments, facial objects may be compared and found to be the same when the similarities between the facial objects are found to be within a predetermined threshold. </p><p id="p0019" num="0019"> [0014] In many embodiments, facial detection may be performed on a frame by frame basis, where each frame may be analyzed using a face detection algorithm. In such embodiments, the frames may be analyzed as static, independent images. Such algorithms may not be very accurate and may incorrectly detect objects that are not faces or may not detect faces that were present. By analyzing the frame information by traversing both forward and backward through the sequence of frames to create a facial track, some of the noise or unreliability of the static face detection algorithms may be eliminated. </p><p id="p0020" num="0020"> [0015] Throughout this specification, like reference numbers signify the same elements throughout the description of the figures. </p><p id="p0021" num="0021"> [0016] When elements are referred to as being "connected" or "coupled," the elements can be directly connected or coupled together or one or more intervening 
<!-- EPO <DP n="5"/>-->
 elements may also be present. In contrast, when elements are referred to as being "directly connected" or "directly coupled," there are no intervening elements present. </p><p id="p0022" num="0022"> [0017] The subject matter may be embodied as devices, systems, methods, and/or computer program products. Accordingly, some or all of the subject matter may be embodied in hardware and/or in software (including firmware, resident software, microcode, state machines, gate arrays, etc.) Furthermore, the subject matter may take the form of a computer program product on a computer-usable or computer-readable storage medium having computer-usable or computer-readable program code embodied in the medium for use by or in connection with an instruction execution system. In the context of this document, a computer-usable or computer-readable medium may be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. </p><p id="p0023" num="0023"> [0018] The computer-usable or computer-readable medium may be, for example but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. </p><p id="p0024" num="0024"> [0019] Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, </p><p id="p0025" num="0025">EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can accessed by an instruction execution system. Note that the computer-usable or computer-readable medium could be paper or another suitable medium upon which the program is printed, as the program can be electronically captured, via, for instance, optical scanning of the paper or other medium, then compiled, interpreted, of otherwise processed in a suitable manner, if necessary, and then stored in a computer memory. </p><p id="p0026" num="0026"> [0020] Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The 
<!-- EPO <DP n="6"/>-->
 term "modulated data signal" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, F, infrared and other wireless media. Combinations of the any of the above should also be included within the scope of computer readable media. </p><p id="p0027" num="0027"> [0021] When the subject matter is embodied in the general context of computer- executable instructions, the embodiment may comprise program modules, executed by one or more systems, computers, or other devices. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Typically, the functionality of the program modules may be combined or distributed as desired in various embodiments. </p><p id="p0028" num="0028"> [0022] Figure 1 is a diagram of an embodiment 100, showing a system for video analysis. Embodiment 100 is a simplified example of a device that may receive video, break the video into shots, and analyze each frame of each shot to detect a track for a face object across the video frames. </p><p id="p0029" num="0029"> [0023] The diagram of Figure 1 illustrates functional components of a system. In some cases, the component may be a hardware component, a software component, or a combination of hardware and software. Some of the components may be application level software, while other components may be operating system level components. In some cases, the connection of one component to another may be a close connection where two or more components are operating on a single hardware platform. In other cases, the connections may be made over network connections spanning long distances. Each embodiment may use different hardware, software, and interconnection architectures to achieve the described functions. </p><p id="p0030" num="0030"> [0024] A system for facial tracking creates a facial track that may span multiple frames of a video shot. The system may use the results of a frame-by- frame facial detection algorithm, then create facial tracks that span multiple frames that may minimize missing or incorrect facial detections. The system may analyze a video shot both forwards and backwards to connect faces in a sequence of frames. </p><p id="p0031" num="0031"> [0025] By examining the sequence of frames to connect faces, frames that may have a missing or unreliably detected face may be included into a facial track. Further, 
<!-- EPO <DP n="7"/>-->
 misinterpreted or incorrect facial detections may be ignored when those detections do not find nearby frames that also contain a matching face. </p><p id="p0032" num="0032"> [0026] The system may have the effect of smoothing errors in a frame-by-frame facial detection system. Many facial detection algorithms may operate well when a person faces the camera directly. As the person turns their head to the side, a typical facial detection system may lose confidence that the object being analyzed is a face as the full facial features may be missing. For example, a person's picture from the profile may contain a single eye, a nose profile, and half of a mouth, which may be not be detected as a face with high reliability. A full, face-on view may contain two eyes, a nose, and a mouth, which may be much more reliably detected. </p><p id="p0033" num="0033"> [0027] The analysis of faces in video may take advantage of the fact that video frames before and after a given frame may contain additional information that may assist in determining if indeed a face is present, as well as fill in when a face may not be properly detected. </p><p id="p0034" num="0034"> [0028] A video analysis system may first break up a video into various shots.</p><p id="p0035" num="0035">Each shot may be a sequence of frames that are similar and may contain the same faces. In some cases, a shot boundary may be determined when a camera operator begins and ends a specific video segment, creating individual shots. In other cases, the scene may change sufficiently that a new shot may be created even when the camera is still recording. Such an event may occur when a camera operator turns quickly and changes the view. </p><p id="p0036" num="0036"> [0029] The shots may be analyzed to find a facial track within the shots. In many embodiments, a facial track may be determined by assuming that the position and size of a face may be consistent from one frame to another. Such an algorithm may not operate as intended across shot boundaries. Consequently, many video parsers may err on the side of creating too many shots from a video than too few. Too many shots may stem from a condition where a video parser is overly sensitive to a change in shots and may detect a shot boundary when one may not actually exist. Too few shots may occur when a video parser may be less sensitized and may not detect an actual shot boundary. </p><p id="p0037" num="0037"> [0030] A face detector may analyze each frame of a video shot to detect faces within the frame. Many embodiments may operate a face detector by analyzing the frames separately and independently from other frames. The face detector may use any type of face detection mechanism to detect faces within the still image of a frame. 
<!-- EPO <DP n="8"/>-->
 [0031] In many cases, a face detector may detect one or more faces and may provide a position and size for the face objects. Some embodiments may include a reliability factor for the detection, which may indicate a confidence that the algorithm may have in the detection. Some embodiments may include various characteristics about the face, such as facial structure analysis, color histograms, or other information derived from the image itself. </p><p id="p0038" num="0038"> [0032] A facial track analyzer may attempt to connect facial objects from one frame to another by analyzing the sequence of frames in both forward and backward directions. In some embodiments, the facial track analyzer may attempt to match the facial objects in nearby frames by comparing just the position and size of the facial objects in nearby frames. Other embodiments may compare additional factors, such as factors derived from image analysis to match facial objects. </p><p id="p0039" num="0039"> [0033] In some embodiments, a first pass for matching facial objects may be made using position and size of the facial objects. A second pass may be performed using image analysis factors to verify or supplement the initial findings made using the position and size analysis. </p><p id="p0040" num="0040"> [0034] The facial track analyzer may create a first set of facial tracks, and then may attempt to join facial tracks within a shot. The process of joining facial tracks may join tracks that are discontinuous but may show the same face. The joining process may select non-overlapping facial tracks and join them using either or both of a position and size analysis or image factor analysis. </p><p id="p0041" num="0041"> [0035] In some embodiments, the facial tracks may be compared to other facial tracks in other shots. In such embodiments, the facial tracks may be compared using image analysis, such as facial structure, color histograms, or other types of analysis to determine that two facial tracks are for the same person. </p><p id="p0042" num="0042"> [0036] The system of embodiment 100 is illustrated as being contained in a single device 102. In many embodiments, various software components may be implemented on many different devices. In some cases, a single software component may be implemented on a cluster of computers. Some embodiments may operate using cloud computing technologies for one or more of the components. </p><p id="p0043" num="0043"> [0037] The system of embodiment 100 may be accessed by various client devices 132. The client devices 132 may access the system through a web browser or other application. In one such embodiment, the device 102 may be implemented as a web 
<!-- EPO <DP n="9"/>-->
 service that may process video in a cloud based system. Such embodiments may operate by receiving video images from various clients, processing the video images in a large datacenter, and returning the analyzed results to the clients. </p><p id="p0044" num="0044"> [0038] In another embodiment, the operations of device 102 may be performed by a personal computer, server computer, or other computing platform within the control of a user. Such an embodiment may be implemented with a software package that may be distributed and installed on a user's computer. </p><p id="p0045" num="0045"> [0039] In still another embodiment, the operations of device 102 may be implemented in a video camera or other specialized device. When implemented in a video camera, the camera may shoot a video segment, and then perform an analysis on the video segment after the fact, for example. </p><p id="p0046" num="0046"> [0040] The device 102 may have a hardware platform 104 and software components 106. The client device 102 may represent any type of device that may communicate with a video source, such as various client devices 132, social network sites 136, or other sources. In some cases, the client device 102 may have a video camera or other capture device that may generate video within the client device 102. </p><p id="p0047" num="0047"> [0041] The hardware components 104 may represent a typical architecture of a computing device, such as a desktop or server computer. In some embodiments, the client device 102 may be a personal computer, game console, network appliance, interactive kiosk, or other device. The client device 102 may also be a portable device, such as a laptop computer, netbook computer, personal digital assistant, mobile telephone, or other mobile device. </p><p id="p0048" num="0048"> [0042] The hardware components 104 may include a processor 108, random access memory 110, and nonvolatile storage 112. The processor 108 may be a single microprocessor, multi-core processor, or a group of processors. The random access memory 110 may store executable code as well as data that may be immediately accessible to the processor 108, while the nonvolatile storage 112 may store executable code and data in a persistent state. </p><p id="p0049" num="0049"> [0043] The hardware components 104 may also include one or more user interface devices 114 and network interfaces 116. The user interface devices 114 may include monitors, displays, keyboards, pointing devices, and any other type of user interface device. In some embodiments, the user interface components may include a camera or 
<!-- EPO <DP n="10"/>-->
 other video capture device. The network interfaces 116 may include hardwired and wireless interfaces through which the device 102 may communicate with other devices. </p><p id="p0050" num="0050"> [0044] The software components 106 may include an operating system 118 on which various applications may execute. </p><p id="p0051" num="0051"> [0045] A video analysis system 120 may process video to detect facial tracks. A video parser 122 may analyze a video image to separate the video into shots. Each shot may be a sequence of frames that are related in space and time. The shot may contain the same scene and, when people are present, the people in scene may move smoothly and continuously. </p><p id="p0052" num="0052"> [0046] A face detector 124 may analyze each frame of a shot to attempt to find faces in the frames. The face detector 124 may analyze each frame as a static image, and may or may not use adjacent frames to detect faces. The face detector 124 may return a set of information for each face. The set of information may vary from one embodiment to another. The set of information may include a position and size for each face, which may be a set of coordinates for the face and a rectangular or other shaped size for the face object. The set of coordinates may be a point in the center or a corner of the face object. In some embodiments, the size may be expressed in a height and width of a rectangle, a radius of a circle, a pair of radii for an ellipse, or some other indication of size. </p><p id="p0053" num="0053"> [0047] In some embodiments, the set of information may include additional information that may be derived from the image itself. Such information may include a color histogram of the facial object, facial structural features, or some other information. Such information may be used to match facial objects by comparing similar image features. </p><p id="p0054" num="0054"> [0048] A facial track analyzer 126 may use the output from the face detector 124 to create sequences of frames that contain the same facial object. Some embodiments may compare just the position and size of faces within a shot to link together facial objects in successive frames. Other embodiments may use information derived from the image to find matching face objects in successive frames. </p><p id="p0055" num="0055"> [0049] In some embodiments, the facial track analyzer 124 may analyze the successive frames both forward and backward within the video sequence. The facial track analyzer 124 may compare a facial object in one frame to groups or clusters of frames in either direction from the given frame. In such embodiments, a clustering analysis or clustering algorithm may be used to identify matches. 
<!-- EPO <DP n="11"/>-->
 [0050] Some embodiments may use an object tracking algorithm to track a facial object across multiple frames. Some object tracking algorithms may determine a possible trajectory for an object across the video frames to determine a track. The facial track analyzer 126 may analyze similar facial objects across multiple frames using various techniques, such as blob tracking, kernel based tracking, contour tracking, or other tracking mechanisms. </p><p id="p0056" num="0056"> [0051] The facial track analyzer 126 may use metadata about the facial objects with an object tracking algorithm. Because a person's face may change characteristics within the video, such as when the person turns their head from being straight towards the camera, to a profile shot, to facing away from the camera, a conventional object tracking mechanism may not be as effective as the facial track analyzer 126 that may use the metadata about the facial objects created by the face detector 124. </p><p id="p0057" num="0057"> [0052] The metadata may include a face object that may be detected from various facial orientations, which may be very different images. The facial track analyzer 126 may associate facial objects together and detect and verify those associations with various object tracking mechanisms. </p><p id="p0058" num="0058"> [0053] A post processor 128 may attempt to join non-overlapping facial tracks into longer facial tracks. The post processor 128 may use position and size analysis to determine if two facial tracks may be related. In some embodiments, the post processor 128 may use image analysis comparisons, such as facial structure comparisons or color histogram analyses, to determine a match. </p><p id="p0059" num="0059"> [0054] In some embodiments, the post processor 128 may attempt to match two facial tracks by finding the most reliably detected face object within a first facial track and compare that face object with a most reliably detected face object within a second facial track. The two reliable facial images may be the best facial representation of each facial track, and comparisons between those images may be more certain than the last image from one track and the first image of a second track. </p><p id="p0060" num="0060"> [0055] The video analysis system 120 may be connected to other devices over a network 130. The network 130 may be a personal area network, local area network, wide area network, the Internet, or any other network. </p><p id="p0061" num="0061"> [0056] Various client devices 132 may have video in various forms. The video databases 134 may be any type of repository that contains video that may be analyzed. The client devices 132 may be personal computers or other devices to which a user may 
<!-- EPO <DP n="12"/>-->
 have uploaded video from various video sources. The client devices 132 may be video cameras, cellular telephones, personal digital assistants, or other video capture devices. </p><p id="p0062" num="0062"> [0057] In some embodiments, various social network sites 136 may contain a video database 138 that users may upload videos to share. The social network sites 136 may be configured to transfer video to the video analysis system 120 to have the video analyzed and detect persons in the video. </p><p id="p0063" num="0063"> [0058] In many embodiments, the output of the video analysis system 120 may be used to attempt to identify actual persons in the video. The output may detect a facial track for a person, and an image matching system may attempt to associate an actual person's name or other information with the video's facial tracks. Such as system is not shown in the embodiment 100 and is merely one use scenario for the video analysis system 120. </p><p id="p0064" num="0064"> [0059] Figure 2 is a flowchart illustration of an embodiment 200 showing a method for analyzing video. Embodiment 200 is a simplified example of a method that may be performed by a video analysis system 120 to parse a video into shots, perform a frame by frame static analysis of the video shot, and use the output of the static facial analysis to create a facial track that spans multiple frames of the video. </p><p id="p0065" num="0065"> [0060] Other embodiments may use different sequencing, additional or fewer steps, and different nomenclature or terminology to accomplish similar functions. In some embodiments, various operations or set of operations may be performed in parallel with other operations, either in a synchronous or asynchronous manner. The steps selected here were chosen to illustrate some principles of operations in a simplified form. </p><p id="p0066" num="0066"> [0061] Embodiment 200 illustrates one method by which video may analyzed to create tracks of faces through the video. After a video is broken into shots, each shot may be analyzed on a frame by frame basis for static facial detection. The frame by frame analysis results may then be used to link multiple frames together to show the movement or progression of a single face through the video. </p><p id="p0067" num="0067"> [0062] In block 202, the video to analyze may be received. The video may be any type of video image that is made up of a series or sequence of individual frames. The video may be separated into discrete shots in block 204. Each shot may represent a single scene or set of related frames. An example of a process that may be performed in block</p><p id="p0068" num="0068">204 is found later in this specification at embodiment 300. 
<!-- EPO <DP n="13"/>-->
 [0063] Each shot may be analyzed in block 206. For each shot in block 206 and for each frame of each shot in block 208, the frame may be analyzed for faces in block 210. The analysis of block 210 may be a static image analysis that may detect faces within the static image. For each face detected in block 212, the size and position of the face is determined in block 214, image analysis of the face may be performed in block 215, and a reliability factor for the analysis may be determined in block 216. All of the analysis results may be stored in a face definition in block 218. </p><p id="p0069" num="0069"> [0064] The analysis of the faces may include a position and size definition. In some embodiments, the position and size may indicate a location within the frame for a particular face. The size may define the area of the image that contains the face. In many embodiments, the position may be the center point of the face boundary, but other embodiments may define a corner or other location. The size of the face may be indicated by a geometric shape, such as a rectangle, square, circle, ellipse, hexagon, octagon, or other shape. In some cases, the size may be defined in one, two, three, or more values. In a typical example of a rectangle shape, the size may be defined using height and width dimensions. </p><p id="p0070" num="0070"> [0065] The image analysis of the face may include various data derived from the image itself, such as color histograms, facial structural variables, or other information. Some embodiments may use image analysis information to compare two face objects to determine if the objects are a match. Such matching may be performed to associate two sequential frames, two separate facial tracks, or for other matches, depending on the embodiment. </p><p id="p0071" num="0071"> [0066] The reliability factor of block 216 may be a statistical or other indicator for the confidence in the analysis. The reliability factor may indicate the confidence the facial detection algorithm may have that the object is indeed a face. Facial detection can be a complex algorithm with a large amount of variability. Each algorithm may have different mechanisms for indicating reliability, such as a numerical score from 0 to 1 or 1 to 10, a qualitative indicator such as high, medium, low, or some other indicator. </p><p id="p0072" num="0072"> [0067] After analyzing each face in the frame and storing the facial objects in block 218, the analyzed frame definition may be stored in block 220. The process of blocks 206 through 220 may be repeated for each frame of each shot. </p><p id="p0073" num="0073"> [0068] A frame within a shot may be selected in block 220. In some </p><p id="p0074" num="0074">embodiments, the frame of block 220 may be any frame within the shot. Some 
<!-- EPO <DP n="14"/>-->
 embodiments may scan the frames within a shot to find the most reliably detected face object within the shot. From that frame, the most reliably detected face object that has not been analyzed may be selected in block 224. </p><p id="p0075" num="0075"> [0069] Using the selected face object, facial tracking may be performed forwards in block 226 through the video sequence and backwards in block 228 in the video sequence. An example embodiment of the process of blocks 226 and 228 may be found later in this specification in embodiment 400. </p><p id="p0076" num="0076"> [0070] The results of the facial tracking analyses may be stored in block 230 and the face objects may be marked as processed in block 232. If there are more faces in the current frame in block 234, the process may return to block 224 to select another face. If there are more frames in the shot that have not been analyzed in block 236, the process may return to block 222 to select another frame. </p><p id="p0077" num="0077"> [0071] Once the frames have been analyzed in block 236, linking analysis may be performed in block 238. An example of a linking analysis may be found later in this specification in embodiment 500. </p><p id="p0078" num="0078"> [0072] Figure 3 is a flowchart illustration of an embodiment 300 showing a method for determining shots within a video. Embodiment 300 is a simplified example of a method that may be performed by a video parser, such as the video parser 122 of embodiment 100. </p><p id="p0079" num="0079"> [0073] Other embodiments may use different sequencing, additional or fewer steps, and different nomenclature or terminology to accomplish similar functions. In some embodiments, various operations or set of operations may be performed in parallel with other operations, either in a synchronous or asynchronous manner. The steps selected here were chosen to illustrate some principles of operations in a simplified form. </p><p id="p0080" num="0080"> [0074] The method of embodiment 300 illustrates one example of how to separate a video sequence into discrete shots. Each shot may be a sequence of frames that are similar and may have the same facial images in a facial track. </p><p id="p0081" num="0081"> [0075] The video to analyze may be received in block 302. For each frame in the video in block 304, the current frame may be characterized in block 306 as well as the next frame in block 308. The characterizations of the frames may be compared in block 310 to determine if the frames are statistically different. If the blocks are not significantly different in block 310, the metadata associated with the frames in block 312 may be 
<!-- EPO <DP n="15"/>-->
 compared to determine if the shots may have changed. If not, the process may return to block 304 to process the next frame. </p><p id="p0082" num="0082"> [0076] If the statistical analysis or metadata analysis indicates that the shot has changed in either block 310 or 312, a new shot may be identified in block 314. The process may return to block 304 to process another frame. The process of embodiment 300 may continue until each frame of the video is processed. </p><p id="p0083" num="0083"> [0077] The statistical comparison of block 310 may compare various statistics or information derived from the images of the frame. Such information may include color histograms, object analysis, or other analyses of the image. When the images change abruptly from one frame to another, a new shot may be indicated. </p><p id="p0084" num="0084"> [0078] The metadata analysis of block 312 may include examining time stamps or other metadata associated with each frame. When the timestamps change significantly from one frame to another, the timestamps may indicate that the camera operator stopped and restarted the camera, indicating a new shot. </p><p id="p0085" num="0085"> [0079] Figure 4 is a flowchart illustration of an embodiment 400 showing a method for facial tracking within a video shot. Embodiment 400 is a simplified example of a method that may be performed by a facial track analyzer, such as facial track analyzer 126 of embodiment 100. </p><p id="p0086" num="0086"> [0080] Other embodiments may use different sequencing, additional or fewer steps, and different nomenclature or terminology to accomplish similar functions. In some embodiments, various operations or set of operations may be performed in parallel with other operations, either in a synchronous or asynchronous manner. The steps selected here were chosen to illustrate some principles of operations in a simplified form. </p><p id="p0087" num="0087"> [0081] Embodiment 400 illustrates one method by which a facial track may be created. A facial track may be a sequence of facial objects that are linked together in a sequence of frames of a video. The facial track may represent the same facial object as it moves and changes through a video shot. </p><p id="p0088" num="0088"> [0082] In block 402, the starting frame and a detected face object may be received. A group of frames in the traversing direction may be identified in block 404. The traversing direction may be forwards or backwards through the video stream and may use frames preceding and subsequent to a starting frame. </p><p id="p0089" num="0089"> [0083] In some embodiments, the starting frame may be selected by scanning the frames within a shot and selecting the most reliably detected face object. A facial track 
<!-- EPO <DP n="16"/>-->
 may be created using the most reliably detected face object, then each subsequent facial track may be created using the same method of selecting the most reliably detected face object that has not already been placed into a facial track. </p><p id="p0090" num="0090"> [0084] The face object in the current frame may be compared to the face objects in the group of frames in block 406 using trajectory analysis. Trajectory analysis may attempt to match the face objects based on the position and size of the face objects. In many embodiments, such an analysis may use only position and size comparisons and may or may not use information derived from the image analysis. </p><p id="p0091" num="0091"> [0085] If there is a successful match in block 408, the group of frames may be added to the facial track in block 414. </p><p id="p0092" num="0092"> [0086] If there is not a successful match in block 408, a match may be attempted using image analysis results in block 410. The image analysis results may use color histograms, facial structure analysis, or other types of comparisons using information derived from the images associated with the face objects. If there is a successful match in block 412, the process may continue to block 414 and the group of frames may be added in block 414. If there is not a successful match in block 412, the track may be ended in block 418. </p><p id="p0093" num="0093"> [0087] When a successful match is found in blocks 408 or 412 and the frames are added to the facial track in block 414, if there are additional frames in the shot in block 416, the current frame may be incremented in block 420 and the process may return to block 402 to be repeated. If there are no additional frames in block 416, the track may be ended in block 418. </p><p id="p0094" num="0094"> [0088] Figure 5 is a flowchart illustration of an embodiment 500 showing a method for linking facial tracks. Embodiment 500 is a simplified example of a method that may be performed by a post processor, such as the post processor 128 of embodiment 100. </p><p id="p0095" num="0095"> [0089] Other embodiments may use different sequencing, additional or fewer steps, and different nomenclature or terminology to accomplish similar functions. In some embodiments, various operations or set of operations may be performed in parallel with other operations, either in a synchronous or asynchronous manner. The steps selected here were chosen to illustrate some principles of operations in a simplified form. </p><p id="p0096" num="0096"> [0090] Embodiment 500 illustrates one method by which facial tracks may be linked together to form a longer facial track through a shot. The linking analysis of 
<!-- EPO <DP n="17"/>-->
 embodiment 500 may attempt to join facial tracks from the same face object into a single, long facial track. </p><p id="p0097" num="0097"> [0091] Part of the operations of embodiment 500 analyzes non-overlapping facial tracks, where non-overlapping facial tracks are those that do not share a common frame. Overlapping facial tracks within a shot may indicate that two separate faces are shown in the same frame. Because the overlapping facial tracks indicate two separate faces, considering joining such facial tracks would be improper. </p><p id="p0098" num="0098"> [0092] A facial track may be detected in block 502. Within the shot, non- overlapping facial tracks may be detected in both forward and backward directions from the given facial track in block 504. The detected facial tracks may be those which are potential matches with the given facial track. </p><p id="p0099" num="0099"> [0093] The object trajectories of the potentially matching facial tracks may be compared in block 506. The object trajectories may use the position and size of the facial objects to compare the facial tracks. In some embodiments, merely the position of the face objects may be compared, while other embodiments may use both position and size in the trajectory analysis. </p><p id="p0100" num="0100"> [0094] Within each facial track, the most reliably detected face objects may be selected in block 508 and compared in block 510. The comparison in block 510 may use image analysis results to determine whether or not the facial tracks represent the same face. If there is a match in block 510, the facial tracks may be added together in block 512. If there is not a match in block 510, and there are more facial tracks within the shot in block 514, the process may return to block 502 to process another facial track. If no more facial tracks are available in block 514, the process may end in block 516. </p><p id="p0101" num="0101"> [0095] In some embodiments, the process of embodiment 500 may be used to link facial tracks from different shots. In such a case, the embodiment 500 may be used without comparing the object trajectories of block 506. Such an embodiment may select a face object from the two potentially matching facial tracks and use image analysis results to determine if the facial tracks are a match. If so, the facial tracks may be joined across the shot boundary. </p><p id="p0102" num="0102"> [0096] Figure 6 is a diagram illustration of an example embodiment 600 showing a facial track from a single shot. Embodiment 600 illustrates five frames that show two faces and an illustration of one of the facial tracks derived from the sequence of frames. Embodiment 600 is a very simplified example for illustration purposes. 
<!-- EPO <DP n="18"/>-->
 [0097] Frames 602, 604, 606, 608, and 610 illustrate successive frames of a single video shot. Within each frame are faces 612 and 614, each of which traverse the frames in sequence. Face 612 moves to the background and to the left in the sequence, while face 614 moves to the front and to the right in the sequence. </p><p id="p0103" num="0103"> [0098] After traversing the frames, a facial track 616 may be generated that links the various position and sizes of face 612 across the frames. Facial track 616 may illustrate the face object as it moves through the successive frames. </p><p id="p0104" num="0104"> [0099] The foregoing description of the subject matter has been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the subject matter to the precise form disclosed, and other modifications and variations may be possible in light of the above teachings. The embodiment was chosen and described in order to best explain the principles of the invention and its practical application to thereby enable others skilled in the art to best utilize the invention in various embodiments and various modifications as are suited to the particular use contemplated. It is intended that the appended claims be construed to include other alternative embodiments except insofar as limited by the prior art. 
</p></description><claims mxw-id="PCLM44997202" ref-ucid="WO-2012122069-A2" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="19"/>-->CLAIMS What is claimed is: </claim-statement><claim id="clm-0001" num="1"><claim-text> 1. A method performed on at least one computer processor, said method comprising: </claim-text><claim-text> receiving a video comprising a sequence of frames; </claim-text><claim-text> for at least one shot in said video, analyzing each of said frames to detect faces, said faces being identified with at least a position and a size; </claim-text><claim-text> creating a first facial track by: </claim-text><claim-text> selecting a first face in a first frame; </claim-text><claim-text> analyzing at least one frame subsequent said first frame to identify said first face; and </claim-text><claim-text> analyzing at least one frame preceding said first frame to identify said first face to create said first facial track. </claim-text></claim><claim id="clm-0002" num="2"><claim-text> 2. The method of claim 1 said creating at least one facial track further comprising: identifying a second facial track; </claim-text><claim-text> determining that said first facial track contains a similar face as said second facial track; and </claim-text><claim-text> combining said first facial track and said second facial track into a single facial track. </claim-text></claim><claim id="clm-0003" num="3"><claim-text> 3. The method of claim 2, said second facial track not sharing a common frame with said first facial track in said sequence of said frames. </claim-text></claim><claim id="clm-0004" num="4"><claim-text> 4. The method of claim 3, said determining that said first facial track contains a similar face as said second facial track being performed using image analysis of at least one face in said first facial track and at least one face said second facial track.</claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method of claim 4, said image analysis comprising color histogram analysis. </claim-text></claim><claim id="clm-0006" num="6"><claim-text> 6. The method of claim 4, said image analysis comprising facial structure analysis.</claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method of claim 1, said first face being identified by comparing said position and said size from said first face in a first frame to said position and said size from said first face in said second frame. </claim-text></claim><claim id="clm-0008" num="8"><claim-text> 8. The method of claim 7, said first face being identified by comparing said position and said size from said first face in a first frame to said position and said size from said first face in a group of frames comprising said second frame. </claim-text></claim><claim id="clm-0009" num="9"><claim-text> 9. The method of claim 8, said comparing using a clustering algorithm. <!-- EPO <DP n="20"/>--> </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. A system comprising: </claim-text><claim-text> a face detector that: </claim-text><claim-text> analyzes each frame of a first shot in said video to identify faces, said faces being identified with at least a position and a size; a facial track analyzer that: </claim-text><claim-text> selects a first face in a first frame; </claim-text><claim-text> analyzes at least one frame after said first frame to identify said first face; and </claim-text><claim-text> analyzes at least one frame before said first frame to identify said first face to create said first facial track; </claim-text><claim-text> said system being executed on at least one processor. </claim-text></claim><claim id="clm-0011" num="11"><claim-text> 11. The system of claim 10, said facial track analyzer that: </claim-text><claim-text> combines a second facial track to said first facial track, said first facial track and second facial track not overlapping. </claim-text></claim><claim id="clm-0012" num="12"><claim-text> 12. The system of claim 11, said facial track analyzer that: </claim-text><claim-text> combines said second facial track to said first facial, a first facial track being in a first shot and said second facial track being in a second shot. </claim-text></claim><claim id="clm-0013" num="13"><claim-text> 13. The system of claim 10, said face detector identifying a reliability factor for said first face. </claim-text></claim><claim id="clm-0014" num="14"><claim-text> 14. The system of claim 13, said facial track analyzer that further: </claim-text><claim-text> analyzes said facial track to determine a second frame comprising said first face and having a high reliability factor; and </claim-text><claim-text> selecting at least a portion of said second frame to represent said first face in said facial track. </claim-text></claim><claim id="clm-0015" num="15"><claim-text> 15. The system of claim 10, said face detector that further: </claim-text><claim-text>generates image content analysis of said faces. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
