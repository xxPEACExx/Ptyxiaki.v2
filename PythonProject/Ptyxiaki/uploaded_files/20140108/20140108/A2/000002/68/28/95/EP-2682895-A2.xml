<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2682895-A2" country="EP" doc-number="2682895" kind="A2" date="20140108" family-id="47997010" file-reference-id="307650" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146584857" ucid="EP-2682895-A2"><document-id><country>EP</country><doc-number>2682895</doc-number><kind>A2</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13158257-A" is-representative="YES"><document-id mxw-id="PAPP154847049" load-source="docdb" format="epo"><country>EP</country><doc-number>13158257</doc-number><kind>A</kind><date>20130307</date><lang>EN</lang></document-id><document-id mxw-id="PAPP222722619" load-source="docdb" format="original"><country>EP</country><doc-number>13158257.9</doc-number><date>20130307</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140556404" ucid="JP-2012150024-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2012150024</doc-number><kind>A</kind><date>20120703</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989317442" load-source="docdb">G06K   9/00        20060101AFI20130823BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989321534" load-source="docdb">G06T   7/20        20060101ALI20130823BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1642417229" load-source="docdb" scheme="CPC">H04N  13/366       20180501 FI20180501BCEP        </classification-cpc><classification-cpc mxw-id="PCL-1861395793" load-source="docdb" scheme="CPC">G06T   7/248       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989619982" load-source="docdb" scheme="CPC">G06K   9/00261     20130101 LI20130823BHEP        </classification-cpc><classification-cpc mxw-id="PCL2094214231" load-source="docdb" scheme="CPC">G06K   9/6807      20130101 LI20140708BHEP        </classification-cpc><classification-cpc mxw-id="PCL2100275157" load-source="docdb" scheme="CPC">G06T2207/30201     20130101 LA20140723BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132359204" lang="DE" load-source="patent-office">Videoanzeigevorrichtung und Videoanzeigeverfahren</invention-title><invention-title mxw-id="PT132359205" lang="EN" load-source="patent-office">Video display apparatus and video display method</invention-title><invention-title mxw-id="PT132359206" lang="FR" load-source="patent-office">Appareil dýaffichage vidéo et procédé dýaffichage vidéo</invention-title><citations><patent-citations><patcit mxw-id="PCIT242942482" load-source="docdb" ucid="JP-2004246618-A"><document-id format="epo"><country>JP</country><doc-number>2004246618</doc-number><kind>A</kind><date>20040902</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942483" load-source="docdb" ucid="JP-2005092262-A"><document-id format="epo"><country>JP</country><doc-number>2005092262</doc-number><kind>A</kind><date>20050407</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit></patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR919542881" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>TOSHIBA KK</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR919537228" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KABUSHIKI KAISHA TOSHIBA</last-name></addressbook></applicant><applicant mxw-id="PPAR919008284" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Kabushiki Kaisha Toshiba</last-name><iid>100153902</iid><address><street>1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo 105-8001</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919535296" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>MARUYAMA EMI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919541369" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>MARUYAMA, EMI</last-name></addressbook></inventor><inventor mxw-id="PPAR919008888" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>MARUYAMA, EMI</last-name><address><street>Toshiba Corporation, Intellectual Property Division 1-1, Shibaura 1-chome, Minato-ku</street><city>Tokyo, 105-8001</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919007387" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Henkel, Feiler &amp; Hänzel</last-name><iid>100060244</iid><address><street>Patentanwälte Maximiliansplatz 21</street><city>80333 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS549738277" load-source="docdb">AL</country><country mxw-id="DS549739944" load-source="docdb">AT</country><country mxw-id="DS549741637" load-source="docdb">BE</country><country mxw-id="DS549907311" load-source="docdb">BG</country><country mxw-id="DS549739583" load-source="docdb">CH</country><country mxw-id="DS549741638" load-source="docdb">CY</country><country mxw-id="DS549739945" load-source="docdb">CZ</country><country mxw-id="DS549738279" load-source="docdb">DE</country><country mxw-id="DS549741639" load-source="docdb">DK</country><country mxw-id="DS549741640" load-source="docdb">EE</country><country mxw-id="DS549812267" load-source="docdb">ES</country><country mxw-id="DS549907312" load-source="docdb">FI</country><country mxw-id="DS549739584" load-source="docdb">FR</country><country mxw-id="DS549738280" load-source="docdb">GB</country><country mxw-id="DS549741645" load-source="docdb">GR</country><country mxw-id="DS549738281" load-source="docdb">HR</country><country mxw-id="DS549739946" load-source="docdb">HU</country><country mxw-id="DS549739585" load-source="docdb">IE</country><country mxw-id="DS549741646" load-source="docdb">IS</country><country mxw-id="DS549907313" load-source="docdb">IT</country><country mxw-id="DS549741647" load-source="docdb">LI</country><country mxw-id="DS549907314" load-source="docdb">LT</country><country mxw-id="DS549812381" load-source="docdb">LU</country><country mxw-id="DS549907315" load-source="docdb">LV</country><country mxw-id="DS549907316" load-source="docdb">MC</country><country mxw-id="DS549812386" load-source="docdb">MK</country><country mxw-id="DS549812387" load-source="docdb">MT</country><country mxw-id="DS549812388" load-source="docdb">NL</country><country mxw-id="DS549739586" load-source="docdb">NO</country><country mxw-id="DS549812389" load-source="docdb">PL</country><country mxw-id="DS549812268" load-source="docdb">PT</country><country mxw-id="DS549739290" load-source="docdb">RO</country><country mxw-id="DS549812269" load-source="docdb">RS</country><country mxw-id="DS549812402" load-source="docdb">SE</country><country mxw-id="DS549738283" load-source="docdb">SI</country><country mxw-id="DS549739587" load-source="docdb">SK</country><country mxw-id="DS549739588" load-source="docdb">SM</country><country mxw-id="DS549741648" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128673029" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A video display apparatus includes an image acquiring module, a face-dictionary face detector, a face determining module and a face tracking module. The image acquiring module is configured to acquire an image captured by an imaging device. The face-dictionary face detector is configured to search the captured image acquired by the image acquiring module for a portion that coincides with a face pattern in a human face dictionary. The face determining module is configured to evaluate the portion based on the captured image and a background image acquired in advance. The face tracking module is configured to track a face based on a feature quantity of the face pattern and a result of the evaluation by the face determining module.</p></abstract><abstract mxw-id="PA128737174" lang="EN" source="EPO" load-source="docdb"><p>A video display apparatus includes an image acquiring module, a face-dictionary face detector, a face determining module and a face tracking module. The image acquiring module is configured to acquire an image captured by an imaging device. The face-dictionary face detector is configured to search the captured image acquired by the image acquiring module for a portion that coincides with a face pattern in a human face dictionary. The face determining module is configured to evaluate the portion based on the captured image and a background image acquired in advance. The face tracking module is configured to track a face based on a feature quantity of the face pattern and a result of the evaluation by the face determining module.</p></abstract><description mxw-id="PDES63959017" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">TECHNICAL FIELD</heading><p id="p0001" num="0001">Embodiments described herein relate generally to a video display apparatus and a video display method.</p><heading id="h0002">BACKGROUND</heading><p id="p0002" num="0002">Hitherto, a stereoscopically-viewable area of a naked-eye stereoscopic video display apparatus with respect to a viewer and speaker directions of an audio apparatus with respect to a listener have been adjusted using position information of the viewer/listener.</p><p id="p0003" num="0003">To acquire such position information of the viewer/listener, feature points of a face of the viewer/listener in an image captured by a camera are detected, and a distance between the feature points (for example, a face width, an interval between the eyes, or the like) is calculated. Then, Actual position coordinates of the viewer/listener are calculated based on the calculated distance between the feature points in the captured image and an average distance between the feature points prepared in advance, using a perspective projection method of a pinhole camera model.</p><p id="p0004" num="0004">Incidentally, apparatus capable of detecting a user's face have been spreading.</p><p id="p0005" num="0005"><patcit id="pcit0001" dnum="JP2005092262A"><text>JP 2005-92262 A</text></patcit> describes that a background image without any person is captured in advance, a person is detected based on a difference between the background image and a newly captured image, and a face portion is estimated by edge detection and finally detected by skin color detection. However, it is difficult to estimate a face portion by the edge detection because the estimation is influenced by the resolution of an image captured by a camera and noise in the captured image. If the accuracy of detection of a face portion is low, the accuracy of the face detection by the skin color detection also becomes low.</p><p id="p0006" num="0006"><patcit id="pcit0002" dnum="JP2004246618A"><text>JP 2004-246618 A</text></patcit>, which is the publication of a Japanese patent application filed by the same applicant as this application, describes a face detecting method. In this technique, various face images are used as sample images, and sample probability images are generated from the sample images. A face is detected by comparing an image captured by a camera with the sample probability images. (In the following description, the sample probability images may be referred to as a "face dictionary," and this detection method may be referred to as a "face dictionary face detecting method".) With this detection method, faces can be detected stably. There may be, however, a case where a portion other than a face is detected. For example, a photograph of a face hung on a wall, a pattern that is similar to a face, etc. may<!-- EPO <DP n="2"> --> be detected erroneously. No method for eliminating such an erroneously detected portion is available.</p><p id="p0007" num="0007">Where only a face feature quantity is used, face tracking may fail temporarily. This is because matching requires a front face; it is difficult to deal with faces that are orientated in all directions in acquired images because an enormous amount of calculation is necessary. If face tracking fails, it is necessary to perform face detection again. However, in apparatus in which viewers are discriminated from each other by assigning identifiers to the respective viewers every time face detection is performed, an event that face tracking fails due to a temporary face motion to necessitate assignment of new identifiers causes much inconvenience.</p><p id="p0008" num="0008">More robust face detection or face tracking techniques which can cope with a variation in environment or a face motion are desired. However, no means for satisfying such a desire has been known.</p><heading id="h0003">SUMMARY</heading><p id="p0009" num="0009">The invention provides a more robust face detection technique which can cope with a variation in environment and/or a face motion.</p><p id="p0010" num="0010">According to one embodiment, a video display apparatus includes an image acquiring module, a face-dictionary face detector, a face determining module and a face tracking module. The image acquiring module is configured to acquire an image captured by an imaging device. The face-dictionary face detector is configured to search the captured image acquired by the image acquiring module for a portion that coincides with a face pattern in a human face dictionary. The face determining module is configured to evaluate the portion based on the captured image and a background image acquired in advance. The face tracking module is configured to track a face based on a feature quantity of the face pattern and a result of the evaluation by the face determining module.</p><heading id="h0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0011" num="0011"><ul><li><figref idrefs="f0001">Fig. 1</figref> is a perspective appearance view showing one example of a digital TV receiver according to an embodiment;</li><li><figref idrefs="f0002">Fig. 2</figref> is a block diagram showing a signal processing system of the digital TV receiver;</li><li><figref idrefs="f0003">Fig. 3</figref> is a functional block diagram of a face-position-coordinate acquiring module according to the embodiment;<!-- EPO <DP n="3"> --></li><li><figref idrefs="f0004">Fig. 4</figref> illustrates an example of a camera image and face coordinates in the embodiment;</li><li><figref idrefs="f0005">Fig. 5</figref> is a flowchart of a face detection/face tracking process according to the embodiment;</li><li><figref idrefs="f0006">Fig. 6</figref> is a flowchart of a process for acquiring a background/reference image according to the embodiment; and</li><li><figref idrefs="f0007">Fig. 7</figref> is a flowchart of a face detection process according to the embodiment.</li></ul></p><heading id="h0005">DETAILED DESCRIPTION</heading><p id="p0012" num="0012">According to one embodiment, a video display apparatus includes an image acquiring module, a face-dictionary face detector, a face determining module and a face tracking module. The image acquiring module is configured to acquire an image captured by an imaging device. The face-dictionary face detector is configured to search the captured image acquired by the image acquiring module for a portion that coincides with a face pattern in a human face dictionary. The face determining module is configured to evaluate the portion based on the captured image and a background image acquired in advance. The face tracking module is configured to track a face based on a feature quantity of the face pattern and a result of the evaluation by the face determining module.</p><p id="p0013" num="0013">Embodiments will be described in detail below with reference to the accompanying drawings.
<ul><li><figref idrefs="f0001">Fig. 1</figref> is a perspective view showing an appearance of a digital TV receiver 1 which is an example of an electronic device according to one embodiment. As shown in <figref idrefs="f0001">Fig. 1</figref>, when viewed from the front side (in a planar view from the front side), the digital TV receiver 1 has a rectangular appearance. The digital TV receiver 1 includes a casing 2 and a display module 3 such as an LCD (liquid crystal display) panel. The display module 3 receives a video signal from a video processor 20 (see <figref idrefs="f0002">Fig. 2</figref>; which will be described later) and displays video such as a still image or a moving image. The casing 2 is supported by a support member 4.</li><li><figref idrefs="f0002">Fig. 2</figref> is a block diagram showing a signal processing system of the digital TV receiver 1. The digital TV receiver 1 serves as a stereoscopic image output apparatus. The digital TV receiver 1 can not only display video based on an ordinary planar (2D) display video signal but also display video based on a stereoscopic (3D) display video signal. Also, the digital TV receiver 1 enables users to view stereoscopic video with naked eyes.</li></ul></p><p id="p0014" num="0014">As shown in <figref idrefs="f0002">Fig. 2</figref>, in the digital TV receiver 1, a broadcast signal on a desired channel can be selected by supplying digital TV broadcast signals received by an antenna 12<!-- EPO <DP n="4"> --> to a tuner module 14 (receiver) via an input terminal 13. The broadcast signal selected by the tuner module 14 is supplied to a demodulating/decoding module 15. The demodulating/decoding module 15 restores a digital video signal and audio signal etc., which are output to an input signal processor 16. In this embodiment, it is assumed that the digital TV receiver 1 includes three tuners (receivers configured to receive digital TV broadcast signals), that is, a tuner A 141 and a tuner B 142 (two tuners for reception of ground-wave digital broadcasts) and a tuner C 143 (one tuner for reception of BS/CS digital broadcasts).</p><p id="p0015" num="0015">The input signal processor 16 performs prescribed digital signal processing on each of the digital video signal and audio signal, which are supplied from the demodulating/decoding module 15.</p><p id="p0016" num="0016">The input signal processor 16 has a conversion-into-stereoscopic-image module 160 which performs stereoscopic image conversion processing of converting a video signal (input video signal) for ordinary planar (2D) display into a video signal for stereoscopic (3D) display.</p><p id="p0017" num="0017">The input signal processor 16 separates an EIT (event information table) being a table, in which event information such as a program name, persons who appear, and a start time are described, from the broadcast signal selected by the tuner module 14. The EIT separated by the input signal processor 16 is input to a controller 23 as program table data. The EIT contains information (event information) relating to a program such as a broadcast date and time and broadcast details including program title information, genre information, and information indicating persons who appear.</p><p id="p0018" num="0018">The input signal processor 16 outputs a digital video signal and an audio signal to a synthesizing processor 17 and an audio processor 18, respectively. The synthesizing processor 17 superimposes an OSD (On-Screen Display) signal (superimposition video signal) such as subtitles, a GUI (Graphical User Interface), or the like generated by an OSD signal generator 19 on the digital video signal supplied from the input signal processor 16, and outputs a resulting signal. In this example, the synthesizing processor 17 superimposes the OSD signal supplied from the OSD signal generator 19 as it is on the digital video signal supplied from the input signal processor 16, and outputs a resulting signal.</p><p id="p0019" num="0019">In the digital TV receiver 1, the digital video signal output from the synthesizing processor 17 is supplied to the video processor 20. The video processor 20 converts the received digital video signal into an analog video signal having such a format as to be displayable by the display module 3 serving as a video output module. The analog video signal output from the video processor 20 is supplied to the display module 3 and used for video output there.<!-- EPO <DP n="5"> --></p><p id="p0020" num="0020">The audio processor 18 converts the received audio signal into analog audio signals having such a format as to be reproducible by downstream speakers 22. The analog audio signals output from the audio processor 18 are supplied to the speakers 22 and used for sound reproduction there.</p><p id="p0021" num="0021">As shown in <figref idrefs="f0002">Fig. 2</figref>, the synthesizing processor 17, the audio processor 18, the OSD signal generator 19, and the video processor 20 constitute an output signal processor 21.</p><p id="p0022" num="0022">As shown in <figref idrefs="f0001">Fig. 1</figref>, the digital TV receiver 1 includes a camera 37 (an example of an imaging device) in the vicinity of the display module 3. The camera 37 is disposed at such a position as to be able to capture a face of a user who is opposed to the digital TV receiver 1.</p><p id="p0023" num="0023">In the digital TV receiver 1, all operations including the above-described various receiving operations are controlled by the controller 23 in a unified manner. The controller 23 incorporates a CPU (Central Processing Unit) 23a. The controller 23 controls individual components in such a manner that the content of a manipulation indicated by manipulation information received from a manipulation module 24 which is a manipulation device provided in the main body of the digital TV receiver 1 or manipulation information transmitted from a remote controller 25 (another example of manipulation device) and received by a receiver 26.</p><p id="p0024" num="0024">The controller 23 incorporates a memory 23b, which mainly includes a ROM (read-only memory) storing control programs to be executed by the CPU 23a, a RAM (random access memory) for providing a work area for the CPU 23a, and a nonvolatile memory for storing various kinds of setting information, control information, and manipulation information supplied from the manipulation module 24 and/or the remote controller 25, and other information.</p><p id="p0025" num="0025">A disc drive 27 is connected to the controller 23. An optical disc 28 such as a DVD (digital versatile disc) is to be inserted into the disc drive 27 in a detachable manner. The disc drive 27 has functions of recording and reproducing digital data on and from the inserted optical disc 28.</p><p id="p0026" num="0026">The controller 23 may perform, according to a manipulation made by a viewer on the manipulation module 24 and/or the remote controller 25, controls so that a digital video signal and a audio signal generated by the demodulating/decoding module 15 are coded and converted by a recording/reproduction processor 29 into signals having a predetermined recording format, which are supplied to the disc drive 27 and recorded on the optical disc 28.</p><p id="p0027" num="0027">The controller 23 may perform, according to a manipulation made by a viewer on the manipulation module 24 and/or the remote controller 25, controls so that a digital video signal and a audio signal are read from the optical disc 28 by the disc drive 27 and decoded by the<!-- EPO <DP n="6"> --> recording/reproduction processor 29, and resulting signals are supplied to the input signal processor 16 so as to be used for video display and audio reproduction (as described above).</p><p id="p0028" num="0028">An HDD (hard disk drive) 30 is connected to the controller 23. The controller 23 may perform, according to a manipulation made by a viewer on the manipulation module 24 and/or the remote controller 25, controls so that a digital video signal and a audio signal generated by the demodulating/decoding module 15 are coded and converted by the recording/reproduction processor 29 into signals having a predetermined recording format, which are supplied to the HDD 30 and recorded on a hard disk 30a.</p><p id="p0029" num="0029">Furthermore, the controller 23 may perform, according to a manipulation made by a viewer on the manipulation module 24 and/or the remote controller 25, controls so that a digital video signal and an audio signal are read from the hard disk 30a by the HDD 30 and decoded by the recording/reproduction processor 29, and resulting signals are supplied to the input signal processor 16 so as to be used for video display and audio reproduction (as described above).</p><p id="p0030" num="0030">By storing various kinds of data in the hard disk 30a, the HDD 30 functions as a background image buffer 301 and a face detection history data storage 304. The face detection history data storage 304, which functions as a human database (DB), stores distances between feature points (for example, a face width which will be described later) and face feature point coordinates (for example, coordinate information of a face contour which will be described later) in such a manner that they are associated with respective viewer IDs.</p><p id="p0031" num="0031">The digital TV receiver 1 has an input terminal 31. The input terminal 31, which is a LAN terminal, a USB terminal, an HDMI terminal, or the like, serves for direct input of a digital video signal and an audio signal from outside the digital TV receiver 1. A digital video signal and an audio signal that are input through the input terminal 31 may be supplied to the input signal processor 16 via the recording/reproduction processor 29 and used for video display and audio reproduction (as described above), under the control of the controller 23.</p><p id="p0032" num="0032">Also, a digital video signal and an audio signal that are input through the input terminal 31 may be supplied to the disc drive 27 or the HDD 30 via the recording/reproduction processor 29 and recorded in the optical disc 28 or the hard disk 30a, under the control of the controller 23.</p><p id="p0033" num="0033">The controller 23 also performs, according to viewer's manipulation on the manipulation module 24 or the remote controller 25, controls so that a digital video signal and an audio signal recorded on the optical disk 28 are transferred to and recorded on the hard disk 30a or a digital video signal and an audio signal recorded on the hard disk 30a are transferred<!-- EPO <DP n="7"> --> to and recorded on the optical disk 28 by the disc drive 27 and the HDD 30.</p><p id="p0034" num="0034">A network interface 32 is connected to the controller 23. The network interface 32 is connected to an external network 34 through an input/output terminal 33. Network servers 35 and 36 for providing various services using a communication function via the network 34 are connected to the network 34. Therefore, the controller 23 can use a service provided by a desired one of the network servers 35 and 36 by accessing it and performing an information communication with it through the network interface 32, the input/output terminal 33, and the network 34. An SD memory card or an USB device may be connected to the network interface 32 though the input/output terminal 33.</p><p id="p0035" num="0035"><figref idrefs="f0003">Fig. 3</figref> is a functional block diagram of a face-position-coordinate acquiring module that generates face position coordinates based on a camera image. The face-position-coordinate acquiring module is a function of the controller 23, for example, implemented by the CPU 23a and the memory 23b. The face-position-coordinate acquiring module may be provided in an audio apparatus such as a camera-equipped TV receiver, a surveillance camera, or the like and acquires face position coordinates in a camera image.</p><p id="p0036" num="0036">This embodiment is intended to solve, by additionally using a difference between a camera image and a background/reference image, a disadvantage of <patcit id="pcit0003" dnum="JP2004246618A"><text>JP 2004-246618 A</text></patcit> that a variety of faces (in orientation, illumination, etc.) cannot be dealt with.</p><p id="p0037" num="0037">The controller 23 functions as a position coordinates detecting device by having the CPU 23a operate according to a control program. As shown in <figref idrefs="f0003">Fig. 3</figref>, the controller 23 includes an image controller 230, an image acquiring module 231, a face-dictionary face detector 233, a face tracking module 237, and a face determining module 238 that detects position coordinates. Functions of the respective modules will be described below.</p><p id="p0038" num="0038">The image acquiring module 231 acquires a captured image from video captured by the camera 37. In the digital TV receiver 1, the image captured by the camera 37 is supplied to the face tracking module 237 and the face-dictionary face detector 233 under the control of the image controller 230.</p><p id="p0039" num="0039">The camera 37 captures an indoor scene. Then, a camera image captured by the camera 37 is input to the image acquiring module 231. The image acquiring module 231 processes the camera image to facilitate discrimination of a face. A background/reference image is stored in the background image buffer 301. The face-dictionary face detector 233 searches for a portion that coincides with any of face patterns in a face dictionary while scanning the camera image. A typical operation of the face-dictionary face detector 233 is described in <patcit id="pcit0004" dnum="JP2004246618A"><text>JP 2004-246618 A</text></patcit>. The face tracking module 237 tracks a face portion in a<!-- EPO <DP n="8"> --> prescribed range around the face-detected position based on feature quantities of the face (coordinates of the eyes, nose, and mouth). The face determining module 238 evaluates a difference between the camera image and a background/reference image, uses an evaluation result to improve the face detection accuracy and enhance the tracking performance, and outputs face position coordinates.</p><p id="p0040" num="0040">Specific description will be given with reference to <figref idrefs="f0003">Fig. 3</figref>. In <figref idrefs="f0003">Fig. 3</figref>, solid-line arrows indicate data flows, and broken-line arrows indicate control relationships.</p><p id="p0041" num="0041">Face detection is first started upon activation of the digital TV receiver 1. Alternatively, the face detection may be started upon activation of the position-coordinate-detection device. The image acquiring module 231 acquires image data from the camera 37 under the control of the image controller 230, and thereafter, a switch SW_A is switched to the "1" side. Face position coordinates from the present time to a time that was a prescribed time before the present time are stored in the face detection history data storage 304. Since it is found by referring to data stored in the face detection history data storage 304 that no face history data exists there, a switch SW_B is switched to the "2" side, and the face-dictionary face detector 233 performs face detection. The face-dictionary face detector 233 may detect a face correctly or erroneously. That is, face position coordinates obtained by the face-dictionary face detector 233 may be face coordinates of a viewer face or face coordinates that have been detected erroneously because of presence of a wall pattern, a photograph, or the like. The face determining module 238 eliminates erroneously detected face coordinates using the reference image stored in the background image buffer 301.</p><p id="p0042" num="0042">The background/reference image(s) are acquired by the following two methods. The first method detects that no person exists and utilizing an image captured by the camera 37 at that time. This kind of image will be referred to as a "background image." Absence of a person is detected when differences among images of several consecutive frames are very small. A background image is captured every prescribed time, and a background image captured in a time slot that is close to a time of the face detection is used by associating each background image with its capturing time. The second method acquires an image every frame or every several frames. This kind of image will be referred to as a "reference image." When an acquired background or reference image is stored in the background image buffer 301, the switch SW_A (see <figref idrefs="f0003">Fig. 3</figref>) is switched to the "2" side.</p><p id="p0043" num="0043">The face determining module 238 determines as to whether or not detected face coordinates are correct ones. The face determining module 238 compares a face area acquired from face coordinates and a face width which are obtained from the face-dictionary<!-- EPO <DP n="9"> --> face detector 233 with the same area in a background image, using the background image obtained by the first method and stored in the background image buffer 301. If a difference between the face areas is smaller than a predetermined value, the face determining module 238 determines that a background pattern was detected erroneously as a face. If the difference is equal to or larger than the predetermined value, the face determining module 238 determines that a face was detected correctly. The comparing of the face areas may be made, for example, by calculating differences between pixel values of pixels at the same positions in the face areas or by comparing statistical data (histograms, maximum values, minimum values, average values, or the like) in the face areas. "A difference that is smaller than the predetermined value" is a difference caused only by camera noise and/or light and enables the face determining module 238 to determine that a captured object(s) are a still object(s) in the image. "A difference(s) that is equal to or larger than the predetermined value" is a difference caused by a motion of a human (for example, a blink and/or vibration due to a breath) that occurs even if he or she is still, and enables the face determining module 238 to determine that a captured object(s) include a human(s). The threshold value (predetermined value) is determined according to the image acquisition method, an S/N ratio of a captured image, the optical characteristics of the camera 37, etc.</p><p id="p0044" num="0044">The face tracking module 237 is activated upon detection of a face. After the image acquiring module 231 acquires image data from the camera 37 under the control of the image controller 230, the switch SW_A is switched to the "1" side, and the data stored in the face detection history data storage 304 are referred to. Since face history data exists there, the switch SW_B is switched to the "1" side, and the face tracking module 237 performs face tracking. If the face tracking has succeeded, the face tracking module 237 supplies face coordinates and a face width to the face determining module 238. If the face tracking has failed, the face tracking module 237 notifies the face determining module 238 of that fact. In this case, the face determining module 238 supplements the face tracking using a background/reference image(s) stored in the background image buffer 301.</p><p id="p0045" num="0045">Description will be given on the case where a background image has been acquired by the first method. When the face tracking has failed, if a difference between a currently captured image and the background image is larger than the predetermined value, it is determined that the face tracking has failed temporarily, and face position coordinates of an image captured at an immediately preceding time when the face tracking succeeded are used. The difference, which is larger than the predetermined value, is a difference that enable discrimination between a background image (without a human) and an image including a<!-- EPO <DP n="10"> --> human.</p><p id="p0046" num="0046">Next, description will be given on the case where a reference image has been acquired by the second method. When the face tracking has failed, a difference between a currently captured image and an image captured at an immediately preceding time when the face tracking succeeded is calculated, and a portion where the difference is larger than the predetermined value is detected. If face coordinates obtained at the immediately preceding time when the face tracking succeeded are included in the detected portion, it is determined that the face tracking has failed temporarily, and the face position coordinates of the image captured at the immediately preceding time when the face tracking succeeded are used. The portion where the difference is larger than the predetermined value should be a portion where a human moves. A portion where the difference is equal to or smaller than the predetermined value is a portion that can be determined to be a background portion. The difference may be calculated by comparing pixel values of pixels at the same position in areas or comparing statistical data values (histograms, maximum values, minimum values, average values, or the like) in the areas.</p><p id="p0047" num="0047">A human position can be calculated from the face position coordinates determined by the face determining module 238 using the known perspective projection conversion of a pinhole camera model. As shown in <figref idrefs="f0004">Fig. 4</figref>, values that are necessary in this conversion are coordinates (x1, y1) (unit: pixel) of the center of gravity of a face in a camera image and a face feature quantity (in the example of <figref idrefs="f0004">Fig. 4</figref>, a face width w (pixels)). A viewer position (X, Y, Z) (world coordinates; unit: mm) can be calculated based on the coordinates in the captured image using a face average width W<sub>A</sub> and the focal length f of the camera 37 in the following manner: <maths id="math0001" num=""><math display="block"><mi mathvariant="normal">X</mi><mo>=</mo><mfenced separators=""><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">1</mn></msub><mo>×</mo><msub><mi mathvariant="normal">W</mi><mi mathvariant="normal">A</mi></msub></mfenced><mo>/</mo><mi mathvariant="normal">w</mi><mfenced><mi>mm</mi></mfenced></math><img id="ib0001" file="imgb0001.tif" wi="45" he="8" img-content="math" img-format="tif"/></maths> <maths id="math0002" num=""><math display="block"><mi mathvariant="normal">Y</mi><mo>=</mo><mfenced separators=""><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">1</mn></msub><mo>×</mo><msub><mi mathvariant="normal">W</mi><mi mathvariant="normal">A</mi></msub></mfenced><mo>/</mo><mi mathvariant="normal">w</mi><mfenced><mi>mm</mi></mfenced></math><img id="ib0002" file="imgb0002.tif" wi="45" he="9" img-content="math" img-format="tif"/></maths> <maths id="math0003" num=""><math display="block"><mi mathvariant="normal">Z</mi><mo>=</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>×</mo><msub><mi mathvariant="normal">W</mi><mi mathvariant="normal">A</mi></msub></mfenced><mo>/</mo><mi mathvariant="normal">w</mi><mfenced><mi>mm</mi></mfenced></math><img id="ib0003" file="imgb0003.tif" wi="45" he="9" img-content="math" img-format="tif"/></maths></p><p id="p0048" num="0048">For example, an optimum viewing range of a glassless TV receiver or an optimum sound field of an audio apparatus can be set using an actual distance.</p><p id="p0049" num="0049">The above operations will be described with reference to flowcharts in which the image controller 230 mainly performs processes. At first, <figref idrefs="f0005">Fig. 5</figref> is a flowchart of a face detection/face tracking process according to this embodiment.</p><p id="p0050" num="0050">Step S51: An image is acquired from the camera 37.</p><p id="p0051" num="0051">Step S52: It is determined as to whether or not face history data exists in the face detection history data storage 304.<!-- EPO <DP n="11"> --></p><p id="p0052" num="0052">Step S53: If the determination result at step S52 is negative, the face-dictionary face detector 233 performs face detection at step S53.</p><p id="p0053" num="0053">Step S54: If the determination result at step S52 is affirmative, the face tracking module 237 performs face tracking at step S54 by.</p><p id="p0054" num="0054">Step S55: The face determining module 238 eliminates an erroneously detected face or determines as to whether or not the face tracking has failed temporarily, based on (i) a background/reference image and (ii) face position coordinates and a face width that are received from the face-dictionary face detector 233 or the face tracking module 237, and outputs face position coordinates and a face width.</p><p id="p0055" num="0055">Step S56: The process is terminated if some error has occurred. If not, the process returns to step S51.</p><p id="p0056" num="0056"><figref idrefs="f0006">Fig. 6</figref> is a flowchart of a process for acquiring a background/reference image according to this embodiment.</p><p id="p0057" num="0057">Step S61: It is determined as to whether or not an image acquisition time comes. If the determination result is negative, step S61 is repeated.</p><p id="p0058" num="0058">Step S62: An image is acquired from the camera 37.</p><p id="p0059" num="0059">Step S63: If a background image should be acquired by the first method, it is determined as to whether or not the image is motionless. If the determination result is negative, the process returns to step S61. If a reference image should be acquired by the second method, the process moves to step S64 with skipping step S63.</p><p id="p0060" num="0060">Step S64: The image is stored in the background image buffer 301.</p><p id="p0061" num="0061">Step S65: The process is terminated if some error has occurred. If not, the process returns to step S61.</p><p id="p0062" num="0062"><figref idrefs="f0007">Fig. 7</figref> is a flowchart of a face detection process according to this embodiment.</p><p id="p0063" num="0063">Step S71: The face-dictionary face detector 233 determines as to whether or not face detection has succeeded. If the determination result is negative, step S71 is repeated.</p><p id="p0064" num="0064">Step S72: The data stored in the face detection history data storage 304 are referred to.</p><p id="p0065" num="0065">Step S73: It is determined as to whether or not data within a predetermined time exists. The process is terminated if the determination result is negative.</p><p id="p0066" num="0066">Step S74: Differences between portions, around face coordinates, of a captured image and a background image stored in the background image buffer 301 are calculated.</p><p id="p0067" num="0067">Step S75: The face coordinates are output if the differences are larger than the threshold value.</p><p id="p0068" num="0068">The embodiment provides an advantage that it enables more robust face tracking and<!-- EPO <DP n="12"> --> face detection than the prior art when it is combined with the technique described in <patcit id="pcit0005" dnum="JP2004246618A"><text>JP 2004-246618 A</text></patcit>.</p><p id="p0069" num="0069">The embodiment is summarized as follows. In a camera-equipped TV receiver, the face detection and the face tracking can be performed robustly by using face detection in which differences from a reference image (or background image) are calculated in addition to a face detecting function of detecting a viewer face from a camera image. A background image that was captured by the camera when no person existed or a reference image that was captured by the camera at a preceding time is used as a background/reference image.
<ul><li>(1. Enhancement of Face Tracking) If a viewer face is lost in the face tracking, it is determined as to whether or not there is a difference from a background image. If the determination result is affirmative, a face position obtained by the face tracking module before the viewer face is lost are used.</li><li>(2. Increase of Accuracy of Face Detection) If a face has been detected by a face detector but a difference from a background image is approximately equal to zero, it is determined that the detected face is an erroneous one, and corresponding face position coordinates are not used.</li></ul></p><p id="p0070" num="0070">A camera image with minimum inter-frame differences is stored in the buffer as the background image, and a camera image is stored in the buffer as a reference image every frame or every several frames. The background image is updated every several hours, and a background image in the same time slot as a current image is used.</p><p id="p0071" num="0071">The above-described embodiment enables the face tracking, which is robust to a face image variation due to a variation in illumination, face orientation, or the like. Furthermore, the probability of erroneous detection (that is, detection of an object other than a face) can be reduced.</p><p id="p0072" num="0072">The invention is not limited to the above embodiment, and can be practiced in such a manner that constituent elements are modified in various manners without departing from the spirit and scope of the invention.</p><p id="p0073" num="0073">Also, various inventive concepts may be conceived by properly combining plural constituent elements disclosed in the embodiment. For example, several ones of the constituent elements of the embodiment may be omitted. Furthermore, constituent elements of different embodiments may be combined appropriately.</p></description><claims mxw-id="PCLM56982038" lang="EN" load-source="patent-office"><!-- EPO <DP n="13"> --><claim id="c-en-0001" num="0001"><claim-text>A video display apparatus comprising:
<claim-text>an image acquiring module configured to acquire an image captured by an imaging device;</claim-text>
<claim-text>a face-dictionary face detector configured to search the captured image acquired by the image acquiring module for a portion that coincides with a face pattern in a human face dictionary;</claim-text>
<claim-text>a face determining module configured to evaluate the portion based on the captured image and a background image acquired in advance; and</claim-text>
<claim-text>a face tracking module configured to track a face based on a feature quantity of the face pattern and a result of the evaluation by the face determining module.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The apparatus of claim 1, further comprising:
<claim-text>a background image buffer configured to acquire, as the background image, the captured image and buffer the acquired background image.</claim-text></claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The apparatus of claim 1, further comprising:
<claim-text>a storage configured to store face detection history data relating to the human face dictionary, which is used to search for the portion.</claim-text></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The apparatus of claim 2, the background image is acquired in frame units of the captured image and buffered.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>A video display method comprising:
<claim-text>acquiring a captured image;</claim-text>
<claim-text>searching the captured and acquired image for a portion that coincides with a face pattern in a human face dictionary;</claim-text>
<claim-text>evaluating the portion based on the captured image and a background image acquired in advance; and</claim-text>
<claim-text>tracking a face based on a feature quantity of the face pattern and a result of the evaluating.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW16670432" load-source="patent-office"><!-- EPO <DP n="14"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="163" he="201" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="15"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="16"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="135" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="17"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="165" he="120" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="18"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="151" he="194" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="19"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="123" he="194" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="20"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="126" he="201" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
