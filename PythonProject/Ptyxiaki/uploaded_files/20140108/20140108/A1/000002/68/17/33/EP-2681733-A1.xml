<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681733-A1" country="EP" doc-number="2681733" kind="A1" date="20140108" family-id="46753308" file-reference-id="252638" date-produced="20180822" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586073" ucid="EP-2681733-A1"><document-id><country>EP</country><doc-number>2681733</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12706128-A" is-representative="NO"><document-id mxw-id="PAPP154848265" load-source="docdb" format="epo"><country>EP</country><doc-number>12706128</doc-number><kind>A</kind><date>20120124</date><lang>EN</lang></document-id><document-id mxw-id="PAPP209887896" load-source="docdb" format="original"><country>EP</country><doc-number>12706128.1</doc-number><date>20120124</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140547117" ucid="US-201113285971-A" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201113285971</doc-number><kind>A</kind><date>20111031</date></document-id></priority-claim><priority-claim mxw-id="PPC140557049" ucid="US-201161449475-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201161449475</doc-number><kind>P</kind><date>20110304</date></document-id></priority-claim><priority-claim mxw-id="PPC140547035" ucid="US-2012022383-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2012022383</doc-number><kind>W</kind><date>20120124</date></document-id></priority-claim></priority-claims><technical-data><classification-ipc><edition>7</edition><main-classification>G10L  11/00</main-classification></classification-ipc><classifications-ipcr><classification-ipcr mxw-id="PCL1989319020" load-source="docdb">G10L  15/10        20060101ALI20120927BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989319911" load-source="docdb">G10L  15/20        20060101ALI20120927BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1927412835" load-source="docdb" scheme="CPC">G10L  15/20        20130101 LI20160613BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1927413370" load-source="docdb" scheme="CPC">G10L  15/10        20130101 FI20160613BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1927416635" load-source="docdb" scheme="CPC">G10L  25/00        20130101 LI20160613BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132362852" lang="DE" load-source="patent-office">SYSTEM UND VERFAHREN ZUR ERKENNUNG VON UMGEBUNGSKLÄNGEN</invention-title><invention-title mxw-id="PT132362853" lang="EN" load-source="patent-office">SYSTEM AND METHOD FOR RECOGNIZING ENVIRONMENTAL SOUND</invention-title><invention-title mxw-id="PT132362854" lang="FR" load-source="patent-office">SYSTÈME ET PROCÉDÉ DE RECONNAISSANCE D'UN SON AMBIANT</invention-title><citations><non-patent-citations><nplcit><text>See references of WO 2012121809A1</text><sources><source mxw-id="PNPL67455789" load-source="docdb" name="SEA"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR919541959" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>QUALCOMM INC</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR919538028" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>QUALCOMM INCORPORATED</last-name></addressbook></applicant><applicant mxw-id="PPAR919016371" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>QUALCOMM Incorporated</last-name><iid>101027011</iid><address><street>Attn: International IP Administration 5775 Morehouse Drive</street><city>San Diego, California 92121-1714</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919513276" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>HWANG KYU WOONG</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919538064" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>HWANG, KYU WOONG</last-name></addressbook></inventor><inventor mxw-id="PPAR919006701" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>HWANG, KYU WOONG</last-name><address><street>5775 Morehouse Drive</street><city>San Diego, California 92121-1714</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919525235" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>KIM TAESU</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919542352" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>KIM, TAESU</last-name></addressbook></inventor><inventor mxw-id="PPAR919008255" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>KIM, TAESU</last-name><address><street>5775 Morehouse Drive</street><city>San Diego, California 92121-1714</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919512189" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>YOU KISUN</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919531157" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>You, Kisun</last-name></addressbook></inventor><inventor mxw-id="PPAR919016850" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>You, Kisun</last-name><address><street>5775 Morehouse Drive</street><city>San Diego, California 92121-1714</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919010742" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Bentall, Mark James</last-name><iid>101259397</iid><address><street>Reddie &amp; Grose LLP 16 Theobalds Road</street><city>London WC1X 8PL</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2012022383-W"><document-id><country>US</country><doc-number>2012022383</doc-number><kind>W</kind><date>20120124</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012121809-A1"><document-id><country>WO</country><doc-number>2012121809</doc-number><kind>A1</kind><date>20120913</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549844240" load-source="docdb">AL</country><country mxw-id="DS549913154" load-source="docdb">AT</country><country mxw-id="DS549833738" load-source="docdb">BE</country><country mxw-id="DS549832600" load-source="docdb">BG</country><country mxw-id="DS549830914" load-source="docdb">CH</country><country mxw-id="DS549833739" load-source="docdb">CY</country><country mxw-id="DS549913155" load-source="docdb">CZ</country><country mxw-id="DS549844241" load-source="docdb">DE</country><country mxw-id="DS549833740" load-source="docdb">DK</country><country mxw-id="DS549833741" load-source="docdb">EE</country><country mxw-id="DS549755912" load-source="docdb">ES</country><country mxw-id="DS549832601" load-source="docdb">FI</country><country mxw-id="DS549830915" load-source="docdb">FR</country><country mxw-id="DS549844246" load-source="docdb">GB</country><country mxw-id="DS549833758" load-source="docdb">GR</country><country mxw-id="DS549844247" load-source="docdb">HR</country><country mxw-id="DS549913156" load-source="docdb">HU</country><country mxw-id="DS549755917" load-source="docdb">IE</country><country mxw-id="DS549833759" load-source="docdb">IS</country><country mxw-id="DS549832606" load-source="docdb">IT</country><country mxw-id="DS549833760" load-source="docdb">LI</country><country mxw-id="DS549832607" load-source="docdb">LT</country><country mxw-id="DS549763576" load-source="docdb">LU</country><country mxw-id="DS549832608" load-source="docdb">LV</country><country mxw-id="DS549832609" load-source="docdb">MC</country><country mxw-id="DS549763593" load-source="docdb">MK</country><country mxw-id="DS549763594" load-source="docdb">MT</country><country mxw-id="DS549755918" load-source="docdb">NL</country><country mxw-id="DS549756666" load-source="docdb">NO</country><country mxw-id="DS549755919" load-source="docdb">PL</country><country mxw-id="DS549832622" load-source="docdb">PT</country><country mxw-id="DS549913157" load-source="docdb">RO</country><country mxw-id="DS549832623" load-source="docdb">RS</country><country mxw-id="DS549755920" load-source="docdb">SE</country><country mxw-id="DS549830916" load-source="docdb">SI</country><country mxw-id="DS549756667" load-source="docdb">SK</country><country mxw-id="DS549756668" load-source="docdb">SM</country><country mxw-id="DS549763595" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA100066376" ref-ucid="WO-2012121809-A1" lang="EN" load-source="patent-office"><p num="0000">A method for recognizing an environmental sound in a client device in cooperation with a server is disclosed. The client device includes a client database having a plurality of sound models of environmental sounds and a plurality of labels, each of which identifies at least one sound model. The client device receives an input environmental sound and generates an input sound model based on the input environmental sound. At the client device, a similarity value is determined between the input sound model and each of the sound models to identify one or more sound models from the client database that are similar to the input sound model. A label is selected from labels associated with the identified sound models, and the selected label is associated with the input environmental sound based on a confidence level of the selected label.</p></abstract><abstract mxw-id="PA100335484" ref-ucid="WO-2012121809-A1" lang="EN" source="national office" load-source="docdb"><p>A method for recognizing an environmental sound in a client device in cooperation with a server is disclosed. The client device includes a client database having a plurality of sound models of environmental sounds and a plurality of labels, each of which identifies at least one sound model. The client device receives an input environmental sound and generates an input sound model based on the input environmental sound. At the client device, a similarity value is determined between the input sound model and each of the sound models to identify one or more sound models from the client database that are similar to the input sound model. A label is selected from labels associated with the identified sound models, and the selected label is associated with the input environmental sound based on a confidence level of the selected label.</p></abstract><abstract mxw-id="PA100066377" ref-ucid="WO-2012121809-A1" lang="FR" load-source="patent-office"><p num="0000">La présente invention concerne un procédé de reconnaissance d'un son ambiant dans un dispositif client en coopération avec un serveur. Le dispositif client contient une base de données cliente comportant une pluralité de modèles de sons ambiants et une pluralité d'étiquettes, chacune d'elles identifiant au moins un modèle de son. Le dispositif client reçoit un son ambiant d'entrée et crée un modèle de son d'entrée sur la base du son ambiant d'entrée. Au niveau du dispositif client, il est déterminé une valeur de similarité entre le modèle de son d'entrée et chacun des modèles de sons de façon à identifier, à partir de la base de données cliente, un ou plusieurs modèles de sons similaires au modèle de son d'entrée. Parmi les étiquettes associées aux modèles de sons identifiés, il est sélectionné une étiquette qui est ensuite associée au son ambiant d'entrée sur la base d'un niveau de confiance de l'étiquette sélectionnée.</p></abstract><abstract mxw-id="PA100335485" ref-ucid="WO-2012121809-A1" lang="FR" source="national office" load-source="docdb"><p>La présente invention concerne un procédé de reconnaissance d'un son ambiant dans un dispositif client en coopération avec un serveur. Le dispositif client contient une base de données cliente comportant une pluralité de modèles de sons ambiants et une pluralité d'étiquettes, chacune d'elles identifiant au moins un modèle de son. Le dispositif client reçoit un son ambiant d'entrée et crée un modèle de son d'entrée sur la base du son ambiant d'entrée. Au niveau du dispositif client, il est déterminé une valeur de similarité entre le modèle de son d'entrée et chacun des modèles de sons de façon à identifier, à partir de la base de données cliente, un ou plusieurs modèles de sons similaires au modèle de son d'entrée. Parmi les étiquettes associées aux modèles de sons identifiés, il est sélectionné une étiquette qui est ensuite associée au son ambiant d'entrée sur la base d'un niveau de confiance de l'étiquette sélectionnée.</p></abstract><description mxw-id="PDES51372841" ref-ucid="WO-2012121809-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> SYSTEM AND METHOD FOR RECOGNIZING ENVIRONMENTAL SOUND CLAIM OF PRIORITY UNDER 35 U.S.C. §119 </p><p id="p0002" num="0002">This application is based upon and claims the benefit of priority from U.S. Provisional Patent Application No. 61/449,475, filed on March 4, 2011, the entire contents of which are incorporated herein by reference. </p><p id="p0003" num="0003">TECHNICAL FIELD </p><p id="p0004" num="0004">The present disclosure relates generally to sound recognition. More specifically, the present disclosure relates to a system and method for recognizing an environmental sound of a client device in communication with a server. </p><p id="p0005" num="0005">BACKGROUND </p><p id="p0006" num="0006">Sound-based environment recognition technology has been used in modern mobile communication systems for providing user services. For example, a device such as a mobile device may be configured to detect ambient sound, and recognize the sound environment surrounding the device. The recognized sound environment may be used to provide the user of the device with targeted information such as advertisement information and social networking information. </p><p id="p0007" num="0007">In daily lives, mobile device users may be subject to a large number of different sound environments. In many cases, the devices may not be capable of storing or handling the sound environmental data due to limited storage capacity or processing power. Further, if a device is used to recognize only ambient sounds of its surroundings, the device may not be able to accurately recognize ambient sounds of new surroundings. For example, if a mobile device is used to capture and recognize the ambient sounds at a new place, such as vacation resort, the device may not accurately recognize sound environment of the surroundings. </p><p id="p0008" num="0008">In addition, users may carry their mobile devices to similar sound environments such as classrooms, meeting rooms, assembly halls, etc. In such instances, the mobile devices may not be able to distinguish and accurately recognize different sound environments. 
<!-- EPO <DP n="4"/>-->
 Thus, there is a need for a method and an apparatus that enable a mobile device to recognize a broader scope of environmental sounds than may be available to the device while increasing the accuracy of environmental sound recognition. </p><p id="p0009" num="0009">SUMMARY The present disclosure provides systems and methods for recognizing an environmental sound of a device in communication with a server. </p><p id="p0010" num="0010">According to one aspect of the present disclosure, a method for recognizing an environmental sound in a client device is disclosed. The method includes providing a client database including a plurality of sound models of environmental sounds and a plurality of labels, each of which identifies at least one sound model. An input environmental sound is received, and input sound model is generated based on the input environmental sound. A similarity values between input sound model and each of the sound models is determined to identify one or more sound models from the client database that are similar to the input sound model. Further, a label is selected from labels associated with the identified sound models. The selected label is associated with the input environmental sound based on a confidence level of the selected label. This disclosure also describes a device, an apparatus, a combination of means, and a computer-readable medium relating to this method. </p><p id="p0011" num="0011">According to another aspect of the present disclosure, a method for recognizing an environmental sound received from a client device in a server is disclosed. The method includes providing a server database including a plurality of sound models of environmental sounds and a plurality of labels, each of which identifies at least one sound model. The method further includes receiving an input sound model representing an input environmental sound from the client device. A similarity value between the input sound model and each of the sound models is determined to identify one or more sound models from the server database that are similar to the input sound model. Then, a label is selected from labels associated with the identified sound models. The selected label is associated with the input environmental sound based on a confidence level of the selected level. This disclosure also describes a server, a computer system, a combination of means, and a computer-readable medium relating to this method. 
<!-- EPO <DP n="5"/>-->
 BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0012" num="0012">FIG. 1 illustrates an exemplary system including a plurality of client devices and a server which communicate via a network. </p><p id="p0013" num="0013">FIG. 2 depicts an exemplary configuration of a client device for recognizing an environmental sound according to one embodiment of the present disclosure. </p><p id="p0014" num="0014">FIG. 3 depicts an exemplary configuration of a server for recognizing an environmental sound according to one embodiment of the present disclosure. </p><p id="p0015" num="0015">FIG. 4A illustrates a flowchart of a method, performed by a client device, for recognizing an environmental sound according to one embodiment of the present disclosure. </p><p id="p0016" num="0016">FIGS. 4B to 4D are more detailed flowcharts of 408 in FIG. 4A for selecting a label for an input environmental sound and determining a confidence level of the label according to some embodiments of the present disclosure. </p><p id="p0017" num="0017">FIG. 5 illustrates a flowchart of a method for generating a sound model by constructing a histogram based on feature vectors extracted from an input environmental sound according to one embodiment of the present disclosure. </p><p id="p0018" num="0018">FIG. 6 illustrates a flowchart of a method, performed by a client device, for recognizing an input environmental sound by using information associated with the input environmental sound according to one embodiment of the present disclosure. </p><p id="p0019" num="0019">FIG. 7 is a schematic diagram of a database structure of location-dependent and/or time- dependent sound models. </p><p id="p0020" num="0020">FIG. 8 shows an exemplary client or server database of sound models and information associated with the sound models, according to one embodiment of the present disclosure. </p><p id="p0021" num="0021">FIG. 9 illustrates a flowchart of a method, performed by a server, for recognizing an environmental sound according to one embodiment of the present disclosure. 
<!-- EPO <DP n="6"/>-->
 FIG. 10 illustrates a flowchart of a method for updating a database of sound models by using location and/or time information associated with sound models according to one embodiment of the present disclosure. </p><p id="p0022" num="0022">FIG. 11 shows a configuration of an exemplary mobile device in a wireless communications system. </p><p id="p0023" num="0023">DETAILED DESCRIPTION </p><p id="p0024" num="0024">Various embodiments are now described with reference to the drawings, wherein like reference numerals are used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of one or more embodiments. It may be evident, however, that such embodiment(s) may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to facilitate describing one or more embodiments. </p><p id="p0025" num="0025">FIG. 1 illustrates an exemplary system 100 including a plurality of client devices 120, 150, 160, 170, and 180 and a server 130, which communicate via a network 140. Each of the client devices 120, 150, 160, 170, and 180 is configured to store a database of sound models for recognizing environmental sounds. The server 130 stores a database of sound models that may be provided to any one of the client devices 120, 150, 160, 170, and 180. Each of the client devices 120, 150, 160, 170, and 180 is configured to capture environmental sound of the surroundings (i.e., input environmental sound) through a sound sensor (e.g., microphone) and recognize the environmental sound to generate a label for the captured environmental sound based on the stored database. If any client device is not able to recognize its captured sound, the device sends a request to the server 130 to recognize the sound. The server 130 is configured to receive environmental sounds from one or more client devices and recognize the environmental sounds by generating associated labels. The labels are then transmitted to the requesting client devices. </p><p id="p0026" num="0026">The present disclosure may be practiced in a crowd-sourcing framework that aggregates sound models from any of the client devices 120, 150, 160, 170, and 180, and shares the aggregated sound models among the client devices 120, 150, 160, 170, and 180. The 
<!-- EPO <DP n="7"/>-->
 crowd-sourcing framework improves the sound recognition accuracy of the client devices 120, 150, 160, 170, and 180 by sharing the sound models with the server 130. </p><p id="p0027" num="0027">The client devices 120, 150, 160, 170, and 180 may be any suitable mobile device, such as a cellular phone, a laptop, a car accessory, or the like, equipped with a sound capturing capability, e.g., a microphone, and communication capability through a data and/or communication network. Alternatively, the client devices 120, 150, 160, 170, and 180 may be any device fixed at a particular location, such as a security camera, a desktop computer, etc. </p><p id="p0028" num="0028">FIG. 2 depicts an exemplary configuration of the client device 120 for recognizing an environmental sound according to one embodiment of the present disclosure. The client device 120 includes a sound sensor 210, a sound recognition unit 220, a communication unit 230, a user interface 240, a database updating unit 250, and a client database 260. Although the client device 120 is illustrated in FIG. 2, the other client devices 150, 160, 170, and 180 may implement a similar configuration. The above described units in the client device 120 may be implemented by hardware, software executed in one or more processors, and/or a combination thereof. </p><p id="p0029" num="0029">The sound sensor 210 is configured to capture an input environmental sound. The sound sensor 210 may include, e.g., one or more microphones or any other type of sound sensors used to measure, record, or otherwise convey any aspect of </p><p id="p0030" num="0030">environmental sounds of the client device 120. Such sound sensor 210 may employ software and/or hardware adapted for sound capturing capability in the client device 120. The sound sensor 210 may also be a microphone provided in the client device 120 that is used for telephone calls or video recording. </p><p id="p0031" num="0031">In one embodiment, the client device 120 may be configured to perform a background process for detecting environmental sounds constantly via the sound sensor 210. </p><p id="p0032" num="0032"> Alternatively, the client device 120 may be configured to detect environmental sounds at a predetermined time, at predetermined intervals, or in response to a user's command. </p><p id="p0033" num="0033">Environmental sounds detected by a client device 120 may include any sounds of the surroundings of the client device 120. Such sounds may include, for example, engine noise, honking of a horn, subway noise, animal sounds, human voices, footsteps, 
<!-- EPO <DP n="8"/>-->
 knocking, tableware sounds, a whistle, etc. </p><p id="p0034" num="0034">The client database 260 stores sound models representing environmental sounds. A sound model is used for sound recognition as a statistical representation of an environmental sound that makes up each recognition target sound. The sound models may be generated based on training sound databases by using any techniques well known in the art including, but not limited to, Gaussian Mixture Model (GMM) or Hidden Markov Model (HMM). In some embodiments, the sound model may be represented as a Gaussian histogram, which will be described in detail below. A storage size and processing time for the sound model may be reduced using the Gaussian histogram. The client database 260 includes predetermined sound models and may also include associated information, such as anti-models, labels, thresholds, location, time, prior occurrence information, etc. Among such information, a label may be used to identify an input environmental sound. For example, an input environmental sound detected by a client device 120 can be associated with a label identifying one or more sound models, for example, by indicating the location and type of activity (or context) for the environmental sound such as a street and traffic information. The location of the environmental sound may indicate a street, an office, a subway station, a car, a restaurant, etc. Also, the activity information of the environmental sound may indicate music, speech, television, walking, traffic, etc. Thus, sound models associated with a location may be represented differently according to different activities occurring in the same location, and sound models associated with an activity may be represented differently according to different locations where the activity occurs. </p><p id="p0035" num="0035">The sound recognition unit 220 recognizes an input environmental sound captured by the sound sensor 210, by accessing the client database 260, to produce a label for the input environmental sound as a recognition result thereof. Specifically, the sound recognition unit 220 generates a sound model for the input environmental sound, and determines a similarity value between the input sound model and each of the predetermined sound models stored in the client database 260 to identify, from the client database 260, one or more sound models similar to the input sound model. Further, the sound recognition unit 220 selects a label associated with a largest number of sound models among the identified sound models and determines a confidence level of the selected label, as described in detail below with reference to FIGS. 4B to 4D. The 
<!-- EPO <DP n="9"/>-->
 sound recognition unit 220 then outputs the selected label based on the confidence level of the selected label. For example, if the confidence level is equal to or greater than a predetermined confidence threshold, the sound recognition unit 220 associates the selected label with the input environmental sound for output to a user as a recognition result. </p><p id="p0036" num="0036">The communication unit 230 is configured to send a request to the server 130 via the network 140 to recognize the input environmental sound if the confidence level is less than the predetermined confidence threshold. In such case, the communication unit 230 receives a label for the input environmental sound and/or one or more sound models corresponding to the input environmental sound from the server 130. The </p><p id="p0037" num="0037">communication unit 230 may also receive, from the server 130, additional information such as a location, a time, an anti-model, a label, prior occurrence information, and/or a threshold associated with each of the one or more sound models received from the server 130. The communication unit 230 may be further configured to transmit the input sound model to the server 130 for requesting the server 130 to recognize the input environmental sound. Further, the communication unit 230 may transmit an input location and input time associated with the input sound model. The input location indicates a particular location where the input environmental sound is detected, and the input time indicates a particular time when the input environmental sound is detected. The input location may be obtained using a location service such as Global Positioning System (GPS) through a network. Further, the input time may be obtained from a clock unit (not shown) in the client device 120 or a network service provider. </p><p id="p0038" num="0038">The user interface 240 is configured to allow the user to confirm if the label provided either from the sound recognition unit 220 or from the server 130 is correct. Through the user interface 240, the user may provide an input to confirm the accuracy of the label or provide a correct label if the provided label is incorrect. </p><p id="p0039" num="0039">The database updating unit 250 updates the client database 260 with the one or more sound models and the additional information, as described above, including the label received from the server 130. In some embodiments, the database updating unit 250 is further configured to update the client database 260 based on the user input provided 
<!-- EPO <DP n="10"/>-->
 through the user interface 240. </p><p id="p0040" num="0040">FIG. 3 depicts an exemplary configuration of the server 130 for recognizing an environmental sound according to one embodiment of the present disclosure. The server 130 includes a sound recognition unit 310, a communication unit 320, a database updating unit 330, and a server database 340. </p><p id="p0041" num="0041">The server database 340 stores sound models representing environmental sounds. In some embodiments, the sound models may be represented as Gaussian histograms. The server database 340 may include predetermined sound models and associated information, such as anti-models, labels, thresholds, locations, times, prior occurrence information, etc. Among such information, a label may be utilized for identifying an input environmental sound. </p><p id="p0042" num="0042">The communication unit 320 is configured to receive a request for recognition of an input environmental sound from the client device 120 via the network 140. Further, the communication unit 320 receives, from the client device 120, an input sound model which characterizes an input environmental sound. In some embodiments, the communication unit 320 receives an input sound model and associated information including a location, a time, and a label, etc. </p><p id="p0043" num="0043">The sound recognition unit 310 recognizes the input environmental sound by accessing the server database 340, to determine a label for the input environmental sound as a recognition result thereof. Specifically, the sound recognition unit 310 determines a similarity value between the input sound model and each of the predetermined sound models stored in the server database 340 to identify one or more sound models from the server database 340 that are similar to the input sound model. Further, the sound recognition unit 310 selects a label associated with a largest number of sound models among the identified sound models and determines a confidence level of the selected label, as described in detail below with reference to FIGS. 4B to 4D. </p><p id="p0044" num="0044">The communication unit 320 is further configured to provide the selected label and/or one or more sound models similar to the input sound model to the client device 120 via the network 140. Also, the communication unit 320 may provide additional information associated with each of the one or more sound models, such as an anti-model, a label, a 
<!-- EPO <DP n="11"/>-->
 threshold, a location, a time, prior occurrence information, to the client device 120. The above information including sound models may be used by the client device 120 for sound recognition. </p><p id="p0045" num="0045">As described above, the user interface 240 of the client device 120 may receive an input from the user to confirm if the selected label from the server 130 is correct. The client device 120 may send the user input to the server 130 via the network 140. The database updating unit 330 then updates the server database 340 based on the user input on the selected label. </p><p id="p0046" num="0046">In some embodiments, the sound recognition unit 310 identifies one or more sound models, stored in the server database 340, based on the associated locations and times that match the location and time of the input sound model. In the sound recognition unit 310, a sound model having a greatest similarity to the input sound model is identified among the one or more sound models. The database updating unit 330 updates the server database 340 by adding the input sound model or merging the input sound model and the sound model having the greatest similarity, which will be described in detail with reference to FIG 10. </p><p id="p0047" num="0047">FIG. 4A illustrates a flowchart of a method, performed by a client device, for recognizing an environmental sound in accordance with one embodiment of the present disclosure. The sound sensor 210 of the client device 120 receives an input </p><p id="p0048" num="0048">environmental sound by detecting and capturing an environmental sound of the surroundings, at 402. The sound recognition unit 220 of the client device 120 generates an input sound model based on the input environmental sound, at 404. The input sound model may be generated based on a sound feature extracted from the input </p><p id="p0049" num="0049">environmental sound. The sound feature may be extracted from the input environmental sound, using any suitable signal processing scheme, including speech compression, enhancement, recognition, and synthesis methods. For example, such signal processing scheme may employ MFCC (Mel-frequency cepstral coefficients), LPC (linear predictive coding), and/or LSP (line spectral pair) techniques, which are well-known methods for speech recognition or speech codec. For example, a sound feature may be extracted from an input environmental sound under the MFCC method as follows. A frame of an input 
<!-- EPO <DP n="12"/>-->
 environmental sound in the time domain (e.g., raw sound signal) is multiplied by a windowing function, e.g., hamming window. The sound signal is then Fourier transformed to the frequency domain, and a power is calculated for each band in the spectrum of the transformed signal in the frequency domain. A logarithm operation and a discrete cosine transform (DCT) operation are performed on each calculated power to obtain DCT coefficients. A mean value over a period of a predetermined time in the past is subtracted from each DCT coefficient for binarization and a set of the binarization results constitutes the sound feature. </p><p id="p0050" num="0050">Further, the sound recognition unit 220 of the client device 120 determines a similarity value between the input sound model and each of the sound models stored in the client database 260 to identify one or more sound models that are similar to the input sound model, at 406. For ease of reference, the one or more sound models identified as being similar to the input sound model are also referred to as similar sound models herein. A similarity value may be represented by a probability that one sound model corresponds to another sound model. For example, a similarity value can be determined by the distance between the input sound model and each of the sound models in the client database 260. In such case, one or more sound models having a distance less than a predetermined distance from the input sound model are identified as being similar to the input sound model. Additionally or alternatively, a predetermined number of sound models having the smallest distances from the input sound model may be identified as being similar to the input sound model. </p><p id="p0051" num="0051">In some embodiments, the sound models are represented by predetermined histograms (e.g. Gaussian histograms). In this case, the distance may be calculated by summing the differences in the histogram counts between a histogram representing the input sound model ("input histogram") and each of the predetermined histograms. 
<!-- EPO <DP n="13"/>-->
 In some other embodiments, the distance between histograms may be calculated by using Kullback-Leibler (KL) divergence. KL divergence is defined as a weighted log ratio between two histograms with the more frequently occurring histogram having a larger weight. In some embodiments, the KL divergence, DKL between two histograms H(A<sub>m</sub>) and H(A<sub>n</sub>) representing sound clips A<sub>m</sub> and A„, respectively, is given as follows: h<sub>k</sub> (A<sub>m</sub> ) </p><p id="p0052" num="0052"> ¾ (HKJ II H )) =∑¾ (AJiog (Equation 1) h i ) </p><p id="p0053" num="0053">where h^A<sub>m</sub>) and hk(A<sub>n</sub>) represent histogram counts, and k is an index of Gaussian clusters that represent the entire set of sound features in the GMM. </p><p id="p0054" num="0054">It should be noted that Equation 1 above is not symmetric for H(A<sub>m</sub>) and H(A<sub>n</sub>), and thus, the distance calculation will be affected by which histograms are used as H(A<sub>m</sub>) or H(A„) between the input histogram and the predetermined histogram. For example, a first KL divergence may be calculated by setting H(A<sub>m</sub>) as the predetermined histogram and setting H(A<sub>n</sub>) as the input histogram. A second KL divergence may be calculated by setting H(A<sub>m</sub>) to the input histogram and setting H(A„) to the predetermined histogram. In some embodiments, both KL divergences may be calculated and the average of the two KL divergences may be used as the distance. It is noted that other schemes for calculating the similarity of two sound models may also be used without departing from the spirit and scope of the present disclosure. </p><p id="p0055" num="0055">At 408, the sound recognition unit 220 of the client device 120 then selects a label for the input environmental sound and determines a confidence level of the selected label. The specific operations for selecting the label and determining the confidence level are described below in detail with reference to FIGS. 4B to 4D. </p><p id="p0056" num="0056">The sound recognition unit 220 of the client device 120 compares the confidence level to a predetermined confidence threshold, which may be set to a fixed value for all predetermined sound models, at 410. Alternatively, different confidence thresholds may be assigned to different predetermined sound models. Further, the thresholds may be changed according to a user input. For example, if a user indicates that the selected label is incorrect, then the threshold corresponding to the recognized sound model may be adjusted to a higher value. 
<!-- EPO <DP n="14"/>-->
 If the confidence level is determined to be equal to or greater than the predetermined confidence threshold ("NO" at 410), the sound recognition unit 220 outputs the selected label to a user, at 416. Otherwise, if the confidence level is less than the predetermined confidence threshold ("YES" at 410), the communication unit 230 of the client device 120 transmits the input sound model to the server 130 for requesting to recognize the input environmental sound, at 412. In response to the request, the server 130 recognizes the input environmental sound for the client device 120 to select a label for the input environmental sound and/or one or more sound models, as described in further detail below. </p><p id="p0057" num="0057">The communication unit 230 of the client device 120 receives the selected label and/or one or more sound models from the server 130, for updating the client database 260, at 414. Alternatively, if the server 130 fails to recognize the input environmental sound, the communication unit 230 may receive an indication of a failure in recognizing the input environmental sound. </p><p id="p0058" num="0058">After receiving the selected label and/or one or more sound models from the server 130, the client device 120 outputs the selected label to the user and receives a user input, at 416. In this operation, the user is allowed to verify the accuracy of the selected label. For example, the user may confirm if the selected label is correct, or provide a correct label if the selected label is inaccurate. In one embodiment, if the user fails to provide a response on the selected label, it may be assumed that the selected label is correct. </p><p id="p0059" num="0059">Based on the selected label, the one or more sound models received from the server 130 and/or the user input on the selected label, the client database 260 is updated, at 418. For example, the updating operation may be implemented by adding the selected label and an associated sound model as a new entry in the client database 260, so that the newly added sound model and selected label may be used later in recognizing input environmental sound. Alternatively, the updating operation may be implemented by merging or replacing a predetermined sound model in the client database 260, with the sound model associated with the selected label. In some embodiments, the input from the user in the client device 120 is transmitted to the server 130 so that the server 130 may update the server database 340 in a manner similar to updating the client database 260. In some other embodiments, the updating operation may be implemented based on 
<!-- EPO <DP n="15"/>-->
 the received sound models and other types of information described above. </p><p id="p0060" num="0060">FIG. 4B is a more detailed flowchart of 408 in FIG. 4A, performed by the sound recognition unit 220, for selecting a label for an input environmental sound and determining a confidence level of the selected label according to one embodiment of the present disclosure. At 422, the sound recognition unit 220 selects a first label associated with the largest number of sound models among the one or more sound models identified as being similar to the input sound model. For selecting the label, the labels of the similar sound models can be examined and a number of the similar sound models may be determined for each label. Thus, the first label is a label attributed to the largest number of sound models. Then, a second label associated with the second largest number of sound models among the similar sound models is selected, at 424. Selecting the second label may be based on the number of similar sound models determined in connection with 422. The sound recognition unit 220 then calculates a sum of similarity values (e.g., probabilities or distances) between the input sound model and the sound models associated with the first label, at 426, and a sum of similarity values between the input sound model and the sound models associated with the second label, at 428. The difference between the above two sums is determined to be the confidence level, at 430. In an alternative embodiment, the sum associated with the first label may be determined to be the confidence level. FIG. 4C is a more detailed flowchart of 408 in FIG. 4A, performed by the sound recognition unit 220, according to another embodiment. Initially, the similar sound models are grouped into one or more sets of sound models based on their associated labels, at 432 such that each set of sound models is associated with a unique label. For each set of sound models, the sound recognition unit 220 calculates a sum of the similarity values between the input sound model and each of the sound models, at 434. Among the sums of similarity values, the sound recognition unit 220 determines and selects a label associated with the largest sum, at 436. A second largest sum is identified among the sums of the similarity values, and the confidence level of the selected label is determined based on a difference between the largest sum and the second largest sum, at 438. In an alternative embodiment, the largest sum may be determined to be the confidence level. 
<!-- EPO <DP n="16"/>-->
 FIG. 4D is a more detailed flowchart of 408 in FIG. 4A, performed by the sound recognition unit 220, according to one embodiment. At 442, the sound recognition unit 220 selects a label associated with the largest number of sound models among the similar sound models, as described above with reference to 422 of FIG. 4B. </p><p id="p0061" num="0061">Alternatively, a label associated with the set of sound models having the largest sum of similarity values may be selected, as described above with reference to 432 to 436 of FIG. 4C. Then, a sound model having the greatest similarity to the input sound model is identified among the sound models associated with the selected label, at 444. </p><p id="p0062" num="0062">A similarity value between the identified sound model and its anti-model is calculated, at 446. The anti-model may be predetermined and loaded from the client database 260, which includes both sound models and their anti-models. An anti-model may be generated based on sound models, which are similar to but distinguishable from a sound model associated with the anti-model, so as to improve the recognition accuracy. When using descriptive labels, users may verify a recognized label for some of the sound models as being correct or incorrect. Sound models, for which the recognized label has been determined to be incorrect, can be merged or averaged to be an anti-model associated with the label and a sound model for which the recognized label has been determined to be correct. At 448, the calculated similarity value determined at 446 is set and used as the confidence level. As described above, an input sound model may be represented by a histogram. In some embodiments, the input sound model is generated from the input environmental sound by constructing a histogram (e.g. a Gaussian histogram) based on sound features of the input environmental sound. Environmental sounds obtained in different environments may have different distributions of sound features. The distribution of sound features may be represented by a "histogram." Thus, a histogram of sound features may be used rather than respective sound features in representing the sound environments. In general, a histogram may represent an approximation of the frequency distribution of data points such as feature vectors representing sound features. </p><p id="p0063" num="0063">An environmental sound captured for a relatively long period of time (e.g., several seconds) generally provides a better representation of a sound environment than an environmental sound captured in a shorter period of time. In addition, sound features 
<!-- EPO <DP n="17"/>-->
 extracted from an environmental sound generally exhibit a characteristic distribution for the sound environment. For example, an environmental sound obtained in a crowded restaurant may exhibit a greater number of sound features relating to tableware sounds, human voices, background music, etc., but a fewer number of sound features relating to outdoor sounds such as engine noise, wind noise, etc. Thus, a histogram of sound features can be used to represent a sound model. </p><p id="p0064" num="0064">FIG. 5 illustrates a flowchart of a method for generating a sound model by constructing a histogram based on feature vectors extracted from an input environmental sound according to one embodiment of the present disclosure. The method shown in FIG. 5 may be performed at 404 in FIG. 4A to generate a histogram as an input sound model. </p><p id="p0065" num="0065">The input environmental sound detected by the sound sensor 210 is received and segmented into a sound clip of a predetermined time period, at 510. The sound clip is then divided into frames, at 520. In some embodiments, the frames are non-overlapping waveform signals and each frame has a length of a specified time period. From each of frames, a feature vector including MFCCs as components is extracted, at 530. </p><p id="p0066" num="0066">After extracting a feature vector from each frame, at 530, the probability of each Gaussian cluster for each feature vector is calculated, at 540. In this operation, P(xi) representing the probability of a feature vector x which appears in the entire distribution represented by GMM, is determined. For example, given a sound clip A<sub>m</sub> composed of L frames, a feature vector x<sub>;</sub>- from the z<sup>'</sup>-th frame that is represented by a universal GMM can be calculated as follows: </p><p id="p0067" num="0067">P(x<sub>t</sub> ) =∑f<sub>j</sub> (*/ ) =∑ w,N(*, I Mj ,∑, ) (Equation 2) i i </p><p id="p0068" num="0068">where Ρ(χΐ) represents the probability of x<sub>;</sub>- appearing in the entire distribution represented by GMM,^(¾) represents the likelihood of the y<sup>'</sup>-th Gaussian cluster for ¾, N represents a normal distribution of a Gaussian cluster having the mean value μ and the variance∑, and Wj represents the prior probability of the y<sup>'</sup>-th Gaussian cluster. 
<!-- EPO <DP n="18"/>-->
 Then, using^(¾) to represent the likelihood of the y<sup>'</sup>-th Gaussian cluster for x,, the probability of each Gaussian cluster for each feature vector can be represented by the following equation: f. (x.) </p><p id="p0069" num="0069"> P( f , \ x,-) = ^ =r—<sup>■</sup>— (Equation 3)</p><p id="p0070" num="0070">∑/ <sup>)</sup> </p><p id="p0071" num="0071"> i </p><p id="p0072" num="0072"> Equation 3 above represents the probability that the feature vector Λ¾ appears in the Gaussian cluster f, or the probability that the Gaussian cluster f includes feature vector Xi. </p><p id="p0073" num="0073">The probabilities of feature vectors extracted from the sound clip are then summed for each Gaussian cluster, to obtain a histogram count for each Gaussian cluster, at 550. Thus, the histogram count of Gaussian cluster f is the summation ofP(f- | ¾) over all feature vectors corresponding to all frames in the sound clip A<sub>m</sub>. In some embodiments, the histogram count may be normalized with the sound clip length or the number of frames L in the sound clip. For example, the histogram count of Gaussian cluster f for the sound clip A<sub>m</sub> may be represented by the following equation: h<sub>j</sub> <sup>A</sup><sub>m</sub> = Y∑<sup>p</sup>(f<sub>j</sub> k ) (Equation ^ </p><p id="p0074" num="0074">Equation 4 above may be understood as representing an approximate frequency at which the Gaussian cluster f appears in the entire sound clip A<sub>m</sub>. </p><p id="p0075" num="0075">Based on the histogram counts for the Gaussian clusters, a histogram that represents the sound clip is generated, at 550. In some embodiments, the normalized histogram representation of the sound clip A<sub>m</sub> may be given as follows: </p><p id="p0076" num="0076">H(A<sub>m</sub> ) = [h<sub>l</sub> (A<sub>m</sub> ) h ... h<sub>K</sub> (A ] (Equation 5) where K is the number of Gaussian clusters that represent the entire set of sound features in the GMM. </p><p id="p0077" num="0077">Further, as discussed above, the input environmental sound may be associated with a location where the input environmental sound is detected and/or a time when the input environmental sound is detected. Other types of information, such as the user's gender or age, device information, and the like, may also be associated with the input 
<!-- EPO <DP n="19"/>-->
 environmental sound. The location and/or time may be utilized to improve the accuracy in recognizing the input environmental sound, as described in FIG. 6. </p><p id="p0078" num="0078">FIG. 6 illustrates a flowchart of a method, performed by the client device 120, for recognizing an input environmental sound by using information associated with the input environmental sound according to one embodiment of the present disclosure. The sound recognition unit 220 of the client device 120 receives the input environmental sound detected by the sound sensor 210, at 610. The sound recognition unit 220 also receives an input location and/or time associated with the input sound model, at 610. The input location and time may be obtained internally or externally through a network, as described above. </p><p id="p0079" num="0079">The sound recognition unit 220 of the client device 120 generates an input sound model based on the input environmental sound, at 620. In some embodiments, the client device 120 obtains the input sound model by constructing a Gaussian histogram as described above, based on the input environmental sound. The input location and/or time is then compared with predetermined locations and/or times associated with predetermined sound models in the client database 260, at 630. In this operation, any type of generally known matching methods, such as string matching methods, may be employed for matching the input location and/or time to any one of the predetermined locations and/or times. </p><p id="p0080" num="0080">At 640, based on the comparison result at 630, the sound recognition unit 220 of the client device 120 compares the input sound model with predetermined sound models from the client database 260, so as to identify one or more predetermined sound models that are similar to the input sound model. In some embodiments, predetermined sound models, which are associated with predetermined locations and/or times that do not match the input location and/or time, are filtered out before being compared with the input sound model, at 640. Thus, only predetermined sound models associated with predetermined locations and/or times matching the input location and/or time are considered in recognizing the input environmental sound. Such filtering operation decreases the recognition time of the input environmental sound by reducing the number of predetermined sound models to be compared with the input sound model. The filtering operation also enhances accuracy of the recognition by ignoring predetermined 
<!-- EPO <DP n="20"/>-->
 sound models that are coincidentally similar but irrelevant to the input sound model in terms of location or time. In some embodiments, instead of being filtered, some predetermined sound models may be given a lower preference or weight in sound recognition than those matching the input location and/or time. The comparison of the sound models may be performed by calculating the distance between each of the predetermined sound models and the input sound model as described above with reference to FIG. 4A. </p><p id="p0081" num="0081">A client device 120 selects a label based on the comparison result and prior occurrence information from the client database 260, at 650. In addition to the location and/or time, this operation considers statistics related to prior occurrences of environmental sounds as prior probability in recognizing the input environmental sound. In general, the same type of environmental sounds may occur dominantly and regularly in a particular location and/or time. For example, in a populated city during rush hour, traffic sounds may have a higher probability of being detected than other type of sounds. Thus, it may be assumed that a particular set of sound models may be recognized more frequently than other sound models at a particular location and/or time. Based on this observation, an input environmental sound in a particular location and/or time may be expected to have a higher probability of matching frequently recognized sound models in such location or time. Thus, utilizing prior occurrence statistics may improve the recognition accuracy. </p><p id="p0082" num="0082">In using prior occurrence information, the client database 260 may also include prior occurrence information including sound occurrence probability information at a particular time or location. In the prior occurrence information, a higher preference or weight may be given to predetermined sound models with higher prior occurrence than predetermined sound models with lower occurrence. </p><p id="p0083" num="0083">In some embodiments, the operation for selecting the label in FIG. 6 is implemented by determining a confidence level of the selected label as described above with reference to FIGS. 4B to 4D. Although the method of FIG 6 has been described above to be performed in the client device 120, the method may also be performed in a similar manner by the server 130 in some embodiments. </p><p id="p0084" num="0084">FIG. 7 is a schematic diagram of an exemplary database structure of location-dependent 
<!-- EPO <DP n="21"/>-->
 and time-dependent sound models. The database shown in FIG. 7 may be implemented either in the client database 260 or in the server database 340. The location-dependent and time-dependent sound models may be stored along with associated locations and times in the database. As shown in FIG. 7, sound models representing the same type of environmental sound (e.g., traffic sound) may be generated and stored for different locations and times. For example, the traffic sound of Seoul may be different from that of San Diego. Thus, there may be more than one similar traffic sound models associated with different locations, one associated with the location "Seoul," and another associated with the location "San Diego." Thus, similar environmental sounds detected at different locations or times may be represented by similar sound models associated with different locations and times. </p><p id="p0085" num="0085">In some embodiments, if sound models obtained from similar locations or similar times are statistically similar to each other, those sound models may be merged with each other, which will be described in detail below with reference to FIG. 10. For example, the merging may be performed by simply averaging the sound models. </p><p id="p0086" num="0086">As there may be a huge number of sound models, in some embodiments, the sound models may be classified and stored in the database according to multi-level structured categories of times and locations, which may serve to save time in looking up a particular sound model among the stored sound models. In some embodiments, a plurality of sound models representing different sound patterns may be associated with the same kind of circumstances in similar locations and times. For example, there may be multiple types of trains or buses running in the same city, which generate different sound patterns. FIG. 8 is an exemplary client or server database of sound models and information associated with the sound models, according to one embodiment of the present disclosure. The database includes sound models such as histograms, corresponding anti-models, labels, locations, times, and prior occurrence information. In this case, the prior occurrence information indicates the number of occurrences of respective sound models being recognized in the associated times and locations. 
<!-- EPO <DP n="22"/>-->
 In some embodiments, sound models related to the same type of environmental sound are stored in association with locations and/or times of different data hierarchical levels in the client or server database. For example, histogram A (labeled as "street-traffic") is associated with a location "Seoul," while the histogram D is associated with a location "City" which is a higher level than the location "Seoul" of histogram A. </p><p id="p0087" num="0087">In some embodiments, each label may identify at least one sound model. For example, a label "street-traffic" identifies sound models represented by histograms A to D while a label "office-speech" identifies a sound model represented by a histogram E, as shown in FIG 8. Such labels may be selected to recognize an environmental sound represented by associated sound models in sound recognition performed by a client device, as described above with reference to FIGS. 4A to 4D. Also, the labels associated with sound models may be selected and used in environmental sound recognition performed by a server, which will be described below with reference to FIG 9. FIG 9 illustrates a flowchart of a method, performed by the server 130, for recognizing an environmental sound according to one embodiment of the present disclosure. </p><p id="p0088" num="0088">The communication unit 320 of the server 130 receives an input sound model from the client device 120, at 910. Alternatively, the communication unit 130 may receive an input environmental sound from the client device 120, and the server 130 may construct an input sound model based on the input environmental sound. In this case, the input sound model is constructed based on a sound feature extracted from the input environmental sound, as describe above. Further, the communication unit 320 may receive additional information, from the client device 120, such as an input location and/or time. At 920, the sound recognition unit 310 of the server 130 identifies, from the server database 340, one or more sound models similar to the input sound model, e.g., by employing the method as described above with reference to FIG 4A. In some embodiments, the server database 340 includes predetermined sound models and associated information, such as anti-models, labels, thresholds, locations, times, prior occurrence information, as shown in FIG 8. In this case, in the sound recognition unit 310, the one or more sound models are identified by comparing the input location 
<!-- EPO <DP n="23"/>-->
 and/or time with locations and/or times associated with the predetermined sound models in a similar manner to the method described above with reference to FIG. 6. </p><p id="p0089" num="0089">At 930, the sound recognition unit 310 of the server 130 then selects a label for the input environmental sound and determines a confidence level of the selected label. This operation may be performed similarly to the methods described above with reference to FIGS. 4B to 4D. If the confidence level is determined to be equal to or greater than a predetermined confidence threshold ("NO" at 940), the communication unit 320 of the server 130 provides the selected label to the client device 120, at 950. The </p><p id="p0090" num="0090">communication unit 320 of the server 130 may also provide one or more sound models associated with the selected label to the client device 120, so that the client database 260 of client device 120 is updated based on the provided sound models and the selected label. Additionally, the communication unit 320 may transmit additional information associated with the sound models, such as locations, times, anti-models, prior occurrence information, etc., which may be later used in recognizing an input environmental sound in the client device 120. </p><p id="p0091" num="0091">After providing the selected label and associated sound models at 950, the </p><p id="p0092" num="0092">communication unit 320 of the server 130 may receive an input from a user of the client device 120 on accuracy of the selected label, at 970. For example, the user input may include information on whether the selected label is correct, or a correct label if the selected label is incorrect. Based on the user input from the client device 120, the database updating unit 330 of the server 130 updates the sound models and/or associated information, such as prior occurrence information, stored in the server database 340, at 980. For example, if the user input indicates that the selected label is correct, the prior occurrence information associated with the label may be updated to indicate that the number of occurrences of the associated sound models is adjusted to be higher. </p><p id="p0093" num="0093">On the other hand, if the confidence level is less than the predetermined confidence threshold ("YES" at 940), the communication unit 320 of the server 130 transmits an indication of a failure in recognizing the input environmental sound to a client device 120, at 960. Upon receiving such indication, the client device 120 may request a user to provide a correct label for the input environmental sound, at 970. In this case, when the 
<!-- EPO <DP n="24"/>-->
 user provides the correct label, the client device 120 transmits the provided label to the server 130. Based on the received label and the input sound model, the server 130 updates the sound models and/or associated information in the server database 340, at 980, so that the input sound model may be stored as a new entry in the server database 340. </p><p id="p0094" num="0094">FIG. 10 illustrates a flowchart of a method, performed by the server 130, for updating a sound model in a database by using a location and/or time associated with the sound model according to one embodiment of the present disclosure. Although the method as shown in FIG. 10 is described to update the server database 340, this method may also be employed to update the client database 260. Specifically, this method may be used in updating the client database 260 at 418 of the method shown in FIG. 4A or updating the server database 340 at 980 of the method shown in FIG. 9. </p><p id="p0095" num="0095">At 1010, an input sound model, and an input location and/or time associated with the input sound model are received by the communication unit 320 of the server 130. The sound recognition unit 310 of the server 130 then identifies, from the server database 340, one or more sound models that are associated with locations and/or times matching the input location and/or time, at 1020. </p><p id="p0096" num="0096">If any sound models are identified ("YES" at 1030), the sound recognition unit 310 identifies a sound model having the greatest similarity to the input sound model among the identified sound models, at 1040. In some embodiments, a similarity may be determined based on the distance between the input sound model and each of the sound models as described above with reference to FIG. 4A. </p><p id="p0097" num="0097">The sound recognition unit 310 of the server 130 then compares the similarity with a predetermined similarity threshold, at 1050. If the similarity is determined to be equal to or greater than the predetermined similarity threshold ("YES" at 1050), the database updating unit 330 merges the input sound model and the identified sound model having the greatest similarity to the input sound model, at 1070. In some embodiments, the merging may be performed by simply averaging the sound models. </p><p id="p0098" num="0098">On the other hand, if any sound models associated with locations and/or times matching the input location and/or time are not identified from the server database 340 ("NO" at 
<!-- EPO <DP n="25"/>-->
 1030), then the input sound model, and the input location and/or time are stored as a new entry in the server database 340, at 1060. Also, if the similarity is determined to be less than the predetermined similarity threshold ("NO" at 1050), the same operation is performed, at 1060. FIG. 11 shows a configuration of an exemplary mobile device 1100 in a wireless communication system. The configuration of the mobile device 1100 may be implemented in the client devices 120, 150, 160, 170, and 180. The mobile device 1100 may be a cellular phone, a terminal, a handset, a personal digital assistant (PDA), a wireless modem, a cordless phone, etc. The wireless communication system may be a Code Division Multiple Access (CDMA) system, a Global System for Mobile </p><p id="p0099" num="0099"> Communications (GSM) system, Wideband CDMA (WCDMA) system, Long Tern Evolution (LTE) system, LTE Advanced system, etc. Further, the mobile device 1100 may communicate directly with another mobile device, e.g., using Wi-Fi Direct, Bluetooth, or FlashLinq technology. The mobile device 1100 is capable of providing bidirectional communication via a receive path and a transmit path. On the receive path, signals transmitted by base stations are received by an antenna 1112 and are provided to a receiver (RCVR) 1114. The receiver 1114 conditions and digitizes the received signal and provides samples such as the conditioned and digitized digital signal to a digital section for further processing. On the transmit path, a transmitter (TMTR) 1116 receives data to be transmitted from a digital section 1120, processes and conditions the data, and generates a modulated signal, which is transmitted via the antenna 1112 to the base stations. The receiver 1114 and the transmitter 1116 may be part of a transceiver that may support CDMA, GSM, LTE, LTE Advanced, etc. The digital section 1120 includes various processing, interface, and memory units such as, for example, a modem processor 1122, a reduced instruction set computer/ digital signal processor (RISC/DSP) 1124, a controller/processor 1126, an internal memory 1128, a generalized audio encoder 1132, a generalized audio decoder 1134, a graphics/display processor 1136, and an external bus interface (EBI) 1138. The modem processor 1122 may perform processing for data transmission and reception, e.g., encoding, modulation, demodulation, and decoding. The RISC/DSP 1124 may perform 
<!-- EPO <DP n="26"/>-->
 general and specialized processing for the mobile device 1100. The controller/processor 1126 may perform the operation of various processing and interface units within the digital section 1120. The internal memory 1128 may store data and/or instructions for various units within the digital section 1120. The generalized audio encoder 1132 may perform encoding for input signals from an audio source 1 142, a microphone 1143, etc. The generalized audio decoder 1134 may perform decoding for coded audio data and may provide output signals to a </p><p id="p0100" num="0100">speaker/headset 1 144. The graphics/display processor 1136 may perform processing for graphics, videos, images, and texts, which may be presented to a display unit 1146. The EBI 1138 may facilitate transfer of data between the digital section 1120 and a main memory 1148. </p><p id="p0101" num="0101">The digital section 1 120 may be implemented with one or more processors, DSPs, microprocessors, RISCs, etc. The digital section 1 120 may also be fabricated on one or more application specific integrated circuits (ASICs) and/or some other type of integrated circuits (ICs). </p><p id="p0102" num="0102">In general, any device described herein may represent various types of devices, such as a wireless phone, a cellular phone, a laptop computer, a wireless multimedia device, a wireless communication personal computer (PC) card, a PDA, an external or internal modem, a device that communicates through a wireless channel, etc. A device may have various names, such as access terminal (AT), access unit, subscriber unit, mobile station, mobile device, mobile unit, mobile phone, mobile, remote station, remote terminal, remote unit, user device, user equipment, handheld device, etc. Any device described herein may have a memory for storing instructions and data, as well as hardware, software, firmware, or combinations thereof. The techniques described herein may be implemented by various means. For example, these techniques may be implemented in hardware, firmware, software, or a </p><p id="p0103" num="0103">combination thereof. Those ordinary skill in the art would further appreciate that the various illustrative logical blocks, modules, circuits, and algorithm steps described in connection with the disclosure herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, the various illustrative components, blocks, modules, circuits, 
<!-- EPO <DP n="27"/>-->
 and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present disclosure. </p><p id="p0104" num="0104">For a hardware implementation, the processing units used to perform the techniques may be implemented within one or more ASICs, DSPs, digital signal processing devices (DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), processors, controllers, micro-controllers, microprocessors, electronic devices, other electronic units designed to perform the functions described herein, a computer, or a combination thereof. </p><p id="p0105" num="0105">Thus, the various illustrative logical blocks, modules, and circuits described in connection with the disclosure herein may be implemented or performed with a general- purpose processor, a DSP, an ASIC, a FPGA or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration. </p><p id="p0106" num="0106">For a firmware and/or software implementation, the techniques may be embodied as instructions stored on a computer-readable medium, such as random access memory (RAM), read-only memory (ROM), non-volatile random access memory (NVRAM), programmable read-only memory (PROM), electrically erasable PROM (EEPROM), FLASH memory, compact disc (CD), magnetic or optical data storage device, or the like. The instructions may be executable by one or more processors and may cause the processor(s) to perform certain aspects of the functionality described herein. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium. Computer-readable media 
<!-- EPO <DP n="28"/>-->
 includes both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage media may be any available media that can be accessed by a computer. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if the software is transmitted from a website, a server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, the fiber optic cable, the twisted pair, the DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. Disk and disc, as used herein, includes CD, laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media. </p><p id="p0107" num="0107">A software module may reside in RAM memory, flash memory, ROM memory, EPROM memory, EEPROM memory, registers, hard disk, a removable disk, a CD- ROM, or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read information from, and write information to, the storage medium. Alternatively, the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user terminal. Alternatively, the processor and the storage medium may reside as discrete components in a user terminal. </p><p id="p0108" num="0108">The previous description of the disclosure is provided to enable any person skilled in the art to make or use the disclosure. Various modifications to the disclosure will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other variations without departing from the spirit or scope of the disclosure. Thus, the disclosure is not intended to be limited to the examples described herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein. 
<!-- EPO <DP n="29"/>-->
 Although exemplary implementations may refer to utilizing aspects of the presently disclosed subject matter in the context of one or more stand-alone computer systems, the subject matter is not so limited, but rather may be implemented in connection with any computing environment, such as a network or distributed computing environment. Still further, aspects of the presently disclosed subject matter may be implemented in or across a plurality of processing chips or devices, and storage may similarly be effected across a plurality of devices. Such devices may include PCs, network servers, and handheld devices. Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims. 
</p></description><claims mxw-id="PCLM44997412" ref-ucid="WO-2012121809-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="30"/>-->WHAT IS CLAIMED IS; </claim-statement><claim id="clm-0001" num="1"><claim-text>1. A method for recognizing an environmental sound in a client device, the method comprising: </claim-text><claim-text>providing a client database including a plurality of sound models of environmental sounds and a plurality of labels, wherein each label identifies at least one sound model; receiving an input environmental sound and generating an input sound model based on the input environmental sound; </claim-text><claim-text>determining a similarity value between the input sound model and each of the sound models to identify one or more sound models from the client database that are similar to the input sound model; </claim-text><claim-text>selecting a label from one or more labels associated with the identified sound models; and </claim-text><claim-text>associating the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The method of Claim 1, wherein the input sound model is generated based on a sound feature extracted from the input environmental sound. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The method of Claim 1, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The method of Claim 3, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method of Claim 1, wherein selecting the label comprises: </claim-text><claim-text>grouping the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models; </claim-text><claim-text>calculating a sum of the similarity values of sound models in each of the sets to determine a largest sum; and </claim-text><claim-text>selecting a label associated with one of the sets having the largest sum. <!-- EPO <DP n="31"/>--> </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The method of Claim 5, wherein a second largest sum is identified among the sums of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method of Claim 1, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The method of Claim 1, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. The method of Claim 1, further comprising: </claim-text><claim-text>if the confidence level is less than a predetermined confidence threshold, </claim-text><claim-text>transmitting the input sound model to a server to recognize the input environmental sound; and </claim-text><claim-text>receiving a label identifying the input environmental sound from the server. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The method of Claim 9, further comprising: </claim-text><claim-text>receiving, from the server, one or more sound models similar to the input sound model; and </claim-text><claim-text>updating the client database with the label and the sound models received from the server. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. The method of Claim 10, further comprising: </claim-text><claim-text>receiving, from the server, at least one of location and time associated with each of the one or more sound models received from the server; and </claim-text><claim-text>updating the client database with the at least one of location and time from the server. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. The method of Claim 10, wherein updating the client database comprises: </claim-text><claim-text>providing the received label to a user of the client device; </claim-text><claim-text>receiving an input from the user on whether the received label matches the input environmental sound; and </claim-text><claim-text>updating the client database based on the input from the user. <!-- EPO <DP n="32"/>--> </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The method of Claim 1, further comprising: </claim-text><claim-text>receiving at least one of location and time associated with the input sound model, wherein the client database further comprises at least one of location and time associated with each of the sound models of environmental sounds. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The method of Claim 13, wherein determining the similarity value comprises: filtering the plurality of sound models of the client database based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The method of Claim 13, wherein the client database further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the client database are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. A method for recognizing an input environmental sound received from a client device in a server, the method comprising: </claim-text><claim-text>providing a server database including a plurality of sound models of environmental sounds and a plurality of labels, wherein each label identifies at least one sound model; receiving an input sound model representing an input environmental sound from the client device; </claim-text><claim-text>determining a similarity value between the input sound model and each of the sound models to identify one or more sound models from the server database that are similar to the input sound model; </claim-text><claim-text>selecting a label from one or more labels associated with the identified sound models; and </claim-text><claim-text>associating the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. The method of Claim 16, further comprising: </claim-text><claim-text>if the confidence level is greater than or equal to a predetermined confidence threshold, providing the selected label and the one or more sound models to the client device. <!-- EPO <DP n="33"/>--> </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. The method of Claim 17, further comprising: </claim-text><claim-text>providing at least one of location and time information associated with each of the one or more sound models to the client device. </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The method of Claim 17, further comprising: </claim-text><claim-text>receiving an input from an user of the client device on whether the provided label matches the input environmental sound; and </claim-text><claim-text>updating the server database with the input from the user. </claim-text></claim><claim id="clm-0020" num="20"><claim-text>20. The method of Claim 17, further comprising: </claim-text><claim-text>receiving at least one of location and time associated with the input sound model from the client device, </claim-text><claim-text>wherein the server database comprises at least one of location and time associated with each of the sound models of environmental sounds. </claim-text></claim><claim id="clm-0021" num="21"><claim-text>21. The method of Claim 20, wherein determining the similarity value comprises: filtering the plurality of sound models of the server database based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0022" num="22"><claim-text>22. The method of Claim 20, wherein the server database further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the server database are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0023" num="23"><claim-text>23. The method of Claim 20, further comprising: </claim-text><claim-text>if the confidence level is less than the predetermined confidence threshold, </claim-text><claim-text>providing the client device with an indication of a failure in recognizing the input environmental sound; </claim-text><claim-text>identifying a set of sound models with at least one of location and time that match the at least one of location and time associated with the input sound model; </claim-text><claim-text>identifying a sound model having a greatest similarity to the input sound model from the set of sound models; and <!-- EPO <DP n="34"/>--> if the similarity between the input sound model and the identified sound model having the greatest similarity is greater than or equal to a predetermined similarity threshold, merging the input sound model and the identified sound model in the server database. </claim-text></claim><claim id="clm-0024" num="24"><claim-text>24. The method of Claim 23, further comprising: </claim-text><claim-text>if the similarity between the input sound model and the identified sound model is less than the predetermined similarity threshold, storing the input sound model in the server database. </claim-text></claim><claim id="clm-0025" num="25"><claim-text>25. The method of Claim 16, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0026" num="26"><claim-text>26. The method of Claim 25, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0027" num="27"><claim-text>27. The method of Claim 16, wherein selecting the label comprises: </claim-text><claim-text>grouping the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models; </claim-text><claim-text>calculating a sum of the similarity values of sound models in each of the sets to determine a largest sum; and </claim-text><claim-text>selecting a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0028" num="28"><claim-text>28. The method of Claim 27, wherein a second largest sum is identified among the sum of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0029" num="29"><claim-text>29. The method of Claim 16, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. </claim-text></claim><claim id="clm-0030" num="30"><claim-text>30. The method of Claim 16, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. <!-- EPO <DP n="35"/>--> </claim-text></claim><claim id="clm-0031" num="31"><claim-text>31. A device for recognizing an environmental sound, comprising: </claim-text><claim-text>a client database configured to store a plurality of sound models of environmental sounds and a plurality of labels, which each label identifies at least one sound model; a sound sensor configured to capture an input environmental sound; and </claim-text><claim-text>a sound recognition unit configured to: </claim-text><claim-text>generate an input sound model from the input environmental sound; </claim-text><claim-text>determine a similarity value between the input sound model and each of the sound models to identify one or more sound models from the client database that are similar to the input sound model; </claim-text><claim-text>select a label from one or more labels associated with the identified sound models; and associate the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0032" num="32"><claim-text>32. The device of Claim 31, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0033" num="33"><claim-text>33. The device of Claim 32, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0034" num="34"><claim-text>34. The device of Claim 31 , wherein the sound recognition unit is further configured to: group the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models; </claim-text><claim-text>calculate a sum of the similarity values of sound models in each of the sets to determine a largest sum; and </claim-text><claim-text>select a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0035" num="35"><claim-text>35. The device of Claim 34, wherein a second largest sum is identified among the sums of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0036" num="36"><claim-text>36. The device of Claim 31, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. <!-- EPO <DP n="36"/>--> </claim-text></claim><claim id="clm-0037" num="37"><claim-text>37. The device of Claim 31, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. </claim-text></claim><claim id="clm-0038" num="38"><claim-text>38. The device of Claim 31 , further comprising a communication unit configured to: if the confidence level is less than a predetermined confidence threshold, transmit the input sound model to a server to recognize the input environmental sound; and receive a label identifying the input environmental sound from the server. </claim-text></claim><claim id="clm-0039" num="39"><claim-text>39. The device of Claim 38, wherein the communication unit is further configured to receive, from the server, one or more sound models similar to the input sound model, and </claim-text><claim-text>wherein the device further comprises a database updating unit configured to update the client database with the label and the sound models received from the server. </claim-text></claim><claim id="clm-0040" num="40"><claim-text>40. The device of Claim 39, wherein the communication unit is further configured to receive, from the server, at least one of location and time associated with each of the one or more sound models received from the server, and the database updating unit is further configured to update the client database with the at least one of location and time from the server. </claim-text></claim><claim id="clm-0041" num="41"><claim-text>41. The device of Claim 39, further comprising a user interface configured to provide the received label to a user of the client device, and to receive an input from the user on whether the received label matches the input environmental sound, </claim-text><claim-text>wherein the database updating unit is further configured to update the client database based on the input from the user. </claim-text></claim><claim id="clm-0042" num="42"><claim-text>42. The device of Claim 31, wherein the communication unit is further configured to receive at least one of location and time associated with the input sound model, and wherein the client database comprises at least one of location information and time associated with each of the sound models of environmental sounds. <!-- EPO <DP n="37"/>--> </claim-text></claim><claim id="clm-0043" num="43"><claim-text>43. The device of Claim 42, wherein the sound recognition unit is further configured to filter the plurality of sound models of the client database based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0044" num="44"><claim-text>44. The device of Claim 42, wherein the client database further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the client database are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0045" num="45"><claim-text>45. A server for recognizing an input environmental sound received from a client device, the server comprising: </claim-text><claim-text>a server database configured to store a plurality of sound models of environmental sounds and a plurality of labels, wherein each label identifies at least one the sound model; </claim-text><claim-text>a communication unit configured to receive an input sound model representing an input environmental sound from the client device; and </claim-text><claim-text>a sound recognition unit configured to: </claim-text><claim-text>determine a similarity value between the input sound model and each of the sound models to identify one or more sound models from the server database that are similar to the input sound model; </claim-text><claim-text>select a label from one or more labels associated with the identified sound models; and associate the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0046" num="46"><claim-text>46. The server of Claim 45, wherein the communication unit is further configured to provide the selected label and the one or more sound models to the client device if the confidence level is greater than or equal to a predetermined confidence threshold. </claim-text></claim><claim id="clm-0047" num="47"><claim-text>47. The server of Claim 46, wherein the communication unit is further configured to provide at least one of location and time information associated with each of the one or more sound models to the client device. <!-- EPO <DP n="38"/>--> </claim-text></claim><claim id="clm-0048" num="48"><claim-text>48. The server of Claim 46, wherein the communication unit is further to receive an input from an user of the client device on whether the provided label matches the input environmental sound; and the server further comprises a database updating unit configured to update the server database with the input from the user. </claim-text></claim><claim id="clm-0049" num="49"><claim-text>49. The server of Claim 46, wherein the communication unit is further configured to receive at least one of location and time associated with the input sound model from the client device, and the server database further comprises at least one of location and time associated with each of the sound models of environmental sounds. </claim-text></claim><claim id="clm-0050" num="50"><claim-text>50. The server of Claim 49, wherein the sound recognition unit is further configured to filter the plurality of sound models of the server database based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0051" num="51"><claim-text>51. The server of Claim 49, wherein the server database further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the server database are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0052" num="52"><claim-text>52. The server of Claim 49, wherein the communication unit is further configured to provide the client device with an indication of a failure in recognizing the input environmental sound if the confidence level is less than the predetermined confidence threshold, </claim-text><claim-text>wherein the sound recognition unit is further configured to: </claim-text><claim-text>identify a set of sound models with at least one of location and time that match the at least one of location and time associated with the input sound model; and </claim-text><claim-text>identify a sound model having a greatest similarity to the input sound model from the set of sound models, </claim-text><claim-text>wherein the database updating unit is further configured to merge the input sound model and the identified sound model in the server database if the similarity between the input sound model and the identified sound model having the greatest similarity is greater than or equal to a predetermined similarity threshold. <!-- EPO <DP n="39"/>--> </claim-text></claim><claim id="clm-0053" num="53"><claim-text>53. The server of Claim 52, wherein the database updating unit is further configured to store the input sound model in the server database if the similarity between the input sound model and the identified sound model is less than the predetermined similarity threshold. </claim-text></claim><claim id="clm-0054" num="54"><claim-text>54. The server of Claim 45, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0055" num="55"><claim-text>55. The server of Claim 54, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0056" num="56"><claim-text>56. The server of Claim 45, wherein the sound recognition unit is further configured to: group the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models; </claim-text><claim-text>calculate a sum of the similarity values of sound models in each of the sets to determine a largest sum; and </claim-text><claim-text>select a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0057" num="57"><claim-text>57. The server of Claim 56, wherein a second largest sum is identified among the sum of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0058" num="58"><claim-text>58. The server of Claim 45, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. </claim-text></claim><claim id="clm-0059" num="59"><claim-text>59. The server of Claim 45, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. <!-- EPO <DP n="40"/>--> </claim-text></claim><claim id="clm-0060" num="60"><claim-text>60. A computer-readable medium comprising instructions for recognizing an environmental sound in a client device, the instructions causing a processor to perform the operations of: </claim-text><claim-text>providing a client database including a plurality of sound models of environmental sounds and a plurality of labels, wherein each label identifies at least one sound model; receiving an input environmental sound and generating an input sound model based on the input environmental sound; </claim-text><claim-text>determining a similarity value between the input sound model and each of the sound models to identify one or more sound models from the client database that are similar to the input sound model; </claim-text><claim-text>selecting a label from one or more labels associated with the identified sound models; and </claim-text><claim-text>associating the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0061" num="61"><claim-text>61. The medium of Claim 60, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0062" num="62"><claim-text>62. The medium of Claim 61, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0063" num="63"><claim-text>63. The medium of Claim 60, wherein selecting the label comprises: </claim-text><claim-text>grouping the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models; </claim-text><claim-text>calculating a sum of the similarity values of sound models in each of the sets to determine a largest sum; and </claim-text><claim-text>selecting a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0064" num="64"><claim-text>64. The medium of Claim 63, wherein a second largest sum is identified among the sums of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. <!-- EPO <DP n="41"/>--> </claim-text></claim><claim id="clm-0065" num="65"><claim-text>65. The medium of Claim 60, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. </claim-text></claim><claim id="clm-0066" num="66"><claim-text>66. The medium of Claim 60, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. </claim-text></claim><claim id="clm-0067" num="67"><claim-text>67. The medium of Claim 60, wherein the operations further comprise: </claim-text><claim-text>if the confidence level is less than a predetermined confidence threshold, </claim-text><claim-text>transmitting the input sound model to a server to recognize the input environmental sound; and </claim-text><claim-text>receiving a label identifying the input environmental sound from the server. </claim-text></claim><claim id="clm-0068" num="68"><claim-text>68. The medium of Claim 67, wherein the operations further comprise: </claim-text><claim-text>receiving, from the server, one or more sound models similar to the input sound model; and </claim-text><claim-text>updating the client database with the label and the sound models received from the server. </claim-text></claim><claim id="clm-0069" num="69"><claim-text>69. The medium of Claim 68, wherein the operations further comprise: </claim-text><claim-text>receiving, from the server, at least one of location and time associated with each of the one or more sound models received from the server; and </claim-text><claim-text>updating the client database with the at least one of location and time from the server. </claim-text></claim><claim id="clm-0070" num="70"><claim-text>70. The medium of Claim 68, wherein updating the client database comprises: </claim-text><claim-text>providing the received label to a user of the client device; </claim-text><claim-text>receiving an input from the user on whether the received label matches the input environmental sound; and </claim-text><claim-text>updating the client database based on the input from the user. </claim-text></claim><claim id="clm-0071" num="71"><claim-text>71. The medium of Claim 60, wherein the operations further comprise: </claim-text><claim-text>receiving at least one of location and time associated with the input sound model, wherein the client database further comprises at least one of location and time associated with each of the sound models of environmental sounds. <!-- EPO <DP n="42"/>--> </claim-text></claim><claim id="clm-0072" num="72"><claim-text>72. The medium of Claim 71, wherein determining the similarity value comprises: filtering the plurality of sound models of the client database based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0073" num="73"><claim-text>73. The medium of Claim 71, wherein the client database further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the client database are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0074" num="74"><claim-text>74. A computer-readable medium comprising instructions for recognizing an input environmental sound received from a client device in a server, the instructions causing a processor to perform the operations of: </claim-text><claim-text>providing a server database including a plurality of sound models of environmental sounds and a plurality of labels, wherein each label identifies at least one sound model; receiving an input sound model representing an input environmental sound from the client device; </claim-text><claim-text>determining a similarity value between the input sound model and each of the sound models to identify one or more sound models from the server database that are similar to the input sound model; </claim-text><claim-text>selecting a label from one or more labels associated with the identified sound models; and </claim-text><claim-text>associating the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0075" num="75"><claim-text>75. The medium of Claim 74, wherein the operations further comprise: </claim-text><claim-text>if the confidence level is greater than or equal to a predetermined confidence threshold, providing the selected label and the one or more sound models to the client device. </claim-text></claim><claim id="clm-0076" num="76"><claim-text>76. The medium of Claim 75, wherein the operations further comprise: </claim-text><claim-text>providing at least one of location and time information associated with each of the one or more sound models to the client device. <!-- EPO <DP n="43"/>--> </claim-text></claim><claim id="clm-0077" num="77"><claim-text>77. The medium of claim 75, wherein the operations further comprise: </claim-text><claim-text>receiving an input from an user of the client device on whether the provided label matches the input environmental sound; and </claim-text><claim-text>updating the server database with the input from the user. </claim-text></claim><claim id="clm-0078" num="78"><claim-text>78. The medium of claim 75, wherein the operations further comprise: </claim-text><claim-text>receiving at least one of location and time associated with the input sound model from the client device, </claim-text><claim-text>wherein the server database comprises at least one of location and time associated with each of the sound models of environmental sounds. </claim-text></claim><claim id="clm-0079" num="79"><claim-text>79. The medium of Claim 78, wherein determining the similarity value comprises: filtering the plurality of sound models of the server database based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0080" num="80"><claim-text>80. The medium of Claim 78, wherein the server database further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the server database are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0081" num="81"><claim-text>81. The medium of Claim 78, wherein the operations further comprise: </claim-text><claim-text>if the confidence level is less than the predetermined confidence threshold, </claim-text><claim-text>providing the client device with an indication of a failure in recognizing the input environmental sound; </claim-text><claim-text>identifying a set of sound models with at least one of location and time that match the at least one of location and time associated with the input sound model; </claim-text><claim-text>identifying a sound model having a greatest similarity to the input sound model from the set of sound models; and </claim-text><claim-text>if the similarity between the input sound model and the identified sound model having the greatest similarity is greater than or equal to a predetermined similarity threshold, merging the input sound model and the identified sound model in the server database. <!-- EPO <DP n="44"/>--> </claim-text></claim><claim id="clm-0082" num="82"><claim-text>82. The medium of claim 81, wherein the operations further comprise: </claim-text><claim-text>if the similarity between the input sound model and the identified sound model is less than the predetermined similarity threshold, storing the input sound model in the server database. </claim-text></claim><claim id="clm-0083" num="83"><claim-text>83. The medium of Claim 74, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0084" num="84"><claim-text>84. The medium of Claim 83, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0085" num="85"><claim-text>85. The medium of Claim 74, wherein selecting the label comprises: </claim-text><claim-text>grouping the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models; </claim-text><claim-text>calculating a sum of the similarity values of sound models in each of the sets to determine a largest sum; and </claim-text><claim-text>selecting a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0086" num="86"><claim-text>86. The medium of Claim 85, wherein a second largest sum is identified among the sum of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0087" num="87"><claim-text>87. The medium of Claim 74, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. </claim-text></claim><claim id="clm-0088" num="88"><claim-text>88. The medium of Claim 74, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. <!-- EPO <DP n="45"/>--> </claim-text></claim><claim id="clm-0089" num="89"><claim-text>89. An apparatus for recognizing an environmental sound, comprising: </claim-text><claim-text>means for storing a plurality of sound models of environmental sounds and a plurality of labels, which each label identifies at least one sound model; </claim-text><claim-text>means for capturing an input environmental sound; </claim-text><claim-text>means for generating an input sound model from the input environmental sound; </claim-text><claim-text>means for determining a similarity value between the input sound model and each of the sound models to identify one or more sound models from the means of storing that are similar to the input sound model; </claim-text><claim-text>means for selecting a label from one or more labels associated with the identified sound models; and </claim-text><claim-text>means for associating the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0090" num="90"><claim-text>90. The apparatus of Claim 89, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0091" num="91"><claim-text>91. The apparatus of Claim 90, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. </claim-text></claim><claim id="clm-0092" num="92"><claim-text>92. The apparatus of Claim 89, wherein the selecting means groups the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models, calculate a sum of the similarity values of sound models in each of the sets to determine a largest sum, and select a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0093" num="93"><claim-text>93. The apparatus of Claim 92, wherein a second largest sum is identified among the sums of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0094" num="94"><claim-text>94. The apparatus of Claim 89, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. <!-- EPO <DP n="46"/>--> </claim-text></claim><claim id="clm-0095" num="95"><claim-text>95. The apparatus of Claim 89, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. </claim-text></claim><claim id="clm-0096" num="96"><claim-text>96. The apparatus of Claim 89, further comprising: </claim-text><claim-text>means for transmitting the input sound model to a server to recognize the input environmental sound if the confidence level is less than a predetermined confidence threshold; and </claim-text><claim-text>means for receiving a label identifying the input environmental sound from the server. </claim-text></claim><claim id="clm-0097" num="97"><claim-text>97. The apparatus of Claim 96, wherein the receiving means further receives, from the server, one or more sound models similar to the input sound model, and </claim-text><claim-text>wherein the apparatus further comprises means for updating the storing means with the label and the sound models received from the server. </claim-text></claim><claim id="clm-0098" num="98"><claim-text>98. The apparatus of Claim 97, wherein the receiving means further receives, from the server, at least one of location and time associated with each of the one or more sound models received from the server, and the updating means further updates the storing means with the at least one of location and time from the server. </claim-text></claim><claim id="clm-0099" num="99"><claim-text>99. The apparatus of Claim 97, further comprising means for providing the received label to a user of the apparatus and receiving an input from the user on whether the received label matches the input environmental sound, </claim-text><claim-text>wherein the updating means further updates the storing means based on the input from the user. </claim-text></claim><claim id="clm-0100" num="100"><claim-text>100. The apparatus of Claim 89, wherein the receiving means further receives at least one of location and time associated with the input sound model, and the storing means further stores at least one of location information and time associated with each of the sound models of environmental sounds. </claim-text></claim><claim id="clm-0101" num="101"><claim-text>101. The apparatus of Claim 100, wherein the determining means filters the plurality of sound models in the storing means based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. <!-- EPO <DP n="47"/>--> </claim-text></claim><claim id="clm-0102" num="102"><claim-text>102. The apparatus of Claim 100, wherein the storing means further comprises prior occurrence information associated with each of the sound models of environmental sounds, and wherein the one or more sound models from the storing means are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0103" num="103"><claim-text>103. A computer system for recognizing an input environmental sound received from a client device, the computer system comprising: </claim-text><claim-text>means for storing a plurality of sound models of environmental sounds and a plurality of labels, wherein each label identifies at least one the sound model; means for receiving an input sound model representing an input environmental sound from the client device; means for determining a similarity value between the input sound model and each of the sound models to identify one or more sound models from the server database that are similar to the input sound model; </claim-text><claim-text>means for selecting a label from one or more labels associated with the identified sound models; and </claim-text><claim-text>means for associating the selected label with the input environmental sound based on a confidence level of the selected label. </claim-text></claim><claim id="clm-0104" num="104"><claim-text>104. The computer system of Claim 103, further comprising: </claim-text><claim-text>means for providing the selected label and the one or more sound models to the client device if the confidence level is greater than or equal to a predetermined confidence threshold. </claim-text></claim><claim id="clm-0105" num="105"><claim-text>105. The computer system of Claim 104, wherein the providing means further provides at least one of location and time information associated with each of the one or more sound models to the client device. </claim-text></claim><claim id="clm-0106" num="106"><claim-text>106. The computer system of Claim 104, wherein the receiving means further receives at least one of location and time associated with the input sound model from the client device, and the storing means further stores at least one of location and time associated with each of the sound models of environmental sounds. <!-- EPO <DP n="48"/>--> </claim-text></claim><claim id="clm-0107" num="107"><claim-text>107. The computer system of Claim 106, wherein the determining means filters the plurality of sound models of the storing means based on the at least one of location and time associated with the input sound model to identify the sound models for which the similarity values of the input sound model are determined. </claim-text></claim><claim id="clm-0108" num="108"><claim-text>108. The computer system of Claim 106, wherein the storing means further stores prior occurrence information associated with each of the sound models of environmental sounds, wherein the one or more sound models from the storing means are identified based on the prior occurrence information. </claim-text></claim><claim id="clm-0109" num="109"><claim-text>109. The computer system of Claim 106, wherein the providing means further provides the client device with an indication of a failure in recognizing the input environmental sound if the confidence level is less than the predetermined confidence threshold, wherein the determining means further identifies identify a set of sound models with at least one of location and time that match the at least one of location and time associated with the input sound model and identify a sound model having a greatest similarity to the input sound model from the set of sound models, </claim-text><claim-text>wherein the updating means merges the input sound model and the identified sound model having the greatest similarity if the similarity between the input sound model and the identified sound model having the greatest similarity is greater than or equal to a predetermined similarity threshold. </claim-text></claim><claim id="clm-0110" num="110"><claim-text>110. The computer system of Claim 109, wherein the updating means stores the input sound model in the storing means if the similarity between the input sound model and the identified sound model is less than the predetermined similarity threshold. </claim-text></claim><claim id="clm-0111" num="111"><claim-text>111. The computer system of Claim 103, wherein the selected label is associated with a largest number of sound models among the identified sound models. </claim-text></claim><claim id="clm-0112" num="112"><claim-text>112. The computer system of Claim 111, wherein another label is associated with a second largest number of sound models from the identified sound models, and wherein the confidence level is determined based on a difference between a sum of the similarity values of the sound models associated with the largest number and a sum of the similarity values of the sound models associated with the second largest number. <!-- EPO <DP n="49"/>--> </claim-text></claim><claim id="clm-0113" num="113"><claim-text>113. The computer system of Claim 103, wherein the selecting means groups the identified sound models into one or more sets of sound models based on the labels associated with the identified sound models, calculate a sum of the similarity values of sound models in each of the sets to determine a largest sum, and select a label associated with one of the sets having the largest sum. </claim-text></claim><claim id="clm-0114" num="114"><claim-text>114. The computer system of Claim 113, wherein a second largest sum is identified among the sum of the similarity values, and the confidence level is determined based on a difference between the largest sum and the second largest sum. </claim-text></claim><claim id="clm-0115" num="115"><claim-text>115. The computer system of Claim 103, wherein the confidence level is determined based on a sum of the similarity values of the sound models associated with the selected label. </claim-text></claim><claim id="clm-0116" num="116"><claim-text>1 16. The computer system of Claim 103, wherein a sound model having a greatest similarity value to the input sound model is identified among the sound models associated with the selected label, and the confidence level is determined based on a similarity between the sound model having the greatest similarity value and an associated anti-model. </claim-text></claim><claim id="clm-0117" num="117"><claim-text>117. The computer system of Claim 104, wherein the receiving means further receives an input from an user of the client device on whether the provided label matches the input environmental sound, and the computer system further comprises means for updating the server database with the input from the user. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
