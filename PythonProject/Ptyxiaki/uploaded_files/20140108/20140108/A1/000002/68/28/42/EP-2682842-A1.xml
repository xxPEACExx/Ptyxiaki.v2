<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2682842-A1" country="EP" doc-number="2682842" kind="A1" date="20140108" family-id="48747395" file-reference-id="263467" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146584910" ucid="EP-2682842-A1"><document-id><country>EP</country><doc-number>2682842</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13175242-A" is-representative="YES"><document-id mxw-id="PAPP154847102" load-source="docdb" format="epo"><country>EP</country><doc-number>13175242</doc-number><kind>A</kind><date>20130705</date><lang>EN</lang></document-id><document-id mxw-id="PAPP210132546" load-source="docdb" format="original"><country>EP</country><doc-number>13175242.0</doc-number><date>20130705</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140557019" ucid="KR-20120073921-A" load-source="docdb"><document-id format="epo"><country>KR</country><doc-number>20120073921</doc-number><kind>A</kind><date>20120706</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989320644" load-source="docdb">G06F   3/01        20060101AFI20131018BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989322137" load-source="docdb">G06F   3/048       20130101ALI20131018BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989323207" load-source="docdb">G06F   3/0485      20130101ALI20131018BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1989626998" load-source="docdb" scheme="CPC">G06F   3/012       20130101 LI20131016BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989641997" load-source="docdb" scheme="CPC">G06F   3/0485      20130101 LI20131016BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989642506" load-source="docdb" scheme="CPC">G06F   3/017       20130101 LI20131016BHEP        </classification-cpc><classification-cpc mxw-id="PCL1991317634" load-source="docdb" scheme="CPC">G06F   3/011       20130101 FI20140102BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132359363" lang="DE" load-source="patent-office">Benutzerschnittstellenverfahren und Vorrichtung dafür</invention-title><invention-title mxw-id="PT132359364" lang="EN" load-source="patent-office">User interface method and apparatus therefor</invention-title><invention-title mxw-id="PT132359365" lang="FR" load-source="patent-office">Procédé d'interface utilisateur et appareil correspondant</invention-title><citations><patent-citations><patcit mxw-id="PCIT242942591" load-source="docdb" ucid="GB-2474536-A"><document-id format="epo"><country>GB</country><doc-number>2474536</doc-number><kind>A</kind><date>20110420</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942589" load-source="docdb" ucid="US-20080141181-A1"><document-id format="epo"><country>US</country><doc-number>20080141181</doc-number><kind>A1</kind><date>20080612</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942590" load-source="docdb" ucid="US-20090079813-A1"><document-id format="epo"><country>US</country><doc-number>20090079813</doc-number><kind>A1</kind><date>20090326</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942592" load-source="docdb" ucid="WO-2011156161-A2"><document-id format="epo"><country>WO</country><doc-number>2011156161</doc-number><kind>A2</kind><date>20111215</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit></patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR919542287" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SAMSUNG ELECTRONICS CO LTD</last-name><address><country>KR</country></address></addressbook></applicant><applicant mxw-id="PPAR919509527" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SAMSUNG ELECTRONICS CO., LTD</last-name></addressbook></applicant><applicant mxw-id="PPAR919005611" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Samsung Electronics Co., Ltd</last-name><iid>101312277</iid><address><street>129, Samsung-ro Yeongtong-gu Suwon-si</street><city>Gyeonggi-Do 443-742</city><country>KR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919514555" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>KANG JI-YOUNG</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919508485" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KANG, JI-YOUNG</last-name></addressbook></inventor><inventor mxw-id="PPAR919008047" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>KANG, JI-YOUNG</last-name><address><street>615-801, 33, Cheoncheon-ro 21beon-gil, Jangna-gu, Suwon-si</street><city>Gyeonggi-do</city><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919519917" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>PARK MI-JUNG</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919517447" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>PARK, MI-JUNG</last-name></addressbook></inventor><inventor mxw-id="PPAR919018985" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>PARK, MI-JUNG</last-name><address><street>145-502, 231, Dongtanbanseok-ro, Hwaseong-si</street><city>Gyeonggi-do</city><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919544917" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>LEE CHI-HOON</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919533583" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>LEE, CHI-HOON</last-name></addressbook></inventor><inventor mxw-id="PPAR919011869" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>LEE, CHI-HOON</last-name><address><street>101-1401, 26-7, Gyeonggi-daero, Seodaemun-gu</street><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919533386" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>OH SAE-GEE</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR919534268" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>Oh, Sae-Gee</last-name></addressbook></inventor><inventor mxw-id="PPAR919018419" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>Oh, Sae-Gee</last-name><address><street>1709-802, 98, Juyeob-ro, Ilsanseo-gu, Goyang-si</street><city>Gyeonggi-do</city><country>KR</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919013060" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Nederlandsch Octrooibureau</last-name><iid>101379333</iid><address><street>P.O. Box 29720</street><city>2502 LS The Hague</city><country>NL</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS549740717" load-source="docdb">AL</country><country mxw-id="DS549740238" load-source="docdb">AT</country><country mxw-id="DS549740719" load-source="docdb">BE</country><country mxw-id="DS549812262" load-source="docdb">BG</country><country mxw-id="DS549907572" load-source="docdb">CH</country><country mxw-id="DS549813003" load-source="docdb">CY</country><country mxw-id="DS549740239" load-source="docdb">CZ</country><country mxw-id="DS549814674" load-source="docdb">DE</country><country mxw-id="DS549740720" load-source="docdb">DK</country><country mxw-id="DS549813004" load-source="docdb">EE</country><country mxw-id="DS549738685" load-source="docdb">ES</country><country mxw-id="DS549812263" load-source="docdb">FI</country><country mxw-id="DS549812264" load-source="docdb">FR</country><country mxw-id="DS549814675" load-source="docdb">GB</country><country mxw-id="DS549740721" load-source="docdb">GR</country><country mxw-id="DS549740722" load-source="docdb">HR</country><country mxw-id="DS549813005" load-source="docdb">HU</country><country mxw-id="DS549907573" load-source="docdb">IE</country><country mxw-id="DS549740723" load-source="docdb">IS</country><country mxw-id="DS549812265" load-source="docdb">IT</country><country mxw-id="DS549813014" load-source="docdb">LI</country><country mxw-id="DS549814676" load-source="docdb">LT</country><country mxw-id="DS549740240" load-source="docdb">LU</country><country mxw-id="DS549814677" load-source="docdb">LV</country><country mxw-id="DS549814682" load-source="docdb">MC</country><country mxw-id="DS549742389" load-source="docdb">MK</country><country mxw-id="DS549742390" load-source="docdb">MT</country><country mxw-id="DS549740724" load-source="docdb">NL</country><country mxw-id="DS549812270" load-source="docdb">NO</country><country mxw-id="DS549740725" load-source="docdb">PL</country><country mxw-id="DS549812271" load-source="docdb">PT</country><country mxw-id="DS549740726" load-source="docdb">RO</country><country mxw-id="DS549812272" load-source="docdb">RS</country><country mxw-id="DS549740727" load-source="docdb">SE</country><country mxw-id="DS549907574" load-source="docdb">SI</country><country mxw-id="DS549814684" load-source="docdb">SK</country><country mxw-id="DS549814685" load-source="docdb">SM</country><country mxw-id="DS549813015" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128673082" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A User Interface method and an apparatus therefor are provided. The method includes dividing a photographing region into a plurality of portions of the photographing region, acquiring corresponding information from respective image signals output while being classified according to the divided portion of the photographing region, verifying a command corresponding to the acquired information, and performing an operation according to the verified command.
<img id="iaf01" file="imgaf001.tif" wi="73" he="131" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA128737227" lang="EN" source="EPO" load-source="docdb"><p>A User Interface method and an apparatus therefor are provided. The method includes dividing a photographing region into a plurality of portions of the photographing region, acquiring corresponding information from respective image signals output while being classified according to the divided portion of the photographing region, verifying a command corresponding to the acquired information, and performing an operation according to the verified command.</p></abstract><description mxw-id="PDES63959070" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>BACKGROUND OF THE INVENTION</b></heading><heading id="h0002"><b><i>1. Field of the Invention:</i></b></heading><p id="p0001" num="0001">The present invention relates to a method and apparatus for controlling and displaying User Interface (UI). More particularly, the present invention relates to a method and apparatus for controlling and displaying a UI which generates input capable of replacing an event according to a push of a button without requiring the push of the button.</p><heading id="h0003"><b><i>2. Description of the Related Art:</i></b></heading><p id="p0002" num="0002">Recently, as portable electronic devices have rapidly become widely adopted, the portable devices have become necessities of life for modem people. These portable electronic devices have evolved into multimedia communication devices capable of providing various data transmission services and various additional services as well as an original voice communication service. As the portable electronic devices are further developed, the services provided by the portable electronic devices correspondingly increase based on similar development endeavors. In addition, User Interface (UI) technologies for controlling the portable electronic devices are similarly being developed.</p><p id="p0003" num="0003">Development of the portable electronic devices focuses on improving convenience to a user for user input by applying a touch screen based on a Graphical User Interface (GUI). In addition, while a utilization degree of each of the portable electronic devices applying the touch screen is enhanced, the user shows a trend of preferring a larger touch screen. Because most buttons for input to the portable electronic devices are configured in the touch screen and implemented in software, the number of hardware-like buttons on such portable electronic devices is reduced.</p><p id="p0004" num="0004">However, according to the related art, if the user touches a capacitive touch screen with the user's hand on which a glove or other covering is being worn, input is not generated. For example, there is no input means capable of controlling an operation of a portable electronic device according to the related art when the portable electronic device is in a state in which it is difficult for the user to touch the capacitive touch screen. Therefore, the degree to which utilization of the portable electronic<!-- EPO <DP n="2"> --> device according to the related art is degraded.</p><p id="p0005" num="0005">Therefore, a need exists for a method and apparatus for controlling and displaying a UI which generates input capable of replacing an event according to a push of a button without requiring the push of the button.</p><p id="p0006" num="0006">The above information is presented as background information only to assist with an understanding of the present disclosure. No determination has been made, and no assertion is made, as to whether any of the above might be applicable as prior art with regard to the present invention.</p><heading id="h0004"><b>SUMMARY OF THE INVENTION</b></heading><p id="p0007" num="0007">Aspects of the present invention are to address at least the above-mentioned problems and/or disadvantages and to provide at least the advantages described below. Accordingly, an aspect of the present invention is to provide a User Interface (UI) method of generating input capable of replacing an event according to a push of a button without pushing the button and an apparatus therefor.</p><p id="p0008" num="0008">Another aspect of the present invention is to provide a UI method of generating input capable of replacing a touch event although a user does not touch a touch screen and an apparatus therefor.</p><p id="p0009" num="0009">Another aspect of the present invention is to provide a UI method of controlling an operation of a portable electronic device according to motions (e.g., a nod, a face direction, user's eyes, and the like) of an object.</p><p id="p0010" num="0010">In accordance with an aspect of the present invention, a method is provided. The method includes dividing a photographing region into a plurality of portions of the photographing regions, acquiring corresponding information from respective image signals output while being classified according to the divided portion of the photographing regions, verifying a command corresponding to the acquired information, and performing an operation according to the verified command.</p><p id="p0011" num="0011">In accordance with another aspect of the present invention, a method is provided. The method includes dividing a photographing region into a plurality of portions of the photographing regions, performing a first operation upon detection that an object is moved from a first portion of the photographing region to a second portion of the photographing region from an image signal output from the photographing<!-- EPO <DP n="3"> --> region, and performing a second operation when a corresponding motion of the object is detected from an image signal output from the second portion of the photographing region.</p><p id="p0012" num="0012">In accordance with another aspect of the present invention, a device is provided. The device includes a camera, at least one processor, a memory, and at least one module, each of which is stored in the memory and is configured to be executed by the at least one processor, wherein at least one of the at least one module divides a photographing region of the camera into a plurality of portions of the photographing region, acquires corresponding information from respective image signals output while being classified according to the divided portion of the photographing region, verifies a command corresponding to the acquired information, and performs an operation according to the verified command.</p><p id="p0013" num="0013">In accordance with another aspect of the present invention, an electronic device is provided. The device includes a camera, at least one processor, a memory, and at least one module, each of which is stored in the memory and is configured to be executed by the at least one processor, wherein at least one of the at least one module divides a photographing region of the camera into a plurality of portion of the photographing region, performs a first operation upon detection that an object is moved from a first portion of the photographing region to a second portion of the photographing region from an image signal output from the photographing region, and performs a second operation when a corresponding motion of the object is detected from an image signal output from the second portion of the photographing region.</p><p id="p0014" num="0014">Other aspects, advantages, and salient features of the invention will become apparent to those skilled in the art from the following detailed description, which, taken in conjunction with the annexed drawings, discloses exemplary embodiments of the invention.</p><heading id="h0005"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading><p id="p0015" num="0015">The above and other aspects, features, and advantages of certain exemplary embodiments of the present invention will be more apparent from the following description taken in conjunction with the accompanying drawings, in which:</p><p id="p0016" num="0016"><figref idrefs="f0001">FIG. 1</figref> is a block diagram illustrating configuration of an electronic<!-- EPO <DP n="4"> --> device according to an exemplary embodiment of the present invention;</p><p id="p0017" num="0017"><figref idrefs="f0002">FIG. 2</figref> illustrates a User Interface (UI) method according to an exemplary embodiment of the present invention;</p><p id="p0018" num="0018"><figref idrefs="f0003">FIG. 3</figref> is a flowchart illustrating a UI method according to an exemplary embodiment of the present invention;</p><p id="p0019" num="0019"><figref idrefs="f0004">FIG. 4</figref> is a flowchart illustrating a process of acquiring information from an image signal of a photographing region such as, for example, a process of acquiring information from the image signal of the photographing region described in relation to step 301 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention;</p><p id="p0020" num="0020"><figref idrefs="f0005">FIG. 5</figref> is a flowchart illustrating a process of acquiring information from an image signal of a photographing region such as, for example, a process of acquiring information from the image signal of the photographing region described in relation to step 301 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention;</p><p id="p0021" num="0021"><figref idrefs="f0006">FIG. 6</figref> is a flowchart illustrating a process of acquiring information from an image signal of a photographing region such as, for example, a process of acquiring information from the image signal of the photographing region described in relation to step 301 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention;</p><p id="p0022" num="0022"><figref idrefs="f0007">FIG. 7</figref> is a flowchart illustrating a process of verifying a command corresponding to information acquired from an image signal of a photographing region such as, for example, a process of verifying the command corresponding to the acquired information described in relation to step 303 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention; and</p><p id="p0023" num="0023"><figref idrefs="f0008 f0009 f0010 f0011">FIGs. 8A to 8D</figref> illustrate a UI method according to an exemplary embodiment of the present invention.</p><p id="p0024" num="0024">Throughout the drawings, it should be noted that like reference numbers are used to depict the same or similar elements, features, and structures.</p><heading id="h0006"><b>DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS</b></heading><p id="p0025" num="0025">The following description with reference to the accompanying drawings is provided to assist in a comprehensive understanding of exemplary embodiments of invention of the invention as defined by the claims and their equivalents. Accordingly, those of ordinary skill in the art will recognize that various changes and modifications<!-- EPO <DP n="5"> --> of the embodiments described herein can be made without departing from the scope and spirit of the invention. In addition, descriptions of well-known functions and constructions may be omitted for clarity and conciseness.</p><p id="p0026" num="0026">The terms and words used in the following description and claims are not limited to the bibliographical meanings, but, are merely used by the inventor to enable a clear and consistent understanding of the invention. Accordingly, it should be apparent to those skilled in the art that the following description of exemplary embodiments of the present invention is provided for illustration purpose only and not for the purpose of limiting the invention as defined by the appended claims and their equivalents.</p><p id="p0027" num="0027">It is to be understood that the singular forms "a," "an," and "the" include plural referents unless the context clearly dictates otherwise. Thus, for example, reference to "a component surface" includes reference to one or more of such surfaces.</p><p id="p0028" num="0028"><figref idrefs="f0001">FIG. 1</figref> is a block diagram illustrating configuration of an electronic device according to an exemplary embodiment of the present invention.</p><p id="p0029" num="0029">This electronic device 100 may be a portable electronic device. The electronic device 100 may be an apparatus such as a portable terminal, a mobile phone, a mobile pad, a media player, a tablet computer, a handheld computer, a Personal Digital Assistant (PDA), a Global Positioning System (GPS) device, and the like. The electronic device 100 may also be a portable electronic device including a device in which two or more functions are combined among the above-identified apparatuses.</p><p id="p0030" num="0030">The electronic device 100 includes a host device 110, an external memory device 120, a camera device 130, a sensor device 140, a wireless communication device 150, an audio device 160, an external port device 170, a touch screen device 180, and other input/control devices 190. The external memory device 120 and the external port device 170 may be a plurality of external memory devices and external port devices, respectively.</p><p id="p0031" num="0031">The host device 110 includes an internal memory 111, one or more processors 112, and an interface 113. The internal memory 111, the one or more processors 112, and the interface 113 may be separately configured or may be configured in one or more Integrated Circuits (ICs).</p><p id="p0032" num="0032">The processor 112 executes several software programs and performs several functions for the electronic device 100. For example, the processor 112<!-- EPO <DP n="6"> --> performs process and control for audio communication, video communication, data communication, and the like. In addition to these general functions, the processor 112 executes a software module (command set) stored in the internal memory 111 and/or the external memory device 120 and performs several functions corresponding to the software module. Also, in addition to these general functions, the processor 112 plays a role in executing a specific software module (command set) stored in the internal memory 111 and/or the external memory device 120 and performing several specific functions corresponding to the specific software module. For example, the processor 112 interworks with software modules stored in the internal memory 111 and/or the external memory devices 120 and performs exemplary embodiments of the present invention. Also, the processor 112 may include one or more data processors, an image processor, or a codec. In addition, the electronic device 100 may include the data processor, the image processor, or the codec, separately.</p><p id="p0033" num="0033">The interface 113 connects the host device 110 with several components of the electronic device 100.</p><p id="p0034" num="0034">Particularly, in accordance with an exemplary embodiment of the present invention, the processor 112 controls the camera device 130. The camera device 130 may perform a camera function such as a photo and video clip recording function, and the like. The camera device 130 includes a Charge Coupled Device (CCD), a Complementary Metal Oxide Semiconductor (CMOS), and/or the like. The camera device 130 adjusts hardware-like configuration changes, for example, lens movement, the F-number of a diaphragm, and the like, according to a camera module executed by the processor 112.</p><p id="p0035" num="0035">Various components of the electronic device 100 may be connected through one or more communication buses (not written in reference numbers) or stream lines (not written in reference numbers).</p><p id="p0036" num="0036">The sensor device 140 may include a motion sensor, an optical sensor, a temperature sensor, and the like, and may perform several functions. For example, the motion sensor may detect motion of the electronic device 100. As another example, the optical sensor may detect light around the electronic device 100.</p><p id="p0037" num="0037">The wireless communication device 150 may perform wireless communication. The wireless communication device 150 may include a Radio Frequency (RF) transceiver and a light (e.g., infrared ray) transceiver. The wireless<!-- EPO <DP n="7"> --> communication device 150 may be designed to operate through one or more communication networks, such as a Global System for Mobile communication (GSM) network, an Enhanced Data GSM Environment (EDGE) network, a Code Division Multiple Access (CDMA) network, a W-CDMA network, a Long Term Evolution (LTE) network, an Orthogonal Frequency Division Multiple Access (OFDMA) network, a Wireless-Fidelity (Wi-Fi) network, a Worldwide interoperability for Microwave Access (WiMax) network, a Bluetooth network, and/or the like.</p><p id="p0038" num="0038">The audio device 160 connects to a speaker 161 and a microphone 162 and is responsible for an audio input and output function such as a voice recognition function, a voice copy function, a digital recording function, a phone call function, and the like. For example, the audio device 160 communicates with a user through the speaker 161 and the microphone 162. Also, the audio device 160 receives a data signal from the host device 110, converts the received data signal into an electric signal, and outputs the converted electric signal through the speaker 161. The speaker 161 converts a band of the electric signal into an audible frequency band and outputs the converted signal. The microphone 162 converts sound waves transmitted from people or other sound sources into electric signals. Also, the audio device 160 receives an electric signal from the microphone 162, converts the received electric signal into an audio data signal, and transmits the converted audio data signal to the host device 110. The audio device 160 may include an earset, a headphone, or a headset which is attachable and detachable to the electronic device 100.</p><p id="p0039" num="0039">The external port device 170 connects the electronic device 100 to another electronic device directly or connects the electronic device 100 to another electronic device indirectly through a network (e.g., the Internet, an intranet, a wireless Local Area Network (LAN), and the like).</p><p id="p0040" num="0040">The touch screen device 180 provides an input and output interface between the electronic device 100 and the user. The touch screen device 180 applies touch sensing technologies, transmits touch input of the user to the host device 110, and provides visual information provided from the host device 110, such as texts, graphics, videos, and the like, to the user. Also, the touch screen device 180 may further apply not only capacitive, resistive, infrared ray, and surface acoustic wave technologies but also certain multi-touch sensing technologies including other proximity sensor arrays or other elements.<!-- EPO <DP n="8"> --></p><p id="p0041" num="0041">The other input/control devices 190 may include an up/down button for volume control. In addition, the other input/control devices 190 may include at least one of pointer devices, such as a push button, a rocker button, a locker switch, a thumb wheel, a dial, a stick, a stylus, and the like, each of which has a corresponding function.</p><p id="p0042" num="0042">The external memory device 120 includes a high-speed Random Access Memory (RAM) such as one or more magnetic storages, a non-volatile memory, one or more optical storages, and/or a flash memory (e.g., a NAND flash memory or a NOR flash memory). The external memory device 120 may include a non-transitory computer-readable storage medium. The external memory device 120 includes software components. The software components include an Operating System (OS) module, a communication module, a graphic module, a User Interface (UI) module, a codec module, a camera module, one or more application modules, and the like. The modules may be expressed in a set of instructions, an instruction set, programs, and/or the like.</p><p id="p0043" num="0043">The OS module may correspond to an embedded OS such as Windows, Linux, Darwin, RTXC, UNIX, OS X, VxWorks, and/or the like. The OS module includes several software components for controlling a general system operation. For example, control of this general system operation includes memory management and control, storage hardware (device) control and management, power control and management, and the like. In addition, the OS module also performs a function for smoothly communicating between several hardware devices and software components (modules).</p><p id="p0044" num="0044">The communication module may communicate with other electronic devices such as a computer, a server, a portable terminal, and the like through the wireless communication device 150 and/or the external port device 170.</p><p id="p0045" num="0045">The graphic module includes several software components for providing and displaying graphics on the touch screen device 180. The term "graphics" includes texts, web pages, icons, digital images, videos, animations, and the like.</p><p id="p0046" num="0046">The UI module includes several software components related to a UI. In addition, the UI module includes content about whether a state of a UI is changed to any state, whether a state of a UI is changed in any condition, and the like.</p><p id="p0047" num="0047">The codec module may include software components related to encoding and decoding of video files.<!-- EPO <DP n="9"> --></p><p id="p0048" num="0048">The camera module includes cameral-related software components capable of performing camera-related processes and functions. Particularly, in accordance with an exemplary embodiment of the present invention, the camera module acquires information from an image signal output from a region at which the camera device 130 photographs an object, verifies a command corresponding to the acquired information, and performs an operation according to the verified command.</p><p id="p0049" num="0049">The camera module divides the photographing region into a plurality of portions of the photographing region and acquires respective information from respective image signals output while being classified according to the corresponding portion of the photographing region. In addition, the camera module acquires information from an image signal output from a previous portion of the photographing region and acquires information from an image signal output from a next portion of the photographing region after a threshold time.</p><p id="p0050" num="0050">When performance of an operation according to a command corresponding to the information acquired from the image signal output from the previous portion of the photographing region is completed, the camera module may perform an operation according to a command corresponding to the information acquired from the image signal output from the next portion of the photographing region. In addition, when performance of an operation according to the command corresponding to the information acquired from the image signal output from the previous portion of the photographing region is completed, the camera module may verify a next portion of the photographing region related to the completed operation and may acquire information from an image signal output from the verified next portion of the photographing region.</p><p id="p0051" num="0051">Also, the camera module may detect a movement direction of an object from the image signal output from the previous portion of the photographing region, may verify a next portion of the photographing region which is in the detected movement direction, and may acquire information from an image signal output from the verified next portion of the photographing region.</p><p id="p0052" num="0052">In addition, when a command corresponding to information acquired from an image signal output from a portion of the photographing region is identical to a previously specified command, the camera module may determine a command for an operation.<!-- EPO <DP n="10"> --></p><p id="p0053" num="0053">According to exemplary embodiments of the present invention, information acquired from a photographing region at the camera module includes motion information of an object, for example, at least one body of a person. Also, the camera module provides a visual cue for previously informing that an operation is any operation before the corresponding operation is performed. In accordance with operations of the camera module, an input or control event, corresponding to an event according to a push of a button of the electronic device 100 and a touch event of the touch screen, may be generated from an image received from the camera device 130.</p><p id="p0054" num="0054">Also, the camera module divides a photographing region of the camera device 130 into a plurality of portions of the photographing region. When the camera module detects that an object is moved from a first portion of the photographing region to a second portion of the photographing region from an image signal from the photographing region, the camera module performs a first operation. When the camera module detects a corresponding motion of the object from an image signal output from the second photographing region, the camera module may perform a second operation. This first operation may be a visual cue for the second operation.</p><p id="p0055" num="0055">Hereinafter, a description will be given in which information acquired from an image photographed by the camera module relates to a person. However, it is obvious that the information may be replaced with various objects such as animals, places, and the like.</p><p id="p0056" num="0056">The application module includes a browser function, an email function, an instant message function, a word processing function, a keyboard emulation function, an address book function, a touch list function, a widget function, a Digital Right Management (DRM) function, a voice recognition function, a voice copy function, a position determining function, a location based service function, and the like. The external memory device 120 may further include additional modules (commands) in addition to the above-described modules.</p><p id="p0057" num="0057">A variety of functions of the electronic device 100 according to an exemplary embodiment of the present invention may be executed by one or more streaming processing, hardware including an Application Specific Integrated Circuit (ASIC), software, and/or combination of thereof.</p><p id="p0058" num="0058"><figref idrefs="f0002">FIG. 2</figref> illustrates a UI method according to an exemplary embodiment of the present invention.<!-- EPO <DP n="11"> --></p><p id="p0059" num="0059">Referring to <figref idrefs="f0002">FIG. 2</figref>, the electronic device 100 includes a speaker 2, a microphone 3, a touch screen 4, a front camera 5, and a sensor 6. The speaker 2 is installed on a front surface 11 of the electronic device 100 and converts a band of an electric signal into an audible frequency band and outputting the converted signal. The microphone 3 converts sound waves transmitted from people or other sound sources into electric signals. The touch screen 4 outputs an image signal and receives a touch of a user 20. The front camera 5 photographs objects, and a sensor 6 detects a motion, light, a temperature, and the like. The electronic device 100 may further include a rear camera 7 installed in a rear surface 12 thereof. The rear camera 7 may photograph objects.</p><p id="p0060" num="0060">According to exemplary embodiments of the present invention, the user 20 may input signals to the electronic device 100 and may control the electronic device 100 through an action of photographing his or her face through the front camera 5 of the electronic device 100.</p><p id="p0061" num="0061">A region photographed by the camera 5 may be divided into a plurality of portion regions. For example, a camera module according to an exemplary embodiment of the present invention may divide a photographing region of the camera 5 into a middle portion of the photographing region 51, a left portion of the photographing region 52, and a right portion of the photographing region 53.</p><p id="p0062" num="0062">The camera module acquires information about motion (e.g., face movement, a nod), a focal length, and the like from image signals, output from the respective portion of the photographing regions (e.g., the middle portion of the photographing region 51, the left portion of the photographing region 52, and/or the right portion of the photographing region 53) about the face of the user 20, and performs an operation according to a command corresponding to the acquired information.</p><p id="p0063" num="0063">Particularly, the camera module specifies respective commands of the electronic device 110 corresponding to the respective portion of the photographing regions (e.g., the middle portion of the photographing region 51, the left portion of the photographing region 52, and/or the right portion of the photographing region 53). When a command verified from each of the portion of the photographing region is identical to a previously specified command, the camera module performs an operation according to the command. In other words, the camera module specifies an operation<!-- EPO <DP n="12"> --> corresponding to the respective photographing regions 51-53 in advance. For example, when previously defined information is acquired from an image signal, the camera module performs the operation.</p><p id="p0064" num="0064">For example, in accordance with an exemplary embodiment of the present invention, the electronic device 100 may generate an input or control signal corresponding to a push event of a button thereof and a touch event using only an image photographed through the camera 5 without using the push event of the button, and/or the touch event through a touch screen. Accordingly, because the user 20 may control an operation of the electronic device 100 without operating a button and a touch screen of the electronic device 100, the electronic device 100 may provide a convenience to the user 20.</p><p id="p0065" num="0065">Operations of the electronic device 100 include an operation (e.g., a GUI operation) about videos, an operation (e.g., a volume adjustment operation) about audio control.</p><p id="p0066" num="0066"><figref idrefs="f0003">FIG. 3</figref> is a flowchart illustrating a UI method according to an exemplary embodiment of the present invention.</p><p id="p0067" num="0067">Referring to <figref idrefs="f0001">FIGs. 1</figref> and <figref idrefs="f0003">3</figref>, the processor 112 acquires information from an image signal output from a photographing region in step 301. For example, the processor 112 may acquire information about face recognition, a face direction, person's eyes, and the like from an image signal about a person output from the photographing region.</p><p id="p0068" num="0068">The processor 112 verifies a command corresponding to the acquired information in step 303.</p><p id="p0069" num="0069">The processor 112 performs an operation according to the verified command in step 305.</p><p id="p0070" num="0070"><figref idrefs="f0004">FIG. 4</figref> is a flowchart illustrating a process of acquiring information from an image signal of a photographing region such as, for example, a process of acquiring information from the image signal of the photographing region described in relation to step 301 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention.</p><p id="p0071" num="0071">Referring to <figref idrefs="f0001">FIGs. 1</figref> and <figref idrefs="f0004">4</figref>, the processor 112 divides a photographing region into a plurality of portions of the photographing region in step 401.</p><p id="p0072" num="0072">The processor 112 acquires respective information from respective image signals output while being classified according to the corresponding portion of<!-- EPO <DP n="13"> --> the photographing region in step 403.</p><p id="p0073" num="0073">This process illustrated in <figref idrefs="f0004">FIG. 4</figref> shows an example in which the processor 112 acquires information from image signals successively output while being classified according to the partial photographing regions.</p><p id="p0074" num="0074"><figref idrefs="f0005">FIG. 5</figref> is a flowchart illustrating a process of acquiring information from an image signal of a photographing region such as, for example, a process of acquiring information from the image signal of the photographing region described in relation to step 301 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention.</p><p id="p0075" num="0075">Referring to <figref idrefs="f0005">FIG. 5</figref>, the illustrated process corresponds to a process of dividing a photographing region into a plurality of portions of the photographing regions.</p><p id="p0076" num="0076">As illustrated in <figref idrefs="f0005">FIG. 5</figref>, the processor 112 completes performance of an operation according to a command corresponding to information acquired from an image signal output from a previous portion of the photographing region in step 501.</p><p id="p0077" num="0077">The processor 112 acquires information from an image signal from a next portion of the photographing region in step 503.</p><p id="p0078" num="0078">If a certain process of acquiring the information from the image signal output from the previous portion of the photographing region, verifying the command corresponding to the acquired information, and performing the operation according to the verified command is completed, the process illustrated in <figref idrefs="f0005">FIG. 5</figref> shows an example in which the processor 112 also applies this process to the other partial photographing region equally.</p><p id="p0079" num="0079"><figref idrefs="f0006">FIG. 6</figref> is a flowchart illustrating a process of acquiring information from an image signal of a photographing region such as, for example, a process of acquiring information from the image signal of the photographing region described in relation to step 301 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention.</p><p id="p0080" num="0080">Referring to <figref idrefs="f0006">FIG. 6</figref>, the illustrated process corresponds to a process of dividing a photographing region into a plurality of portions of the photographing region.</p><p id="p0081" num="0081">As illustrated in <figref idrefs="f0006">FIG. 6</figref>, the processor 112 completes performance of an operation according to a command corresponding to information acquired from an image signal output from a previous portion of the photographing region in step 601.</p><p id="p0082" num="0082">The processor 112 verifies a next portion of the photographing region<!-- EPO <DP n="14"> --> related to the completed operation in step 603.</p><p id="p0083" num="0083">The processor 112 acquires information from an image signal output from the verified next portion of the photographing region in step 605.</p><p id="p0084" num="0084">If a certain process of acquiring the information from the image signal output from the previous portion of the photographing region, verifying the command corresponding to the acquired information, and performing the operation according to the verified command is completed, the process illustrated in <figref idrefs="f0006">FIG. 6</figref> shows an example in which the processor 112 also applies this process to the next portion of the photographing region related to the completed operation.</p><p id="p0085" num="0085"><figref idrefs="f0007">FIG. 7</figref> is a flowchart illustrating a process of verifying a command corresponding to information acquired from an image signal of a photographing region such as, for example, a process of verifying the command corresponding to the acquired information described in relation to step 303 of <figref idrefs="f0003">FIG. 3</figref> according to an exemplary embodiment of the present invention.</p><p id="p0086" num="0086">Referring to <figref idrefs="f0007">FIG. 7</figref> the illustrated process corresponds to a process of dividing a photographing region into a plurality of portions of the photographing region.</p><p id="p0087" num="0087">As illustrated in <figref idrefs="f0007">FIG. 7</figref>, the processor 112 verifies a command corresponding to information acquired from each portion of the photographing region in step 701.</p><p id="p0088" num="0088">When the verified command is identical to a command which is previously specified at each portion of the photographing region, the processor 112 determines the command in step 703.</p><p id="p0089" num="0089">The process of <figref idrefs="f0007">FIG. 7</figref> shows that the processor 112 permits only operations according to the commands which are previously specified while being classified according to the corresponding portion of the photographing region. For this reason, the information acquired from the image signals output from the partial photographing regions is specified. For example, when previously specified information must be acquired from the image signals of the portion of the photographing region, a corresponding operation may be performed.</p><p id="p0090" num="0090"><figref idrefs="f0008 f0009 f0010 f0011">FIGs. 8A to 8D</figref> illustrate a UI method according to an exemplary embodiment of the present invention.</p><p id="p0091" num="0091">Referring to <figref idrefs="f0008 f0009 f0010 f0011">FIGs. 8A to 8D</figref>, a description will be given with respect to<!-- EPO <DP n="15"> --> a webpage screen.</p><p id="p0092" num="0092">As illustrated in <figref idrefs="f0008 f0009 f0010 f0011">FIGs. 8A to 8D</figref>, a camera module divides a photographing region of a camera into a middle portion of the photographing region 51, a left portion of the photographing region 52, and a right portion of the photographing region 53. Also, the camera module specifies an operation matched with the middle portion of the photographing region 51 as an operation for providing a visual cue for screen change. The camera module specifies an operation matched with the left portion of the photographing region 52 as a change operation to a previous screen and specifies an operation matched with the right portion of the photographing region 53 as a change operation to a next screen.</p><p id="p0093" num="0093">Also, the camera module specifies motion information matched with the middle portion of the photographing region 51 as a left movement or right movement of a face of a user 20. If the left movement of the face is verified, the camera module provides a visual cue for informing change to a previous webpage in advance. If the left movement of the face is verified, the camera module provides a visual cue for informing change to a next webpage in advance. The visual cue does not immediately result in performance of a change operation to the previous or next webpage. Rather, the visual cue results in informing the user 20 of the change operation in advance.</p><p id="p0094" num="0094">In addition, the camera module provides a visual cue for informing screen change to a previous webpage in advance according to a command verified from an image signal output from the middle portion of the photographing region 51. The camera module performs an operation according to a command corresponding to information acquired from an image signal output from the left portion of the photographing region 52 related to the visual cue. Also, the camera module provides a visual cue for informing screen change to a next webpage in advance according to the command verified from the image signal output from the middle portion of the photographing region 51. The camera module performs an operation according to a command corresponding to information acquired from an image signal output from the right portion of the photographing region 53 related to the visual cue.</p><p id="p0095" num="0095">For example, the camera module verifies a next portion of the photographing region related to an operation performed from the image signal output from the middle portion of the photographing region 51, acquires information from an image signal output from the verified next portion of the photographing region, and<!-- EPO <DP n="16"> --> performs an operation according to a command corresponding to the acquired information.</p><p id="p0096" num="0096">The camera module may determine a next portion of the photographing region related to information from the information acquired from an image signal output from the middle portion of the photographing region 51, may acquire information from an image signal output from the determined next portion of the photographing region, and may perform an operation according to a command corresponding to the acquired information. For example, the camera module may perform a certain process of detecting a movement direction of the face of the user from an image signal output from the middle portion of the photographing region 51, providing a visual cue for informing screen change to a previous webpage and screen change to a next webpage in advance, verifying a next portion of the photographing region which is in the detected movement direction of the face, acquiring information from an image signal output from the verified next portion of the photographing region, and performing an operation according to a command corresponding to the verified information.</p><p id="p0097" num="0097">According to exemplary embodiments of the present invention, camera module specifies motion information matched with the left portion of the photographing region 52 as a nod. If the nod is verified, the camera module performs an operation changed to a previous webpage. Also, the camera module specifies motion information matched with the right portion of the photographing region 52 as a nod. If the nod is verified, the camera module performs an operation to change to a next webpage.</p><p id="p0098" num="0098">As described above, when the camera module provides a visual cue for informing change to a previous webpage, the camera module acquires information from an image signal output from the left portion of the photographing region 52 related to the visual cue. When the information is information corresponds to a nod, the camera module performs an operation changed to the previous webpage. Also, when the camera module provides a visual cue for informing change to a next webpage, the camera module acquires information from an image signal output from the right portion of the photographing region 53 related to the visual cue. When the information is information corresponds to a nod, the camera module performs an operation changed to the next webpage.<!-- EPO <DP n="17"> --></p><p id="p0099" num="0099"><figref idrefs="f0008 f0009 f0010 f0011">FIGs. 8A to 8D</figref> illustrate a visual cue 55 for informing change to a next webpage screen when the face of the user is moved to a right side and then illustrate change to a next webpage screen by a nod according to an exemplary embodiment of the present invention.</p><p id="p0100" num="0100"><figref idrefs="f0008 f0009 f0010 f0011">FIGs. 8A to 8D</figref> illustrate, but are not limited to, screen change taken as an example according to an exemplary embodiment of the present invention. For example, exemplary embodiments of the present invention may be applied to screen enlargement or reduction and video or audio-related operations related to other electronic devices.</p><p id="p0101" num="0101">The camera module divides a photographing region of the camera into a plurality of portions of the photographing region. When the camera module detects that an object is moved from a first portion of the photographing region to a second portion of the photographing region from an image signal from the photographing region, the camera module performs a first operation. When the camera module detects a corresponding motion of the object from an image signal output from the second photographing region, the camera module may perform a second operation. For example, when the camera module detects movement of a face of the user from the middle portion of the photographing region 51 to the left portion of the photographing region 52 from an image signal output from a photographing region, the camera module provides a visual cue for informing change to a previous webpage screen in advance. When the camera module detects a nod from an image signal output from the left portion of the photographing region 52, the camera module performs change to a previous webpage screen.</p><p id="p0102" num="0102">According to exemplary embodiments of the present invention, other visual cues for confirming or verifying a desired operation may include a facial expression, movement of the user's 20 eyes, and the like.</p><p id="p0103" num="0103">Methods according to exemplary embodiments described herein may be implemented as hardware, software, or combinational type of the hardware and the software.</p><p id="p0104" num="0104">When the method is implemented by the software, a non-transitory computer-readable storage medium for storing one or more programs (software modules) may be provided. The one or more programs stored in the non-transitory computer-readable storage medium are configured for being executed by one or more<!-- EPO <DP n="18"> --> processors in an electronic device. The one or more programs include instructions for allowing an electronic device to execute the methods according to exemplary embodiments described herein.</p><p id="p0105" num="0105">These programs (software module, software) may be stored in a RAM, a non-volatile memory including a flash memory, a Read Only Memory (ROM), an Electrically Erasable Programmable ROM (EEPROM), a magnetic disc storage device, a Compact Disc-ROM (CD-ROM), a Digital Versatile Disc (DVD) or an optical storage device of a different type, and a magnetic cassette. Or, the programs may be stored in a memory configured by combination of some or all of them. Also, the configured memory may include a plurality of memories.</p><p id="p0106" num="0106">Also, the programs may be stored in an attachable storage device capable of accessing an electronic device through each of communication networks such as the Internet, an intranet, a Local Area Network (LAN), a Wide LAN (WLAN), a Storage Area Network (SAN), and the like, or a communication network configured by combination thereof. This storage device may connect to the electronic device through an external port.</p><p id="p0107" num="0107">Also, a separate storage device on a communication network may connect to a portable electronic device.</p><p id="p0108" num="0108">A UI method and an apparatus therefor according to an exemplary embodiment of the present invention may control an operation of a portable electronic device without operating a button of the portable electronic device. Also, a UI method and an apparatus therefor according to an exemplary embodiment of the present invention may improve availability of a portable electronic device by generating input capable of replacing a button push event or a touch event of the portable electronic device from a motion of a user.</p><p id="p0109" num="0109">While the present invention has been shown and described with reference to certain exemplary embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims and their equivalents.</p></description><claims mxw-id="PCLM56982095" lang="EN" load-source="patent-office"><!-- EPO <DP n="19"> --><claim id="c-en-0001" num="0001"><claim-text>A method comprising:
<claim-text>dividing a photographing region into a plurality of portions of the photographing region;</claim-text>
<claim-text>acquiring corresponding information from respective image signals output while being classified according to the divided portion of the photographing region;</claim-text>
<claim-text>verifying a command corresponding to the acquired information; and</claim-text>
<claim-text>performing an operation according to the verified command.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The method of claim 1, wherein the acquiring of the corresponding information from the respective image signals output while being classified according to the divided portion of the photographing region comprises:
<claim-text>acquiring information from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region; and</claim-text>
<claim-text>acquiring information from an image signal output from a next portion of the photographing region after a threshold time.</claim-text></claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The method of claim 1, wherein the acquiring of the corresponding information from the respective image signals output while being classified according to the divided portion of the photographing region comprises:
<claim-text>performing an operation according to a command corresponding to information acquired from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region; and</claim-text>
<claim-text>acquiring information from an image signal output from a next portion of the photographing region when the operation is completed.</claim-text></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The method of claim 1, wherein the acquiring of the corresponding information from the respective image signals output while being classified according to the divided portion of the photographing region comprises:<!-- EPO <DP n="20"> -->
<claim-text>performing an operation according to a command corresponding to information acquired from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region;</claim-text>
<claim-text>verifying a next portion of the photographing region related to the operation;<br/>
and</claim-text>
<claim-text>acquiring information from an image signal output from the verified next portion of the photographing region.</claim-text></claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The method of claim 1, wherein the acquiring of the corresponding information from the respective image signals output while being classified according to the divided portion of the photographing region comprises:
<claim-text>detecting a movement direction of an object from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region;</claim-text>
<claim-text>verifying a next portion of the photographing region which is in the detected movement direction;</claim-text>
<claim-text>acquiring information from an image signal output from the verified next portion of the photographing region.</claim-text></claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The method of claim 1, wherein the acquiring of the corresponding information from the respective image signals output while being classified according to the divided portion of the photographing region comprises acquiring motion information of an object.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The method of claim 1, wherein the verification of the command corresponding to the acquired information comprises determining the command when the command corresponding to the information acquired from the image signal output from each of the portion of the photographing region is identical to a command which is previously specified at each of the portion of the photographing region.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The method of claim 1, wherein the performing of the operation according to the verified command comprises providing a visual cue.<!-- EPO <DP n="21"> --></claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>An electronic device comprising:
<claim-text>a camera;</claim-text>
<claim-text>at least one processor;</claim-text>
<claim-text>a memory; and</claim-text>
<claim-text>at least one module, each of which is stored in the memory and is configured to be executed by the at least one processor,</claim-text>
<claim-text>wherein at least one of the at least one module divides a photographing region of the camera into a plurality of portions of the photographing region, acquires corresponding information from respective image signals output while being classified according to the divided portion of the photographing region, verifies a command corresponding to the acquired information, and performs an operation according to the verified command.</claim-text></claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>The device of claim 9, wherein at least one of the at least one module acquires information from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region and acquires information from an image signal output from a next portion of the photographing region after a threshold time.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The device of claim 9, wherein at least one of the at least one module performs an operation according to a command corresponding to information acquired from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region and acquires information from an image signal output from a next portion of the photographing region when the operation is completed.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>The device of claim 9, wherein at least one of the at least one module performs an operation according to a command corresponding to information acquired from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region, verifies a next portion of the photographing region related to the operation, and acquires information from an image signal output from the verified next portion of the photographing region.<!-- EPO <DP n="22"> --></claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>The device of claim 9, wherein at least one of the at least one module detects a movement direction of an object from an image signal output from a previous portion of the photographing region among the divided portion of the photographing region, verifies a next portion of the photographing region which is in the detected movement direction, and acquires information from an image signal output from the verified next portion of the photographing region.</claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>The device of claim 9, wherein each of the one or more modules acquires motion information of an object from the respective image signals output while being classified according to the divided portion of the photographing regions.</claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>The device of claim 9, wherein at least one of the at least one module determines the command when the command corresponding to the information acquired from the image signal output from each of the portions of the photographing region is identical to a command which is previously specified at each of the portions of the photographing region.</claim-text></claim></claims><drawings mxw-id="PDW16670485" load-source="patent-office"><!-- EPO <DP n="23"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="200" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="24"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="200" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="25"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="26"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="27"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="28"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="29"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="30"> --><figure id="f0008" num="8A"><img id="if0008" file="imgf0008.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="31"> --><figure id="f0009" num="8B"><img id="if0009" file="imgf0009.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> --><figure id="f0010" num="8C"><img id="if0010" file="imgf0010.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> --><figure id="f0011" num="8D"><img id="if0011" file="imgf0011.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="156" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="158" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
