<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681711-A1" country="EP" doc-number="2681711" kind="A1" date="20140108" family-id="45809360" file-reference-id="318301" date-produced="20180822" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586095" ucid="EP-2681711-A1"><document-id><country>EP</country><doc-number>2681711</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12707651-A" is-representative="NO"><document-id mxw-id="PAPP154848287" load-source="docdb" format="epo"><country>EP</country><doc-number>12707651</doc-number><kind>A</kind><date>20120224</date><lang>EN</lang></document-id><document-id mxw-id="PAPP173226905" load-source="docdb" format="original"><country>EP</country><doc-number>12707651.1</doc-number><date>20120224</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140551247" ucid="EP-11305223-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>11305223</doc-number><kind>A</kind><date>20110302</date></document-id></priority-claim><priority-claim mxw-id="PPC140556440" ucid="EP-12707651-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>12707651</doc-number><kind>A</kind><date>20120224</date></document-id></priority-claim><priority-claim mxw-id="PPC140547850" ucid="IB-2012050855-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>IB</country><doc-number>2012050855</doc-number><kind>W</kind><date>20120224</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989316533" load-source="docdb">A61B   6/00        20060101ALI20120919BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989320825" load-source="docdb">G06T   7/00        20060101AFI20120919BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989321968" load-source="docdb">G06T   7/20        20060101ALI20120919BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861390315" load-source="docdb" scheme="CPC">G06T   7/30        20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1861391056" load-source="docdb" scheme="CPC">G06T   7/246       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059330633" load-source="docdb" scheme="CPC">G06T2207/10121     20130101 LA20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059331657" load-source="docdb" scheme="CPC">G06T2207/30021     20130101 LA20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059333521" load-source="docdb" scheme="CPC">A61B   6/032       20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059333629" load-source="docdb" scheme="CPC">A61B   6/4441      20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059333912" load-source="docdb" scheme="CPC">A61B   6/487       20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059334074" load-source="docdb" scheme="CPC">A61B   6/503       20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059335081" load-source="docdb" scheme="CPC">G06T2207/10072     20130101 LA20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059335901" load-source="docdb" scheme="CPC">A61B   6/5264      20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059337304" load-source="docdb" scheme="CPC">A61B   6/5288      20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059338736" load-source="docdb" scheme="CPC">A61B   6/12        20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059339590" load-source="docdb" scheme="CPC">G06T2207/30048     20130101 LA20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059339901" load-source="docdb" scheme="CPC">G06T2207/30101     20130101 LA20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2059340591" load-source="docdb" scheme="CPC">A61B   6/5247      20130101 LI20150506BHEP        </classification-cpc><classification-cpc mxw-id="PCL2003767609" load-source="docdb" scheme="CPC">G06T   7/0012      20130101 FI20140116BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132362918" lang="DE" load-source="patent-office">VISUALISIERUNG FÜR EINE NAVIGATIONSFÜHRUNG</invention-title><invention-title mxw-id="PT132362919" lang="EN" load-source="patent-office">VISUALIZATION FOR NAVIGATION GUIDANCE</invention-title><invention-title mxw-id="PT132362920" lang="FR" load-source="patent-office">VISUALISATION POUR UN GUIDAGE DE NAVIGATION</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR919545825" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>KONINKL PHILIPS NV</last-name><address><country>NL</country></address></addressbook></applicant><applicant mxw-id="PPAR919538727" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KONINKLIJKE PHILIPS N.V.</last-name></addressbook></applicant><applicant mxw-id="PPAR919018963" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Koninklijke Philips N.V.</last-name><iid>101391185</iid><address><street>High Tech Campus 5</street><city>5656 AE Eindhoven</city><country>NL</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919523853" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CATHIER PASCAL YVES FRANCOIS</last-name><address><country>NL</country></address></addressbook></inventor><inventor mxw-id="PPAR919506195" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CATHIER, PASCAL, YVES, FRANCOIS</last-name></addressbook></inventor><inventor mxw-id="PPAR919012137" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>CATHIER, Pascal, Yves, François</last-name><address><street>c/o High Tech Campus Building 44</street><city>NL-5656 AE Eindhoven</city><country>NL</country></address></addressbook></inventor><inventor mxw-id="PPAR919530731" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>FLORENT RAOUL</last-name><address><country>NL</country></address></addressbook></inventor><inventor mxw-id="PPAR919504878" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>FLORENT, RAOUL</last-name></addressbook></inventor><inventor mxw-id="PPAR919009897" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>FLORENT, RAOUL</last-name><address><street>c/o High Tech Campus Building 44</street><city>NL-5656 AE Eindhoven</city><country>NL</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919017649" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Steffen, Thomas</last-name><iid>100958149</iid><address><street>Philips Intellectual Property &amp; Standards P.O. Box 220</street><city>5600 AE Eindhoven</city><country>NL</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="IB-2012050855-W"><document-id><country>IB</country><doc-number>2012050855</doc-number><kind>W</kind><date>20120224</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012117321-A1"><document-id><country>WO</country><doc-number>2012117321</doc-number><kind>A1</kind><date>20120907</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549832882" load-source="docdb">AL</country><country mxw-id="DS549756244" load-source="docdb">AT</country><country mxw-id="DS549832883" load-source="docdb">BE</country><country mxw-id="DS549834071" load-source="docdb">BG</country><country mxw-id="DS549844769" load-source="docdb">CH</country><country mxw-id="DS549756882" load-source="docdb">CY</country><country mxw-id="DS549756249" load-source="docdb">CZ</country><country mxw-id="DS549832884" load-source="docdb">DE</country><country mxw-id="DS549756883" load-source="docdb">DK</country><country mxw-id="DS549756884" load-source="docdb">EE</country><country mxw-id="DS549764036" load-source="docdb">ES</country><country mxw-id="DS549834072" load-source="docdb">FI</country><country mxw-id="DS549834073" load-source="docdb">FR</country><country mxw-id="DS549832885" load-source="docdb">GB</country><country mxw-id="DS549756893" load-source="docdb">GR</country><country mxw-id="DS549832890" load-source="docdb">HR</country><country mxw-id="DS549756250" load-source="docdb">HU</country><country mxw-id="DS549844770" load-source="docdb">IE</country><country mxw-id="DS549756894" load-source="docdb">IS</country><country mxw-id="DS549834082" load-source="docdb">IT</country><country mxw-id="DS549756895" load-source="docdb">LI</country><country mxw-id="DS549831130" load-source="docdb">LT</country><country mxw-id="DS549913256" load-source="docdb">LU</country><country mxw-id="DS549831131" load-source="docdb">LV</country><country mxw-id="DS549831132" load-source="docdb">MC</country><country mxw-id="DS549913257" load-source="docdb">MK</country><country mxw-id="DS549913258" load-source="docdb">MT</country><country mxw-id="DS549764049" load-source="docdb">NL</country><country mxw-id="DS549834083" load-source="docdb">NO</country><country mxw-id="DS549913259" load-source="docdb">PL</country><country mxw-id="DS549844771" load-source="docdb">PT</country><country mxw-id="DS549764050" load-source="docdb">RO</country><country mxw-id="DS549844772" load-source="docdb">RS</country><country mxw-id="DS549913260" load-source="docdb">SE</country><country mxw-id="DS549832891" load-source="docdb">SI</country><country mxw-id="DS549834084" load-source="docdb">SK</country><country mxw-id="DS549913261" load-source="docdb">SM</country><country mxw-id="DS549756896" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA99835356" ref-ucid="WO-2012117321-A1" lang="EN" load-source="patent-office"><p num="0000">The present invention relates to visualizing information of an object. In order to provide spatial information and in addition situation specific data to the user while ensuring an effective perceptibility, a method (110) is provided comprising the steps of: a) providing (112) pre-navigation data (114) of a region of interest of an object (22); wherein the pre-navigation data comprises spatial geometrical data (116) and a functional parameter surface (118) in correspondence to the spatial geometrical data; b) acquiring (120) live image data (122) of the region of interest; c) detecting (124) an element (126) in the live image data; d) determining (128) spatial relation (130) of the pre-navigation data and the live image data; e) determining (132) the position (134) of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and computing (136) a predetermined related point of location (138) on the functional parameter surface; f) generating (140) a combination (142) of a simplified surface representation (144) of the region of interest, which simplified surface representation is based on a visualization of the functional parameter surface, and a marker (146) indicating the computed predetermined related point of location; and g) displaying (148) the combination as navigation guidance (150).</p></abstract><abstract mxw-id="PA100331190" ref-ucid="WO-2012117321-A1" lang="EN" source="national office" load-source="docdb"><p>The present invention relates to visualizing information of an object. In order to provide spatial information and in addition situation specific data to the user while ensuring an effective perceptibility, a method (110) is provided comprising the steps of: a) providing (112) pre-navigation data (114) of a region of interest of an object (22); wherein the pre-navigation data comprises spatial geometrical data (116) and a functional parameter surface (118) in correspondence to the spatial geometrical data; b) acquiring (120) live image data (122) of the region of interest; c) detecting (124) an element (126) in the live image data; d) determining (128) spatial relation (130) of the pre-navigation data and the live image data; e) determining (132) the position (134) of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and computing (136) a predetermined related point of location (138) on the functional parameter surface; f) generating (140) a combination (142) of a simplified surface representation (144) of the region of interest, which simplified surface representation is based on a visualization of the functional parameter surface, and a marker (146) indicating the computed predetermined related point of location; and g) displaying (148) the combination as navigation guidance (150).</p></abstract><abstract mxw-id="PA99835357" ref-ucid="WO-2012117321-A1" lang="FR" load-source="patent-office"><p num="0000">La présente invention porte sur la visualisation d'informations d'un objet. Afin de fournir des informations spatiales et en outre des données spécifiques à une situation à l'utilisateur tout en assurant un caractère perceptible efficace, l'invention porte sur un procédé (110) comprenant les étapes consistant à : a) fournir (112) des données de prénavigation (114) d'une région d'intérêt d'un objet (22) ; les données de prénavigation comprenant des données géométriques spatiales (116) et une surface de paramètre fonctionnel (118) en correspondance aux données géométriques spatiales ; b) acquérir (120) des données d'image en direct (122) de la région d'intérêt ; c) détecter (124) un élément (126) dans les données d'image en direct ; d) déterminer (128) une relation spatiale (130) des données de prénavigation et des données d'image en direct ; e) déterminer (132) la position (134) de l'élément détecté dans les données géométriques spatiales, laquelle détermination est basée sur la relation spatiale, et calculer (136) un point apparenté prédéterminé de localisation (138) sur la surface de paramètre fonctionnel ; f) générer (140) une combinaison (142) d'une représentation de surface simplifiée (144) de la région d'intérêt, laquelle représentation de surface simplifiée est basée sur une visualisation de la surface de paramètre fonctionnel, et un marqueur (146) indiquant le point de localisation apparenté prédéterminé calculé ; et g) afficher (148) la combinaison en tant que guidage de navigation (150).</p></abstract><abstract mxw-id="PA100331191" ref-ucid="WO-2012117321-A1" lang="FR" source="national office" load-source="docdb"><p>La présente invention porte sur la visualisation d'informations d'un objet. Afin de fournir des informations spatiales et en outre des données spécifiques à une situation à l'utilisateur tout en assurant un caractère perceptible efficace, l'invention porte sur un procédé (110) comprenant les étapes consistant à : a) fournir (112) des données de prénavigation (114) d'une région d'intérêt d'un objet (22) ; les données de prénavigation comprenant des données géométriques spatiales (116) et une surface de paramètre fonctionnel (118) en correspondance aux données géométriques spatiales ; b) acquérir (120) des données d'image en direct (122) de la région d'intérêt ; c) détecter (124) un élément (126) dans les données d'image en direct ; d) déterminer (128) une relation spatiale (130) des données de prénavigation et des données d'image en direct ; e) déterminer (132) la position (134) de l'élément détecté dans les données géométriques spatiales, laquelle détermination est basée sur la relation spatiale, et calculer (136) un point apparenté prédéterminé de localisation (138) sur la surface de paramètre fonctionnel ; f) générer (140) une combinaison (142) d'une représentation de surface simplifiée (144) de la région d'intérêt, laquelle représentation de surface simplifiée est basée sur une visualisation de la surface de paramètre fonctionnel, et un marqueur (146) indiquant le point de localisation apparenté prédéterminé calculé ; et g) afficher (148) la combinaison en tant que guidage de navigation (150).</p></abstract><description mxw-id="PDES51232973" ref-ucid="WO-2012117321-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> VISUALIZATION FOR NAVIGATION GUIDANCE </p><p id="p0002" num="0002">FIELD OF THE INVENTION </p><p id="p0003" num="0003"> The present invention relates to visualizing information of an object, in particular to a method for visualizing information of an object of interest, a device for visualizing, a medical imaging system for visualizing as well as to a computer program element and a computer readable medium. </p><p id="p0004" num="0004">BACKGROUND OF THE INVENTION </p><p id="p0005" num="0005"> To visualize information about a situation of an object, for example a patient, it is known to overlay two-dimensional live images to preoperative images. The preoperative image data contains information, for example, about the vascular structure, whereas the 2D live image provides information about the current situation. For example, in document WO 2008/107814 Al, a method is described to provide a user with such information. However, the information thus presented only provides limited information, for example for </p><p id="p0006" num="0006">cardiovascular interventional guidance. However, with the development of new examination and treatment procedures, for example for interventional cardiovascular procedures, specifically, for complex procedures such as ablations and device implantations in cardiac electrophysiology, the demand for provided information increases constantly. </p><p id="p0007" num="0007">SUMMARY OF THE INVENTION </p><p id="p0008" num="0008"> Thus, there may be a need to provide spatial information and in addition situation specific data to the user while ensuring an increased perceptibility. </p><p id="p0009" num="0009"> In the present invention, this is solved by the subject-matter of the independent claims, wherein further embodiments are incorporated in the dependent claims. </p><p id="p0010" num="0010"> It should be noted that the following described aspects of the invention apply also for the device for visualizing information of an object of interest, the medical imaging system for visualizing information of an object of interest, the computer program element, and 
<!-- EPO <DP n="4"/>-->
 the computer readable medium. </p><p id="p0011" num="0011"> According to an exemplary embodiment of the invention, a method for visualizing information of an object of interest is provided that comprises the following steps: </p><p id="p0012" num="0012"> a) providing pre-navigation data of a region of interest of an object, wherein the pre-navigation data comprises spatial geometrical data and a functional parameter surface in correspondence to the spatial geometrical data; </p><p id="p0013" num="0013"> b) acquiring live image data of the region of interest; </p><p id="p0014" num="0014"> c) detecting an element in the live image data; </p><p id="p0015" num="0015"> d) determining spatial relation of the pre-navigation data and the live image data; </p><p id="p0016" num="0016"> e) determining the position of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and computing a predetermined related point of location on the functional parameter surface; </p><p id="p0017" num="0017"> f) generating a combination of a simplified surface representation of the region of interest, which simplified surface representation is based on a visualization of the functional parameter surface, and a marker indicating the computed predetermined related point of location; and </p><p id="p0018" num="0018"> g) displaying the combination as navigation guidance. </p><p id="p0019" num="0019"> According to a further exemplary embodiment of the invention, a device is provided, comprising a processing unit, an interface unit, and a display. </p><p id="p0020" num="0020"> The interface unit is adapted to provide pre-navigation data of a region of interest of an object, wherein the pre-navigation data comprises spatial geometrical data and a functional parameter surface in correspondence to the spatial geometrical data. The interface unit is further adapted to provide live image data of the region of interest. </p><p id="p0021" num="0021"> The processing unit is adapted to detect an element in the live image data. The processing unit is further adapted to determine spatial relation of the pre-navigation data and the live image data. The processing unit is also adapted to determine the position of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and to compute a predetermined related point of location on the functional parameter surface. The processing unit is still further adapted to generate a combination of a simplified surface representation of the region of interest, which simplified surface representation is based on a visualization of the functional parameter surface, and a marker indicating the computed predetermined related point of location. 
<!-- EPO <DP n="5"/>-->
 The display is adapted to display the combination as navigation guidance. </p><p id="p0022" num="0022"> According to a further exemplary embodiment of the invention, a medical imaging system for visualizing information of an object of interest is provided, comprising a device according to the above mentioned exemplary embodiment, and image acquisition means. The image acquisition means are adapted to acquire the live image data. </p><p id="p0023" num="0023"> It can be seen as a gist of the invention to provide both spatial information and functional information. Both types of information are provided in one image in form of the navigation guidance. To allow for a quick understanding that does not require complex imagination on the side of the user, a simplified surface representation is chosen wherein the functional parameter surface is visualized, and the marker provides the spatial information about the current situation. </p><p id="p0024" num="0024"> These and other aspects of the present invention will become apparent from and elucidated with reference to exemplary embodiments described hereinafter. BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0025" num="0025"> Exemplary embodiments of the invention will be described in the following with reference to the following drawings: </p><p id="p0026" num="0026">Fig. 1 illustrates a medical imaging system with a device according to an exemplary embodiment of the invention. </p><p id="p0027" num="0027"> Fig. 2 schematically illustrates the basic steps of an exemplary embodiment of the invention. </p><p id="p0028" num="0028"> Figs. 3 to 8 show method steps of further exemplary embodiments. Fig. 9 schematically describes a further exemplary embodiment of the invention.</p><p id="p0029" num="0029">Figs. 10 to 20 show exemplary embodiments of navigation guidance provided to the user according to the invention. </p><p id="p0030" num="0030">DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS </p><p id="p0031" num="0031"> Fig. 1 schematically shows a medical image system 10, for the use in </p><p id="p0032" num="0032">cardiovascular laboratory, for example. The medical image system 10 for visualizing information of an object of interest comprises image acquisition means 12. For example, the image acquisition means 12 are X-ray image acquisition means provided with a source of X- 
<!-- EPO <DP n="6"/>-->
 ray radiation 14 to generate X-ray radiation, indicated by an X-ray beam 16. Further an X-ray image detection module 18 is located opposite the source of X-ray radiation 14 such that, for example, during a radiation procedure, an object, for example a patient 20, can be located between the source of X-ray radiation 14 and the detection module 18. Further a table 22 is provided to receive the object to be examined, i.e. a patient 20. </p><p id="p0033" num="0033"> According to another exemplary embodiment, although not shown, the medical image system 10 comprises image acquisition means 12 in form of ultrasound image acquisition means. Of course, in case of ultrasound image acquisition means, the source of X- ray radiation, the X-ray beam 16, as well as the image detection module 18 are replaced by an ultrasound transducer emitting ultrasound waves into the object, for example the patient, and receiving reflected ultrasound waves. </p><p id="p0034" num="0034"> According to a further exemplary embodiment, the image acquisition means 12 acquire spatial geometrical data by MR (Magnetic Resonance Imaging MRI) or NMRI (Nuclear Magnetic Resonance Imaging). Of course, other nuclear imaging such as SPECT (Single Photon Emission Computed Tomography) or PET (Positron Emission Tomography) is also provided in exemplary embodiments (not shown). </p><p id="p0035" num="0035"> Further, the medical image system 10 of Fig. 1 comprises a device 24 for visualizing information of an object. The device 24 comprises a processing unit 26, an interface unit 28, and a display 30. </p><p id="p0036" num="0036"> The interface unit 28 is adapted to provide pre-navigation data of a region of interest of an object, wherein the pre-navigation data comprises spatial geometrical data and a functional parameter surface in correspondence to the spatial geometrical data. The interface unit 28 is further adapted to provide live image data of the region of interest. </p><p id="p0037" num="0037"> The processing unit 26 is adapted to detect an element in the live image data. The processing unit 26 is also adapted to determine spatial relation of the pre-navigation data and the live image data. The processing unit 26 is also adapted to determine the position of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and to compute a predetermined related point of location on the functional parameter surface. The processing unit 26 is still further adapted to generate a combination of a simplified surface representation of the region of interest, which simplified surface </p><p id="p0038" num="0038">representation is based on a visualization of the functional parameter surface, and a marker indicating the computed predetermined related point of location. </p><p id="p0039" num="0039"> The display 30 is adapted to display the combination as navigation guidance. 
<!-- EPO <DP n="7"/>-->
 Further, the acquisition means 12 are adapted to acquire the live image data.</p><p id="p0040" num="0040">The acquired live image data is then provided to the interface unit 28 which is indicated by the first connecting line 32. The interface unit 28 then provides the live image data to the processing unit which is indicated by a second connecting line 34. The generated combination is provided by the processing unit 26 to a display 30 which is indicated by a third connecting line 36. Of course, the data connections of the above mentioned units and means can also be realized with a wireless connection. </p><p id="p0041" num="0041"> It is noted that the example shown is a so-called CT-image acquisition device. Of course, the invention also relates to other types of X-ray image acquisition means, such as a C-arm X-ray image acquisition device with a C-arm instead of a circular gentry, as shown in Fig. 1. </p><p id="p0042" num="0042"> The procedure according to the invention is described in more detail below with reference to Fig. 2. </p><p id="p0043" num="0043"> First, in a providing step 112, pre-navigation providing data 114 of a region of interest of an object 22 is provided. The pre-navigation data 114 comprises spatial geometrical data 116 and a functional parameter surface 118 in correspondence to the spatial geometrical data 116, wherein the term "correspondence" relates to spatial correspondence. </p><p id="p0044" num="0044"> Further, in an acquisition step 120, live image data 122 of the region of interest is acquired. </p><p id="p0045" num="0045"> In a detection step 124, an element 126 in the live image is detected. </p><p id="p0046" num="0046"> Further, in a determination step 128, spatial relation 130 of the pre-navigation data 114 and the live image data 122 is determined. </p><p id="p0047" num="0047"> In another step, a sub-step determination step 132 is provided in which the position 134 of the detected element in the spatial geometrical data is determined, which determination is based on the spatial relation 130. Further, in a computational sub-step 136, a predetermined related point of location 138 on the functional parameter surface 118 is computed. </p><p id="p0048" num="0048"> Then, in a generating step 140, a combination 142 of a simplified surface representation 144 of the region of interest is generated, wherein the simplified surface representation 144 is based on a visualization of the functional parameter surface 118. Further, in the generating step 140, a marker 146 is generated indicating the computed predetermined related point of location 138. </p><p id="p0049" num="0049"> In a display step 148, the combination is displayed as navigation guidance 150. 
<!-- EPO <DP n="8"/>-->
 It is noted that the steps described above in Fig. 2 are only shown in an exemplary order of the steps. Of course, other orders of the steps are also possible. For example, step d) is performed before step c). Still further, the acquisition steps a) and b) provide image data that are already registered to one another. In such case, further </p><p id="p0050" num="0050">determination of the spatial relation as in step d) is not necessary anymore. </p><p id="p0051" num="0051"> According to a further aspect, the steps can be repeated continuously with a predetermined time rate. </p><p id="p0052" num="0052"> As mentioned above, the spatial geometrical data can be acquired by Computed</p><p id="p0053" num="0053">Tomography. </p><p id="p0054" num="0054"> According to another aspect, the spatial geometrical data is acquired by</p><p id="p0055" num="0055">Ultrasound. </p><p id="p0056" num="0056"> According to a further aspect, the spatial geometrical data 116 is acquired by Magnetic Resonance Imaging or Nuclear Magnetic Resonance Imaging. The spatial geometrical data can also be acquired by Nuclear Imaging such as Single Photon Emission Computed Tomography or Positron Emission Tomography. </p><p id="p0057" num="0057"> According to a further aspect, the spatial geometrical data 116 comprises three- dimensional, i.e. volume information about the region of interest. </p><p id="p0058" num="0058"> According to a further aspect, the spatial geometrical data also comprises temporal information. In other words, as the spatial geometrical data, 3D+t image data or 4D image data is provided. </p><p id="p0059" num="0059"> According to an aspect of the invention, the functional parameter surface comprises a bull's-eye view which is illustrated and described below. </p><p id="p0060" num="0060"> According to a further aspect, the functional parameter surface comprises a three-dimensional mesh view. </p><p id="p0061" num="0061"> According to a further aspect, the functional parameters refer to anatomy locations of the object, i.e. the patient. </p><p id="p0062" num="0062"> According to a further aspect, the functional parameter surface is in a known spatial relation to the spatial geometrical data. </p><p id="p0063" num="0063"> For example, for registering the spatial geometrical data and the functional parameter surface, the same imaging modality is used for acquiring the spatial geometrical data and the functional parameter surface. </p><p id="p0064" num="0064"> According to another aspect of the invention, different imaging modalities are used for acquiring the spatial geometrical data and the functional parameter surface. 
<!-- EPO <DP n="9"/>-->
 It is noted that in the context of describing the present invention, the providing step 112 is also referred to as step a), the acquisition step 120 as step b), the detection step 124 as step c), the determination step 128 as step d), the determination step 132 as step e), the generating step 140 as step f), and the displaying step 148 as step g). </p><p id="p0065" num="0065"> According to a further aspect of the invention, the object of interest comprises a tubular structure, for example a vascular structure. For example, an active area of research is advanced guidance methods or interventional cardiovascular procedures. Specifically, for complex procedures such as ablations and device implantations in cardiac electrophysiology (EP), according to the invention, the pre-procedural imaging information can be registered and overlaid to the interventional X-ray images. </p><p id="p0066" num="0066"> According to an aspect of the invention, the location of an element, for example a catheter, guide wire or lead is visualized in relation to the target zones for ablation or pacer lead deployment. </p><p id="p0067" num="0067"> According to a further aspect, the functional parameter surface 118 and the spatial geometrical data 116 of the providing step 112 are each registered to the live image data 122 of the acquisition step 120, which is indicated by a first arrow 152 and a second arrow 154. The step of registering the two types of data is further indicated by a box overlaid to the first and second arrow 152, 154, which box is illustrated with a dashed line, indicated with reference numeral 156. </p><p id="p0068" num="0068"> According to an aspect of the invention, for cardiac examination, the functional parameters of the functional parameter surface 118 comprise, for example, at least one of the groups or a combination thereof, comprising scar location and burden, mechanical function, electrical activation, perfusion/viability or others. The functional parameters can also comprise any combination of parameters calculated there from. </p><p id="p0069" num="0069"> Whereas the element 126 is detected in the live image data 122, according to a further aspect, the element can also be visible in the spatial geometrical data 116. </p><p id="p0070" num="0070"> With respect to step b), according to an aspect of the invention, the live image data 122 comprises 2D image data, for example 2D fluoroscopy images. </p><p id="p0071" num="0071"> According to a further aspect, the live image data 122 also comprises spatial geometrical data. </p><p id="p0072" num="0072"> With respect to step c), the detection 124 comprises tracking the element in a sequence of live images acquired in step b). </p><p id="p0073" num="0073"> According to an aspect of the invention, the element is constrained to move 
<!-- EPO <DP n="10"/>-->
 inside a sub-volume of the region of interest, for example inside a tubular structure, such as a vessel tree. This provides the advantage, that in case such a sub-volume is provided in the acquired live image data, for example by image analysis, the element can be located only inside such a volume as a vessel tree, for example, which leads to an improvement of the detection of the element. </p><p id="p0074" num="0074"> With respect to step d), the determination 128 may comprise projective mapping 158 of the element in the live image data into the spatial geometrical data, which is indicated by a mapping box shown in a dotted line and an arrow 160 from step b) to step a), i.e. from the acquisition box 120 to the providing box 112. </p><p id="p0075" num="0075"> According to a further aspect, step d) comprises registering the spatial geometrical data 116 and the live image data 122. </p><p id="p0076" num="0076"> According to a further aspect, step d) comprises a 2D-3D registration based on at least three marker points that are determined in the live image data and that are determined in the spatial geometrical data (not shown). </p><p id="p0077" num="0077"> According to a further aspect, step d) comprises a registration based on image intensity. </p><p id="p0078" num="0078"> According to a further aspect, step d) of determining the spatial relation 130 comprises localizing the element. </p><p id="p0079" num="0079"> For example, the spatial relation 130 is based on registering the element. </p><p id="p0080" num="0080"> According to a further aspect, the spatial relation 130 is based upon registering landmarks. </p><p id="p0081" num="0081"> For example, the spatial relation can also be achieved by using the same imaging modality for acquiring the spatial geometrical data 116 and the live image data 122. </p><p id="p0082" num="0082"> According to a further aspect, step d) comprises correcting for misalignment, for example breathing or cardiac motion compensation. </p><p id="p0083" num="0083"> According to a further aspect, in case that the spatial geometrical data 116 also comprises temporal information, for the determination of the spatial relation, a temporal spatial relation is determined, although not further shown. </p><p id="p0084" num="0084"> With respect to step e), according to a further aspect, the predetermined relation can be preset by the user. </p><p id="p0085" num="0085"> According to a further aspect, the predetermined relation comprises at least one parameter. </p><p id="p0086" num="0086"> According to an exemplary embodiment of the invention, the predetermined 
<!-- EPO <DP n="11"/>-->
 related point of location is the closest point. </p><p id="p0087" num="0087"> According to a further exemplary embodiment, shown in Fig. 5, the detection of the element in step c) comprises determining 162 the 2D position 164 of the element in the live image data and for determining the position in step e), the detected 2D position is mapped 166 to the spatial geometrical data 116 by a direct projection line 168. </p><p id="p0088" num="0088"> According to a further aspect, for determining the position, spatial parameters are provided by acquisition means. For example, the spatial parameters are provided by electromagnetic localization or with one or two 2D images with different viewing planes. </p><p id="p0089" num="0089"> According to a further aspect, step e) comprises correcting for misalignment, for example breathing or cardiac motion compensation. </p><p id="p0090" num="0090"> According to a further exemplary embodiment, shown in Fig. 6, step e) comprises a back projection 170 of the tracked location through the spatial geometrical data, to estimate spatial position 172 of the element. </p><p id="p0091" num="0091"> According to an exemplary aspect, this can be performed in a loop-like manner, indicated by two arrows 174a and 174b entering and leaving the back projection box 170. </p><p id="p0092" num="0092"> According to an exemplary embodiment of the invention, in case that the back projection does not intersect with the anatomy according to the spatial geometrical data 116, the closest or statistically most likely point in the anatomy is used (not shown). </p><p id="p0093" num="0093"> According to a further exemplary embodiment, the simplified surface representation is an unfolded map image (examples for an unfolded map image are described further below). </p><p id="p0094" num="0094"> According to a further aspect, the visualizing of functional parameters is also referred to as functional image data. </p><p id="p0095" num="0095"> According to a further aspect, for generating the combination, a 3D image representation of the spatial geometrical data 116 and the determined 3D position are mapped forward into a flat representation (see below). </p><p id="p0096" num="0096"> According to a further aspect, the representation comprises a 2D map with colour coded information as further dimension besides the two geometrical dimensions. </p><p id="p0097" num="0097"> According to a further exemplary embodiment of the invention, a projection axis from the detected element in the spatial geometrical data to the computed related point of location on the functional parameter surface is provided. Further, the simplified surface representation is a 3D image and the 3D image is aligned with the projection axis. </p><p id="p0098" num="0098"> According to a further aspect, step f) comprises computing a marker position in 
<!-- EPO <DP n="12"/>-->
 the unfolded representation based on the determined position of step e). </p><p id="p0099" num="0099"> According to a further aspect, the surface representation is also referred to as parametric surface. </p><p id="p0100" num="0100"> According to a further aspect, the unfolded 2D map encodes the information of the functional parameter surface 118, for example a functional mesh. </p><p id="p0101" num="0101"> According to a further aspect, step f) comprises generating a segmented mesh.</p><p id="p0102" num="0102">According to a further exemplary embodiment of the invention, the unfolded map, as indicated above, is a bull's eye view (see further below for an example of such a bull's eye view). </p><p id="p0103" num="0103"> According to a further aspect, the simplified surface representation comprises a</p><p id="p0104" num="0104">2D surface map, for example of the left atrium in case of cardiovascular interventions. </p><p id="p0105" num="0105"> According to a further exemplary embodiment of the invention, step f) comprises combining navigation anatomy with the simplified surface representation 144. For example, as shown in Fig. 7, step f) comprises combining live image data 122 provided in step b) as the navigation anatomy. In Fig. 7, this is indicated by an arrow 176 from the acquisition box 120 to the generating box 140. For example, the navigation anatomy comprises the live image data in form of a fluoroscopic 2D image. </p><p id="p0106" num="0106"> According to a further exemplary embodiment, the navigation anatomy comprises a projection 178 of the vascular structure. </p><p id="p0107" num="0107"> According to an aspect, shown in Fig. 8, the projection 178 is derived by segmenting or modelling 180 the spatial geometrical data 116 provided in step e) which is indicated by an arrow 181 from the box of the providing step 112 to the segmenting or modelling step 180. The projection 178 thus created, or computed respectively, is then entered to the generating box 140 which is indicated by an arrow 182 entering the box of step f). </p><p id="p0108" num="0108"> According to an exemplary embodiment of the invention, the navigation guidance 150 is a 2D representation comprising functional parameter surface information and 3D positioning information of the element in relation to the region of interest. For an example of the 2D representation, see below. </p><p id="p0109" num="0109"> According to an aspect of the invention, the navigation guidance also provides temporal information in case the pre-navigation data comprises 3D+t image data. </p><p id="p0110" num="0110"> In the following, an exemplary embodiment of the invention is described with reference to Fig. 9. In the left part of the image, pre-navigation data 214 of a region of interest of an object is provided. The pre-navigation data comprises spatial geometrical data 216, or in 
<!-- EPO <DP n="13"/>-->
 other words, the pre-navigation data 214 comprises three-dimensional image data of the region of interest. For example, geometrical data 216 shown in Fig. 9 provides three- dimensional anatomic information, for example in a cardiovascular intervention. The pre- navigation data 214 also comprises a functional parameter surface 218 in correspondence to the spatial geometrical data 216. As an exemplary embodiment, shown in Fig. 9, the functional parameter surface 218 comprises a functional mesh. As can be seen, the functional mesh follows the contour, i.e. the three-dimensional form of the anatomy, and provides additional information about functional parameters, as mentioned above. These functional parameters are provided, for example, by a colour coding. For example, the target area is shown in green, whereas the other surfaces are shown in a red colour. Of course, this can also be achieved by different patterns. </p><p id="p0111" num="0111"> In the right part of Fig. 9, a perspective or slightly distorted live image data 222 of the region of interest is provided, for example in form of a 2D fluoroscopy X-ray image. In the fluoroscopy image 222, an element 226 is visible and can thus be detected. For example, the element 226 is a guide wire with a guide wire or lead tip 227, indicated by a red point, for example. As indicated by a coded line 225, the element 226 has already been detected in Fig. 9. </p><p id="p0112" num="0112"> Further, the spatial relation of the pre-navigation data 214 and the live image data 222 is also already determined. The tip 227 is then taken as a position 224 of the element 226 such that the position 224 of the detected element can be determined in the spatial geometrical data 216. Since the spatial relation of the two types of data is known, the determination is achieved by a line projection in X-ray geometry, which is indicated by a direct arrow 232 from the tip position 224 in direction of the spatial geometrical data 216. As indicated by a point 233, the position of the guide wire in 3D is achieved by projective mapping into the mesh of the constraining anatomy which is representing the spatial geometrical data 216. Thus, the position of the detected element is determined in the spatial geometrical data. </p><p id="p0113" num="0113"> Further, in a computing step, a predetermined related point of location on the functional parameter surface is computed. According to the example shown, the </p><p id="p0114" num="0114">predetermined related point of location can be a closest point 238 on the functional mesh with respect to the detected lead position indicated by point 233. The computing is indicated by an arrow 239. </p><p id="p0115" num="0115"> In order to be able to provide the user with an easily perceptive information, a 
<!-- EPO <DP n="14"/>-->
 simplified surface representation 244 of the region of interest is generated. According to the example shown, the simplified surface representation is a so-called bull's-eye representation which is also referred to as a bull's-eye plot. Such a bull's-eye representation shows the volumetric information of the region of interest in a two-dimensional, i.e. flat unfolded map. The individual volumetric parts enclosing the volume in the spatial geometrical data 216 are divided into a number of segments 245, as indicated in the bull's-eye plot in the middle of Fig. 9. The surface parts from the spatial geometrical data 216 are number coded. It is further possible to transfer the computed position into the unfolded map projection by a forward mapping. As can be seen in the simplified surface representation 244, the segments indicated by numbers are also overlaid by coded patterns or colours. For example, the target area is indicated by a certain predetermined pattern 244a whereas the rest is coded with another predetermined coded pattern 244b. The closest point computed in the determination step before is indicated with a marker 246. The forward mapping is indicated by an arrow 240. Thus, a combination 242 is generated which can then be displayed as navigation guidance. </p><p id="p0116" num="0116"> According to a further aspect of the invention, instead of an unfolded map, for example a bull's-eye view, it is also possible to generate a three-dimensional left ventricle figure as shown in Fig. 10 for the simplified surface representation. According to the reference numbers used with respect to the bull's-eye view, Fig. 10 shows a combination 342 of a simplified surface representation 344 with a marker 36 indicating the computed predetermined related point of location. Further, it is also possible to indicate the target area with a coded pattern 344a. </p><p id="p0117" num="0117"> In another example, shown in Fig. 11 , the left atrium is shown in an unrolled manner as a simplified surface representation 444 with a marker 446 indicated the computed predetermined related point of location, and also a target area indicated with a coded pattern 444a. </p><p id="p0118" num="0118"> According to another example, a simplified perspective view of a left atrium is shown in Fig. 12. Thus, the left atrium represents a simplified surface representation 544 with a marker 546 and a target area 544a. </p><p id="p0119" num="0119"> The left atrium of Fig. 12 can also be displayed in an unfolded map, shown in Fig. 13. The unfolded map represents a simplified surface representation 644 with a marker 646 and a target area 644a. </p><p id="p0120" num="0120"> According to a further example, shown in Fig. 14, as a simplified surface representation 744, a right and left atria are shown in unfolded manner. Also here, a marker 
<!-- EPO <DP n="15"/>-->
 746 as well as a target area 744a is indicated. Fig. 14 shows an unfolded map of the left 744d and right atria 744e with further anatomy information, e.g. superior vena cava 744f, interior vena cava 744g, septum 744h, fossa ovalis 744i, coronary sinus 744j and pulmonary veins 744k. </p><p id="p0121" num="0121"> Fig. 15 shows a further example of a simplified surface representation 844 in form of a perspective wire frame of the left ventricle, for example. As can be seen, a marker 846 as well as a target area 844a is indicated. Further, the wire frame, indicated by reference numeral 844c is added by anatomy information, for example by a representation 844b of a vascular structure. </p><p id="p0122" num="0122"> Fig. 15 also shows another aspect of the invention: According to a further exemplary embodiment of the invention, a projection axis is provided from the detected element in the spatial geometrical data to the computed related point of location on the functional parameter surface. For example, this projection axis is used for determining a closest point. Further, the simplified surface representation in Fig. 15 is a 3D image. As can be seen, this 3D image is aligned with the projection axis. In other words, the 3D image is automatically orientated such that the visualization axis is aligned with the direction of projection of, for example the catheter / guidewire / lead to the target surface. With such an orientation, the catheter / guidewire / lead is always visible and is superimposed with its projection of the surface without any parallax error. Therefore, it becomes very easy to check that the catheter / guidewire / lead has reached its target location with respect to the surface. </p><p id="p0123" num="0123"> Fig. 16 shows a similar simplified surface representation 944 with a marker 946, a pattern indicating a target area 944a as well as anatomy information by a vessel representation 944c. In addition, the combination of the simplified surface representation 944 and the marker 946 is overlaid to a live image, for example a fluoroscopy image 922 which is used as the live image data 122 according to the above described embodiments. </p><p id="p0124" num="0124"> According to a further example, the simplified surface representation 844 of Fig. 15 can also be displayed in an unrolled manner, shown in Fig. 17. A simplified surface representation 1044 is combined with a marker 1046 indicating the computed predetermined related point of location. Further, it is also possible to indicate a target area with a coded pattern 1044a. According to Fig. 15, an anatomy structure, for example a vessel tree 1044c is also shown. </p><p id="p0125" num="0125"> According to a further aspect, shown in Fig. 18, a fluoro image 1122 is combined with a perspective view of a left ventricle, for example according to Fig. 10. Thus, a 
<!-- EPO <DP n="16"/>-->
 simplified surface representation 1144 in form of the left ventricle is combined with a marker 1146. In addition, it is also possible to indicate a target area with a coded pattern 1144a. Further, as indicated with reference numeral 1144c, a vessel tree is also indicated. </p><p id="p0126" num="0126"> According to a further exemplary embodiment, shown in Fig. 19, an unrolled map 1244 is combined with a marker 1246 as well as a coded pattern representing a target area 1244a. Further, a vessel tree representation 1244c is overlaid to the combination. </p><p id="p0127" num="0127"> According to a further example shown in Fig. 20, a bull's-eye view 1344 is representing the simplified surface representation, which is combined with a marker 1346 as well as a target area 1344a. Further, anatomy information is overlaid in form of a vessel tree projection or modelling 1344c. </p><p id="p0128" num="0128"> According to an aspect of the invention, for example in relation with advanced guidance methods for interventional cardiovascular procedures, a visualization of the location of catheters / guidewires / leads in relation to the target zones for ablation or pacer lead deployment is provided. It has been shown that overlay and registration of a 3D pre-operative surface onto a 2D X-ray projection requires frequent reorientation of the X-ray imaging plane to allow for a full understanding of the actual locus of interrogation, mostly because of hidden parts and/or parallax errors. The present invention therefore provides more accurate information of, e.g. the catheter position in the 3D space to the surgeon. As an advantage, the burden of navigating a catheter / guidewire / lead in 3D towards an interrogation target is minimized. </p><p id="p0129" num="0129"> According to one aspect, the present invention is embodied in interventional guidance software that can help identifying and reaching interrogation targets easier. Less reorientations of the XR system are required to obtain sufficient information on 3D location of catheter / guidewire / lead in relation to target zones. </p><p id="p0130" num="0130"> According to one aspect, an image representing a 2D projection of a 3D area is created and displayed in real time together with a computed position of e.g. the tip of a catheter. </p><p id="p0131" num="0131"> According to one aspect, other known projection techniques such as those usually used to get planispheres, are used alternatively. Thus, navigation takes place in such a 2D unfolded map image. </p><p id="p0132" num="0132"> As an advantage, this approach simplifies navigation in complex 3D anatomy and helps reaching therapy targets very efficiently. </p><p id="p0133" num="0133"> In an exemplary embodiment where 3D images are used, the method may 
<!-- EPO <DP n="17"/>-->
 comprise the following steps: providing a registered 3D image of the region of interest; </p><p id="p0134" num="0134">providing a real time X-ray fluoroscopy 2D image in real time which is registered to the navigation anatomy (this image may show the invasive instrument); determining/tracking the position of the instrument in real time; there may be various ways to do this, e.g. by analyzing the fluoro image; computing a marker position of the instrument in the unfolded </p><p id="p0135" num="0135">representation based on the position determined in the previous step; and displaying an image showing the unfolded representation including the marker at the computed position. </p><p id="p0136" num="0136"> For example, a 2D intra-operative X-ray projection is provided of the interventional tool (e.g. catheter tip). Further, a pre or intra-operative acquisition of the 3D anatomy is provided where the interventional tool is constrained to move, e.g. coronary veins). According to another aspect, this can be replaced by another method to localize the catheter in 3D, e.g. electromagnetic localization or with two X-ray-projections at different angles. Still further, a surface representation of the intervention target is provided, e.g. left ventricule, on which can be displayed additional information, e.g. scar location and burden, mechanical function, electrical activation etc.. This surface can also be displayed in unfolded representations, e.g. a bulls-eye view. </p><p id="p0137" num="0137"> According to one aspect, a registration of the 3D anatomy, where the catheter / guidewire / lead is moving and of the surface, where the target area is defined, is provided. The navigation anatomy and the parametric surface are registered together. If the same imaging modality is used to acquire those two elements (e.g. MRI for both LV endocardium and coronary veins), the registration is implicit. Otherwise (e.g. CTA for coronary veins and echography for LV function), a specific registration method needs to be applied (e.g. rigid transform estimated on common landmarks). Alternatively, the registration of the navigation anatomy and the parametric surface can be obtained by registering both of them independently to the 2D XR projection as mentioned above. </p><p id="p0138" num="0138"> According to another aspect, a registration of the anatomy items as above with the live imaging used during the intervention is provided (e.g. 2D XR projection or "fluoro"). The navigation anatomy and the surface where the target area is defined are registered, separately or as a whole, to the 2D XR projection. For example, a point based 2D3D perspective registration approach can be used. After acquisition of an XR coronary venous angiogram, at least 3 marker points are determined in a 2D projection at specific anatomic locations such as vein bifurcations, CS, sharp bends in the vessel etc. The same anatomic point locations are determined in the segmented 3D coronary venous tree e.g. from cardiac MR. 
<!-- EPO <DP n="18"/>-->
 2D-3D registration is performed using e.g. a 6 degree of freedom (6DOF, translation and rotation) registration approach employing an optimization. Here, the squared distances between the points in the 2D X-ray projection and the projected 3D CMR points need to be minimized (least squares of projected point distances): m<sub>T</sub> |PH— PZB <sup>*</sup>( }|.. with T being the 6 DOF registration transform to be optimized, P¾} being the selected point set in the X-ray angiogram and F^<sup>R</sup>( ) = X - T - P^<sup>R</sup> being the projected and transformed selected point set in the 3D CMR vein tree, where X represents the X-ray projection matrix. During the registration process, the perspective geometry of the XR system needs to be taken into account, which can be derived from specific calibration steps. </p><p id="p0139" num="0139"> According to another aspect, a detection or tracking of the catheter / guidewire</p><p id="p0140" num="0140">/ lead in the live images used during the intervention is provided. This may comprise the detection or tracking of the catheter tip (or guidewire or lead tips) in real-time within the 2D XR projection. For this purpose, an appropriate template correlation filter can be used. In the specific case of CRT, a guidewire / pacer lead can be tracked within the coronary venous system. </p><p id="p0141" num="0141"> According to another aspect, a 3D localization of the catheter / guidewire / lead is provided. This may comprises a back projection of the tracked location through the navigation anatomy where the catheter tip is constrained to navigate. Using the back projection of the tracked point in XR geometry, a corresponding 3D location (e.g. along the centreline of the coronary vein from pre-operative imaging) within the 3D navigation anatomy can be identified. For CRT, the specific location in 3D of the guidewire / lead within the coronary venous system can be determined in this way. If the back projection line does not intersect the navigation anatomy, for example due to 2D3D mis-registration or motion, the closest (or the statistically most likely) point in the constraining anatomy can be used. </p><p id="p0142" num="0142">Deformations due to breathing and cardiac motion can be taken into account with template- based breathing motion tracking, ECG gating or other motion compensation approaches. </p><p id="p0143" num="0143"> According to a further aspect, another method is used for the 3D location of the catheter tip, e.g. electromagnetic localization (EM localization) or with 2 XR projections at different angles. In this case, there is no need for the constraining navigation anatomy and this invention can be applied to navigation in larger anatomical regions such as cardiac chambers (left atrium for AF ablation or left ventricule for VT ablation). 
<!-- EPO <DP n="19"/>-->
 According to another aspect, a projection of the catheter / guidewire / lead onto the surface where the target area is defined is provided. This may comprise the calculation of the closest point of the determined 3D location to a target location on the 3D parametric surface. In the specific case of CRT, the closest point on the left-ventricular (LV) surface (assuming it is represented by a discrete mesh), from the current 3D location within the coronary venous system is determined by: πώ¾ |Ρ¾^— ¾Β<sup>-ΚΜ£ΐι</sup>(ί where pj^ is the </p><p id="p0144" num="0144">3D point within the venous system and F ¾<sup>_£riesh</sup>(i} is the 3D coordinate of the LV mesh at vertex i. </p><p id="p0145" num="0145"> According to another aspect, a displaying of the catheter / guidewire / lead and the target area on the simplified view (unfolded 2D view or automatically oriented 3D) is provided. Preferably, the navigation anatomy, e.g. the coronary veins, are also displayed on the same view. </p><p id="p0146" num="0146"> Once the 3D location of the catheter tip
<img id="imgf000019_0001" he="6" wi="8" file="imgf000019_0001.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 and its closest target point on the parametric surface P§<sup>II SII</sup> are both available, we propose new ways to display automatically the navigation information in the easiest way. The first proposal is to orientate the 3D view automatically so that
<img id="imgf000019_0002" he="7" wi="21" file="imgf000019_0002.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 ) is aligned to the visualization axis. The second proposal is to display P^ ** on the unfolded map, so that one can directly see its position with respect to the information displayed in the map. Additionally, if available, the navigation anatomy can also be overlaid on top of the unfolded map, providing information on the possible paths for the catheter tip. </p><p id="p0147" num="0147"> For the specific case of CRT, the closest point on the LV with regard to the tracked guidewire / lead location within the coronary venous system is displayed in a bulls-eye plot as already mentioned. According to the American Heart Association, the LV can be separated into 17 segments, 6 basal, 6 equatorial and 5 apical segments. Functional Ultrasound or MR allows the visualization of important functional information in such bulls-eye plots. This way, the current location of the guidewire / lead can be visualized in real-time in relation to target spots in the bulls-eye. For CRT, the bulls-eye should practically contain information such as mechanical activation, scar burden and electrical activation. </p><p id="p0148" num="0148"> According to another aspect, the invention can be used for CRT or any other interventional procedure requiring catheter / lead / guidewire position in relation to anatomy 
<!-- EPO <DP n="20"/>-->
 and specific parameters i.e. function, scar burden, electrical activity etc. (e.g. ventricular tachycardia VT ablation, stem cell injection etc.). If applied for VT ablation, the bulls-eye can contain information such as scar burden and electrical activation. If applied for atrial fibrillation ablation, a 2D surface map of the left atrium can be used instead of a LV bulls-eye, indicating the ostia of the pulmonary veins. </p><p id="p0149" num="0149"> In another exemplary embodiment of the present invention (not shown), a computer program or a computer program element is provided i.e. characterized by being adapted to execute the method steps of the method according to one of the preceding embodiments, on an appropriate system. </p><p id="p0150" num="0150"> The computer program element might therefore be stored on a computer unit, which might also be part of an embodiment of the present invention. This computing unit may be adapted to perform or induce a performing of the steps of the method described above. Moreover, it may be adapted to operate the components of the above described apparatus. The computing unit can be adapted to operate automatically and/or to execute the orders of a user. A computer program may be loaded into a working memory of a data processor. The data processor may thus be equipped to carry out the method of the invention. </p><p id="p0151" num="0151"> This exemplary embodiment of the invention covers both, a computer program that right from the beginning uses the invention and a computer program that by means of an up-date turns an existing program into a program that uses the invention. </p><p id="p0152" num="0152"> Further on, the computer program element might be able to provide all necessary steps to fulfil the procedure of an exemplary embodiment of the method as described above. </p><p id="p0153" num="0153"> According to a further exemplary embodiment of the present invention (not shown), a computer readable medium, such as a CD-ROM, is presented wherein the computer readable medium has a computer program element stored on it which computer program element is described by the preceding section. </p><p id="p0154" num="0154"> A computer program may be stored and/or distributed on a suitable medium, such as an optical storage medium or a solid state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the internet or other wired or wireless telecommunication systems. </p><p id="p0155" num="0155"> However, the computer program may also be presented over a network like the World Wide Web and can be downloaded into the working memory of a data processor from such a network. According to a further exemplary embodiment of the present invention, a 
<!-- EPO <DP n="21"/>-->
 medium for making a computer program element available for downloading is provided, which computer program element is arranged to perform a method according to one of the previously described embodiments of the invention. </p><p id="p0156" num="0156"> It has to be noted that embodiments of the invention are described with reference to different subject matters. In particular, some embodiments are described with reference to method type claims whereas other embodiments are described with reference to the device type claims. However, a person skilled in the art will gather from the above and the following description that, unless otherwise notified, in addition to any combination of features belonging to one type of subject matter also any combination between features relating to different subject matters is considered to be disclosed with this application. </p><p id="p0157" num="0157"> However, all features can be combined providing synergetic effects that are more than the simple summation of the features. </p><p id="p0158" num="0158"> In the claims, the word "comprising" does not exclude other elements or steps, and the indefinite article "a" or "an" does not exclude a plurality. A single processor or other unit may fulfil the functions of several items re-cited in the claims. The mere fact that certain measures are re-cited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. 
<!-- EPO <DP n="22"/>-->
</p><p id="p0159" num="0159">MEDICAL IMAGING SYSTEM AND METHOD FOR PROVIDING AN IMAGE </p><p id="p0160" num="0160">REPRESENTATION SUPPORTING ACCURATE GUIDANCE OF AN INTERVENTION DEVICE IN A VESSEL INTERVENTION PROCEDURE </p><p id="p0161" num="0161">FIELD OF THE INVENTION </p><p id="p0162" num="0162"> The present invention relates to a medical imaging system and a method for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure, especially suitable for fluoroscopy guided interventions, e.g. atrial fibrillation ablation procedures. </p><p id="p0163" num="0163">BACKGROUND OF THE INVENTION </p><p id="p0164" num="0164"> Electrophysiology is a specific domain of interventional cardiology where physicians use intra-cardiac catheters to locate and cure electrical dysfunctions of the heart rhythm, under X-Ray fluoroscopy guidance. A challenging electrophysiology procedure is radio-frequency ablation for the treatment of atrial fibrillation. Electrophysiologists need a special training to perfectly know the anatomy and the access pathways to all the sites of interest and some practice to select the correct devices and manipulate them to the desired target. The patient's anatomy can be recorded with 3D imaging devices, e.g. through CT or MRI, or by injecting contrast agent locally just at the beginning of the intervention, e.g. into the left atrium and ostium of the pulmonary veins for atrial fibrillation or into coronary veins and sinus for a cardiac resynchronization therapy. The physician basically has to perform a mental registration to navigate in the live fluoro images where the structural information is not visible anymore. For atrial fibrillation procedures, knowing the exact positions of the catheters when measuring electrical potentials is the key to find the sources that cause fibrillation, e.g. the ectopic foci or the reentry loop. Even more important is an anatomical mapping of the ablation sites in order to perform the desired ablation patterns, such as pulmonary vein isolation or roof line ablation in the left atrium. </p><p id="p0165" num="0165"> Tracking a third-party object like an interventional tool or a visible anatomical landmark is mandatory in interventional X-ray if one wants to compensate the motion of an anatomy of interest, e.g. a heart chamber or the coronary sinus, if said organ is mostly </p><p id="p0166" num="0166">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="23"/>-->
 invisible. </p><p id="p0167" num="0167"> However, the relationship between the motion of a tracked intervention device and the part of the anatomy that is interesting for electrophysilogic intervention may be complex. In the chest area, for example, the motion of an intervention device is mostly induced by two factors, the heart beat and the breathing motion. </p><p id="p0168" num="0168"> Since in the chest area these two distinct motion sources effect their surrounding differently, it may be considered necessary to separate these motion sources </p><p id="p0169" num="0169"> Known measures for the separation of different motion sources, which motion sources produce motions with distinct frequency bands, are based on filtering the overall motion of the tracked objects to recover the motion originating from either one of the source. However, filtering may result in lags that alter the quality of the motion compensation. </p><p id="p0170" num="0170">Furthermore, the outcome of filtering may degrade quickly when the image rate of the fluoroscopy drops down. </p><p id="p0171" num="0171"> Another known measure consists of using a temporal model of one (or several) of the motion sources. The temporal model(s) is (are) then fitted to the recorded motion. This may not be very flexible for coping with a huge motion variability, e.g. arrythmia of the cardiac or breathing motion. </p><p id="p0172" num="0172">SUMMARY OF THE INVENTION </p><p id="p0173" num="0173"> Therefore there may be a need for a robust medical imaging system and method implementing an image representation of an anatomy of interest supporting an accurate guidance of an intervention device inserted into a vessel, which vessel is subject to an induced motion caused by at least two different motion sources. The robustness should address the variation in rhythm and to framerate. </p><p id="p0174" num="0174"> Such need may be met with the subject-matter of the independent claims. Further embodiments of the invention are defined in the dependent claims. </p><p id="p0175" num="0175"> According to an aspect of the present invention, a medical imaging system for providing an image representation of an anatomy of interest supporting the accurate guidance of an intervention device in a vessel intervention procedure is proposed. The medical imaging system is adapted to perform the following steps, preferably but not exclusively in the indicated order: </p><p id="p0176" num="0176"> (i) acquiring a first sequence of images of the vessel region of interest with the INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="24"/>-->
 intervention device inserted into the vessel region of interest during a time frame without a first motion of the patient; </p><p id="p0177" num="0177"> (ii) detecting a first motion sequence of a periodic motion of the intervention device during a second motion cycle by analyzing the motion of the intervention device in the first sequence of acquired images; </p><p id="p0178" num="0178"> (iii) creating a motion model of the intervention device solely induced by the second motion; </p><p id="p0179" num="0179"> (iv) determining an operator defining the relationship between the first motion sequence of the intervention device and the second motion; </p><p id="p0180" num="0180"> (iv) acquiring live images of the vessel region of interest with the intervention device inserted into the vessel region of interest of the patient; </p><p id="p0181" num="0181"> (v) subtracting the motion of the intervention device induced by the second motion using the operator and determining the motion of the intervention device induced by the first motion of the patient; and </p><p id="p0182" num="0182"> (vi) registering of a representation of the object of interest based on the first motion. </p><p id="p0183" num="0183"> According to the invention the state of the different sources of motion present e.g. as a cardiac motion and a perspirating motion is recovered by assessing the actual shape of an intervention device, realized as one or several third party objects, and deducing the pose of another object of interest, typically a heart chamber, based on the relevant motion. </p><p id="p0184" num="0184"> At the heart of the invention is the assumption that different sources of motion deform the shape of an intervention device in different ways, so that the respective influence of these sources on the motion can be recovered unambiguously. With the help of one or several motion models, the motion induced by the different factors can then be inferred and propagated to other objects such as the anatomy of interest. By contrast to the existing techniques, a key benefit of this approach is the lack of dependency of the results to the framerate, and its ability to produce an estimate of the position of the anatomy of interest without time lag. </p><p id="p0185" num="0185"> The acquisition of the first set of images of the anatomy region of interest may be conducted by an X-ray imaging apparatus that is able to provide a series of subsequent 2D- images. The intervention device may comprise one or several distinguishing features or landmarks that are feasible for recognition in an X-ray image for the purpose of defining a model of the intervention device and further for detecting a first motion sequence of a motion </p><p id="p0186" num="0186">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="25"/>-->
 of the intervention device induced by a second motion source, which may be cyclic but not necessarily periodic, e.g. a cardiac motion. The definition of the model for the intervention device may be conducted by a regression or other applicable methods. </p><p id="p0187" num="0187"> For the definition of a relationship between the second motion source influencing the shape of the intervention device and the state of the intervention device itself the definition of a motion model is mandatory. Such a motion model may be set up as a matrix or a vector that may be multiplied by a first motion source vector, such as a first motion scalar, e.g. a radian value of a recurring motion, for achieving a position vector describing the position of the intervention device depending on the state of the first motion source. In general, the motion model takes the shape of the interventional device as an input and outputs a phase or a displacement either through analytically or experimentally determined </p><p id="p0188" num="0188">relationship. The input should preferably not be a derivative such as a velocity, since in the most basic mode the motion model should be usable based on a single frame. Furthermore, in low framerate sequences a velocity can hardly be estimated. </p><p id="p0189" num="0189"> The determination of an operator as a reversed motion model may be conducted by transposing the motion model of the intervention device through a convenient transposition function depending on the nature of the motion model and its dimensionality. </p><p id="p0190" num="0190"> An acquisition of live fluoroscopy images of the anatomy region of interest with the intervention device inserted into the anatomy region of interest is necessary for making the intervention device visible, which intervention device is to be guided to a certain spot such as the left atrium. It is pointed out that the live images may be acquired without restrictions regarding the motion of the patient. </p><p id="p0191" num="0191"> In a following step a motion model of the intervention device based on the first motion source only, e.g. a perspirating motion, is conducted by analyzing the motion of the intervention device according to the first learning step in which the shape deformations are only induced by the second motion source. When the shape of the intervention device is monitored under the influence of both second motion source and first motion source the shape deformation of the intervention device induced by the second motion source may be subtracted under utilization of the operator as the reversed motion model described above. Thereby, a motion model as a relationship between the first motion source and the motion of the intervention device may be gained. </p><p id="p0192" num="0192"> This relationship helps in registering of a previously acquired image of the anatomy of interest and the live fluoroscopy images based on especially the perspirating </p><p id="p0193" num="0193">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="26"/>-->
 motion. This may be helpful, since e.g. the motion of the left atrium substantially does not depend on the second motion source, especially the cardiac motion, but is rigidly moved under the influence of the perspiration. </p><p id="p0194" num="0194"> In a formalized manner there are n<sub>s</sub> sources and n<sub>0</sub> objects that are tracked along time. Each source and object is unambiguously described by its state, which is noted respectively S<sub>i</sub> and T<sub>i</sub> and which may or may not be multidimensional. In the following it is assumed that the states T<sub>;</sub> of the objects depend on the states of the motions sources S<sub>i</sub> only, that is, </p><p id="p0195" num="0195"> T<sub>i</sub> = T(S<sub>l</sub> ,...,S<sub>n^</sub> ), </p><p id="p0196" num="0196"> where T<sub>i</sub> represents a motion model. It is further assumed that this action is reversable with the knowledge of the state of all objects, i.e. that for each source, there is an operator such that</p><p id="p0197" num="0197"><img id="imgf000026_0001" he="6" wi="30" file="imgf000026_0001.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/></p><p id="p0198" num="0198"> The state A of an object of interest, assuming again that it depends on the 5<sup>*</sup>. only, is then recovered by </p><p id="p0199" num="0199"> A = A(S<sub>l t</sub>...,S<sub>mi</sub>) = A(H<sub>l</sub> ( JJ , .. J<sub>Ko</sub> ), ...H<sub>¾</sub>
<img id="imgf000026_0002" he="6" wi="18" file="imgf000026_0002.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 , assuming a motion model A . It may be possible that the anatomy of interest itself is counted in the interventional devices if its observable state is somehow insufficient and needs to be augmented by the observation of other intervention devices or other third party object. </p><p id="p0200" num="0200"> The learning of the models is necessarily specific to the application. Models may either be universally applicable or need patient-specific learning. </p><p id="p0201" num="0201"> The proposed medical imaging system may be applied in X-ray catheterization laboratory systems as used potentially in an operation room environment. Furthermore, it may also be exploited in other situations where guidance support may be meaningful. Other applications where this invention can be used are minimal invasive surgery where locating interventional instruments such as biopsy needles is of high interest. </p><p id="p0202" num="0202"> According to a further aspect of the present invention, a computer program or a computer program element is proposed that is characterized by being adapted to execute the method steps as defined above with respect to the proposed medical imaging device when executed on an appropriate computing device or programmable system. In fact, a computing </p><p id="p0203" num="0203">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="27"/>-->
 device or programmable system on which such computer program is executed and which furthermore comprises for example suitable interfaces, processors and memory for acquiring the respective anatomy representation data and X-ray image data as an input for subsequent data processing for providing the anatomy-angiogram-registration, the angiogram- fluoroscopy-registration and finally the anatomy- fluoroscopy-registration, may be operative as the above-defined medical imaging device. </p><p id="p0204" num="0204"> According to a further aspect of the present invention, a computer-readable medium such as a CD-ROM is presented wherein the computer-readable medium has a computer program as described in the preceding section stored thereon. However, the computer program may also be presented over a network and may be downloaded into the working memory of a data processor from such network. Accordingly, the computer-readable medium may be a medium for making a computer program available for downloading. </p><p id="p0205" num="0205"> It has to be noted that features and advantages of embodiments of the invention are described herein with reference to different subject-matters. In particular, some embodiments are described with respect to method type features whereas other embodiments are described with respect to device type features. However, a person skilled in the art will gather from the above and the following description that, unless otherwise notified, in addition to any combination of features belonging to one type of subject-matter also any combination between features relating to different subject-matters is considered to be disclosed with this application. Particularly, features can be combined providing synergic effects that are more than the simple sum of the features. </p><p id="p0206" num="0206">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0207" num="0207"> The features and embodiments defined above and further features and advantages of the present invention can also be derived from the examples of embodiments to be described herein after and are explained with reference to examples of embodiments, but to which the invention is not limited. The invention will be described in more detail hereinafter with reference to the drawings. </p><p id="p0208" num="0208"> Fig 1 shows an overview of shape deformations of an intervention device during a complete cardiac cycle. </p><p id="p0209" num="0209"> Fig. 2 shows a position of an anatomy of interest relative to the intervention device at a given phase. </p><p id="p0210" num="0210"> INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="28"/>-->
 Fig. 3 shows an estimated position of an anatomy of interest relative to the intervention device at another phase. </p><p id="p0211" num="0211"> Fig. 4 shows an overview of a medical imaging system according to the invention. </p><p id="p0212" num="0212"> Fig. 5 shows a diagrammatic overview of a method according to the invention.</p><p id="p0213" num="0213">The figures are only schematically and not to scale. </p><p id="p0214" num="0214">DETAILED DESCRIPTION OF EMBODIMENTS </p><p id="p0215" num="0215"> Fig. 1 shows an overview of shape deformations of an intervention device during a complete cardiac cycle. An intervention device 2 is shown for three different phases (pi, q½ and φ<sub>3</sub> of a cardiac cycle, wherein two of these phases may be achieved by end of diastolic (EoD) and end of systolic (EoS) timed triggering. The intervention device 2 may be realized as a coronary sinus catheter (CSC) that is commonly used for occlusion of the coronary sinus, for dispensing cardioplegic solutions and also for monitoring the pressure of the coronary sinus during cardiopulmonary bypass surgery. For electrophysiology applications the coronary sinus catheter comprises electrodes 6 at its tip that may be used as clearly visible landmarks for the shape deformation to which the shape of the intervention device may be reduced. </p><p id="p0216" num="0216"> The three different shape deformations in Fig. 1 are recorded during at least one cardiac cycle when the patient is not conducting a perspirating motion. An instance of a cardiac cycle can automatically be detected by analyzing the motion of the intervention device 2 as it basically conducts a recurring motion. The state of the heart during a cardiac cycle may be represented by a variable S £ [ 0, 2π [ since the motion is cyclic. Furthermore, this state may be proportional to time. </p><p id="p0217" num="0217"> The determination of the relationship between the shape deformation and the cardiac cycle, or the displacement at the cardiac cycle, is a key of the proposed system and method as the perspirating motion is assumed to rigidly affect the intervention device 2. The shape deformation induced by the cardiac cycle and the perspirating motion are superposed and may only be separated through the isolated determination of cardiac cycle induced motion of the intervention device. The operator H giving the state S from the shape of the intervention device 2 may be determined for example by regression techniques on the detected cardiac cycle using various shape features of the intervention device 2, such as orientation, curvature, or the relative position of the electrodes. </p><p id="p0218" num="0218"> INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="29"/>-->
 If it is possible to obtain a three-dimensional position of the electrodes from their two-dimensional projections, e.g. using an anatomic shape prior and a three-dimensional model of the intervention device, a three-dimensional position of the electrodes 6 as the state of the intervention device 2 throughout the heart cycle may be learned. Otherwise, the state may be limited to a two-dimensional position of the electrodes 6, and the model may only be valid until a C-arm of a medical imaging system according to the invention is rotated. </p><p id="p0219" num="0219"> This process may be considered a "learning phase" during which a motion model defining the relationship between intervention device shape deformations and the cardiac cycle is conducted. Thereby, knowing the position within a cardiac cycle, the shape deformation of the intervention device may be determined precisely. </p><p id="p0220" num="0220"> In Fig. 2 a schematic overview of the anatomy of interest is shown. Here, the anatomy of interest is a left atrium (LA) 4. The two main sources of motion correspond to the heart beat (cardiac cycle) and the breathing (perspirating motion). It is assumed that the perspirating motion rigidly moves both the left atrium 4 and the intervention device 2, and that the heart beat does not affect the left atrium 4, which is a reasonable assumption near the roof of the left atrium 4, which is the main region of interest during ablation procedures. </p><p id="p0221" num="0221"> The arrangement of the left atrium 4 and the intervention device 2 is given by the time the image has been acquired. For the image to be acquired a contrast agent may have been applied to the patient for the purpose of making the left atrium 4 visible in an X-ray image acquisition process. The contrast agent shows the anatomy of the left atrium 4, and therefore a model or a previously achieved image can be placed on a screen to the operator, either manually or automatically. To initialize the model in the same frame as the </p><p id="p0222" num="0222">interventional device 2 only a single cardiac motion phase is needed to make an appropriate link. Useful phases for this task are the ones that are easy to detect automatically, showing extreme positions on the course of the intervention device, e.g. corresponding roughly to the EoD and EoS. </p><p id="p0223" num="0223"> Exemplarily, the intervention device 2 has the position ( i in fig. 2. Since the position of the intervention device 2 relative to the left atrium 4 is known and the motion model of the intervention device 2 is determined, the position of the left atrium 4 is determinable for any occuring heart phase. Therefore, on a new image the shape of the intervention device 2 is analyzed, yielding the heart phase. </p><p id="p0224" num="0224"> Using the motion model learned above the motion induced by the heart can be determined. A remaining rigid motion is then entirely attributed to the perspirating motion, </p><p id="p0225" num="0225"> INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="30"/>-->
 thus giving the breathing state, and in turn the position of the left atrium 4. </p><p id="p0226" num="0226"> In its simplest form, the perspirating motion is assumed to be a vertical translation applied uniformly to the intervention device 2 and the left atrium 4. The remaining translation of the intervention device 2 once the heart beat is factored out, can therefore directly be applied to the model of the left atrium 4. </p><p id="p0227" num="0227"> Therefore, after having acquired the image of the left atrium 4 and having determined a motion model of the intervention device 2 the position of the left atrium 4 without contrast agent is determinable for all cardiac cycle phases and during a perspirating motion. The relative position of the left atrium 4 can be made visible to the person conducting the intervention process in registering and superposing the previously acquired image of the left atrium 4 or a model of the left atrium 4 and live fluoroscopy images yielding the actual shape of the intervention device, as depicted in Fig. 3. </p><p id="p0228" num="0228"> Thereby, the detected shape of the intervention device 2 under usage of the determined motion model from the learning phase as depicted in Fig. 1 enables to determine the relationship between a perspirating motion and the relative position of the left atrium 4. </p><p id="p0229" num="0229"> The motion correction may further be conducted through an estimation of the shape 8 of the intervention device 2 during the cardiac phase. For example, if the actual shape of the intervention device 2 equals the shape according to the phase φ<sub>3</sub> of the cardiac cycle as depicted in fig.1 the corresponding shape of the intervention device 2 at the phase (pi may be estimated by means of the motion model. This is indicated by an arrow pointing from the shape of the intervention device 2 at φ<sub>3</sub> to the estimated shape at ( i that may be named cpi'. This estimation may then be used to determine the position of the left atrium 4 as shown in fig. 2. </p><p id="p0230" num="0230"> Fig. 4 shows a diagrammatic overview of a medical imaging system according to the invention. The medical imaging system 10 comprises an X-ray image acquisition device with a source of X-ray radiation 12 provided to generate X-ray radiation. A table 14 is provided to receive an object to be examined. Further, an X-ray image detection module 16 is located opposite the source of X-ray radiation 12. During the radiation procedure, the examined object is located between the source of X-ray radiation 12 and the detection module 16. The latter sends data to a data processing unit 18, which is connected to both the X-ray image detection module 16 and the X-ray radiation source 12. The data processing unit 18 may exemplarily be located underneath the table 14 for saving space within the examination room. It is clear that it could also be located at a different place, such as in a different room or </p><p id="p0231" num="0231">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="31"/>-->
 a different laboratory. Furthermore, an output unit 20 is exemplarily equipped with a display and therefore may be arranged in the vicinity of the table 14 for displacing information to the person operating the medical viewing system, which can be a clinician such as a cardiologist or a cardiac surgeon. Preferably, the display is movably mounted to allow for an individual adjustment depending on the examination situation. Also, an interface unit 22 is arranged to input information by the user. </p><p id="p0232" num="0232"> It is not necessary to use a standalone output unit 20, it may also be possible to include the output unit 20 in the data processing unit 18, where an overlaying and combining process is conducted and provided at suitable output ports for further purposes. </p><p id="p0233" num="0233"> Basically, the image detection module 16 generates images by exposing this subject to X-ray radiation, wherein said images are further processed in the data processing unit 18. It is noted that the example shown is of a so-called C-type X-ray image acquisition device. The X-ray image acquisition device comprises an arm in form of a C where the detection module 16 is arranged at one end of the C-arm and the source of X-ray radiation 12 is located at the opposite end of the C-arm. The C-arm is movably mounted and can be rotated around the object of interest located on the table 14. In other words, it is possible to acquire images with different directions of view. </p><p id="p0234" num="0234"> The data processing unit 18 may be adapted to conduct the method according to the invention and thus can be considered as or comprise the data processing unit for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure. Thereby, a data processor and preferably a storage means for storing the motion model of the intervention device and the model of the object of interest is provided as well as a related software that leads one program element for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure according to exemplary embodiments of the above-described method. The software can be transferred into the data processing unit 18 by means of a computer-readable medium or through a network and may be realized as a complete new operating system or an update. </p><p id="p0235" num="0235"> As becoming apparent in Fig. 5 the method according to the present invention comprises the following process steps: </p><p id="p0236" num="0236"> (i) acquiring 24 a first sequence of images of the vessel region of interest with the intervention device 2 inserted into the vessel region of interest during a time frame without a first motion of the patient; </p><p id="p0237" num="0237"> (ii) detecting 26 a first motion sequence of a cyclic motion of the intervention </p><p id="p0238" num="0238">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="32"/>-->
 device 2 during a second motion cycle by analyzing the motion of the intervention device 2 in the first sequence of acquired images; </p><p id="p0239" num="0239"> (iii) creating 28 a motion model of the intervention device solely induced by the second motion; </p><p id="p0240" num="0240"> (iv) determining 30 an operator defining the relationship between the first motion sequence of the intervention device 2 and the second motion; </p><p id="p0241" num="0241"> (iv) acquiring 32 live images of the vessel region of interest with the intervention device 2 inserted into the vessel region of interest; </p><p id="p0242" num="0242"> (v) subtracting 34 the motion of the intervention device induced by the second motion using the operator and determining the motion of the intervention device 2 induced by the first motion; and </p><p id="p0243" num="0243"> (vi) registering 36 of a representation of an object of interest based on the first motion. </p><p id="p0244" num="0244"> It is pointed out that the live images may be acquired without restrictions regarding the motion of the patient. </p><p id="p0245" num="0245"> The registered representation of the object of interest may furthermore be superposed 38 to the acquired live image and displayed 40. </p><p id="p0246" num="0246"> Finally, it is to be noted that herein the word "comprising" does not exclude other elements or steps, and the indefinite article "a" or "an" does not exclude a plurality. A single processor or other unit may fulfil the functions of several items re-cited in the claims. The mere fact that certain measures are re-cited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. Any reference signs in the claims should not be construed as limiting the scope. </p><p id="p0247" num="0247">INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="33"/>-->
 LIST OF REFERENCE SIGNS: </p><p id="p0248" num="0248">2 intervention device </p><p id="p0249" num="0249"> 4 left atrium (LA) </p><p id="p0250" num="0250"> 6 electrode </p><p id="p0251" num="0251"> 8 shape </p><p id="p0252" num="0252"> 10 medical imaging system </p><p id="p0253" num="0253"> 12 source of X-ray radiation </p><p id="p0254" num="0254"> 14 table </p><p id="p0255" num="0255"> 16 detection module </p><p id="p0256" num="0256"> 18 data processing unit </p><p id="p0257" num="0257"> 20 standalone output unit </p><p id="p0258" num="0258"> 22 interface unit </p><p id="p0259" num="0259"> 24 acquiring a first sequence of images </p><p id="p0260" num="0260"> 26 detecting a first motion sequence </p><p id="p0261" num="0261"> 28 creating a motion model </p><p id="p0262" num="0262"> 30 determining an operator </p><p id="p0263" num="0263"> 32 acquiring live images </p><p id="p0264" num="0264"> 34 subtracting motion of the intervention device </p><p id="p0265" num="0265"> 36 registering of a representation of an object of interest </p><p id="p0266" num="0266"> 38 superposing </p><p id="p0267" num="0267"> 40 displaying </p><p id="p0268" num="0268">INCORPORATED BY REFERENCE (RULE 20.6) 
</p></description><claims mxw-id="PCLM125309645" ref-ucid="WO-2012117321-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="34"/>--> CLAIMS: </claim-statement><claim id="clm-0001" num="1"><claim-text>1. A method (110) for visualizing information of an object of interest, the method comprising the steps of: </claim-text><claim-text> a) providing (112) pre-navigation data (114) of a region of interest of an object (22); wherein the pre-navigation data comprises spatial geometrical data (116) and a functional parameter surface (118) in correspondence to the spatial geometrical data; </claim-text><claim-text> b) acquiring (120) live image data (122) of the region of interest; c) detecting (124) an element (126) in the live image data; </claim-text><claim-text> d) determining (128) spatial relation (130) of the pre-navigation data and the live image data; </claim-text><claim-text> e) determining (132) the position (134) of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and computing (136) a predetermined related point of location (138) on the functional parameter surface; </claim-text><claim-text> f) generating (140) a combination (142) of a simplified surface representation (144) of the region of interest, which simplified surface representation is based on a visualization of the functional parameter surface, and a marker (146) indicating the computed predetermined related point of location; and </claim-text><claim-text> g) displaying (148) the combination as navigation guidance (150). </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. Method according to one of claims 1, wherein the simplified surface representation is an unfolded map image. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. Method according to claim 2, wherein the unfolded map is a bull's-eye view. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. Method according to claim 1, wherein the predetermined related point of location is the closest point. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. Method according to claim 1, wherein the detection of the element comprises determining (162) the 2D position (164) of the element in the live image data and for determining the position in step e), the detected 2D position is mapped (166) to the spatial geometrical data by a direct projection line (168). 
<!-- EPO <DP n="35"/>-->
</claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. Method according to claim 1, wherein step e) comprises a back projection (170) of the tracked location through the spatial geometrical data, to estimate (172) spatial position of the element. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. Method according to claim 6, wherein in case that the back projection does not intersect with the anatomy according to the spatial geometrical data, the closest or statistically most likely point in the anatomy is used. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. Method according to claim 1, wherein a projection axis from the detected element in the spatial geometrical data to the computed related point of location (138) on the functional parameter surface is provided; wherein the simplified surface representation is a 3D image; and wherein the 3D image is aligned with the projection axis. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. Method according to claim 1, wherein step f) comprises combining navigation anatomy with the simplified surface representation. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. Method according to claim 1, wherein the navigation anatomy comprises a projection of the vascular structure. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. Method according to claim 1 , wherein the navigation guidance is a 2D representation comprising functional parameter surface information and 3D positioning information of the element in relation to the region of interest. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. A device (24) for visualizing information of an object of interest, comprising a processing unit (26); </claim-text><claim-text> an interface unit (28); and </claim-text><claim-text> a display (30); </claim-text><claim-text> wherein the interface unit (28) is adapted to provide pre-navigation data of a region of interest of an object, wherein the pre-navigation data comprises spatial geometrical data and a functional parameter surface in correspondence to the spatial geometrical data; and to provide live image data of the region of interest; </claim-text><claim-text> wherein the processing unit (26) is adapted to detect an element in the live 
<!-- EPO <DP n="36"/>-->
 image data; to determine spatial relation of the pre-navigation data and the live image data; to determine the position of the detected element in the spatial geometrical data, which determining is based on the spatial relation, and to compute a predetermined related point of location on the functional parameter surface; and to generate a combination of a simplified surface representation of the region of interest, which simplified surface representation is based on a visualization of the functional parameter surface, and a marker indicating the computed predetermined related point of location; and </claim-text><claim-text> wherein the display (30) is adapted to display the combination as navigation guidance. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. A medical imaging system (10) for visualizing information of an object of interest, comprising: </claim-text><claim-text> a device (24) according to claim 12; and </claim-text><claim-text> image acquisition means (12); </claim-text><claim-text> wherein the image acquisition means (12) are adapted to acquire the live image data of the region of interest. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. A computer program element for controlling a device according to claim 12, which, when being executed by a processing unit, is adapted to perform the method steps of one of the claims 1 to 11. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. A computer readable medium having stored the program element of claim 14. 
<!-- EPO <DP n="37"/>-->
</claim-text><claim-text>CLAIMS: </claim-text><claim-text>1. A medical imaging system (10) for providing an image representation supporting accurate guidance of an intervention device (2) in a vessel intervention procedure in a region of interest, in which region the position of an object of interest is depending on at least a first source of motion, the medical imaging system being adapted for: </claim-text><claim-text> (i) acquiring (24) a first sequence of images of the vessel region of interest with the intervention device inserted into the vessel region of interest during a time frame without a first motion of the patient; </claim-text><claim-text> (ii) detecting (26) a first motion sequence of a cyclic motion of the intervention device during a second motion cycle by analyzing the motion of the intervention device in the first sequence of acquired images; </claim-text><claim-text> (iii) creating (28) a motion model of the intervention device solely induced by the second motion; </claim-text><claim-text> (iv) determining (30) an operator defining the relationship between the first motion sequence of the intervention device and the second motion; </claim-text><claim-text> (iv) acquiring (32) live images of the vessel region of interest with the intervention device inserted into the vessel region of interest of the patient; </claim-text><claim-text> (v) subtracting (34) the motion of the intervention device induced by the second motion using the operator and determining the motion of the intervention device induced by the first motion of the patient; and </claim-text><claim-text> (vi) registering (36) of a representation of the object of interest based on the first motion. </claim-text><claim-text>2. System of claim 1, wherein the first motion source is a perspirating motion. 3. System of claim 1 or 2, wherein the second motion source is a cardiac motion. </claim-text><claim-text>4. System of any one of claims 1 to 3, wherein the region of interest is a region surrounding an aortic root. </claim-text><claim-text>INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="38"/>-->
</claim-text><claim-text>5. System according to any one of claims 1 to 4, wherein the object of interest is a left atrium (4). 6. System of any one of claims 1 to 5, further being adapted for superposing (38) the representation of the object of interest onto the live images and displaying (40) the resulting images onto a screen. </claim-text><claim-text>7. Method for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure in a region of interest, in which region the position of an object of interest is depending on at least a first source of motion, the method comprising: </claim-text><claim-text> (i) acquiring (24) a first sequence of images of the vessel region of interest with the intervention device inserted into the vessel region of interest during a time frame without a first motion of the patient; </claim-text><claim-text> (ii) detecting (26) a first motion sequence of a periodic motion of the intervention device during a second motion cycle by analyzing the motion of the intervention device in the first sequence of acquired images; </claim-text><claim-text> (iii) creating (28) a motion model of the intervention device solely induced by the second motion; </claim-text><claim-text> (iv) determining (30) an operator (H) defining the relationship between the first motion sequence of the intervention device and the second motion; </claim-text><claim-text> (iv) acquiring (32) live images of the vessel region of interest with the intervention device inserted into the vessel region of interest of the patient; </claim-text><claim-text> (v) subtracting (34) the motion of the intervention device induced by the second motion using the operator (H) and determining the motion of the intervention device induced by the first motion of the patient; and </claim-text><claim-text> (vi) registering (36) of a previously acquired image of an object of interest based on the first motion. </claim-text><claim-text>8. A data processing unit (18) for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure, the data processing unit (18) comprising a data processor, which is adapted for performing the method </claim-text><claim-text>INCORPORATED BY REFERENCE (RULE 20.6) 
<!-- EPO <DP n="39"/>-->
 as set forth in claim 7. </claim-text><claim-text>9. A computer-readable medium on which there is stored a computer program for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure, </claim-text><claim-text> the computer program, when being executed by a data processing unit (18) , is adapted for controlling the method as set forth in claim 7. </claim-text><claim-text>10. A program element for providing an image representation supporting accurate guidance of an intervention device in a vessel intervention procedure, </claim-text><claim-text> the program element, when being executed by a data processing unit (18), is adapted for controlling the method as set forth in claim 7. </claim-text><claim-text>INCORPORATED BY REFERENCE (RULE 20.6) 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
