<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2682710-A1" country="EP" doc-number="2682710" kind="A1" date="20140108" family-id="48699660" file-reference-id="315100" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146585023" ucid="EP-2682710-A1"><document-id><country>EP</country><doc-number>2682710</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13174626-A" is-representative="YES"><document-id mxw-id="PAPP154847215" load-source="docdb" format="epo"><country>EP</country><doc-number>13174626</doc-number><kind>A</kind><date>20130702</date><lang>EN</lang></document-id><document-id mxw-id="PAPP177814796" load-source="docdb" format="original"><country>EP</country><doc-number>13174626.5</doc-number><date>20130702</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140557267" ucid="JP-2012149596-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2012149596</doc-number><kind>A</kind><date>20120703</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-2040291459" load-source="docdb">G01B  11/00        20060101AFI20150626BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-2040295332" load-source="docdb">H04N  13/02        20060101ALI20150626BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-2040297808" load-source="docdb">G06T   7/00        20060101ALI20150626BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1624975403" load-source="docdb" scheme="CPC">G06T   7/593       20170101 FI20180616BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1645101229" load-source="docdb" scheme="CPC">H04N  13/239       20180501 LA20180504BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1835140620" load-source="docdb" scheme="CPC">G06T2207/30164     20130101 LA20170324BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1835140622" load-source="docdb" scheme="CPC">G05B2219/37567     20130101 LA20170324BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1835140623" load-source="docdb" scheme="CPC">G01B  11/002       20130101 LI20170324BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1835140624" load-source="docdb" scheme="CPC">G06T2207/10012     20130101 LA20170324BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1835140625" load-source="docdb" scheme="CPC">G06T   7/564       20170101 LI20170324BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1835140626" load-source="docdb" scheme="CPC">H04N2013/0081      20130101 LA20170324BHEP        </classification-cpc><classification-cpc mxw-id="PCL1991318674" load-source="docdb" scheme="CPC">Y10S 901/46        20130101 LA20140102BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132359702" lang="DE" load-source="patent-office">Vorrichtung und Verfahren zur dreidimensionaler Messung und Robotersystem mit dieser Vorrichtung</invention-title><invention-title mxw-id="PT132359703" lang="EN" load-source="patent-office">Apparatus and method for three-dimensional measurement and robot system comprising said apparatus</invention-title><invention-title mxw-id="PT132359704" lang="FR" load-source="patent-office">Appareil et méthode de mesure tridimensionnel et système robotique comprenant cet appareil</invention-title><citations><patent-citations><patcit mxw-id="PCIT242942887" load-source="docdb" ucid="JP-2003248814-A"><document-id format="epo"><country>JP</country><doc-number>2003248814</doc-number><kind>A</kind><date>20030905</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942889" load-source="docdb" ucid="US-20080089557-A1"><document-id format="epo"><country>US</country><doc-number>20080089557</doc-number><kind>A1</kind><date>20080417</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942890" load-source="docdb" ucid="US-20100156896-A1"><document-id format="epo"><country>US</country><doc-number>20100156896</doc-number><kind>A1</kind><date>20100624</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942888" load-source="docdb" ucid="US-6614928-B1"><document-id format="epo"><country>US</country><doc-number>6614928</doc-number><kind>B1</kind><date>20030902</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>BELHAOUA A ET AL: "Estimation of 3D reconstruction errors in a stereo-vision system", PROCEEDINGS OF SPIE - INTERNATIONAL SOCIETY FOR OPTICAL ENGINEERING, S P I E - INTERNATIONAL SOCIETY FOR OPTICAL ENGINEERING, US, vol. 7390, 15 June 2009 (2009-06-15), pages 1 - 10, XP002693538, ISSN: 0277-786X, DOI: 10.1117/12.827346</text><sources><source mxw-id="PNPL55078293" load-source="docdb" name="SEA" category="A"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR919516629" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CANON KK</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR919514691" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CANON KABUSHIKI KAISHA</last-name></addressbook></applicant><applicant mxw-id="PPAR919014707" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Canon Kabushiki Kaisha</last-name><iid>101093230</iid><address><street>30-2 Shimomaruko 3-chome Ohta-ku</street><city>Tokyo 146-8501</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919508040" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SUZUKI HIDEAKI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919538010" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SUZUKI, HIDEAKI</last-name></addressbook></inventor><inventor mxw-id="PPAR919009234" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SUZUKI, HIDEAKI</last-name><address><street>c/o Canon Kabushiki Kaisha 30-2, Shimomaruko 3-chome, v</street><city>Tokyo 146-8501</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919011457" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>TBK-Patent</last-name><iid>100061560</iid><address><street>Bavariaring 4-6</street><city>80336 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS549743762" load-source="docdb">AL</country><country mxw-id="DS549908114" load-source="docdb">AT</country><country mxw-id="DS549743764" load-source="docdb">BE</country><country mxw-id="DS549814417" load-source="docdb">BG</country><country mxw-id="DS549814523" load-source="docdb">CH</country><country mxw-id="DS549741603" load-source="docdb">CY</country><country mxw-id="DS549908115" load-source="docdb">CZ</country><country mxw-id="DS549813165" load-source="docdb">DE</country><country mxw-id="DS549743773" load-source="docdb">DK</country><country mxw-id="DS549741604" load-source="docdb">EE</country><country mxw-id="DS549817144" load-source="docdb">ES</country><country mxw-id="DS549814426" load-source="docdb">FI</country><country mxw-id="DS549814427" load-source="docdb">FR</country><country mxw-id="DS549813170" load-source="docdb">GB</country><country mxw-id="DS549743774" load-source="docdb">GR</country><country mxw-id="DS549813171" load-source="docdb">HR</country><country mxw-id="DS549741609" load-source="docdb">HU</country><country mxw-id="DS549814524" load-source="docdb">IE</country><country mxw-id="DS549743775" load-source="docdb">IS</country><country mxw-id="DS549814428" load-source="docdb">IT</country><country mxw-id="DS549743776" load-source="docdb">LI</country><country mxw-id="DS549813172" load-source="docdb">LT</country><country mxw-id="DS549908116" load-source="docdb">LU</country><country mxw-id="DS549813173" load-source="docdb">LV</country><country mxw-id="DS549813178" load-source="docdb">MC</country><country mxw-id="DS549908117" load-source="docdb">MK</country><country mxw-id="DS549908118" load-source="docdb">MT</country><country mxw-id="DS549908119" load-source="docdb">NL</country><country mxw-id="DS549814525" load-source="docdb">NO</country><country mxw-id="DS549908120" load-source="docdb">PL</country><country mxw-id="DS549817145" load-source="docdb">PT</country><country mxw-id="DS549908121" load-source="docdb">RO</country><country mxw-id="DS549817150" load-source="docdb">RS</country><country mxw-id="DS549908122" load-source="docdb">SE</country><country mxw-id="DS549813180" load-source="docdb">SI</country><country mxw-id="DS549814530" load-source="docdb">SK</country><country mxw-id="DS549814531" load-source="docdb">SM</country><country mxw-id="DS549743785" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128673195" lang="EN" load-source="patent-office"><p id="pa01" num="0001">The three-dimensional measurement apparatus includes: a base camera for capturing a workpiece to acquire a first image; a reference camera for capturing the workpiece from a view point different from the base camera to acquire a second image; and a camera control unit for extracting multiple edge lines from the first and second images, calculating line-of-sight errors with respect to provisional three-dimensional coordinates, wherein the line-of-sight errors are calculated at an endpoint of a first edge line selected from the multiple edge lines of the first image and an endpoint of a second edge line extracted from the multiple edge lines of the second image to correspond to the first edge line, setting an evaluation function from the line-of-sight errors, and a three-dimensional constraint condition set based on a shape of the workpiece, and making an optimization calculation of the evaluation function to measure the three-dimensional coordinates of the workpiece.
<img id="iaf01" file="imgaf001.tif" wi="105" he="101" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA128737345" lang="EN" source="EPO" load-source="docdb"><p>The three-dimensional measurement apparatus includes: a base camera for capturing a workpiece to acquire a first image; a reference camera for capturing the workpiece from a view point different from the base camera to acquire a second image; and a camera control unit for extracting multiple edge lines from the first and second images, calculating line-of-sight errors with respect to provisional three-dimensional coordinates, wherein the line-of-sight errors are calculated at an endpoint of a first edge line selected from the multiple edge lines of the first image and an endpoint of a second edge line extracted from the multiple edge lines of the second image to correspond to the first edge line, setting an evaluation function from the line-of-sight errors, and a three-dimensional constraint condition set based on a shape of the workpiece, and making an optimization calculation of the evaluation function to measure the three-dimensional coordinates of the workpiece.</p></abstract><description mxw-id="PDES63959183" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">BACKGROUND OF THE INVENTION</heading><heading id="h0002">Field of the Invention</heading><p id="p0001" num="0001">The present invention relates to a three-dimensional measurement apparatus for capturing images of a workpiece from two or more view points to make a three-dimensional measurement or view the workpiece stereoscopically, and a robot system including the same.</p><heading id="h0003">Description of the Related Art</heading><p id="p0002" num="0002">Conventionally, as a method of measuring the three-dimensional shape of a workpiece in a non-contact manner, a stereo method has been known, in which the workpiece is captured from cameras located at two or more view points to measure a three-dimensional shape of the workpiece from the captured two or more images. In the stereo method, edge lines of the workpiece on the captured images may be used to measure the three-dimensional shape of the workpiece with a small amount of information.</p><p id="p0003" num="0003">The stereo method for measuring the three-dimensional shape using edge lines can obtain an exact three-dimensional shape of the workpiece under ideal conditions, but there is an error in calibrating a camera or an error in extracting an edge line in practice. This causes a problem that edge lines that should have been connected are recreated in twisted positions or the angle between edge lines is inaccurate in the measured three-dimensional<!-- EPO <DP n="2"> --> shape.</p><p id="p0004" num="0004">In contrast to this, there is proposed a technique for using a condition that a plurality of edge lines of a workpiece lies on the same plane (hereinafter called "plane constraint condition") to correct a corresponding point in order to improve the accuracy of three-dimensional measurement (see Japanese Patent Application Laid-Open No. <patcit id="pcit0001" dnum="JP2003248814A"><text>2003-248814</text></patcit>).</p><p id="p0005" num="0005">However, in the technique disclosed in Japanese Patent Application Laid-Open No. <patcit id="pcit0002" dnum="JP2003248814A"><text>2003-248814</text></patcit>, the plane constraint condition that a group of points on the same plane should meet to determine a plane parameter in order to correct a corresponding point on an image. Therefore, multiple three-dimensional constraint conditions, such as a relative angle between edge lines that do not lie on the same plane and a stereoscopic positional relationship between edge lines, cannot be added.</p><p id="p0006" num="0006">When three-dimensional data is actually generated from captured images, it is often the case that multiple three-dimensional constraint conditions, such as "edge lines are perpendicular to each other", "edge lines are parallel to each other", and "edge lines lie on the same straight line", are already known by design. Therefore, a three-dimensional measurement apparatus is desired, which applies these multiple three-dimensional constraint conditions to improve the accuracy of three-dimensional measurement.<!-- EPO <DP n="3"> --></p><p id="p0007" num="0007">The present invention provides a three-dimensional measurement apparatus capable of measuring a highly accurate three-dimensional shape and a robot system including the same.</p><heading id="h0004">SUMMARY OF THE INVENTION</heading><p id="p0008" num="0008">The present invention includes: a first camera for capturing a workpiece to acquire a first image; a second camera for capturing the workpiece from a view point different from that of the first camera to acquire a second image; and a control unit for extracting a plurality of edge lines of the workpiece respectively from the first image and the second image, calculating each of line-of-sight errors with respect to provisional three-dimensional coordinates, wherein the line-of-sight errors are calculated from an endpoint of a first edge line selected from the plurality of edge lines of the first image and an endpoint of a second edge line extracted from the plurality of edge lines of the second image so as to correspond to the first edge line, setting an evaluation function from the respective line-of-sight errors and a three-dimensional constraint condition set based on the three-dimensional shape of the workpiece, and making an optimization calculation based on the evaluation function to determine three-dimensional coordinates of the workpiece.</p><p id="p0009" num="0009">The present invention further provides a three-dimensional measurement apparatus including: a first<!-- EPO <DP n="4"> --> camera for capturing a workpiece to acquire a first image; a second camera for capturing the workpiece from a view point different from that of the first camera to acquire a second image; and a control unit for extracting a plurality of edge lines of the workpiece respectively from the first image and the second image, calculating a line-of-sight error at an endpoint of a first edge line with respect to provisional three-dimensional coordinates, wherein the first edge line is selected from the plurality of edge lines of the first image, calculating a viewing plane error in a second edge line with respect to the provisional three-dimensional coordinates, wherein the second edge line is extracted from the plurality of edge lines of the second image so as to correspond to the first edge line, setting an evaluation function from the line-of-sight error at the endpoint of the first edge line, the viewing plane error in the second edge line, and a three-dimensional constraint condition set based on the three-dimensional shape of the workpiece, and making an optimization calculation based on the evaluation function to determine three-dimensional coordinates of the workpiece.</p><p id="p0010" num="0010">The present invention further provides a three-dimensional measurement method including: extracting a plurality of edge lines of a workpiece from a first image acquired by capturing the workpiece with a first camera; extracting a plurality of edge lines of the workpiece from<!-- EPO <DP n="5"> --> a second image acquired by capturing the workpiece with a second camera; calculating an endpoint of a second edge line corresponding to an endpoint of a first edge line, wherein the second edge line is extracted from the plurality of edge lines of the second image so as to correspond to the first edge line selected from the plurality of edge lines of the first image; calculating each of line-of-sight errors at the endpoint of the first edge line and the endpoint of the second edge line with respect to provisional three-dimensional coordinates; setting an evaluation function from the line-of-sight errors respectively set at the endpoint of the first edge line and the endpoint of the second edge line, and a three-dimensional constraint condition set based on the three-dimensional shape of the workpiece; and making an optimization calculation to minimize or maximize the value of the evaluation function to determine three-dimensional coordinates of the workpiece.</p><p id="p0011" num="0011">Further, the present invention provides a three-dimensional measurement method including: extracting a plurality of edge lines of a workpiece from a first image acquired by capturing the workpiece with a first camera; extracting a plurality of edge lines of the workpiece from a second image acquired by capturing the workpiece with a second camera; calculating a line-of-sight error at an endpoint of a first edge line with respect to provisional three-dimensional coordinates, wherein the first edge line<!-- EPO <DP n="6"> --> is arbitrarily selected from the plurality of edge lines of the first image, and a viewing plane error in a second edge line with respect to the provisional three-dimensional coordinates, wherein the second edge line is extracted from the plurality of edge lines of the second image so as to correspond to the first edge line; setting an evaluation function from the line-of-sight error at the endpoint of the first edge line, the viewing plane error in the second edge line, and a three-dimensional constraint condition set based on the three-dimensional shape of the workpiece; and making an optimization calculation to minimize or maximize the value of the evaluation function to determine three-dimensional coordinates of the workpiece.</p><p id="p0012" num="0012">Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p><heading id="h0005">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0013" num="0013"><figref idrefs="f0001">FIG. 1</figref> is a perspective view illustrating a schematic structure of a robot system according to an exemplary embodiment of the present invention.</p><p id="p0014" num="0014"><figref idrefs="f0002">FIG. 2</figref> is a block diagram illustrating the configuration of a control device in the robot system according to the exemplary embodiment.</p><p id="p0015" num="0015"><figref idrefs="f0003">FIG. 3</figref> is a block diagram illustrating the configuration of a camera control unit according to a<!-- EPO <DP n="7"> --></p><heading id="h0006">first embodiment.</heading><p id="p0016" num="0016"><figref idrefs="f0004">FIG. 4A and FIG. 4B</figref> are flowcharts illustrating a measurement of three-dimensional coordinates according to the first embodiment.</p><p id="p0017" num="0017"><figref idrefs="f0005">FIG. 5</figref> is a schematic diagram illustrating a base camera, a reference camera, and a workpiece according to the first embodiment.</p><p id="p0018" num="0018"><figref idrefs="f0006">FIG. 6</figref> is a diagram for describing how to determine corresponding points using epipolar lines.</p><p id="p0019" num="0019"><figref idrefs="f0007">FIG. 7</figref> is a schematic diagram for describing line-of-sight errors at a first endpoint of a first edge line and a second endpoint of a second edge line.</p><p id="p0020" num="0020"><figref idrefs="f0008">FIG. 8</figref> is a table illustrating a data structure when mapping is done between edge lines according to the first embodiment.</p><p id="p0021" num="0021"><figref idrefs="f0009">FIG. 9</figref> is a schematic diagram for describing a three-dimensional constraint condition between edge lines according to the first embodiment.</p><p id="p0022" num="0022"><figref idrefs="f0010">FIG. 10</figref> is a block diagram illustrating the configuration of a camera control unit according to a second embodiment.</p><p id="p0023" num="0023"><figref idrefs="f0011">FIG. 11A and FIG. 11B</figref> are flowcharts illustrating a measurement of three-dimensional coordinates according to the second embodiment.</p><p id="p0024" num="0024"><figref idrefs="f0012">FIG. 12</figref> is a schematic diagram for describing a line-of-sight error and a viewing plane error according to the second embodiment.<!-- EPO <DP n="8"> --></p><heading id="h0007">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p0025" num="0025">Exemplary embodiments of the present invention will now be described in detail with reference to the accompanying drawings.</p><p id="p0026" num="0026">&lt;First Embodiment&gt;</p><p id="p0027" num="0027">A robot system 1 according to a first embodiment of the present invention will be described below with reference to <figref idrefs="f0001 f0002 f0003 f0004 f0005 f0006 f0007 f0008 f0009">FIG. 1 to FIG. 9</figref>. First, a schematic structure of the robot system 1 will be described with reference to <figref idrefs="f0001 f0002 f0003">FIG. 1 to FIG. 3</figref>. <figref idrefs="f0001">FIG. 1</figref> is a perspective view illustrating the robot system 1 according to an exemplary embodiment of the present invention. <figref idrefs="f0002">FIG. 2</figref> is a block diagram illustrating the configuration of a control device 4 in the robot system 1 according to the exemplary embodiment. <figref idrefs="f0003">FIG. 3</figref> is a block diagram illustrating the configuration of a camera control unit 5 according to the first embodiment.</p><p id="p0028" num="0028">As illustrated in <figref idrefs="f0001">FIG. 1</figref>, the robot system 1 according to the first embodiment includes a robot 2 for assembling a workpiece, a mounting base 3 on which the robot 2 assembles the workpiece, and the control device 4 for controlling the robot 2.</p><p id="p0029" num="0029">The robot 2 is a six-axis articulated general-purpose robot including a robot arm 20, a hand 21 attached to the tip of the robot arm, a base camera 22 as a first camera, and a reference camera 23 as a second camera. In the embodiment, two cameras, i.e. the base camera 22 and<!-- EPO <DP n="9"> --> the reference camera 23 are used, but a compound-eye camera such as a stereo camera may be used.</p><p id="p0030" num="0030">The robot arm 20 is equipped with six actuators (not illustrated) for driving each joint to rotate about each joint axis. The six actuators are selectively driven, respectively, to move the hand 21 attached to the tip to any three-dimensional position. The hand 21 is equipped with a gripping portion 21a for gripping the workpiece and an actuator, not illustrated, for driving the gripping portion 21a. The hand 21 is moved by the driving of the robot arm 20 to a position in which the actuator is so driven that the hand 21 will grip the workpiece. A force sensor, not illustrated, is provided in the hand 21 to regulate a grip force when the workpiece is gripped. The hand 21 is detachably attached to the tip of the robot arm 20 in a manner to be changeable according to the shape of the workpiece to work on.</p><p id="p0031" num="0031">The base camera 22 is attached to the tip of the robot arm 20 to capture an image of the workpiece in order to acquire a first image of the workpiece. The reference camera 23 is attached adjacent to the base camera 22 at the tip of the robot arm 20 to capture an image of the workpiece from a view point different from the base camera 22 in order to acquire a second image of the workpiece.</p><p id="p0032" num="0032">The mounting base 3 is formed into a rectangular box shape, and provided with a planar placement section 30 with the workpiece placed on the top face thereof. A jig<!-- EPO <DP n="10"> --> 31 used to assemble the workpiece is provided substantially in a central portion of the placement section 30. A support 32 for immovably supporting the base end section of the robot arm 20, a parts feeder 33, and a tool holder 34 are provided at the corners of the placement section 30, respectively. Four casters 35 for moving the mounting base 3 and four fixing brackets 36 for fixing the mounting base 3 to the floor are provided at the bottom of the mounting base 3 so that the mounting base 3 can be fixed after the mounting base 3 is moved to any position.</p><p id="p0033" num="0033">As illustrated in <figref idrefs="f0002">FIG. 2</figref>, the control device 4 is so configured that the robot arm 20, the hand 21, the base camera 22, and the reference camera 23 are connected through a bus to a computer main body having a calculating device 40 and a storing device 41. An input device 42, a teaching pendant 43, a display 44, a speaker 45, a recording medium reading device 46, a communication device 47, and the like are also connected to the computer main body through the bus. In <figref idrefs="f0002">FIG. 2</figref>, interfaces for these connections are not illustrated.</p><p id="p0034" num="0034">The calculating device 40 includes a CPU 40a, an image processing device 40b, and a sound processing device 40c. The CPU 40a includes a camera control unit 5 and a robot control unit 6. The camera control unit 5 measures the three-dimensional position of a workpiece according to various programs stored in the storing device 41 and the<!-- EPO <DP n="11"> --> settings input from the input device 42.</p><p id="p0035" num="0035">As illustrated in <figref idrefs="f0003">FIG. 3</figref>, the camera control unit 5 includes a first image edge line extraction unit 51, a second image edge line extraction unit 52, a corresponding point calculation unit 53, a line-of-sight error calculation unit 54, an evaluation function setting unit 55, and a three-dimensional coordinate measurement unit 56.</p><p id="p0036" num="0036">The first image edge line extraction unit 51 extracts a plurality of edge lines of the workpiece from a first image acquired by capturing the workpiece with the base camera 22. The second image edge line extraction unit 52 extracts a plurality of edge lines of the workpiece from a second image acquired by capturing the workpiece with the reference camera 23. The corresponding point calculation unit 53 extracts, from the plurality of edge lines of the second image, a second edge line corresponding to a first edge line arbitrarily selected from the plurality of edge lines of the first image, and calculates an endpoint of the extracted second edge line corresponding to an endpoint of the first edge line. The line-of-sight error calculation unit 54 sets provisional three-dimensional coordinates and calculates respective line-of-sight errors at the endpoint of the first edge line and the endpoint of the second edge line with respect to the provisional three-dimensional coordinates. The evaluation function setting unit 55 sets an evaluation<!-- EPO <DP n="12"> --> function including the line-of-sight errors and three-dimensional constraint conditions from the line-of-sight errors respectively set at the endpoint of the first edge line and the endpoint of the second edge line, and the three-dimensional constraint conditions set based on the shape of the workpiece. The three-dimensional coordinate measurement unit 56 makes an optimization calculation to minimize or maximize the evaluation function in order to measure the three-dimensional coordinates of the workpiece. The measurement of the three-dimensional position of the workpiece by the camera control unit 5 will be described in detail later.</p><p id="p0037" num="0037">Based on the three-dimensional position of the workpiece measured by the camera control unit 5, the various programs stored in the storing device 41, and the settings input from the input device 42, the robot control unit 6 controls the robot arm 20 and the hand 21. The description of the control of the robot arm 20 and the hand 21 performed by the robot control unit 6 will be omitted.</p><p id="p0038" num="0038">The image processing device 40b controls the display 44 in accordance with a drawing instruction from the CPU 40a to display a predetermined image on the screen. The sound processing device 40c generates a sound signal in accordance with a sound generation instruction from the CPU 40a to output the sound signal to the speaker 45.</p><p id="p0039" num="0039">The storing device 41 is connected to the CPU<!-- EPO <DP n="13"> --> 40a through the bus, and includes a ROM 41a in which the various programs and data are stored, and a RAM 41b reserved as a working area of the CPU 40a. In the embodiment, the various programs for measuring the workpiece three-dimensionally are stored in the ROM 41a in addition to drive programs for the robot arm 20 and the hand 21. For example, a three-dimensional measurement program for executing a first image edge line extraction process, a second image edge line extraction process, a corresponding point calculation process, a line-of-sight error calculation process, an evaluation function setting process, and a three-dimensional coordinate measurement process to be described later is stored. When an operator enters three-dimensional constraint conditions and data, such as camera parameters of the base camera 22 and the reference camera 23, via the input device 42, these are also stored in the ROM 41a.</p><p id="p0040" num="0040">The input device 42 includes a keyboard 42a and a mouse 42b to enable input of information necessary to measure the three-dimensional position of the workpiece and other instructions. The recording medium reading device 46 is used to read a computer-readable recording medium 48 that has recorded, for example, the three-dimensional measurement program, and to store the three-dimensional measurement program in the ROM 41a. The communication device 47 is used to download the three-dimensional measurement program distributed from the<!-- EPO <DP n="14"> --> Internet through the communication device 47 without using the recording medium 48 mentioned above.</p><p id="p0041" num="0041">Next, a method of measuring the three-dimensional coordinates of the workpiece by the camera control unit 5 in the robot system 1 structured as mentioned above will be described along flowcharts illustrated in <figref idrefs="f0004">FIG. 4A and FIG. 4B</figref> while referring to <figref idrefs="f0005 f0006 f0007 f0008 f0009">FIG. 5 to FIG. 9</figref>. <figref idrefs="f0004">FIG. 4A and FIG. 4B</figref> are flowcharts illustrating a measurement of three-dimensional coordinates according to the first embodiment. <figref idrefs="f0005">FIG. 5</figref> is a schematic diagram illustrating the base camera 22, reference camera 23, and the workpiece according to the first embodiment. <figref idrefs="f0006">FIG. 6</figref> is a diagram for describing how to determine corresponding points using epipolar lines. <figref idrefs="f0007">FIG. 7</figref> is a schematic diagram for describing line-of-sight errors at a first endpoint of the first edge line and a second endpoint of the second edge line. <figref idrefs="f0008">FIG. 8</figref> is a table illustrating a data structure when mapping is done between edge lines according to the first embodiment. <figref idrefs="f0009">FIG. 9</figref> is a schematic diagram for describing a three-dimensional constraint condition between edge lines according to the first embodiment.</p><p id="p0042" num="0042">First, as illustrated in <figref idrefs="f0005">FIG. 5</figref>, the Z-axis of each camera coordinate system is set to be perpendicular to each image plane with their origins at respective lens principal points O<sub>L</sub> and OR of the base camera 22 and the reference camera 23 placed to have a common field of view<!-- EPO <DP n="15"> --> from different view points. Hereafter, a coordinate system representative of the base camera 22 is denoted as a base camera coordinate system C<sub>L</sub>, a coordinate system representative of the reference camera 23 is denoted as a reference camera coordinate system C<sub>R</sub>, the image plane of the base camera 22 is denoted as a base image plane 22a, and the image plane of the reference camera 23 is denoted as a reference image plane 23a.</p><p id="p0043" num="0043">Next, internal parameters of the base camera 22, internal parameters of the reference camera 23, and external parameters of the cameras are determined and stored in the ROM 41a. The internal parameters are parameters required to calculate a projection from any three-dimensional coordinates expressed in each camera coordinate system to a point on each image plane, which can be determined by a known camera calibration technique. In the embodiment, the detailed description of how to calculate the internal parameters will be omitted, and the following expression is used as the internal parameters.</p><p id="p0044" num="0044">Suppose that f denotes the focal length of a lens, c<sub>u</sub> denotes the column coordinate of an intersection point of the optical axis of a camera with an image plane, c<sub>v</sub> denotes the row coordinate of the intersection point of the optical axis of the camera with the image plane, s<sub>x</sub> denotes the width of an image sensor per pixel, and s<sub>y</sub> denotes the height of the image sensor per pixel. The column coordinate c<sub>u</sub> and the row coordinate c<sub>v</sub> means that<!-- EPO <DP n="16"> --> an image pixel in the c<sub>u</sub>-th column from the left and the c<sub>v</sub>-th row from the top is the intersection point of the optical axis of the camera with the image plane. The values of c<sub>u</sub> and c<sub>v</sub> do not need to be integers, which may be determined with subpixel accuracy. Further, the focal distance f, the width s<sub>x</sub> per pixel, and the height s<sub>y</sub> per pixel are represented in typical units of length (e.g., millimeters or meters).</p><p id="p0045" num="0045">The internal parameters of the base camera 22 can be expressed as (f<sub>L</sub>, c<sub>uL</sub>, c<sub>vL</sub>, s<sub>xL</sub>, s<sub>yL</sub>), and the internal parameters of the reference camera 23 can be expressed as (f<sub>R</sub>, c<sub>uR</sub>, c<sub>vR</sub>, s<sub>xR,</sub> s<sub>yR</sub>). Note that the subscript L indicates the internal parameters of the base camera 22, and the subscript R indicates the internal parameters of the reference camera 23.</p><p id="p0046" num="0046">Using the internal parameters, a point of the image coordinates (u, v) can be transformed to coordinates P=<sup>T</sup>(x, y, z) in the camera coordinate system as follows. <maths id="math0001" num=""><math display="block"><mi mathvariant="bold">P</mi><mo>=</mo><mfenced><mtable><mtr><mtd><mi>x</mi></mtd></mtr><mtr><mtd><mi>y</mi></mtd></mtr><mtr><mtd><mi>z</mi></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced><mtable><mtr><mtd><msub><mi>s</mi><mi>x</mi></msub><mo>⁢</mo><mfenced separators=""><mi>u</mi><mo>-</mo><msub><mi>c</mi><mi>u</mi></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>s</mi><mi>y</mi></msub><mo>⁢</mo><mfenced separators=""><mi>v</mi><mo>-</mo><msub><mi>c</mi><mi>v</mi></msub></mfenced></mtd></mtr><mtr><mtd><mi>f</mi></mtd></mtr></mtable></mfenced></math><img id="ib0001" file="imgb0001.tif" wi="57" he="30" img-content="math" img-format="tif"/></maths></p><p id="p0047" num="0047">The external parameters are parameters representing relative postures of the base camera 22 and the reference camera 23 in six degrees of freedom, which can be determined by the known camera calibration technique. In the embodiment, the detailed description of<!-- EPO <DP n="17"> --> how to calculate the external parameters between the base camera 22 and the reference camera 23 will be omitted, and the external parameters are represented by a rotation matrix R and a translation vector t.</p><p id="p0048" num="0048">Here, the rotation matrix R and the translation vector t represent a coordinate transformation from the base camera coordinate system C<sub>L</sub> to the reference camera coordinate system C<sub>R</sub>, and any three-dimensional coordinates <sup>(R)</sup>X described in the reference camera coordinate system C<sub>R</sub> are expressed using the external parameters in the base camera coordinate system C<sub>L</sub> as follows. <maths id="math0002" num=""><math display="block"><mi mathvariant="bold">X</mi><mo>⁢</mo><msup><mo>=</mo><mfenced><mi>L</mi></mfenced></msup><mspace width="1em"/><mi mathvariant="bold">X</mi><mo>=</mo><mi mathvariant="bold">R</mi><mo>⁢</mo><msup><mo>•</mo><mfenced><mi mathvariant="normal">R</mi></mfenced></msup><mspace width="1em"/><mi mathvariant="bold">X</mi><mo>+</mo><mi>t</mi></math><img id="ib0002" file="imgb0002.tif" wi="55" he="13" img-content="math" img-format="tif"/></maths></p><p id="p0049" num="0049">In the above, the subscripts <sup>(L)</sup> and <sup>(R)</sup> are used to make them clear to be the respective coordinate values expressed in the base camera coordinate system C<sub>L</sub> and the reference camera coordinate system C<sub>R</sub>. In the following description, it is assumed that the base camera coordinate system is the entire base coordinate system, and the coordinates without any subscript indicative of a coordinate system are described in the base camera coordinate system. In this case, O<sub>L</sub>=<sup>T</sup>(0, 0, 0) and O<sub>R</sub>=t. There are various forms of expression of the internal parameters and external parameters of the cameras, such as a matrix form, but it is not essential to use which form in the embodiment. Further, in the embodiment, no lens<!-- EPO <DP n="18"> --> distortion is taken into account, but parameters that take the lens distortion into account may be used.</p><p id="p0050" num="0050">Next, preparations for an evaluation function are made. In the embodiment, the evaluation function for evaluating an error from a line of sight with respect to an image point and an error from a three-dimensional constraint condition assumes an important role. In the embodiment, the evaluation function is denoted by F. The evaluation function F according to the embodiment is expressed as the sum of a line-of-sight error term portion and a three-dimensional constraint condition term portion. The line-of-sight error term portion of the evaluation function F is denoted by F<sub>sight(N)</sub> and the three-dimensional constraint condition term portion of the evaluation function F is denoted by F<sub>const(M)</sub>, i.e., the evaluation function F is given as follows. <maths id="math0003" num=""><math display="block"><mi>F</mi><mo>=</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mfenced><mi>N</mi></mfenced></mrow></msub><mo>+</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>M</mi></mfenced></mrow></msub></math><img id="ib0003" file="imgb0003.tif" wi="57" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0051" num="0051">The initial values upon starting a processing flow illustrated in <figref idrefs="f0004">FIGS. 4A and 4B</figref> are F<sub>sight(N)</sub>=0 and F<sub>const(M)</sub>=0, and a term is added by looping. The subscript N indicates the line-of-sight error term on the first to N-th edge mappings, and the subscript M indicates the constraint condition term at the time of setting M constraint conditions.</p><p id="p0052" num="0052">Next, a workpiece 10 is captured with the base camera 22 to acquire a base image 22b as the first image,<!-- EPO <DP n="19"> --> and edge line extraction processing is performed on the acquired base image 22b to extract a plurality of edge lines of the workpiece 10 (step S101: first image edge line extraction process). Specifically, an edge extraction filter such as Sobel filter or Canny filter is first used for the base image 22b to extract, as an edge point group, points at which the luminance change is maximized. Next, linear approximation processing is performed on the extracted edge point group while dividing them into line segments to determine image coordinates (pixel position) of each endpoint. Although the image coordinates may take pixel-wise values, it is desired to use processing with subpixel accuracy in order to make a high accuracy measurement. In the embodiment, a line segment obtained by approximating an edge point group is called an edge line. Similarly, edge line extraction processing is performed on a reference image 23b as the second image obtained by capturing the workpiece 10 with the reference camera 23 to extract a plurality of edge lines of the workpiece 10 (step S101: second image edge line extraction process).</p><p id="p0053" num="0053">Next, mapping processing is performed to identify where the plurality of edge lines extracted in the base image 22b are imaged in the reference image 23b, and the results are stored in the ROM 41a. In other words, mapping between the plurality of edge lines extracted from the base image 22b and the plurality of edge lines<!-- EPO <DP n="20"> --> extracted from the reference image 23b is done and the results are stored. The mapping processing is continuously performed until the end of the mapping processing is determined (steps S102 to S106). The mapping may be done by specifying edge lines believed to be the same by a user while viewing images on a UI, or done by using a known technique in which the camera control unit 5 detects similar image patterns using image correlation to make a selection. Specifically, one edge line (first edge line) is selected from the plurality of edge lines extracted from the base image 22b (step S103) to select an edge line (second edge line) corresponding thereto within the reference image 23b (step S104). Processing to be described below is performed on the associated first edge line and second edge line to determine one edge line in a three-dimensional space.</p><p id="p0054" num="0054">Because of restrictions such as lighting conditions, it is not always true that endpoints of the edge lines mapped as mentioned above (e.g., first edge line and second edge line) are the same points. Therefore, epipolar constraints are imposed on the mapped edge lines to determine correspondences between points on the edge lines (step S105: corresponding point calculation process).</p><p id="p0055" num="0055">In step S101, for example, since the image coordinates of the endpoint of the selected first edge line are known, an intersection point of an epipolar line on the reference image 23b corresponding to the endpoint<!-- EPO <DP n="21"> --> of the first edge line and the second edge line on the reference image 23b can be determined to determine the coordinates of a corresponding point. For example, as illustrated in <figref idrefs="f0006">FIG. 6</figref>, the epipolar constraints are used to determine a part imaged in common between the base image 22b and the reference image 23b in order to determine respective endpoints. In <figref idrefs="f0006">FIG. 6</figref>, epipolar lines indicated by the same type of broken line between the base image 22b and the reference image 23b correspond to each other. This corresponding point calculation processing is performed to determine the image coordinates of image points P<sub>Li1</sub>, P<sub>Li2</sub>, P<sub>Ri1</sub>, and P<sub>Ri2</sub> on the respective images with respect to i-th edge line correspondences.</p><p id="p0056" num="0056">Next, a line-of-sight error term on an unknown three-dimensional coordinates X<sub>ij</sub> is determined using the image coordinates of corresponding points determined in step S105 (step S106: line-of-sight error calculation process). Here, the subscript i indicates the i-th edge line correspondences and the subscript j, where j=1, 2, corresponds to both endpoints of the respective edge lines. When a base image line of sight 24 and a reference image line of sight 25, which connect the lens principal points O<sub>L</sub>, OR and image points P<sub>Lij</sub>, P<sub>Rij</sub>, respectively, are considered, the line-of-sight error term is so defined that the values thereof will be large according to distances d<sub>Lij</sub>, d<sub>Rij</sub> from the base image line of sight and the reference image line of sight to the three-dimensional<!-- EPO <DP n="22"> --> coordinates X<sub>ij</sub>. Here, as illustrated in <figref idrefs="f0007">FIG. 7</figref>, the "distances d<sub>Lij</sub>, d<sub>Rij</sub> from the base image line of sight 24 and the reference image line of sight 25 to the three-dimensional coordinates X<sub>ij</sub>" are the lengths of perpendicular lines from the three-dimensional coordinates X<sub>ij</sub> to the base image line of sight 24 and the reference image line of sight 25, respectively. In the embodiment, the sum of squared distances d<sub>Lij</sub> and d<sub>Rij</sub> from the base image line of sight 24 and the reference image line of sight 25 to the three-dimensional coordinates X<sub>ij</sub> is used as the line-of-sight error term.</p><p id="p0057" num="0057">The following will describe the calculation of the line-of-sight error term using mathematical expressions. First, if the image coordinates of the image point P<sub>Lij</sub> are denoted by (u<sub>Lij</sub>, v<sub>Lij</sub>) and the image coordinates of the image point P<sub>Rij</sub> are denoted by (u<sub>Rij</sub>, v<sub>Rij</sub>), the coordinate values of the image points P<sub>Lij</sub> and P<sub>Rij</sub> in the respective camera coordinate systems can be expressed in the following equations.<br/>
<!-- EPO <DP n="23"> -->For left image point PLij: <maths id="math0004" num=""><math display="block"><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Lij</mi></msub><mo>=</mo><mfenced><mtable><mtr><mtd><msub><mi>x</mi><mi mathvariant="italic">Lij</mi></msub></mtd></mtr><mtr><mtd><msub><mi>y</mi><mi mathvariant="italic">Lij</mi></msub></mtd></mtr><mtr><mtd><msub><mi>z</mi><mi mathvariant="italic">Lij</mi></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced><mtable><mtr><mtd><msub><mi>s</mi><mi mathvariant="italic">xL</mi></msub><mo>⁢</mo><mfenced separators=""><msub><mi>u</mi><mi mathvariant="italic">Lij</mi></msub><mo>-</mo><msub><mi>c</mi><mi mathvariant="italic">uL</mi></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>s</mi><mi mathvariant="italic">yL</mi></msub><mo>⁢</mo><mfenced separators=""><msub><mi>v</mi><mi mathvariant="italic">Lij</mi></msub><mo>-</mo><msub><mi>c</mi><mi mathvariant="italic">vL</mi></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>f</mi><mi>L</mi></msub></mtd></mtr></mtable></mfenced></math><img id="ib0004" file="imgb0004.tif" wi="66" he="28" img-content="math" img-format="tif"/></maths><br/>
For right image point PRij: <maths id="math0005" num=""><math display="block"><msub><mmultiscripts><mi mathvariant="bold">P</mi><mprescripts/><none/><mfenced><mi mathvariant="normal">R</mi></mfenced></mmultiscripts><mi mathvariant="italic">Rij</mi></msub><mo>=</mo><mfenced><mtable><mtr><mtd><msubsup><mi>x</mi><mi mathvariant="italic">Rij</mi><mi mathvariant="italic">ʹ</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>y</mi><mi mathvariant="italic">Rij</mi><mi mathvariant="italic">ʹ</mi></msubsup></mtd></mtr><mtr><mtd><msubsup><mi>z</mi><mi mathvariant="italic">Rij</mi><mi mathvariant="italic">ʹ</mi></msubsup></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced><mtable><mtr><mtd><msub><mi>s</mi><mi mathvariant="italic">xR</mi></msub><mo>⁢</mo><mfenced separators=""><msub><mi>u</mi><mi mathvariant="italic">Rij</mi></msub><mo>-</mo><msub><mi>c</mi><mi mathvariant="italic">uR</mi></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>s</mi><mi mathvariant="italic">yR</mi></msub><mo>⁢</mo><mfenced separators=""><msub><mi>v</mi><mi mathvariant="italic">Rij</mi></msub><mo>-</mo><msub><mi>c</mi><mi mathvariant="italic">vR</mi></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>f</mi><mi>R</mi></msub></mtd></mtr></mtable></mfenced></math><img id="ib0005" file="imgb0005.tif" wi="71" he="24" img-content="math" img-format="tif"/></maths> <maths id="math0006" num=""><math display="block"><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Rij</mi></msub><mo>=</mo><mi mathvariant="bold">R</mi><mo>⁢</mo><msup><mo>×</mo><mfenced><mi mathvariant="normal">R</mi></mfenced></msup><mspace width="1em"/><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Rij</mi></msub><mo>+</mo><mi>t</mi></math><img id="ib0006" file="imgb0006.tif" wi="45" he="13" img-content="math" img-format="tif"/></maths></p><p id="p0058" num="0058">Next, line-of-sight unit vectors from the origins O<sub>L</sub> and OR of the respective camera coordinate systems toward the image points P<sub>Lij</sub> and P<sub>Rij</sub> are determined. If the line-of-sight unit vectors are denoted by e<sub>Lij</sub> and e<sub>Rij</sub>, the line-of-sight unit vectors are expressed as follows. <maths id="math0007" num=""><math display="block"><msub><mi mathvariant="bold">e</mi><mi mathvariant="italic">Lij</mi></msub><mo>=</mo><mtable><mtr><mtd><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Lij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>L</mi></msub></mtd></mtr><mtr><mtd><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Lij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>L</mi></msub></mtd></mtr></mtable><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="2em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0007" file="imgb0007.tif" wi="88" he="19" img-content="math" img-format="tif"/></maths> <maths id="math0008" num=""><math display="block"><msub><mi mathvariant="bold">e</mi><mi mathvariant="italic">Rij</mi></msub><mo>=</mo><mtable><mtr><mtd><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Rij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>R</mi></msub></mtd></mtr><mtr><mtd><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Rij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>R</mi></msub></mtd></mtr></mtable><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="2em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0008" file="imgb0008.tif" wi="88" he="19" img-content="math" img-format="tif"/></maths></p><p id="p0059" num="0059">At this point, the line-of-sight unit vector e<sub>Lij</sub> and the line-of-sight unit vector e<sub>Rij</sub> in the base camera coordinate system are constants that can be determined uniquely using the corresponding points extracted from the images and the internal parameters and the external parameters of the cameras.<!-- EPO <DP n="24"> --></p><p id="p0060" num="0060">Here, unknown three-dimensional coordinates X<sub>ij</sub>=(x<sub>ij,</sub> y<sub>ij</sub>, z<sub>ij</sub>) are introduced to determine the distances between X<sub>ij</sub> and the lines of sight. The distance d<sub>Lij</sub> and distance d<sub>Rij</sub> to be determined are expressed using the outer products as follows. <maths id="math0009" num=""><math display="block"><msub><mi>d</mi><mi mathvariant="italic">Lij</mi></msub><mo>=</mo><mfenced open="|" close="|" separators=""><msub><mi mathvariant="bold">e</mi><mi mathvariant="italic">Lij</mi></msub><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mi mathvariant="italic">ij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>L</mi></msub></mfenced></mfenced><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="2em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0009" file="imgb0009.tif" wi="102" he="12" img-content="math" img-format="tif"/></maths> <maths id="math0010" num=""><math display="block"><msub><mi>d</mi><mi mathvariant="italic">Rij</mi></msub><mo>=</mo><mfenced open="|" close="|" separators=""><msub><mi mathvariant="bold">e</mi><mi mathvariant="italic">Rij</mi></msub><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mi mathvariant="italic">ij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>R</mi></msub></mfenced></mfenced><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="2em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0010" file="imgb0010.tif" wi="102" he="14" img-content="math" img-format="tif"/></maths></p><p id="p0061" num="0061">At the end of step S106, the line-of-sight error term corresponding to the distances d<sub>Lij</sub>, d<sub>Rij</sub> between the unknown three-dimensional coordinates X<sub>ij</sub> and the lines of sight is added to the evaluation function. In the embodiment, the sum of squared distances d<sub>Lij</sub> and d<sub>Rij</sub> is added to the evaluation function F. If the line-of-sight error term portion of the evaluation function F at the point of adding the i-th line-of-sight error term is written as F<sub>sight(i)</sub>, F<sub>sight(i)</sub> is given as follows. <maths id="math0011" num=""><math display="block"><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mfenced><mi>i</mi></mfenced></mrow></msub><mo>=</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mo>⁢</mo><mfenced separators=""><mi>i</mi><mo>-</mo><mn>1</mn></mfenced></mrow></msub><mo>+</mo><msup><msub><mi>d</mi><mi mathvariant="italic">Lij</mi></msub><mn>2</mn></msup><mo>+</mo><msup><msub><mi>d</mi><mi mathvariant="italic">Rij</mi></msub><mn>2</mn></msup></math><img id="ib0011" file="imgb0011.tif" wi="77" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0062" num="0062">After the line-of-sight error term is added, the procedure returns to step S102 to determine whether to add mapping, and processing steps S103 to S106 are repeated until the end of the mapping processing is determined. A line-of-sight error term is added each time the processing is repeated. As a result, a line-of-sight error term portion F<sub>sight(N)</sub> of the evaluation function after N<!-- EPO <DP n="25"> --> iterations is as follows. <maths id="math0012" num=""><math display="block"><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mfenced><mi>N</mi></mfenced></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover></mstyle><mfenced separators=""><msup><msub><mi>d</mi><mi mathvariant="italic">Lij</mi></msub><mn>2</mn></msup><mo>+</mo><msup><msub><mi>d</mi><mi mathvariant="italic">Rij</mi></msub><mn>2</mn></msup></mfenced></math><img id="ib0012" file="imgb0012.tif" wi="66" he="24" img-content="math" img-format="tif"/></maths></p><p id="p0063" num="0063">Here, the line-of-sight error term portion F<sub>sight(N)</sub> of the evaluation function is a function having 6N variables (x<sub>11</sub>, y<sub>11</sub>, z<sub>11</sub>, x<sub>12</sub>, y<sub>12</sub>, z<sub>12</sub>, ..., x<sub>N1</sub>, y<sub>N1</sub>, z<sub>N1</sub>, x<sub>N2</sub>, y<sub>N2</sub>, z<sub>N2</sub>). To aid understanding, an example of the mapping data structure at the point of exiting the loop is illustrated in <figref idrefs="f0008">FIG. 8</figref>. When mapping is done between an edge line L<sub>α(i)</sub> of the base image 22b and an edge line R<sub>β(i)</sub> of the reference image 23b at the time of the i-th mapping, a three-dimensional edge line Ei is defined for the combination. In this case, image points P<sub>Li1</sub> and P<sub>Li2</sub> on the edge line L<sub>α(i)</sub> of the base image 22b and image points P<sub>Ri1</sub> and P<sub>Ri2</sub> on the edge line R<sub>β(i)</sub> of the reference image 23b are determined as corresponding points, respectively. Further, the three-dimensional edge line Ei has two endpoints X<sub>i1</sub>=<sup>T</sup>(x<sub>i1</sub>, y<sub>i1</sub>, z<sub>i1</sub>) and X<sub>i2</sub>=<sup>T</sup>(x<sub>i2</sub>, y<sub>i2</sub>, z<sub>i2</sub>). The line-of-sight error term (d<sub>Lij</sub><sup>2</sup>, d<sub>Rij</sub><sup>2</sup>) is defined for the three-dimensional coordinates X<sub>ij</sub> (i=1, 2, ..., N, j=1, 2) of each endpoint.</p><p id="p0064" num="0064">Next, a constraint condition between two edge lines is set and a constraint condition term is determined (steps S107 to S111). First, in step S107, it is determined whether to add a constraint condition. This determination may be made by entering the necessity of<!-- EPO <DP n="26"> --> addition based on the user's discretion, or using an automatic determination algorithm. The subsequent processing steps S108 to S111 are repeated until the addition of constraint conditions is ended.</p><p id="p0065" num="0065">Next, two three-dimensional edge lines used to set the constraint conditions are selected. Note that one three-dimensional edge line corresponds to one combination set in steps S103 and S104. For example, E1 is selected in step S108 and E2 is selected in step S109 from a list of three-dimensional edge lines illustrated in <figref idrefs="f0008">FIG. 8</figref>.</p><p id="p0066" num="0066">Further, a geometric three-dimensional constraint condition between edge lines is selected for the two edge lines selected in steps S108 and S109. In other words, an already known positional relationship between edge lines of the workpiece 10 is set. In the embodiment, among geometric relations between two edge lines, four kinds, namely "two edge lines lie on the same plane," "two edge lines lie on the same straight line," "the directions of two edge lines are parallel," and "the directions of two edge lines are perpendicular" are handled. Among the above four kinds of three-dimensional constraint conditions, one is selected and the procedure proceeds to step S111.</p><p id="p0067" num="0067">Next, a three-dimensional constraint condition term f<sub>const(k)</sub> is added to the evaluation function according to the three-dimensional constraint condition set in step S110 (see <figref idrefs="f0004">FIG. 4B</figref>). First, in step S201, a three-dimensional<!-- EPO <DP n="27"> --> constraint condition term to be added is selected. The following will describe the content of a three-dimensional constraint condition term according to each three-dimensional constraint condition. In the following description of steps S202 to S205, it is assumed that both endpoints of the edge line selected in step S108 are denoted by X<sub>a1</sub>, X<sub>a2</sub> and both endpoints of the edge line selected in step S109 are denoted by X<sub>b1</sub>, X<sub>b2</sub>.</p><p id="p0068" num="0068">When a three-dimensional constraint condition that "two edge lines lie on the same plane (add same plane term: step S202)" is selected, the following processing is performed. As the same plane term, a function for minimizing the value thereof when two edge lines lie on the same plane is used. For example, a condition that four points exist on the same plane can be expressed in the following equation because X<sub>a2</sub> also exists on a plane with three points X<sub>a1</sub>, X<sub>b1</sub>, and X<sub>b2</sub>. <maths id="math0013" num=""><math display="block"><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>•</mo><mfenced open="{" close="}" separators=""><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced></mfenced><mo>=</mo><mn>0</mn></math><img id="ib0013" file="imgb0013.tif" wi="94" he="13" img-content="math" img-format="tif"/></maths></p><p id="p0069" num="0069">Therefore, a same plane term f<sub>plane</sub> as mentioned below is substituted as the k-th constraint condition term f<sub>const(k)</sub>. <maths id="math0014" num=""><math display="block"><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub><mo>=</mo><msub><mi>f</mi><mi mathvariant="italic">plane</mi></msub><mo>=</mo><msub><mi mathvariant="italic">λ</mi><mi>k</mi></msub><mo>‖</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>•</mo><mfenced open="{" close="}" separators=""><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced></mfenced><mo>‖</mo></math><img id="ib0014" file="imgb0014.tif" wi="125" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0070" num="0070">Here, λ<sub>k</sub> is a weighting factor that is a constant used to adjust weighting between different three-dimensional constraint conditions. The weighting factor<!-- EPO <DP n="28"> --> λ<sub>k</sub> may be set by the user individually or using an algorithm for determining weighting according to the length of each edge line.</p><p id="p0071" num="0071">Next, when a three-dimensional constraint condition that "two edge lines lie on the same straight line" is selected, the following processing is performed. As the same straight line term, a function for minimizing the value thereof when two edge lines lie on the same straight line is used. For example, a condition that three points lie on the same straight line can be expressed in the following equation using the outer product. <maths id="math0015" num=""><math display="block"><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>=</mo><mn>0</mn></math><img id="ib0015" file="imgb0015.tif" wi="67" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0072" num="0072">Therefore, a same straight line term f<sub>linear</sub> is substituted as the k-th constraint condition term f<sub>const(k)</sub>. <maths id="math0016" num=""><math display="block"><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub><mo>=</mo><msub><mi>f</mi><mi mathvariant="italic">linear</mi></msub><mo>=</mo><msub><mi mathvariant="italic">λ</mi><mrow><mi>k</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>‖</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>‖</mo><mo>+</mo><msub><mi mathvariant="italic">λ</mi><mrow><mi>k</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>‖</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>‖</mo></math><img id="ib0016" file="imgb0016.tif" wi="152" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0073" num="0073">Here, λ<sub>k1</sub> and λ<sub>k2</sub> are weighting factors that are constants used to adjust weighting between different three-dimensional constraint conditions. Like the same plane term, the weighting factors λ<sub>k1</sub> and λ<sub>k2</sub> may be set by the user individually or using an algorithm for determining weighting according to the length of each edge line.</p><p id="p0074" num="0074">Next, when a three-dimensional constraint condition that "two edge lines are parallel" is selected,<!-- EPO <DP n="29"> --> the following processing is performed. As the parallelism term, a function for minimizing the value thereof when two edge lines are parallel is used. The condition that "two edge lines are parallel" can be expressed in the following mathematical expression using the outer product of vectors. <maths id="math0017" num=""><math display="block"><mo>‖</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>‖</mo><mo>=</mo><mn>0</mn></math><img id="ib0017" file="imgb0017.tif" wi="72" he="18" img-content="math" img-format="tif"/></maths></p><p id="p0075" num="0075">Therefore, a parallelism term f<sub>parallel</sub> as mentioned below is substituted as the k-th constraint condition term f<sub>const(k)</sub>. <maths id="math0018" num=""><math display="block"><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub><mo>=</mo><msub><mi>f</mi><mi mathvariant="italic">parallel</mi></msub><mo>=</mo><msub><mi mathvariant="italic">λ</mi><mi>k</mi></msub><mo>‖</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>b</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>‖</mo></math><img id="ib0018" file="imgb0018.tif" wi="99" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0076" num="0076">Here, λ<sub>k</sub> is a weighting factor that is a constant used to adjust weighting between different three-dimensional constraint conditions. Like the other constraint condition terms, the weighting factor λ<sub>k</sub> may be set by any method.</p><p id="p0077" num="0077">Next, when a three-dimensional constraint condition that "two edge lines are perpendicular" is selected, the following processing is performed. As the perpendicularity term, a function for minimizing the value thereof when two edge lines are perpendicular is used. The condition that two edge lines are perpendicular can be expressed in the following mathematical expression using the inner product of vectors. <maths id="math0019" num=""><math display="block"><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>•</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>=</mo><mn>0</mn></math><img id="ib0019" file="imgb0019.tif" wi="68" he="17" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="30"> --></p><p id="p0078" num="0078">Therefore, a perpendicularity term f<sub>vertical</sub> is substituted as the k-th constraint condition term f<sub>const(k)</sub>. <maths id="math0020" num=""><math display="block"><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub><mo>=</mo><msub><mi>f</mi><mi mathvariant="italic">vertical</mi></msub><mo>=</mo><msub><mi mathvariant="italic">λ</mi><mi>k</mi></msub><mo>‖</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>•</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>-</mo><msub><mi mathvariant="bold">X</mi><mrow><mi>a</mi><mo>⁢</mo><mn>1</mn></mrow></msub></mfenced><mo>‖</mo></math><img id="ib0020" file="imgb0020.tif" wi="107" he="17" img-content="math" img-format="tif"/></maths></p><p id="p0079" num="0079">Here, λ<sub>k</sub> is a weighting factor that is a constant used to adjust weighting between different three-dimensional constraint conditions. Like the other constraint condition terms, the weighting factor λ<sub>k</sub> may be set by any method. In the embodiment, the constraint condition term f<sub>const(k)</sub> is a function having twelve variables (x<sub>a1</sub>, y<sub>a1</sub>, z<sub>a1</sub>, x<sub>a2</sub>, y<sub>a2</sub>, z<sub>a2</sub>, x<sub>b1</sub>, y<sub>b1</sub>, z<sub>b1</sub>, x<sub>b2</sub>, y<sub>b2</sub>, z<sub>b2</sub>).</p><p id="p0080" num="0080">After the constraint condition term f<sub>const(k)</sub> is added in step S111, the procedure returns to step S107 to determine whether further to add a three-dimensional constraint condition. The processing steps S108 to S111 are repeated until the addition of constraint conditions is ended. Any number of three-dimensional constraint condition terms f<sub>const(k)</sub> can be set within a consistent range. For example, two three-dimensional constraint conditions "on the same plane" and "perpendicular" can be set for certain two edge lines. When M three-dimensional constraint conditions are set through the loop of steps S107 to S111, the constraint condition term portion F<sub>const(M)</sub> of the evaluation function F is as follows.<!-- EPO <DP n="31"> --> <maths id="math0021" num=""><math display="block"><msub><mi>F</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>M</mi></mfenced></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub></math><img id="ib0021" file="imgb0021.tif" wi="56" he="26" img-content="math" img-format="tif"/></maths></p><p id="p0081" num="0081">F<sub>const(M)</sub> is a function having 6N variables (x<sub>11</sub>, y<sub>11</sub>, z<sub>11</sub>, x<sub>12</sub>, y<sub>12</sub>, z<sub>12</sub>, ..., x<sub>N1</sub>, y<sub>N1</sub>, z<sub>N1</sub>, x<sub>N2</sub>, y<sub>N2</sub>, z<sub>N2</sub>) at a maximum. Note that the actual number of variables included in F<sub>const(M)</sub> corresponds to the number of edge lines selected in steps S108 and S109.</p><p id="p0082" num="0082">Next, the evaluation function F is set (evaluation function setting process) from the line-of-sight error term portion F<sub>sight(N)</sub> added in steps S102 to S106 and the constraint condition term portion F<sub>const(M)</sub> added in the processing steps S107 to S111. As mentioned above, the evaluation function F is expressed as the sum of the line-of-sight error term portion F<sub>sight(N)</sub> and the constraint condition term portion F<sub>const(M)</sub>, i.e., expressed as follows. <maths id="math0022" num=""><math display="block"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mfenced separators=""><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">12</mn></msub><mo>…</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub></mfenced></mtd></mtr><mtr><mtd><mspace width="3em"/><mo>=</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mfenced><mi>N</mi></mfenced></mrow></msub><mo>+</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>M</mi></mfenced></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover></mstyle><mfenced separators=""><msup><msub><mi>d</mi><mi mathvariant="italic">Lij</mi></msub><mn>2</mn></msup><mo>+</mo><msup><msub><mi>d</mi><mi mathvariant="italic">Rij</mi></msub><mn>2</mn></msup></mfenced><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub></mtd></mtr></mtable></math><img id="ib0022" file="imgb0022.tif" wi="141" he="30" img-content="math" img-format="tif"/></maths></p><p id="p0083" num="0083">Next, the three-dimensional coordinates X<sub>ij</sub> (i=1, 2, ..., j=1, 2) is determined by minimizing the evaluation function F (three-dimensional coordinate measurement process), i.e., the following set of variables (x<sub>11</sub>, y<sub>11</sub>, z<sub>11</sub>, x<sub>12</sub>, y<sub>12</sub>, z<sub>12</sub>, ..., x<sub>N1</sub>, y<sub>N1</sub>, z<sub>N1</sub>, x<sub>N2</sub>, y<sub>N2</sub>, z<sub>N2</sub>) is determined for the evaluation function F.<!-- EPO <DP n="32"> --> <maths id="math0023" num=""><math display="block"><mi>F</mi><mfenced separators=""><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">12</mn></msub><mo>…</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub></mfenced><mo>⇒</mo><mi>min</mi></math><img id="ib0023" file="imgb0023.tif" wi="136" he="17" img-content="math" img-format="tif"/></maths></p><p id="p0084" num="0084">This calculation may be made by using a known multivariable optimization method. For example, the Nelder-Mead downhill simplex method as one of nonlinear search techniques can be used.</p><p id="p0085" num="0085">The coordinate values (x<sub>11</sub>, y<sub>11</sub>, z<sub>11</sub>, x<sub>12</sub>, y<sub>12</sub>, z<sub>12</sub>, ..., x<sub>N1</sub>, y<sub>N1</sub>, z<sub>N1</sub>, x<sub>N2</sub>, y<sub>N2</sub>, z<sub>N2</sub>) determined by the above processing form a shape recreated in consideration of the three-dimensional constraint conditions between edge lines. In addition, according to the embodiment, any three-dimensional constraint condition can be applied between a plurality of edge lines stereoscopically arranged. Therefore, a highly accurate shape can be obtained in comparison with the conventional techniques.</p><p id="p0086" num="0086">&lt;Second Embodiment&gt;</p><p id="p0087" num="0087">Next, a robot system 1A according to a second embodiment of the present invention will be described with reference to <figref idrefs="f0010 f0011 f0012">FIG. 10 to FIG. 12</figref> while using <figref idrefs="f0001">FIG. 1</figref> and <figref idrefs="f0002">FIG. 2</figref>. In the second embodiment, a method of measuring the three-dimensional coordinates of a workpiece by the camera control unit 5A is different from that in the first embodiment. Therefore, in the second embodiment, an emphasis is placed on a point different from the first embodiment, i.e. on the method of measuring the three-dimensional position by the camera control unit, and the<!-- EPO <DP n="33"> --> same components as those in the first embodiment are given the same reference numerals to omit the description.</p><p id="p0088" num="0088">First, a schematic structure of the robot system 1A will be described with reference to <figref idrefs="f0010">FIG. 10</figref> while using <figref idrefs="f0001">FIG. 1</figref> and <figref idrefs="f0002">FIG. 2</figref>. <figref idrefs="f0010">FIG. 10</figref> is a block diagram illustrating the configuration of a camera control unit 5A according to the second embodiment.</p><p id="p0089" num="0089">As illustrated in <figref idrefs="f0001">FIG. 1</figref> and <figref idrefs="f0002">FIG. 2</figref>, the robot system 1A according to the second embodiment includes the robot 2, the mounting base 3, and a control device 4A for controlling the robot 2. The control device 4A includes a calculating device 40A and the storing device 41. The calculating device 40A includes a CPU 40aA, the image processing device 40b, and the sound processing device 40c. The CPU 40aA includes the camera control unit 5A and the robot control unit 6.</p><p id="p0090" num="0090">As illustrated in <figref idrefs="f0010">FIG. 10</figref>, the camera control unit 5A includes the first image edge line extraction unit 51, the second image edge line extraction unit 52, the corresponding point calculation unit 53, an error calculation unit 57, the evaluation function setting unit 55, and the three-dimensional coordinate measurement unit 56. The error calculation unit 57 sets provisional three-dimensional coordinates to calculate a line-of-sight error at an endpoint of a first edge line with respect to the provisional three-dimensional coordinates. The error calculation unit 57 also calculates a viewing plane error<!-- EPO <DP n="34"> --> at an endpoint of a second edge line with respect to the provisional three-dimensional coordinates.</p><p id="p0091" num="0091">Next, a method of measuring the three-dimensional position of a workpiece by the camera control unit 5A when the hand 21 in the robot system 1A structured as mentioned above grips the workpiece will be described along flowcharts illustrated in <figref idrefs="f0011">FIG. 11A and 11B</figref> while referring to <figref idrefs="f0012">FIG. 12</figref>. <figref idrefs="f0011">FIG. 11A and 11B</figref> are flowcharts illustrating a measurement of three-dimensional coordinates according to the second embodiment. <figref idrefs="f0012">FIG. 12</figref> is a schematic diagram for describing a line-of-sight error and a viewing plane error according to the second embodiment.</p><p id="p0092" num="0092">As a basic flow, edge lines are first extracted from a base image 22b and a reference image 23b imaged with the base camera 22 and the reference camera 23 from a plurality of different view points (step S301). Next, it is determined whether to repeat the addition of mapping (step S302). Then, a first edge line is selected from the plurality of edge lines of the base image 22b, and a corresponding second edge line is selected from the reference image 23b (mapping) (S303 and S304). Then, a line-of-sight error term on the first edge line of the base image 22b is calculated (S305). Similarly, a viewing plane error term on the reference image 23b is calculated (S306). Further, a geometric three-dimensional constraint condition is set between mapped two edge lines to<!-- EPO <DP n="35"> --> calculate a three-dimensional constraint condition term (S307 to S311). Finally, an optimization problem is solved so as to minimize the value of an evaluation function including the line-of-sight error term, the viewing plane error term, and the three-dimensional constraint condition term to determine the three-dimensional coordinates (S312).</p><p id="p0093" num="0093">In the second embodiment, the processing steps S301 to S304 until the selection of the second edge line mapped after the selection of the first edge line are the same as those in the first embodiment. The processing steps S307 to S311 to add a three-dimensional constraint condition are also the same as those in the first embodiment. Therefore, the detailed description thereof will be omitted. The following will describe points different from the first embodiment, i.e. processing for adding a line-of-sight error term on an endpoint of the first edge line and adding a viewing plane error term on the second edge line (steps S305 and S306), and a calculation of the three-dimensional coordinates (S312).</p><p id="p0094" num="0094">First, a line-of-sight error term on an endpoint of the first edge line selected in step S303 is determined and added to the evaluation function. Although the way to think about the line-of-sight error term is the same as that in the first embodiment, only the line-of-sight error term on the base image line of sight 24 is added. The line-of-sight error term d<sub>Lij</sub><sup>2</sup> on the base image line of<!-- EPO <DP n="36"> --> sight 24 is expressed as follows. <maths id="math0024" num=""><math display="block"><msup><msub><mi>d</mi><mi mathvariant="italic">Lij</mi></msub><mn>2</mn></msup><mo>=</mo><msup><mfenced open="|" close="|" separators=""><mfrac><mrow><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Lij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>L</mi></msub></mrow><mfenced open="|" close="|" separators=""><msub><mi mathvariant="bold">P</mi><mi mathvariant="italic">Lij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>L</mi></msub></mfenced></mfrac><mo>×</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mi mathvariant="italic">ij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>L</mi></msub></mfenced></mfenced><mn>2</mn></msup><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="1em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0024" file="imgb0024.tif" wi="109" he="20" img-content="math" img-format="tif"/></maths></p><p id="p0095" num="0095">Next, a viewing plane error term is determined from both endpoints of the second edge line selected in step S304, and added to the evaluation function. The viewing plane error term is distance between unknown three-dimensional coordinates X and a viewing plane created by a point group on the selected second edge line. A normal vector n<sub>Ri</sub> for defining the viewing plane is determined as follows. <maths id="math0025" num=""><math display="block"><msub><mi mathvariant="bold">n</mi><mi mathvariant="italic">Ri</mi></msub><mo>=</mo><msub><mi mathvariant="bold">e</mi><mrow><mi mathvariant="italic">Ri</mi><mo>⁢</mo><mn>2</mn></mrow></msub><mo>×</mo><msub><mi mathvariant="bold">e</mi><mrow><mi mathvariant="italic">Ri</mi><mo>⁢</mo><mn>1</mn></mrow></msub></math><img id="ib0025" file="imgb0025.tif" wi="39" he="14" img-content="math" img-format="tif"/></maths></p><p id="p0096" num="0096">Distance d between the reference image viewing plane and the unknown point X is the magnitude of a normal direction component of a vector directed from the origin 0 to X. Therefore, the viewing plane error term d'<sub>Rij</sub> can be expressed as follows. <maths id="math0026" num=""><math display="block"><msubsup><mi>d</mi><mi mathvariant="italic">Rij</mi><mi>ʹ</mi></msubsup><mo>=</mo><mfenced open="|" close="|" separators=""><msub><mi mathvariant="bold">n</mi><mi mathvariant="italic">Ri</mi></msub><mo>•</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mi mathvariant="italic">ij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>R</mi></msub></mfenced></mfenced><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="1em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0026" file="imgb0026.tif" wi="93" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0097" num="0097">Assuming that the evaluation is performed using the squared sum, the following term is added to the evaluation function. <maths id="math0027" num=""><math display="block"><msup><msubsup><mi>d</mi><mi mathvariant="italic">Rij</mi><mi>ʹ</mi></msubsup><mn>2</mn></msup><mo>=</mo><msup><mfenced open="{" close="}" separators=""><msub><mi mathvariant="bold">n</mi><mi mathvariant="italic">Ri</mi></msub><mo>•</mo><mfenced separators=""><msub><mi mathvariant="bold">X</mi><mi mathvariant="italic">ij</mi></msub><mo>-</mo><msub><mi mathvariant="bold">O</mi><mi>R</mi></msub></mfenced></mfenced><mn>2</mn></msup><mspace width="2em"/><mfenced separators=""><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>N</mi><mo>,</mo><mspace width="1em"/><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn></mfenced></math><img id="ib0027" file="imgb0027.tif" wi="98" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0098" num="0098">After the error term is added, the procedure<!-- EPO <DP n="37"> --> returns to step S302 to determine whether to end the addition of mapping, and the processing steps S302 to S306 are repeated until the end of addition is determined. Each time the processing is repeated, the line-of-sight error term d<sub>Lij</sub><sup>2</sup> and the viewing plane error term d'<sub>Rij</sub><sup>2</sup> are added. As a result, an error term portion F<sub>sight(N)</sub> of the evaluation function F after N iterations is as follows. <maths id="math0028" num=""><math display="block"><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mfenced><mi>N</mi></mfenced></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mspace width="1em"/><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover></mstyle><mfenced separators=""><msup><msub><mi>d</mi><mi mathvariant="italic">Lij</mi></msub><mn>2</mn></msup><mo>+</mo><msup><msubsup><mi>d</mi><mi mathvariant="italic">Rij</mi><mi>ʹ</mi></msubsup><mn>2</mn></msup></mfenced></math><img id="ib0028" file="imgb0028.tif" wi="58" he="21" img-content="math" img-format="tif"/></maths></p><p id="p0099" num="0099">After the end of addition is determined in step S302, the processing for adding a constraint condition is performed in steps S307 to S311. As mentioned above, since the processing for adding a three-dimensional constraint condition is the same as the processing steps S107 to S111 in the first embodiment, the description thereof will be omitted. Even when the viewing plane error term is used instead of the line-of-sight error term like in this embodiment, the same calculation formula can be applied as the three-dimensional constraint condition term.</p><p id="p0100" num="0100">Next, an evaluation function F is set by integrating the line-of-sight error term and the viewing plane error term added in steps S302 to S306, and the three-dimensional constraint condition term added in the processing steps S307 to S311 (evaluation function setting process). As mentioned above, the evaluation function F<!-- EPO <DP n="38"> --> is expressed as the sum of the error term portion F<sub>sight(N)</sub> and the constraint condition term portion F<sub>const(M)</sub>, i.e., expressed as follows. <maths id="math0029" num=""><math display="block"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mfenced separators=""><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">12</mn></msub><mo>…</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub></mfenced></mtd></mtr><mtr><mtd><mspace width="3em"/><mo>=</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">sight</mi><mfenced><mi>N</mi></mfenced></mrow></msub><mo>+</mo><msub><mi>F</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>M</mi></mfenced></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover></mstyle><mfenced separators=""><msup><msubsup><mi>d</mi><mi mathvariant="italic">Lij</mi><mi>ʹ</mi></msubsup><mn>2</mn></msup><mo>+</mo><msup><msub><mi>d</mi><mi mathvariant="italic">Rij</mi></msub><mn>2</mn></msup></mfenced><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover></mstyle><msub><mi>f</mi><mrow><mi mathvariant="italic">const</mi><mfenced><mi>k</mi></mfenced></mrow></msub></mtd></mtr></mtable></math><img id="ib0029" file="imgb0029.tif" wi="137" he="30" img-content="math" img-format="tif"/></maths></p><p id="p0101" num="0101">Next, the evaluation function F is minimized to determine the three-dimensional coordinates X<sub>ij</sub> (i=1, 2, ..., N, j=1, 2) (three-dimensional coordinate measurement process). That is, the following set of variables (x<sub>11</sub>, y<sub>11</sub>, z<sub>11</sub>, x<sub>12</sub>, y<sub>12</sub>, z<sub>12</sub>, ..., x<sub>N1</sub>, y<sub>N1</sub>, z<sub>N1</sub>, x<sub>N2</sub>, y<sub>N2</sub>, z<sub>N2</sub>) is determined for the evaluation function F. <maths id="math0030" num=""><math display="block"><mi>F</mi><mfenced separators=""><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">11</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mn mathvariant="normal">12</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mn mathvariant="normal">12</mn></msub><mo>…</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">1</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">x</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">y</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">z</mi><mrow><mi mathvariant="normal">N</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub></mfenced><mo>⇒</mo><mi>min</mi></math><img id="ib0030" file="imgb0030.tif" wi="141" he="19" img-content="math" img-format="tif"/></maths></p><p id="p0102" num="0102">Like in the first embodiment, this calculation for determining the set of variables can be made by using a known multivariable optimization method.</p><p id="p0103" num="0103">As described above, according to the method of measuring a three-dimensional position of this embodiment, a three-dimensional shape measurement in which a plurality of known stereoscopic constraint conditions are reflected together for the positional relationship between lines obtained by approximating edge lines of a workpiece can be performed. Therefore, data on a highly accurate three-dimensional shape can be obtained in comparison with the<!-- EPO <DP n="39"> --> conventional techniques.</p><p id="p0104" num="0104">Further, according to the method of measuring a three-dimensional position of the embodiment, the need to perform processing for "uniquely determining corresponding points using the epipolar constraints" is eliminated to calculate the three-dimensional coordinates. This eliminates the need to determine corresponding points on the reference image 23b ahead with respect to the first edge line extracted from the base image 22b. When the epipolar constraints are not satisfied due to an error in camera parameter or an error in extracting an edge line, the corresponding points become inaccurate in the case of using the method in which the corresponding points are determined. In this case, use of the method in the embodiment can reduce this influence.</p><p id="p0105" num="0105">While the embodiments of the present invention have been described, the present invention is not limited to the aforementioned embodiments. Further, the advantageous effects of the embodiments of the present invention are merely listed as most preferred effects derived from the present invention, and the effects of the present invention are not limited to those described in the embodiments of the present invention.</p><p id="p0106" num="0106">For example, in the embodiments, the description has been made by using two cameras, i.e. the base camera and the reference camera for the sake of simplicity, but the present invention is not limited thereto. For example,<!-- EPO <DP n="40"> --> even when the number of cameras is three or more, the method of the present invention can be applied by adding a viewing plane error term with respect to each reference camera. Further, if the view point of the workpiece varies, images obtained by capturing the workpiece plural times while moving the same camera can also be used.</p><p id="p0107" num="0107">Further, for example, the three-dimensional coordinates are determined by minimizing the value of the evaluation function in the embodiments, but the present invention is not limited thereto. For example, the three-dimensional coordinates may be determined by maximizing the value of the evaluation function.</p><p id="p0108" num="0108">Other embodiments</p><p id="p0109" num="0109">Embodiments of the present invention can also be realized by a computer of a system or apparatus that reads out and executes computer executable instructions recorded on a storage medium (e.g., non-transitory computer-readable storage medium) to perform the functions of one or more of the above-described embodiment(s) of the present invention, and by a method performed by the computer of the system or apparatus by, for example, reading out and executing the computer executable instructions from the storage medium to perform the functions of one or more of the above-described embodiment(s). The computer may comprise one or more of a central processing unit (CPU), micro processing unit (MPU), or other circuitry, and may include a network of separate<!-- EPO <DP n="41"> --> computers or separate computer processors. The computer executable instructions may be provided to the computer, for example, from a network or the storage medium. The storage medium may include, for example, one or more of a hard disk, a random-access memory (RAM), a read only memory (ROM), a storage of distributed computing systems, an optical disk (such as a compact disc (CD), digital versatile disc (DVD), or Blu-ray Disc (BD)™), a flash memory device, a memory card, and the like.</p><p id="p0110" num="0110">According to the present invention, since the optimization calculation of an evaluation function in which multiple three-dimensional constraint conditions to be met together are set is performed, a highly accurate three-dimensional shape can be measured.</p><p id="p0111" num="0111">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.<br/>
The three-dimensional measurement apparatus includes: a base camera for capturing a workpiece to acquire a first image; a reference camera for capturing the workpiece from a view point different from the base camera to acquire a second image; and a camera control unit for extracting<!-- EPO <DP n="42"> --> multiple edge lines from the first and second images, calculating line-of-sight errors with respect to provisional three-dimensional coordinates, wherein the line-of-sight errors are calculated at an endpoint of a first edge line selected from the multiple edge lines of the first image and an endpoint of a second edge line extracted from the multiple edge lines of the second image to correspond to the first edge line, setting an evaluation function from the line-of-sight errors, and a three-dimensional constraint condition set based on a shape of the workpiece, and making an optimization calculation of the evaluation function to measure the three-dimensional coordinates of the workpiece.</p></description><claims mxw-id="PCLM56982209" lang="EN" load-source="patent-office"><!-- EPO <DP n="43"> --><claim id="c-en-0001" num="0001"><claim-text>A three-dimensional measurement apparatus comprising:
<claim-text>a first camera arranged to capture a workpiece to acquire a first image;</claim-text>
<claim-text>a second camera arranged to capture the workpiece from a view point different from that of the first camera to acquire a second image; and</claim-text>
<claim-text>a control unit configured to<br/>
extract a plurality of edge lines of the workpiece respectively from the first image and the second image,<br/>
calculate each of line-of-sight errors with respect to provisional three-dimensional coordinates, wherein the line-of-sight errors are calculated from an endpoint of a first edge line selected from the plurality of edge lines of the first image and an endpoint of a second edge line extracted from the plurality of edge lines of the second image so as to correspond to the first edge line,<br/>
set an evaluation function from the respective line-of-sight errors and a three-dimensional constraint condition set based on a three-dimensional shape of the workpiece, and<br/>
make an optimization calculation based on the evaluation function to determine three-dimensional coordinates of the workpiece.</claim-text><!-- EPO <DP n="44"> --></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>A three-dimensional measurement apparatus comprising:
<claim-text>a first camera arranged to capture a workpiece to acquire a first image;</claim-text>
<claim-text>a second camera arranged to capture the workpiece from a view point different from that of the first camera to acquire a second image; and</claim-text>
<claim-text>a control unit configured to<br/>
extract a plurality of edge lines of the workpiece respectively from the first image and the second image,<br/>
calculate a line-of-sight error at an endpoint of a first edge line with respect to provisional three-dimensional coordinates, wherein the first edge line is selected from the plurality of edge lines of the first image,<br/>
calculate a viewing plane error in a second edge line with respect to the provisional three-dimensional coordinates, wherein the second edge line is extracted from the plurality of edge lines of the second image so as to correspond to the first edge line,<br/>
set an evaluation function from the line-of-sight error at the endpoint of the first edge line, the viewing plane error in the second edge line, and a three-dimensional constraint condition set based on a three-dimensional shape of the workpiece, and<br/>
<!-- EPO <DP n="45"> -->make an optimization calculation based on the evaluation function to determine three-dimensional coordinates of the workpiece.</claim-text></claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The three-dimensional measurement apparatus according to claim 1, wherein the three-dimensional constraint condition is obtained by setting one or more of conditions that two edge lines of the plurality of edge lines of the first image or the second image lie on a same plane, they lie on a same straight line, they are perpendicular to each other, and they are parallel to each other.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>A robot system comprising:
<claim-text>the three-dimensional measurement apparatus for three-dimensional measurement of a workpiece, according to any one of claims 1 to 3; and</claim-text>
<claim-text>a robot arm arranged to grip the workpiece based on a three-dimensional position of the workpiece measured by the three-dimensional measurement apparatus.</claim-text></claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>A three-dimensional measurement method comprising:
<claim-text>extracting a plurality of edge lines of a workpiece from a first image acquired by capturing the workpiece with a first camera;</claim-text>
<claim-text>extracting a plurality of edge lines of the workpiece from a second image acquired by capturing the workpiece with a second camera;</claim-text>
<claim-text>calculating an endpoint of a second edge line corresponding to an endpoint of a first edge line, wherein<!-- EPO <DP n="46"> --> the second edge line is extracted from the plurality of edge lines of the second image so as to correspond to the first edge line selected from the plurality of edge lines of the first image;</claim-text>
<claim-text>calculating each of line-of-sight errors at the endpoint of the first edge line and the endpoint of the second edge line with respect to provisional three-dimensional coordinates;</claim-text>
<claim-text>setting an evaluation function from the line-of-sight errors respectively set at the endpoint of the first edge line and the endpoint of the second edge line, and a three-dimensional constraint condition set based on a three-dimensional shape of the workpiece; and</claim-text>
<claim-text>making an optimization calculation to minimize or maximize a value of the evaluation function to determine three-dimensional coordinates of the workpiece.</claim-text></claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>A three-dimensional measurement method comprising:
<claim-text>extracting a plurality of edge lines of a workpiece from a first image acquired by capturing the workpiece with a first camera;</claim-text>
<claim-text>extracting a plurality of edge lines of the workpiece from a second image acquired by capturing the workpiece with a second camera;</claim-text>
<claim-text>calculating a line-of-sight error at an endpoint of a first edge line with respect to provisional three-dimensional coordinates, wherein the first edge line is arbitrarily selected from the plurality of edge lines of<!-- EPO <DP n="47"> --> the first image, and a viewing plane error in a second edge line with respect to the provisional three-dimensional coordinates, wherein the second edge line is extracted from the plurality of edge lines of the second image so as to correspond to the first edge line;</claim-text>
<claim-text>setting an evaluation function from the line-of-sight error at the endpoint of the first edge line, the viewing plane error in the second edge line, and a three-dimensional constraint condition set based on a three-dimensional shape of the workpiece; and</claim-text>
<claim-text>making an optimization calculation to minimize or maximize a value of the evaluation function to determine three-dimensional coordinates of the workpiece.</claim-text></claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>A three-dimensional measurement program for causing a computer to execute the method according to claim 5 or 6.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>A computer-readable recording medium on which the three-dimensional measurement program according to claim 7 is recorded.</claim-text></claim></claims><drawings mxw-id="PDW16670597" load-source="patent-office"><!-- EPO <DP n="48"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="161" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="49"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="217" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="50"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="165" he="205" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="51"> --><figure id="f0004" num="4A,4B"><img id="if0004" file="imgf0004.tif" wi="165" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="52"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="165" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="53"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="165" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="54"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="165" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="55"> --><figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="165" he="220" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="56"> --><figure id="f0009" num="9"><img id="if0009" file="imgf0009.tif" wi="165" he="200" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="57"> --><figure id="f0010" num="10"><img id="if0010" file="imgf0010.tif" wi="165" he="200" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="58"> --><figure id="f0011" num="11A,11B"><img id="if0011" file="imgf0011.tif" wi="165" he="212" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="59"> --><figure id="f0012" num="12"><img id="if0012" file="imgf0012.tif" wi="165" he="212" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="158" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="161" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
