<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681718-A1" country="EP" doc-number="2681718" kind="A1" date="20140108" family-id="46753067" file-reference-id="298922" date-produced="20180822" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586088" ucid="EP-2681718-A1"><document-id><country>EP</country><doc-number>2681718</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12707969-A" is-representative="NO"><document-id mxw-id="PAPP154848280" load-source="docdb" format="epo"><country>EP</country><doc-number>12707969</doc-number><kind>A</kind><date>20120302</date><lang>EN</lang></document-id><document-id mxw-id="PAPP220440006" load-source="docdb" format="original"><country>EP</country><doc-number>12707969.7</doc-number><date>20120302</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140557451" ucid="US-201113310529-A" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201113310529</doc-number><kind>A</kind><date>20111202</date></document-id></priority-claim><priority-claim mxw-id="PPC140553261" ucid="US-201161449500-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201161449500</doc-number><kind>P</kind><date>20110304</date></document-id></priority-claim><priority-claim mxw-id="PPC140550832" ucid="US-2012027426-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2012027426</doc-number><kind>W</kind><date>20120302</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989317839" load-source="docdb">G06K   9/62        20060101ALI20120925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989319205" load-source="docdb">G06T  19/00        20110101ALI20120925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989320181" load-source="docdb">G06K   9/00        20060101ALI20120925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989321050" load-source="docdb">G06K   9/46        20060101ALI20120925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989321352" load-source="docdb">G06T   7/20        20060101AFI20120925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989321652" load-source="docdb">G06K   9/32        20060101ALI20120925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989322434" load-source="docdb">G06Q  30/00        20120101ALI20120925BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861393246" load-source="docdb" scheme="CPC">G06T   7/248       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989622082" load-source="docdb" scheme="CPC">G06T2207/10016     20130101 LA20130806BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989623015" load-source="docdb" scheme="CPC">G06K   9/32        20130101 FI20130321BHEP        </classification-cpc><classification-cpc mxw-id="PCL1989638975" load-source="docdb" scheme="CPC">G06K   9/00671     20130101 LI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL2070350577" load-source="docdb" scheme="CPC">G06K   9/2063      20130101 LI20140415BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132362897" lang="DE" load-source="patent-office">DYNAMISCHE SCHABLONENVERFOLGUNG</invention-title><invention-title mxw-id="PT132362898" lang="EN" load-source="patent-office">DYNAMIC TEMPLATE TRACKING</invention-title><invention-title mxw-id="PT132362899" lang="FR" load-source="patent-office">SUIVI DE MODÈLE DYNAMIQUE</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR919511149" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>QUALCOMM INC</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR919523663" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>QUALCOMM INCORPORATED</last-name></addressbook></applicant><applicant mxw-id="PPAR919012435" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Qualcomm Incorporated</last-name><iid>101331406</iid><address><street>5775 Morehouse Drive</street><city>San Diego, CA 92121-1714</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919545734" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SWEET III CHARLES WHEELER</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919544004" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SWEET III, CHARLES WHEELER</last-name></addressbook></inventor><inventor mxw-id="PPAR919018476" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SWEET III, CHARLES WHEELER</last-name><address><street>5775 Morehouse Drive</street><city>San Diego, California 92121</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919012810" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Schmidbauer, Andreas Konrad</last-name><iid>101222806</iid><address><street>Wagner &amp; Geyer Patent- und Rechtsanwälte Gewürzmühlstrasse 5</street><city>80538 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2012027426-W"><document-id><country>US</country><doc-number>2012027426</doc-number><kind>W</kind><date>20120302</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012122008-A1"><document-id><country>WO</country><doc-number>2012122008</doc-number><kind>A1</kind><date>20120913</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549833980" load-source="docdb">AL</country><country mxw-id="DS549763861" load-source="docdb">AT</country><country mxw-id="DS549833994" load-source="docdb">BE</country><country mxw-id="DS549832820" load-source="docdb">BG</country><country mxw-id="DS549756789" load-source="docdb">CH</country><country mxw-id="DS549844695" load-source="docdb">CY</country><country mxw-id="DS549763862" load-source="docdb">CZ</country><country mxw-id="DS549831061" load-source="docdb">DE</country><country mxw-id="DS549833995" load-source="docdb">DK</country><country mxw-id="DS549844696" load-source="docdb">EE</country><country mxw-id="DS549756124" load-source="docdb">ES</country><country mxw-id="DS549832821" load-source="docdb">FI</country><country mxw-id="DS549832826" load-source="docdb">FR</country><country mxw-id="DS549833996" load-source="docdb">GB</country><country mxw-id="DS549833997" load-source="docdb">GR</country><country mxw-id="DS549834002" load-source="docdb">HR</country><country mxw-id="DS549844697" load-source="docdb">HU</country><country mxw-id="DS549756790" load-source="docdb">IE</country><country mxw-id="DS549834003" load-source="docdb">IS</country><country mxw-id="DS549832827" load-source="docdb">IT</country><country mxw-id="DS549844702" load-source="docdb">LI</country><country mxw-id="DS549831066" load-source="docdb">LT</country><country mxw-id="DS549763863" load-source="docdb">LU</country><country mxw-id="DS549831067" load-source="docdb">LV</country><country mxw-id="DS549831068" load-source="docdb">MC</country><country mxw-id="DS549913224" load-source="docdb">MK</country><country mxw-id="DS549913225" load-source="docdb">MT</country><country mxw-id="DS549756137" load-source="docdb">NL</country><country mxw-id="DS549763864" load-source="docdb">NO</country><country mxw-id="DS549756138" load-source="docdb">PL</country><country mxw-id="DS549763873" load-source="docdb">PT</country><country mxw-id="DS549756139" load-source="docdb">RO</country><country mxw-id="DS549844703" load-source="docdb">RS</country><country mxw-id="DS549756140" load-source="docdb">SE</country><country mxw-id="DS549844704" load-source="docdb">SI</country><country mxw-id="DS549913226" load-source="docdb">SK</country><country mxw-id="DS549913227" load-source="docdb">SM</country><country mxw-id="DS549756791" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA100066122" ref-ucid="WO-2012122008-A1" lang="EN" load-source="patent-office"><p num="0000">Various arrangements for tracking a target within a series of images is presented. The target may be detected within a first image at least partially based on a correspondence between the target and a stored reference template. A tracking template may be created for tracking the target using the first image. The target may be located within a second image using the tracking template.</p></abstract><abstract mxw-id="PA100335230" ref-ucid="WO-2012122008-A1" lang="EN" source="national office" load-source="docdb"><p>Various arrangements for tracking a target within a series of images is presented. The target may be detected within a first image at least partially based on a correspondence between the target and a stored reference template. A tracking template may be created for tracking the target using the first image. The target may be located within a second image using the tracking template.</p></abstract><abstract mxw-id="PA100066123" ref-ucid="WO-2012122008-A1" lang="FR" load-source="patent-office"><p num="0000">L'invention concerne divers agencements pour suivre une cible dans une série d'images. La cible peut être détectée dans une première image au moins partiellement sur la base d'une correspondance entre la cible et un modèle de référence mémorisé. Un modèle de suivi peut être créé pour suivre la cible en utilisant la première image. La cible peut être localisée dans une seconde image en utilisant le modèle de suivi.</p></abstract><abstract mxw-id="PA100335231" ref-ucid="WO-2012122008-A1" lang="FR" source="national office" load-source="docdb"><p>L'invention concerne divers agencements pour suivre une cible dans une série d'images. La cible peut être détectée dans une première image au moins partiellement sur la base d'une correspondance entre la cible et un modèle de référence mémorisé. Un modèle de suivi peut être créé pour suivre la cible en utilisant la première image. La cible peut être localisée dans une seconde image en utilisant le modèle de suivi.</p></abstract><description mxw-id="PDES51372717" ref-ucid="WO-2012122008-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> Dynamic Template Tracking </p><p id="p0002" num="0002">CROSS REFERENCES </p><p id="p0003" num="0003">[0001] This Patent Cooperation Treaty application claims priority to U.S. Provisional Application No. 61/449,500, filed March 4, 201 1, entitled "Dynamic Template Tracking," attorney docket number 11 1018P1 and U.S. Non-Provisional Application No. 13/310,529, filed December 2, 2011, entitled "Dynamic Template Tracking," attorney docket number 11 1018. These applications are incorporated by reference in their entirety for all purposes. </p><p id="p0004" num="0004">BACKGROUND </p><p id="p0005" num="0005"> [0002] Detection and tracking of a target within a set of images may be useful for various purposes, such as augmented reality. For example, detecting and tracking a target may be used to augment a real-world situation with additional information. Images captured by an image capture device may be augmented with information that provides a user with additional sensory input, such as graphics or sound related to entities in the field of view of the image capture device. Some forms of augmented reality involve a device having a camera being pointed at an entity that a user wishes to augment with additional sensory input. Performing this form of augmented reality can involve at least three stages: first, detecting the entity within an image; second, tracking the entity in subsequent images; and third, augmenting the image with additional information. Following detecting the target within an image, tracking the target as its position shifts and/or the camera's perspective of the target changes in subsequent images may be unreliable and/or processor-intensive. </p><p id="p0006" num="0006">SUMMARY </p><p id="p0007" num="0007">[0003] Systems, methods, devices, computer programs, and apparatuses are described for tracking a target within a series of images. An example of method for tracking a target within a series of images may be presented. The method may include capturing, by a mobile device, a first image. Within the first image may be the target. The target may be an entity to be tracked. The method may include locating, by the mobile device, the target within a second image using a tracking template created using the first image. </p><p id="p0008" num="0008"> [0004] Embodiments of such a method may include one or more of the following: The method may include creating, by the mobile device, the tracking template for tracking the 
<!-- EPO <DP n="4"/>-->
 target using the first image. The method may include transmitting, by the mobile device, at least a portion of the first image to a remote server. The method may include receiving, by the mobile device, the tracking template created using the first image from the remote server. The method may include detecting, by the mobile device, the target within the first image at least partially based on a correspondence between the target and a stored reference template. Detecting, by the mobile device, the target within the first image at least partially based on the correspondence between the target and the stored reference template may comprise: transmitting, by the mobile device, at least a portion of the first image to a remote server, and receiving, by the mobile device, the stored reference template from the remote server. </p><p id="p0009" num="0009">Detecting, by the mobile device, the target within the first image at least partially based on the correspondence between the target and the stored reference template may comprise transmitting, by the mobile device, at least one descriptor based on the first image to a remote server, and receiving, by the mobile device, the stored reference template from the remote server. Detecting, by the mobile device, the target within the first image that corresponds to the stored reference template may comprise comparing, by the mobile device, at least one descriptor of the first image to at least one descriptor of the stored reference template. The method may include receiving, by the mobile device, at least partially based on detecting the target within the first image, data linked with the stored reference template. </p><p id="p0010" num="0010">[0005] Embodiments of such a method may also include one or more of the following: The method may include presenting, by the mobile device, user-discernable information at least partially based on at least a portion of the data linked with the stored reference template. Presenting, by the mobile device, user-discernable information at least partially based on at least the portion of the data linked with the stored reference template may comprise presenting the user-discernable information at least partially based on at least the portion of the data linked with the stored reference template concurrently with presenting the second image. Presenting, the user-discernable information at least partially based on at least the portion of the data linked with the stored reference template concurrently with presenting the second image may comprise: use of an augmented reality display, and a location of display of the at least the portion of the data linked with the stored reference template on the augmented reality display is affected by a location of the target in the second image. The method may include, following locating the target within the second image using the tracking template, locating, by the mobile device, the target within each image of additional images of the series of images using the tracking template. The method may include generating, by the mobile device, pose information using the first image, wherein the pose information represents a 
<!-- EPO <DP n="5"/>-->
 relationship between the stored reference template and the target. The method may include rectifying, by the mobile device, the first image using the pose information. The method may include cropping, by the mobile device, the first image to create the tracking template. </p><p id="p0011" num="0011">[0006] An example of a computer program residing on a non-transitory processor-readable medium may be presented. The computer program may comprise processor-readable instructions configured to cause a processor to cause a first image to be captured. Within the first image may be a target. The target may be an entity to be tracked. The processor- readable instructions may be configured to cause the processor to cause a second image to be captured. The processor-readable instructions may be configured to cause the processor to locate the target within the second image using a tracking template created using the first image. </p><p id="p0012" num="0012">[0007] Embodiments of such a computer program may include one or more of the following: The processor-readable instructions may be configured to cause the processor to create the tracking template for tracking the target using the first image. The processor- readable instructions may be configured to cause the processor to cause at least a portion of the first image to be transmitted to a remote server. The processor-readable instructions may be configured to cause the processor to receive the tracking template created using the first image from the remote server. The processor-readable instructions may be configured to cause the processor to detect the target within the first image at least partially based on a correspondence between the target and a stored reference template. The processor-readable instructions configured to cause the processor to detect the target within the first image at least partially based on the correspondence between the target and the stored reference template may comprise processor-readable instructions to: cause at least a portion of the first image to be transmitted to a remote server; and cause the stored reference template to be received from the remote server. The processor-readable instructions configured to cause the processor to detect the target within the first image at least partially based on the </p><p id="p0013" num="0013">correspondence between the target and the stored reference template comprises processor- readable instructions to: cause at least one descriptor based on the first image to be transmitted to a remote server; and cause the stored reference template to be received from the remote server. </p><p id="p0014" num="0014">[0008] Embodiments of such a computer program may also include one or more of the following: The processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template comprise processor-readable instructions configured to cause the processor to compare at least one 
<!-- EPO <DP n="6"/>-->
 descriptor of the first image to at least one descriptor of the stored reference template. The processor-readable instructions may be configured to cause the processor to cause, at least partially based on detecting the target within the first image, data linked with the stored reference template to be received. The processor-readable instructions may be configured to cause the processor to cause user-discernable information at least partially based on at least a portion of the data to be presented. The processor-readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion of the data to be presented may comprise processor-readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion the data to be presented concurrently with the second image being presented. The processor-readable instructions configured to cause the processor to cause the user- discernable information at least partially based on at least the portion of the data to be presented concurrently with presenting the second image may further comprise processor- readable instructions configured to cause the processor to: use an augmented reality display; and adjust a location of display of the user-discernable information at least partially based on at least the portion of the data on the augmented reality display based on a location of the target in the second image. The processor-readable instructions may be configured to cause the processor to, following locating the target within the second image using the tracking template, locate the target within each image of a series of images using the tracking template. The processor-readable instructions may be configured to cause the processor to generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. The processor-readable instructions may be configured to cause the processor to rectify the first image using the pose information, and crop the first image to create the tracking template. [0009] An example of a device for tracking a target within a series of images may be presented. The device may include an image capture device. The device may include a processor. The device may include a memory communicatively coupled with and readable by the processor and having stored therein a series of processor-readable instructions which, when executed by the processor, cause the processor to cause a first image to be captured by the image capture device. Within the first image may be the target. The target may be an entity to be tracked. The processor-readable instructions, when executed by the processor, may be configured to cause a second image to be captured. The processor-readable instructions, when executed by the processor, may be configured to locate the target within the second image using a tracking template created using the first image. 
<!-- EPO <DP n="7"/>-->
 [0010] Embodiments of such a method may include one or more of the following: The processor-readable instructions, when executed by the processor, may be configured to cause the processor to create the tracking template for tracking the target using the first image. The device may further comprise processor-readable instructions configured to cause the processor to cause at least a portion of the first image to be transmitted to a remote server; and receive the tracking template created using the first image from the remote server. The device may further comprise processor-readable instructions configured to cause the processor to detect the target within the first image at least partially based on a </p><p id="p0015" num="0015">correspondence between the target and a stored reference template. The processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template may comprise processor-readable instructions configured to cause the processor to cause at least a portion of the first image to be transmitted to a remote server; and cause the stored reference template to be received from the remote server. The processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template may comprise processor-readable instructions configured to cause the processor to cause at least one descriptor based on the first image to be transmitted to a remote server; and cause the stored reference template to be received from the remote server. The processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template may comprise processor-readable instructions configured to cause the processor to: compare at least one descriptor of the first image to at least one descriptor of the stored reference template. </p><p id="p0016" num="0016">[0011] Embodiments of such a method may also include one or more of the following: The device may further comprise processor-readable instructions configured to cause the processor to cause, at least partially based on detecting the target within the first image, data linked with the stored reference template to be received, and cause user-discernable information at least partially based on at least a portion of the data to be presented. The processor-readable instructions configured to cause the processor to cause the user- discernable information at least partially based on at least the portion of the data to be presented may comprise processor-readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion of the data to be presented concurrently with the second image being presented. The processor- readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion of the data to be presented 
<!-- EPO <DP n="8"/>-->
 concurrently with presenting the second image may further comprise processor-readable instructions configured to cause the processor to: use an augmented reality display; and adjust a location of display of the at least the portion of the data on the augmented reality display based on a location of the target in the second image. The device may further comprise processor-readable instructions configured to cause the processor to, following locating the target within the second image captured by the image capture device using the tracking template, locate the target within each image of the series of images captured by the image capture device using the tracking template. The device may further comprise processor- readable instructions configured to cause the processor to generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. The device may further comprise processor-readable instructions configured to cause the processor to rectify and crop the first image to create the tracking template. The device may be selected from a group consisting of: a cellular phone, a tablet computer, and a smartphone. [0012] An example of an apparatus for tracking a target within a series of images may be presented. The apparatus may include means for capturing a first image. Within the first image may be the target. The target may be an entity to be tracked. The apparatus may include means for locating the target within a second image captured by the means for capturing the first image using a tracking template created using the first image. [0013] Embodiments of such an apparatus may include one or more of the following: The apparatus may include means for creating the tracking template for tracking the target using the first image. The apparatus may include means for transmitting at least a portion of the first image to a remote server. The apparatus may include means for receiving the tracking template created using the first image from the remote server. The apparatus may include means for detecting the target within the first image at least partially based on a </p><p id="p0017" num="0017">correspondence between the target and a stored reference template. The means for detecting the target within the first image that corresponds to the stored reference template may comprise: means for transmitting the first image to a remote server; and means for receiving the stored reference template. The means for detecting the target within the first image that corresponds to the stored reference template may comprise: means for transmitting at least one descriptor based on the first image to a remote server; and means for receiving the stored reference template. The means for detecting the target within the first image that corresponds to the stored reference template may comprise means for comparing at least one descriptor of the first image to at least one descriptor of the stored reference template. The apparatus may 
<!-- EPO <DP n="9"/>-->
 include means for receiving, at least partially based on detecting the target within the first image, data linked with the stored reference template. The apparatus may include means for presenting user-discernable information at least partially based on at least a portion of the data. [0014] Embodiments of such an apparatus may also include one or more of the following: the means for presenting the user-discernable information at least partially based on at least the portion of the data may comprise means for presenting the user-discernable information at least partially based on at least the portion of the data concurrently with presenting the second image. The means for presenting user-discernable information at least partially based on at least the portion of the data concurrently with presenting the second image may comprise: means for using an augmented reality display; and means for adjusting a location of display of the at least the portion of the data on the augmented reality display at least partially based on a location of the target in the second image. The apparatus may include means for locating the target within each additional image of the series of images captured by the means for capturing the first image using the tracking template. The apparatus may include means for generating pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. The apparatus may include means for rectifying the first image using the pose information. The apparatus may include means for cropping the first image to create the tracking template. [0015] An example of a method for tracking a target within a series of images may be presented. The method may include receiving, by a computer system, a first image from a mobile device. Within the first image may be the target. The target may be an entity to be tracked. The method may include creating, by the computer system, a tracking template based on the first image, wherein the tracking template is used for tracking the target in the series of images by the mobile device. The method may include transmitting, by the computer system, the tracking template to the mobile device. </p><p id="p0018" num="0018">[0016] Embodiments of such a method may include one or more of the following: The method may include identifying, by the computer system, using the first image, a stored reference template that corresponds to the target. The method may include transmitting, by the computer system, the stored reference template to the mobile device. The method may include transmitting, by the computer system, data linked with the stored reference template to the mobile device. The method may include generating, by the computer system, pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. Creating the tracking template may 
<!-- EPO <DP n="10"/>-->
 comprise rectifying, by the computer system, the first image using the pose information. Creating the tracking template further may comprises cropping, by the computer system, the rectified first image. </p><p id="p0019" num="0019">[0017] An example of a computer program residing on a non-transitory processor-readable medium may be presented. The computer program may comprise processor-readable instructions configured to cause a processor to receive a first image from a mobile device. The first image may be a target. The target may be an entity to be tracked. The processor- readable instructions may be configured to cause the processor to create a tracking template based on the first image, wherein the tracking template is used for tracking the target in a series of images by the mobile device. The processor-readable instructions may be configured to cause the processor to cause the tracking template to be transmitted to the mobile device. </p><p id="p0020" num="0020">[0018] Embodiments of such a computer program may include one or more of the following: The processor-readable instructions may be configured to cause the processor to identify, using the first image, a stored reference template that corresponds to the target. The processor-readable instructions may be configured to cause the processor to cause the stored reference template to be transmitted to the mobile device. The processor-readable instructions may be configured to cause the processor to cause data linked with the stored reference template to be transmitted to the mobile device. The processor-readable instructions may be configured to cause the processor to generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. The processor-readable instructions configured to cause the processor to create the tracking template based on the first image may comprise processor-readable instructions configured to rectify the first image using the pose information. The processor-readable instructions configured to cause the processor to create the tracking template based on the first image may comprise processor-readable instructions configured to crop the rectified first image. </p><p id="p0021" num="0021">[0019] An example of a device for tracking a target within a series of images may be presented. The device may include a processor. The device may include a memory communicatively coupled with and readable by the processor and having stored therein a series of processor-readable instructions which, when executed by the processor, cause the processor to receive a first image from a mobile device. Within the first image may be the target. The target may be an entity to be tracked. The processor-readable instructions may be further configured to cause the processor to create a tracking template based on the first 
<!-- EPO <DP n="11"/>-->
 image, wherein the tracking template is used for tracking the target in the series of images by the mobile device. The processor-readable instructions may be further configured to cause the processor to cause the tracking template to be transmitted to the mobile device. </p><p id="p0022" num="0022">[0020] Embodiments of such a device may include one or more of the following: The series of processor-readable instructions may further comprise processor-readable instructions configured to cause the processor to identify, using the first image, a stored reference template that corresponds to the target. The series of processor-readable instructions may further comprising processor-readable instructions configured to cause the processor to cause the stored reference template to be transmitted to the mobile device. The series of processor-readable instructions further comprise processor-readable instructions configured to cause the processor to cause data linked with the stored reference template to be transmitted to the mobile device. The series of processor-readable instructions may further comprise processor-readable instructions configured to cause the processor to generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. The processor-readable instructions configured to cause the processor to create the tracking template based on the first image may comprise processor-readable instructions configured to rectify the first image using the pose information. The processor-readable instructions configured to cause the processor to create the tracking template based on the first image may comprise processor- readable instructions configured to crop the rectified first image. </p><p id="p0023" num="0023">[0021] An example of an apparatus for tracking a target within a series of images may be presented. The apparatus may include means for receiving a first image from a mobile device. Within the first image may be the target. The target may be an entity to be tracked. The apparatus may include means for creating a tracking template based on the first image, wherein the tracking template is used for tracking the target in the series of images. The apparatus may include means for transmitting the tracking template to the mobile device. </p><p id="p0024" num="0024">[0022] Embodiments of such an apparatus may include one or more of the following: The apparatus may include means for identifying using the first image, a stored reference template that corresponds to the target. The apparatus may include means for transmitting the stored reference template to the mobile device. The apparatus may include means for transmitting data linked with the stored reference template to the mobile device. The apparatus may include means for generating pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. The means for creating the tracking template may comprise means for rectifying the first 
<!-- EPO <DP n="12"/>-->
 image using the pose information. The means for creating the tracking template may further comprise means for cropping the rectified first image. </p><p id="p0025" num="0025">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0026" num="0026">[0023] A further understanding of the nature and advantages of the present invention may be realized by reference to the following drawings. In the appended figures, similar components or features may have the same reference label. Further, various components of the same type may be distinguished by following the reference label by a dash and a second label that distinguishes among the similar components. If only the first reference label is used in the specification, the description is applicable to any one of the similar components having the same first reference label irrespective of the second reference label. </p><p id="p0027" num="0027">[0024] FIG. 1 illustrates an embodiment of a mobile device configured to detect and track an entity. </p><p id="p0028" num="0028">[0025] FIG. 2 illustrates an embodiment of a system configured for dynamic template tracking. </p><p id="p0029" num="0029">[0026] FIG. 3 illustrates an embodiment of a captured image. </p><p id="p0030" num="0030">[0027] FIG. 4 illustrates an embodiment of a reference template that represents a target in the captured image of FIG. 3. </p><p id="p0031" num="0031">[0028] FIG. 5 illustrates another embodiment of a captured image. [0029] FIG. 6 illustrates an embodiment of the captured image of FIG. 5 rectified. </p><p id="p0032" num="0032">[0030] FIG. 7 illustrates an embodiment of the rectified image of FIG. 6 cropped. </p><p id="p0033" num="0033">[0031] FIG. 8 illustrates an embodiment of a method for dynamic template tracking. </p><p id="p0034" num="0034">[0032] FIG. 9 illustrates an embodiment of a method for dynamic template tracking performed by a mobile device. [0033] FIG. 10 illustrates an embodiment of a method for dynamic template tracking performed by a mobile device in communication with a remote server. </p><p id="p0035" num="0035">[0034] FIG. 11 illustrates another embodiment of a method for dynamic template tracking performed by a mobile device in communication with a remote server. </p><p id="p0036" num="0036">[0035] FIG. 12 illustrates an embodiment of a computer system. 
<!-- EPO <DP n="13"/>-->
 DETAILED DESCRIPTION </p><p id="p0037" num="0037">[0036] Detecting and tracking a target within a set of images may be useful for various purposes, such as augmented reality. Once an entity desired to be tracked, referred to as a target, has been detected, the location of the target within subsequent images may be tracked. For example, if a user is aiming a camera of a mobile device, such as a cellular phone, in the general direction of an entity, the entity may first be detected as a target and then tracked in subsequent images. The location of the entity within subsequent images may change due to motion of the user holding the mobile device and/or motion of the target being tracked. [0037] Various forms of augmented reality may rely on an entity within an image being detected and tracked in subsequent images. Such images may be displayed to a user containing additional information (that is, augmenting the image with additional </p><p id="p0038" num="0038">information). The additional information presented to the user may be at least partially based on which entities are present in the image. Such an entity may be detected as a target and tracked in subsequent captured images. The additional information may be presented to the user concurrently with the target. As such, as the target is being tracked in subsequent images, the location of the additional information presented to the user may be affected by the location of the target within the subsequent images. </p><p id="p0039" num="0039">[0038] As an example, while visiting a movie rental store, a user may point a camera of a mobile device, such as a cellular phone, at a DVD case. While pointed at the DVD case, the mobile device may detect the DVD case as a target (because the DVD case is sufficiently similar to a stored reference template) and retrieve information linked with the movie of that particular DVD case. In this example, the information is a movie trailer for the DVD. While the DVD case is visible in subsequent images captured by the cell phone's camera, playback of the movie's trailer is overlaid the subsequently captured images, with the trailer's sound track possibly played aloud by the cell phone's audio system. As such, when the subsequent images are displayed to the user via the mobile device's display screen, the user can view the trailer of the movie overlaying the DVD case. As the user moves the mobile device and its camera, the DVD case identified as the target is continued to be tracked in subsequent captured images. The positioning and/or perspective of the trailer as presented on the display screen is adjusted such that it appears to be displayed directly on the DVD case as the location of the DVD case within the subsequent images changes. If the user moves the cell phone such that a second DVD case is present in images captured by the mobile device's 
<!-- EPO <DP n="14"/>-->
 camera, a trailer associated with the movie associated with the second DVD case may be overlaid the second DVD case. </p><p id="p0040" num="0040">[0039] The above example represents one possible application of detection and tracking of an entity for use with augmented reality. More generally, augmented reality can be used to supplement real-world entities with additional information. Such real-world entities are near limitless: barcodes, consumer products, posters, advertisements, constellations, buildings, landmarks, people, text, and symbols can all be entities that can be augmented with additional information, such as video, text, and/or audio. Besides augmented reality, arrangements for detection and tracking of entities described herein may have additional uses. [0040] In the case of a camera capturing images and the images being overlaid with additional information, detection and tracking may be performed to track the target associated with the additional information. In detection, a computerized device attempts to identify one or more entities present in the image. Once an entity (referred to as the "target") has been recognized in an image, tracking may be performed. The target may be tracked in one or more subsequently captured images. Typically, tracking of a target uses less processing than detection of the target. </p><p id="p0041" num="0041">[0041] In some forms of detection, information (e.g., keypoints, descriptors) from an image captured by the camera is compared to information (e.g., keypoints, descriptors) linked with stored reference templates. If a stored reference template is found to be a match (e.g., similar within a certain amount of tolerance), the stored reference template can be used for tracking in subsequent images captured by the camera. However, the use of a stored reference template for tracking may not be effective for successful tracking. For example, while a stored reference template may be similar to the target present in an image captured by the camera and may be considered a "match," the differences between the stored reference template and the target may adversely impact tracking in subsequent images. </p><p id="p0042" num="0042">[0042] As such, once a stored reference template has been matched to a target appearing in an image, rather than using the stored reference template for tracking, a tracking template may be created from the image. As such, it may be expected that the tracking template will very closely match the target because the tracking template was created using an image of the actual target. Accordingly, tracking may be more successful and/or less processor-intensive because the tracking template being used is more similar to the target than a reference template. The reference template and/or information associated with the reference template may be retrieved and presented to the user, possibly as part of an augmented reality display. 
<!-- EPO <DP n="15"/>-->
 [0043] FIG. 1 illustrates an embodiment of a system 100 that includes a mobile device 1 10- 1 configured to detect and track an entity using dynamic template tracking. System 100 is an exemplary arrangement configured to detect and track (e.g., locate in multiple images) one or more entities. In the illustrated embodiment, the entity is identified and tracked such that additional information related to the entity can be presented to the user via an augmented reality display. </p><p id="p0043" num="0043">[0044] Mobile device 1 10-1 is being held by user 130 (for simplicity, FIG. 1 illustrates only the user's hand). Mobile device 1 10-1 may be a cellular phone, such as a smartphone. Mobile device 110-1 can also be some other type of portable electronic device capable of capturing images and displaying images to user 130, such as a portable music player, handheld gaming device, tablet computer, laptop computer, or PDA. Mobile device 110-1 includes image capture device 120. Image capture device 120 may be configured to capture still images and/or video. In some embodiments, image capture device 120 is capable of capturing approximately 30 images per second. The field of view of image capture device 120 is illustrated by imaginary lines 170-1 and 170-2. </p><p id="p0044" num="0044">[0045] In system 100, image capture device 120 of mobile device 110-1 is capturing one or more images (e.g., a series of images, which may be video or a series of still images) that includes DVD case 140. Some or all of the images may be presented to user 130 via a display screen of mobile device 1 10-1. DVD case 140 displays DVD title 160 and graphic design 150. DVD title 160 and graphic design 150 may be specific to a particular movie or TV show (collectively referred to as the "video content"). As such, the images captured by image capture device 120 of DVD case 140 may contain sufficient information to identify the video content associated with DVD case 140 (e.g., without requiring user 130 to specify data specific to DVD case 140). A display (not illustrated) of mobile device 1 10-1 may display the field of view of image capture device 120, which includes DVD case 140 (including DVD title 160 and graphic design 150), to user 130. A system, such as system 200 of FIG. 2, may be used to: 1) identify an entity in the field of view of mobile device 110-1, such as DVD case 140; 2) track DVD case 140 in subsequent images captured by image capture device 120 using a tracking template created for a previous image of DVD case 140; 3) identify additional data, such as video content, associated with DVD case 140; and 4) present the additional data to the user, such as via an augmented reality display of mobile device 1 10-1. If the additional information is a movie trailer, the movie trailer may be overlaid some or all of DVD case 140 on the display screen of mobile device 110-1. For example, the DVD trailer may overlay (and, therefore, at least partially hide) graphic design 150. DVD title 160 
<!-- EPO <DP n="16"/>-->
 may remain visible on the display of mobile device 110-1 to user 130. As the hand of user 130 moves mobile device 110-1, the position of DVD case 140 within images captured by image capture device 120 may change. As such, the positioning and perspective of the trailer displayed via mobile device 110-1 may be adjusted. [0046] System 100 of FIG. 1 represents a possible application of a mobile device performing dynamic template tracking. However, it should be understood that system 100 is only an example. Many other applications, besides augmented reality of DVD covers may be possible. For example, images of barcodes, text, products, people, advertisements, buildings, landmarks, and/or symbols may be other entities that could be used in other embodiments for dynamic template tracking and/or augmented reality. </p><p id="p0045" num="0045">[0047] In order to detect a target within an image captured by mobile device 110-1, information derived from the image may be compared with stored reference information. For example, various keypoints within an image captured by mobile device 110-1, along with the creation of descriptors that detail the gradient of luminescence in the vicinity of some or all of the keypoints, may be used to identify a representative stored reference template. One or more keypoints and/or descriptors from the captured image may be compared with one or more keypoints and/or descriptors from one or more stored reference templates. A comparison between the keypoints and/or descriptors of the image captured by mobile device 110-1 and the keypoints and/or descriptors of the reference templates may allow a reference template that likely represents the target present in the image captured by mobile device 1 10- 1 to be identified. A reference template that "represents" an entity in the image captured by mobile device 110-1 may be a reference template that is similar enough to the entity in the captured image that the reference template and entity in the captured image are expected to be linked with a same set of additional information. As an example of this, consider a DVD case and a stored reference template of a movie poster for the same movie. While the arrangement of information on the case and reference template may vary (e.g., different location, slightly different graphics), both the DVD case and the reference template are related to the same movie trailer, which may be the additional information linked with the reference template. As such, while the reference template has differences from the DVD case, both are related to the same underlying information (the same movie) and the trailer linked with the reference template is relevant to the DVD case. </p><p id="p0046" num="0046">[0048] FIG. 2 illustrates an embodiment of a system 200 configured for dynamic template tracking. System 200 may include components such as: mobile devices 110, wireless network 210, network 220, remote server 230, and reference databases 240. 
<!-- EPO <DP n="17"/>-->
 [0022] Mobile devices 110 can communicate wirelessly with wireless network 210. </p><p id="p0047" num="0047">Wireless network 210 may represent a cellular network. In some embodiments, some or all of mobile devices 1 10 may communicate with a local wireless network, such as a household Wi-Fi network. Other embodiments of wireless networks are also possible. Wireless network 210 may communicate with remote server 230 via network 220. Network 220 may represent a public network (such as the Internet), private network (such as a corporate intranet), or some combination thereof. </p><p id="p0048" num="0048">[0049] Remote server 230 may represent a computer system that mobile devices 1 10 are configured to communicate with when performing dynamic template tracking. Remote server 230 may have locally stored, or may have remote access to, stored reference templates. The reference templates may be stored in a reference database 240-1. The reference templates may be a set of images (or keypoints and descriptors) of entities that may be targets in images captured by mobile devices 1 10. For example, tens, hundreds, thousands, or even hundreds of thousands of images (or keypoints and descriptors) of various entities may be stored as reference templates. Some or all of these reference templates may be linked with additional information. If a large number of reference templates are present, reference database 240-1 may require significant storage capacity. Each reference template may have associated with it various keypoints (such as 100 keypoints) and descriptors associated with some or all of those keypoints. These keypoints and descriptors may be used to determine if the reference template represents an entity present in an image captured by mobile device 110-1 within a certain amount of tolerance. In some embodiments, reference templates contain information sufficient to identify entities as targets and do not contain an image of the entity the reference template represents. </p><p id="p0049" num="0049">[0050] In some embodiments, rather than reference database 240-1 being in communication with a remote server 230 and accessible by mobile devices 110 via wireless network 210 and network 220, the reference templates may be stored locally by mobile devices. Referring to mobile device 110-1, a reference database 240-2 may be stored locally. Reference database 240-2 may contain similar data to reference database 240- 1. In some embodiments, reference database 240-2 may first be searched for a representative reference template, and if none is found, reference database 240-1 may be searched. If only reference database 240-2 is used, communication with remote server 230 may not be necessary. Reference templates stored locally in reference database 240-2 may be fewer in number than reference templates stored in reference database 240-1 due to the amount of available storage space at mobile device 
<!-- EPO <DP n="18"/>-->
 110-1. Reference databases 240 may also store additional data linked with each reference template. In some embodiments, such data is stored in a separate storage arrangement. </p><p id="p0050" num="0050">[0051] Mobile devices 1 10 may include three mobile devices as illustrated. This number of mobile devices 110 is for exemplary purposes only; more or fewer mobile devices may be present. Mobile device 1 10-1 is illustrated as having additional components. Other mobile devices may also have these or similar components. Mobile device 1 10-1 contains processor 212, image capture device 120, display 214, and reference database 240-2. Processor 212 may be used to process images captured by image capture device 120, such as to identify keypoints and/or descriptors. Image capture device 120 may capture still images and/or video (e.g., a series of images captured shortly after the previous image). Display 214 may be used to present some or all of the images captured by image capture device 120 to a user. Display 214 may be used to present the user with an augmented reality display, such as images captured by image capture device 120 overlaid with data retrieved from remote server 230. Display 214 may be a display screen of a mobile device. In some embodiments, the display may be some form of head-mounted display, such as glasses that are worn by a user. In such embodiments, the image capture device may be mounted on the glasses and/or pointed in the direction that the glasses are facing (such that image capture device 120 captures an image containing a similar scene to what the user is likely viewing). Reference database 240-2 may be used to store reference templates for identifying targets in images captured by image capture device 120. </p><p id="p0051" num="0051">[0052] In some embodiments, processing of the images captured by a mobile device 110-1 is performed by mobile device 110-1. In other embodiments, some or all of the image processing may be performed remotely, such as by remote server 230. In such embodiments, mobile device 110-1 may transmit an image captured to remote server 230 for processing. Following processing by remote server 230, tracking may be performed locally by mobile device 1 10-1. </p><p id="p0052" num="0052">[0053] FIG. 3 illustrates an embodiment of an image 300 captured by a mobile device. In this embodiment, a user has used a mobile device, such as mobile device 1 10-1 of FIG. 1, to capture an image of movie poster 310. In the illustrated embodiment, movie poster 310 is for a movie called "Super Action Movie: The Revenge." On movie poster 310 are graphics associated with the movie. Movie poster 310 appears skewed because the user operating the mobile device used to capture the image captured the image from an angle to the poster. In addition to movie poster 310, other entities may be present in the captured image. As an example, two lights 320 are shown. 
<!-- EPO <DP n="19"/>-->
 [0054] From this image captured by the mobile device, keypoints and/or descriptors may be identified (either locally by the mobile device or by a remote server, such as remote server 230 of FIG. 2). The keypoints and/or descriptors may be used to identify a reference template that is either stored locally or remotely. The reference template may be stored among a plurality of reference templates by either the mobile device or by a remote storage device, such as reference database 240-2 or reference database 240-1, respectively. </p><p id="p0053" num="0053">[0055] FIG. 4 illustrates an embodiment of a reference template 400. Reference template 400 is an image of a DVD case with a protruding DVD for the movie "Super Action Movie: The Revenge." Reference template 400 may have been selected from a plurality of reference templates based on the similarity of keypoints and/or descriptors linked with image 300 of FIG. 3 and keypoints and/or descriptors linked with reference template 400 of FIG. 4. As such, the identified reference template 400 of FIG. 4 correctly relates to the DVD cover of image 300 of FIG. 3. While movie poster 310 and the DVD case of reference template 400 are different entities, both relate to the same underlying information: the movie "Super Action Movie: The Revenge." As such, information relevant to the reference template 400 is likely to be relevant to the movie poster, such as a trailer for the movie. </p><p id="p0054" num="0054">[0056] Differences between the target (movie poster 310) of FIG. 3 and reference template 400 of FIG. 4 are present. For example, one major difference between reference template 400 and the target of image 300 is that a DVD is partially protruding from the DVD case in reference template 400. Additionally, the title of the movie is located in a different position in reference template 400 than movie poster 310 of image 300. Other differences are also present, such as the "A Studio 27 Production" language at the bottom of reference template 400. Such differences may be insubstantial enough that detection can still be performed accurately. However, such differences may adversely affect tracking if reference template 400 is used for tracking the target of image 300 in subsequent images captured by the mobile device. </p><p id="p0055" num="0055">[0057] As such, rather than using the reference template for tracking, dynamic template tracking allows for a tracking template to be created from a captured image of the target to be tracked. Once a target in an image has been detected using a reference template, the image captured by the mobile device may undergo various processing in order to create a tracking template that will be used for tracking in subsequent images captured by the mobile device. Referring to the example of FIGS. 3 and 4, reference template 400 may be used to detect the target of movie poster 310 in image 300. However, for tracking the target, a tracking template may be created from image 300. 
<!-- EPO <DP n="20"/>-->
 [0058] FIGS. 6 and 7 illustrate images that may be created based on a captured image of FIG. 5. In FIGS. 6 and 7, the checkerboard has been identified as a target identified through comparison of keypoints and/or descriptors of image 500 of FIG. 5 with keypoints and descriptors of a reference template. [0059] FIG. 5 illustrates an embodiment of an image 500 captured by a mobile device, such as mobile device 110-1 of FIG. 1. Based on a comparison between the keypoints and/or descriptors of image 500 and the keypoints and/or descriptors of one or more reference templates, a particular reference template may be identified as representing a target within image 500. This reference template may indicate that the checkerboard is the target. As such, for tracking purposes, a tracking template may be created from image 500. FIG. 6 illustrates an embodiment of a rectified image created using image 500. Rectified image 600 corrects image distortion (such as by the mobile device capturing image 500 being held at an angle to the checkerboard) by transforming the image to a standard coordinate system. </p><p id="p0056" num="0056">Rectifying may be performed using methods such as Affine warping or perspective warping. Rectifying may make rectified image 600 checkerboard appear viewed from approximately straight on. FIG. 7 illustrates an embodiment of a cropped image created using rectified image 600. Image 700 may be cropped such that only the target (and objects on the target) are substantially present in image 700. As such, image 700 may be used as a tracking template for tracking the checkerboard in subsequent images captured by the mobile device that captured image 500. </p><p id="p0057" num="0057">[0060] Mobile devices, such as mobile device 110-1, along with system 200, or some other system that is configured for dynamic template tracking, may be used to perform various methods for dynamic template tracking. FIG. 8 illustrates an embodiment of a method 800 for dynamic template tracking performed using a mobile device. Each stage of method 800 may be performed by a mobile device, such as mobile device 1 10-1. Alternatively, each stage of method 800 may be performed by a computer system, such as remote server 230. </p><p id="p0058" num="0058">[0061] At stage 810, a target within a first image may be detected using a stored set of reference tracking templates. In order to detect the entity within the first image that is the target, one or more keypoints and/or descriptors for the first image may be identified. These keypoints and descriptors may be compared with keypoints and/or descriptors for one or more of the reference templates. If the keypoints and/or descriptors for one of the reference templates is sufficiently close enough (e.g., within a predetermined amount of tolerance) of the keypoints and/or descriptors for the first image, the reference template may be considered 
<!-- EPO <DP n="21"/>-->
 to represent a target within the first image. Because this target matches a reference template, it may be tracked. </p><p id="p0059" num="0059">[0062] At stage 820, a tracking template may be created for the target detected within the first image. The tracking template may be created using the first image. The first image may be rectified and/or cropped to create an image that substantially contains only the target to be tracked. As such the tracking template may be created from the first image that was used for comparison with the reference templates. In some embodiments, the tracking template may be created by some other image captured before or after the first image of the target was captured. [0063] At stage 830, the target may be located in a second image using the tracking template. The second image may be captured after the first image. It should be understood that one or more other images may have been captured between when the first image and the second image were captured. For example, an image capture device may continually capture images in the form of video. The target may also be located in images captured subsequent to the second image using the tracking template. As such, the tracking template can be used to track the target in a series of captured images, such as video. Tracking the target using the tracking template, which was created using an image of the actual target being tracked, may be more effective (e.g., more likely to correctly follow the target and/or be less processor- intensive) than tracking using the reference template. [0064] FIG. 9 illustrates an embodiment of a method 900 for dynamic template tracking performed by a mobile device. In embodiments of method 900, reference templates are stored locally by the mobile device. Method 900 may be performed using system 200 or some other system that is configured for dynamic template tracking. Referring to system 200 of FIG. 2, mobile device 110-1 may use reference templates stored in reference database 240- 2. </p><p id="p0060" num="0060">[0065] At stage 910, a plurality of reference templates may be stored by the mobile device. These reference templates may be stored using a tangible (e.g., non-transitory) computer- readable medium. These reference templates may be stored prior to an image being captured for dynamic template tracking at stage 920. In some embodiments, the number of reference templates present in the plurality of reference templates may be small, such as 10. In other embodiments, the number of reference templates stored may number in the thousands or hundreds of thousands. The reference templates may be stored on the mobile device using a database. In some embodiments, all of the reference templates may be directed to a particular 
<!-- EPO <DP n="22"/>-->
 topic. For example, each reference template may be related to a movie. The reference templates may be images. The images may be linked with keypoints and descriptors. In some embodiments, the reference templates may only include keypoints and descriptors, with no corresponding image being stored. Each reference template may be linked with additional information either stored locally by the mobile device or remotely, such as at a remote server. </p><p id="p0061" num="0061">[0066] At stage 920, the mobile device may capture an image. Referring to mobile device 110-1 of FIG. 2, image capture device 120 may be used to capture an image. The image may be a still image or a frame of video. At stage 930, various keypoints and/or descriptors within this image may be identified by the mobile device. In some embodiments, approximately one hundred keypoints are identified in the image captured at stage 920. For some or all of the keypoints, descriptors (which represent the gradient of luminescence in the vicinity of the corresponding keypoint) may be created. </p><p id="p0062" num="0062">[0067] At stage 940, the computer-readable storage medium storing the reference templates may be accessed and a reference template that represents an entity within the captured image may be retrieved. The keypoints and descriptors identified for the captured first image at stage 930 may be used to search the stored reference templates at stage 940. The search may be an attempt to identify a reference template that has similar keypoints and/or descriptors (within some amount of tolerance). If no reference template has similar keypoints and/or descriptors within some amount of tolerance to the first image, this may mean that no target to be tracked is present (or at least could be successfully identified) within the image. If a reference template having similar keypoints and descriptors is found, this reference template may be determined to represent an entity that is the target in the image captured at stage 920. As an example, image 300 contains a target of a poster related to the same movie as reference template 400 of FIG. 4. While similar, the movie poster of image 300 and reference template 400 are not identical, but may be similar enough that the reference template is considered to represent the target. </p><p id="p0063" num="0063">[0068] At stage 950, pose information may be generated by the mobile device. A technique, such as homography, may be used to generate pose information that represents the relationship between the reference template retrieved at stage 940 and the target of the image captured at stage 920. For example, this pose information may serve to describe the relationship between a target in an image captured at an angle (such as movie poster 310 of image 300 of FIG. 3) to a reference template (such as reference template 400 of FIG. 4). 
<!-- EPO <DP n="23"/>-->
 [0069] At stage 960, the captured image containing the target may be rectified. Rectifying the image captured at stage 920 can correct image distortion (such as image distortion caused by the mobile device capturing image 500 being held at an angle to the target) by </p><p id="p0064" num="0064">transforming the image to a standard coordinate system. This may involve creating a new image based on the first image or modifying the first image. Methods such as Affine warping or perspective warping may be used. As such, following stage 960, an image may be created that appears as if the target is viewed from approximately straight on. </p><p id="p0065" num="0065">[0070] At stage 970, the rectified image of stage 960 may be cropped. The cropping may eliminate some or all portions of the rectified captured image besides the target. Cropping the image may involve editing the rectified image or creating a new image from the rectified image. As such, following stage 970, a rectified, cropped image of the target may have been created by the mobile device. While the previous three stages refer to various image processing functions, it should be understood that other image processing may also be performed, such as contrast adjustment, image rotation, brightness adjustment, color adjustment, etc. </p><p id="p0066" num="0066">[0071] At stage 980, the rectified and cropped image created at stage 970 may be used as the tracking template for tracking the target in images captured by the mobile device subsequent to the capture of the first image. Such tracking can involve locating the target in each (or at least some) subsequent images captured by an image capture device of the mobile device. Use of this rectified, cropped image as the tracking template may allow for efficient tracking because the tracking template is created from an actual image of the target being tracked in subsequent images. As such, tracking using the tracking template may be more successful than tracking using the reference template, due to possible variances between the reference template that is determined to represent the target and the actual target. [0072] At stage 990, user-discernable information (or data) that is linked with the reference template retrieved at stage 940 may be presented to the user. This user-discernable data may be stored locally by the mobile device or remotely, such as by a remote server. If stored remotely, the mobile device may first request the data based on the reference template selected. Some or all of the user-discernable data may be presented to the user. For example, referring to the examples of FIGS. 3 and 4, the data associated with reference template 400 of FIG. 4 may be a trailer for the movie on the DVD. This trailer may be presented by the mobile device to the user. In some embodiments, the data associated with the reference template is presented to the user via an augmented reality display. Referring again to the example of the movie trailer, the trailer may be presented on a screen of the mobile device to 
<!-- EPO <DP n="24"/>-->
 the user. The screen may show a series of images (e.g., a video stream) captured by the mobile device's image capture device. The target may continue to be tracked in this series of images. Overlaying the series of images may be the trailer. For example, the trailer, possibly using the pose information generated at stage 950, may be displayed to appear to be playing back on the movie poster present in the field of view of the mobile device. If the image capture device is moved, and thus the movie poster target in the field of view of the image capture device moves, the portion of the screen used to display the trailer may change such that it still appears that the trailer is being played back on the movie poster. While this is occurring, images captured by the image capture device of the mobile device may be searched for additional targets by identifying keypoints and descriptors and comparing such keypoints and descriptors to stored reference templates. </p><p id="p0067" num="0067">[0073] FIG. 10 illustrates an embodiment of a method for dynamic template tracking performed by a mobile device that communicates with a remote server. In embodiments of method 1000, reference templates are stored remote from the mobile device used to capture images. Method 1000 may be performed using system 200 or some other system that is configured for dynamic template tracking. Referring to system 200 of FIG. 2, mobile device 110-1 may capture images while the reference template is retrieved from a remote server, such as remote server 230 of system 200. </p><p id="p0068" num="0068">[0074] At stage 1005, a plurality of reference templates may be stored by the remote computer system. These reference templates may be stored using a tangible (e.g., non- transitory) computer readable medium. These reference templates may be stored prior to an image being captured for dynamic template tracking at stage 1010. In some embodiments, the number of reference templates may be small, such as 10. In other embodiments, the number of reference templates in the plurality of reference templates may number in the thousands or hundreds of thousands. The reference templates may be stored by the remote server in a database. In some embodiments, all of the reference templates may be directed to a particular topic. For example, each reference template may be related to a movie. The reference templates may be images. The images may be linked with keypoints and descriptors. In some embodiments, the reference templates may only include keypoints and descriptors, with no actual image being stored. Each reference template may be linked with additional information either stored locally by the mobile device or remotely, such as at a remote server. </p><p id="p0069" num="0069">[0075] At stage 1010, the mobile device may capture an image. Referring to mobile device 110-1 of FIG. 2, image capture device 120 may be used to capture an image. The image may 
<!-- EPO <DP n="25"/>-->
 be a still image or a frame of video. At stage 1020, various keypoints and/or descriptors within this image may be identified by the mobile device. In some embodiments, approximately one hundred keypoints are identified in the image captured at stage 1010. For some or all of the keypoints, descriptors (which represent the gradient of luminescence in the vicinity of the corresponding keypoint) may be created. </p><p id="p0070" num="0070">[0076] At stage 1030, the keypoints and/or the descriptors may be transmitted to the remote server by the mobile device using one or more networks. At stage 1032, the keypoints and/or descriptors may be received by the remote server. In some embodiments only keypoints are used, and in some embodiments keypoints and descriptors may be used. It may also be possible to use only descriptors. </p><p id="p0071" num="0071">[0077] At stage 1034, based on the keypoints and/or descriptors, the remote server may retrieve a reference template linked with similar keypoints and/or descriptors (within an amount of tolerance) from a tangible (e.g., non-transitory, manufactured) computer readable medium. For example, the remote server may access a database, such as reference database 240-2 of FIG. 2, stored on a computer readable medium to search the reference templates. A reference template that is retrieved may be selected from among hundreds, thousands, or hundreds of thousands of reference templates stored by the remote server. At stage 1036, the reference template determined to represent a target in the first image retrieved by the remote server may be transmitted to the mobile device. If no reference template is determined to represent a target in the first image, a response indicating as such may be transmitted to the mobile device. At stage 1040, the reference template, if any, may be received by the mobile device. </p><p id="p0072" num="0072">[0078] At stage 1050, redetection using the reference template received from the remote server may be performed by the mobile device. Redetection may involve redetecting the target within an image captured more recently than the image captured at stage 1010. For example, in some embodiments, the mobile device may be capturing images periodically, such as several times per second. During the time that elapsed between stages 1010 and 1040, a user may have moved the mobile device such that the target is being viewed from a different angle and/or in a different position within the image capture device's field of view. During this time, the mobile device may have continued to capture images. As such, redetection may allow for a more recent captured image to be used for generating pose information to allow for a (more) accurate relationship between the reference template and the target. Accordingly, stage 1050 may involve the capture of one or more additional images of the target to be used for redetection. The redetection of stage 1050 may involve 
<!-- EPO <DP n="26"/>-->
 identifying the keypoints and/or the descriptors present within the more recently captured image. </p><p id="p0073" num="0073">[0079] At stage 1060, pose information may be generated by the mobile device. A technique, such as homography, may be used to generate pose information that represents the relationship between the reference template received at stage 1040 and the target of the image captured at stage 1010 or 1050. </p><p id="p0074" num="0074">[0080] At stage 1070, the captured image containing the target may be rectified. Rectifying the image captured at stage 1010 can correct image distortion (such as image distortion caused by the mobile device capturing image 500 being held at an angle to the target) by transforming the image to a standard coordinate system. This may involve creating a new image based on the first image or modifying the first image. Methods such as Affine warping or perspective warping may be used. As such, following stage 1070, an image may be created that appears as if the target is viewed from approximately straight on. </p><p id="p0075" num="0075">[0081] At stage 1080, the rectified image of stage 1070 may be cropped. Cropping may eliminate some or all portions of the rectified captured image besides the target. Cropping the image may involve editing the rectified image or creating a new image from the rectified image. As such, following stage 1080, a rectified, cropped image of the target may have been created by the mobile device. </p><p id="p0076" num="0076">[0082] At stage 1090, a second image may be captured. One or more additional images may have been captured by the image capture device of the mobile device between stages 1010 and 1090, such as at stage 1050. </p><p id="p0077" num="0077">[0083] At stage 1095, the rectified and cropped image created at stage 1080 may be used as the tracking template for tracking the target in images captured by the mobile device subsequent to the capture of the first image, such as the second image captured at stage 1090. Such tracking can involve identifying the target in each (or at least some) subsequent images captured by an image capture device of the mobile device. Use of this rectified, cropped image as the tracking template may allow for efficient tracking because the tracking template is created from an actual image of the target to be tracked in subsequent images. As such, tracking using the tracking template may be more successful than tracking using the reference template, due to possible variance between the reference template that represents the target and the actual target. 
<!-- EPO <DP n="27"/>-->
 [0084] Concurrent with stage 1095, data that is linked with the reference template received at stage 1040 may be presented to the user. This data may be retrieved from the remote server. The remote server may transmit this data in response to retrieving the reference template at stage 1034. Some or all of the data may be presented to the user. In some embodiments, the data associated with the reference template (and/or an image of the reference template) is presented to the user via an augmented reality display. </p><p id="p0078" num="0078">[0085] FIG. 11 illustrates another embodiment of a method 1 100 for dynamic template tracking performed by a mobile device that communicates with a remote server. In embodiments of method 1100, reference templates are stored remote from the mobile device used to capture images. Further, processing of the images captured by the mobile device is performed by the remote server. Method 1100 may be performed using system 200 or some other system that is configured for dynamic template tracking. Referring to system 200 of FIG. 2, mobile device 1 10-1 may capture images, while the processing is performed by the remote server, such as remote server 230 of system 200. [0086] At stage 1105, a plurality of reference templates may be stored by the remote computer system. These reference templates may be stored using a computer-readable medium. These reference templates may be stored prior to an image being captured for dynamic template tracking at stage 11 10. In some embodiments, the number of reference templates present in the plurality of reference templates may be small, such as 10. In other embodiments, the number of reference templates may number in the thousands or hundreds of thousands. The reference templates may be stored by the remote server in a database. In some embodiments, all of the reference templates may be directed to a particular topic. The reference templates may be images. The images may be linked with keypoints and descriptors. In some embodiments, the reference templates may only include keypoints and descriptors, with no actual image being stored. Each reference template may be linked with additional information stored remote from the mobile device, such as at a remote server. </p><p id="p0079" num="0079">[0087] At stage 1 110, the mobile device may capture an image. Referring to mobile device 110-1 of FIG. 2, image capture device 120 may be used to capture an image. The image may be a still image or a frame of video. At stage 1 1 15, the first image captured by the mobile device may be transmitted to the remote server via one or more networks. At stage 1120, the remote server may receive the first image. </p><p id="p0080" num="0080">At stage 1125, various keypoints and/or descriptors within this image may be identified by the remote server. In some embodiments, approximately 100 keypoints are identified in the 
<!-- EPO <DP n="28"/>-->
 image captured at stage 11 10. For some or all of the keypoints, descriptors (which represent the gradient of luminescence in the vicinity of the corresponding keypoint) may be created. </p><p id="p0081" num="0081">[0088] Based on the keypoints and/or descriptors, the remote server may retrieve a reference template linked with similar keypoints and/or descriptors (within an amount of tolerance) from a computer-readable medium at stage 1 130. For example, the remote server may access a database, such as reference database 240-2 of FIG. 2, stored on a computer readable medium to search the reference templates. A reference template that is retrieved may be selected from among hundreds, thousands, or hundreds of thousands of reference templates stored by the remote server. If no reference template is determined to represent a target found in the image captured, a response indicating as such may be transmitted to the mobile device. </p><p id="p0082" num="0082">[0089] At stage 1 135, pose information may be generated by the remote server. A technique, such as homography, may be used to generate pose information that represents the relationship between the reference template retrieved at stage 1130 and the target of the image captured at stage 1 110. </p><p id="p0083" num="0083">[0090] At stage 1 140, the captured image containing the target may be rectified. Rectifying the image captured at stage 11 10 can correct image distortion (such as image distortion caused by the mobile device capturing image 500 being held at an angle to the target) by transforming the image to a standard coordinate system. This may involve creating a new image based on the first image or modifying the first image. Methods such as Affine warping or perspective warping may be used to warp the image using the pose generated at stage 1135. As such, following stage 1140, an image may be created that appears as if the target is viewed from approximately straight on. </p><p id="p0084" num="0084">[0091] At stage 1 145, the rectified image of stage 1 140 may be cropped. The cropping may eliminate some or all portions of the rectified captured image besides the target. </p><p id="p0085" num="0085"> Cropping the image may involve editing the rectified image or creating a new image from the rectified image. As such, following stage 1 145, a rectified, cropped image of the target may have been created by the remote server that is to be used as the tracking template. Such creation of the tracking template at the remote server, rather than the mobile device, may be more efficient because of increased processing abilities being available at the remote server and/or less of a concern over power consumption (e.g., if a battery is being used by the mobile device). At stage 1 150, the tracking template may be transmitted to the mobile device. At stage 1155, the tracking template may be received by the mobile device. At stage 
<!-- EPO <DP n="29"/>-->
 1160, the mobile device may store the tracking template using a local computer-readable medium, such as memory. </p><p id="p0086" num="0086">[0092] At stage 1165, redetection using the reference template received from the remote server may be performed by the mobile device. Redetection may involve redetecting the target within an image captured more recently than the image captured at stage 11 10. For example, in some embodiments, the mobile device may be capturing images periodically, such as several times per second. During the time that elapsed between stages 1 110 and 1165, a user may have moved the mobile device such that the target is being viewed from a different angle and/or in a different position within the image capture device's field of view. During this time, the mobile device may have continued to capture images. As such, redetection may allow for a more recent captured image to be used for generating pose information to allow for a (more) accurate relationship between the reference template and the target. Accordingly, stage 1 165 may involve the capture of one or more additional images to be used for redetection. The redetection of stage 1 165 may involve identifying the keypoints and the descriptors present within the more recently captured image. </p><p id="p0087" num="0087">[0093] At stage 1 170, a second image may be captured. To be clear, one or more additional images may have been captured by the image capture device of the mobile device between stages 11 10 and 1 170, such as at stage 1165. </p><p id="p0088" num="0088">[0094] At stage 1 175, the rectified and cropped image received by the mobile device at stage 1 155 may be used as the tracking template for tracking the target in the second image captured by the mobile device. Such tracking can involve identifying the target in the second image. Use of this rectified, cropped image as the tracking template may allow for effective (e.g., more accurate) tracking because the tracking template is created from an actual image of the target being tracked in subsequent images. As such, tracking using the tracking template may be more successful than tracking using the reference template, due to possible variances between the reference templates that represent the target and the actual target. </p><p id="p0089" num="0089">[0095] Images may continue to be captured by the mobile device. At stage 1 180, the tracking template used to locate the target in the second image may be used to track the image in these subsequently captured images. The tracking template may be used to continue to locate the target in subsequently captured images until the target is no longer in the field of view of the image capture device or is no longer desired to be tracked. Some or all of the images captured by the image capture device of the mobile device may be presented to the user, possibly in the form of an augmented display overlaid with data. In using the tracking 
<!-- EPO <DP n="30"/>-->
 template to locate the target in subsequently captured images, additional information from previously captured images may also be used (e.g., the location of the target in the previous image). </p><p id="p0090" num="0090">[0096] Concurrent with stages 1170 and on, data that is linked with the reference template retrieved at stage 1 130 may be presented to the user. This data may be retrieved from the remote server. The remote server may transmit this data in response to retrieving the reference template at stage 1 130. Some or all of the data may be presented to the user. In some embodiments, the data associated with the reference template (and/or an image of the reference template) is presented to the user via an augmented reality display. [0097] The previously detailed mobile devices, such as mobile devices 1 10 including mobile device 110-1, may be computerized. FIG. 12 illustrates an embodiment of a computer system. Computer system 1200 may represent some of the components of the mobile devices and may represent the described remote servers, such as remote server 230 of FIG. 2. FIG. 12 provides a schematic illustration of one embodiment of a computer system 1200 that can perform the methods provided by various other embodiments, as described herein, and/or can function as the host computer system and/or mobile devices. FIG. 12 is meant only to provide a generalized illustration of various components, any or all of which may be utilized as appropriate. FIG. 12, therefore, broadly illustrates how individual system elements may be implemented in a relatively separated or relatively more integrated manner. [0098] The computer system 1200 is shown comprising hardware elements that can be electrically coupled via a bus 1205 (or may otherwise be in communication, as appropriate). The hardware elements may include one or more processors 1210, including without limitation one or more general-purpose processors and/or one or more special-purpose processors (such as digital signal processing chips, graphics acceleration processors, and/or the like); one or more input devices 1215, which can include without limitation a mouse, a keyboard, and/or the like; and one or more output devices 1220, which can include without limitation a display device, a printer, and/or the like. Such a display device may be some form of head-mounted display, such as glasses. </p><p id="p0091" num="0091">[0099] The computer system 1200 may further include (and/or be in communication with) one or more storage devices 1225, which can comprise, without limitation, local and/or network accessible storage and/or can include, without limitation, a disk drive, a drive array, an optical storage device, a solid-state storage device such as a random access memory ("RAM") and/or a read-only memory ("ROM"), which can be programmable, flash- 
<!-- EPO <DP n="31"/>-->
 updateable, and/or the like. Such storage devices may be configured to implement any appropriate data stores, including without limitation various file systems, database structures, and/or the like. </p><p id="p0092" num="0092">[0100] The computer system 1200 might also include a communications subsystem 1230, which can include without limitation a modem, a network card (wireless or wired), an infrared communication device, a wireless communication device and/or chipset (such as a Bluetooth™ device, an 802.1 1 device, a WiFi device, a WiMax device, cellular </p><p id="p0093" num="0093">communication facilities, etc.), and/or the like. The communications subsystem 1230 may permit data to be exchanged with a network (such as the network described below, to name one example), other computer systems, and/or any other devices described herein. In many embodiments, the computer system 1200 will further comprise a working memory 1235, which can include a RAM or ROM device, as described above. </p><p id="p0094" num="0094">[0101] The computer system 1200 also can comprise software elements, shown as being currently located within the working memory 1235, including an operating system 1240, device drivers, executable libraries, and/or other code, such as one or more application programs 1245, which may comprise computer programs provided by various embodiments, and/or may be designed to implement methods, and/or configure systems, provided by other embodiments, as described herein. Merely by way of example, one or more procedures described with respect to the method(s) discussed above might be implemented as code and/or instructions executable by a computer (and/or a processor within a computer); in an aspect, then, such code and/or instructions can be used to configure and/or adapt a general purpose computer (or other device) to perform one or more operations in accordance with the described methods. Processor-readable instructions also may refer to dedicated hardware that has been configured to perform such various instructions. For example, such processor- readable instructions may be performed by an application-specific integrated circuit (ASIC). </p><p id="p0095" num="0095">[0102] A set of these instructions and/or code might be stored on a computer-readable storage medium, such as the storage device(s) 1225 described above. In some cases, the storage medium might be incorporated within a computer system, such as computer system 1200. In other embodiments, the storage medium might be separate from a computer system (e.g., a removable medium, such as a compact disc), and/or provided in an installation package, such that the storage medium can be used to program, configure, and/or adapt a general purpose computer with the instructions/code stored thereon. These instructions might take the form of executable code, which is executable by the computer system 1200, and/or might take the form of source and/or installable code, which, upon compilation and/or 
<!-- EPO <DP n="32"/>-->
 installation on the computer system 1200 (e.g., using any of a variety of generally available compilers, installation programs, compression/decompression utilities, etc.), then takes the form of executable code. </p><p id="p0096" num="0096">[0103] It will be apparent to those skilled in the art that substantial variations may be made in accordance with specific requirements. For example, customized hardware might also be used, and/or particular elements might be implemented in hardware, software (including portable software, such as applets, etc.), or both. Further, connection to other computing devices such as network input/output devices may be employed. </p><p id="p0097" num="0097">[0104] As mentioned above, in one aspect, some embodiments may employ a computer system (such as the computer system 1200) to perform methods in accordance with various embodiments of the invention. According to a set of embodiments, some or all of the procedures of such methods are performed by the computer system 1200 in response to processor 1210 executing one or more sequences of one or more instructions (which might be incorporated into the operating system 1240 and/or other code, such as an application program 1245) contained in the working memory 1235. Such instructions may be read into the working memory 1235 from another computer-readable medium, such as one or more of the storage device(s) 1225. Merely by way of example, execution of the sequences of instructions contained in the working memory 1235 might cause the processor(s) 1210 to perform one or more procedures of the methods described herein. [0105] The terms "machine-readable medium" and "computer-readable medium," as used herein, refer to any medium that participates in providing data that causes a machine to operate in a specific fashion. In an embodiment implemented using the computer system 1200, various computer-readable media might be involved in providing instructions/code to processor(s) 1210 for execution and/or might be used to store and/or carry such </p><p id="p0098" num="0098">instructions/code. In many implementations, a computer-readable medium is a physical and/or tangible storage medium. Such a medium may take the form of a non-volatile media or volatile media. Non-volatile media include, for example, optical and/or magnetic disks, such as the storage device(s) 1225. Volatile media include, without limitation, dynamic memory, such as the working memory 1235. [0106] Common forms of physical and/or tangible computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, or any other magnetic medium, a CD-ROM, any other optical medium, punchcards, papertape, any other physical medium with patterns of holes, a RAM, a PROM, EPROM, a FLASH-EPROM, any other 
<!-- EPO <DP n="33"/>-->
 memory chip or cartridge, or any other medium from which a computer can read instructions and/or code. </p><p id="p0099" num="0099">[0107] Various forms of computer-readable media may be involved in carrying one or more sequences of one or more instructions to the processor(s) 1210 for execution. Merely by way of example, the instructions may initially be carried on a magnetic disk and/or optical disc of a remote computer. A remote computer might load the instructions into its dynamic memory and send the instructions as signals over a transmission medium to be received and/or executed by the computer system 1200. </p><p id="p0100" num="0100">[0108] The communications subsystem 1230 (and/or components thereof) generally will receive signals, and the bus 1205 then might carry the signals (and/or the data, instructions, etc. carried by the signals) to the working memory 1235, from which the processor(s) 1210 retrieves and executes the instructions. The instructions received by the working memory 1235 may optionally be stored on a storage device 1225 either before or after execution by the processor(s) 1210. [0109] The methods, systems, and devices discussed above are examples. Various configurations may omit, substitute, or add various procedures or components as appropriate. For instance, in alternative configurations, the methods may be performed in an order different from that described, and/or various stages may be added, omitted, and/or combined. Also, features described with respect to certain configurations may be combined in various other configurations. Different aspects and elements of the configurations may be combined in a similar manner. Also, technology evolves and, thus, many of the elements are examples and do not limit the scope of the disclosure or claims. </p><p id="p0101" num="0101">[0110] Specific details are given in the description to provide a thorough understanding of example configurations (including implementations). However, configurations may be practiced without these specific details. For example, well-known circuits, processes, algorithms, structures, and techniques have been shown without unnecessary detail in order to avoid obscuring the configurations. This description provides example configurations only, and does not limit the scope, applicability, or configurations of the claims. Rather, the preceding description of the configurations will provide those skilled in the art with an enabling description for implementing described techniques. Various changes may be made in the function and arrangement of elements without departing from the spirit or scope of the disclosure. 
<!-- EPO <DP n="34"/>-->
 [0111] Also, configurations may be described as a process which is depicted as a flow diagram or block diagram. Although each may describe the operations as a sequential process, many of the operations can be performed in parallel or concurrently. In addition, the order of the operations may be rearranged. A process may have additional steps not included in the figure. Furthermore, examples of the methods may be implemented by hardware, software, firmware, middleware, microcode, hardware description languages, or any combination thereof. When implemented in software, firmware, middleware, or microcode, the program code or code segments to perform the necessary tasks may be stored in a computer-readable medium such as a storage medium which may be tangible and/or manufactured. Processors may perform the described tasks. </p><p id="p0102" num="0102">[0112] Having described several example configurations, various modifications, alternative constructions, and equivalents may be used without departing from the spirit of the disclosure. For example, the above elements may be components of a larger system, wherein other rules may take precedence over or otherwise modify the application of the invention. Also, a number of steps may be undertaken before, during, or after the above elements are considered. Accordingly, the above description does not bound the scope of the claims. 
</p></description><claims mxw-id="PCLM44997285" ref-ucid="WO-2012122008-A1" lang="EN" load-source="patent-office"><claim id="clm-0001" num="1"><!-- EPO <DP n="35"/>--><claim-text/><claim-text>WHAT IS CLAIMED IS: 1. A method for tracking a target within a series of images, the method comprising: </claim-text><claim-text> capturing, by a mobile device, a first image, wherein: </claim-text><claim-text> within the first image is the target; and </claim-text><claim-text> the target is an entity to be tracked; and </claim-text><claim-text> locating, by the mobile device, the target within a second image using a tracking template created using the first image. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The method for tracking the target within the series of images of claim 1 , further comprising: </claim-text><claim-text> creating, by the mobile device, the tracking template for tracking the target using the first image. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The method for tracking the target within the series of images of claim 1 , further comprising: </claim-text><claim-text> transmitting, by the mobile device, at least a portion of the first image to a remote server; and </claim-text><claim-text> receiving, by the mobile device, the tracking template created using the first image from the remote server. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The method for tracking the target within the series of images of claim 1 , further comprising: </claim-text><claim-text> detecting, by the mobile device, the target within the first image at least partially based on a correspondence between the target and a stored reference template. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method for tracking the target within the series of images of claim 4, wherein: </claim-text><claim-text> detecting, by the mobile device, the target within the first image at least partially based on the correspondence between the target and the stored reference template comprises: </claim-text><claim-text> transmitting, by the mobile device, at least a portion of the first image to a remote server; and <!-- EPO <DP n="36"/>--> receiving, by the mobile device, the stored reference template from the remote server. </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The method for tracking the target within the series of images of claim 4, wherein: </claim-text><claim-text> detecting, by the mobile device, the target within the first image at least partially based on the correspondence between the target and the stored reference template comprises: </claim-text><claim-text> transmitting, by the mobile device, at least one descriptor based on the first image to a remote server; and </claim-text><claim-text> receiving, by the mobile device, the stored reference template from the remote server. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method for tracking the target within the series of images of claim 4, wherein: </claim-text><claim-text> detecting, by the mobile device, the target within the first image that corresponds to the stored reference template comprises: </claim-text><claim-text> comparing, by the mobile device, at least one descriptor of the first image to at least one descriptor of the stored reference template. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The method for tracking the target within the series of images of claim 4, the method further comprising: </claim-text><claim-text> receiving, by the mobile device, at least partially based on detecting the target within the first image, data linked with the stored reference template; and </claim-text><claim-text> presenting, by the mobile device, user-discernable information at least partially based on at least a portion of the data linked with the stored reference template. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. The method for tracking the target within the series of images of claim 8, wherein presenting, by the mobile device, user-discernable information at least partially based on at least the portion of the data linked with the stored reference template comprises presenting the user-discernable information at least partially based on at least the portion of the data linked with the stored reference template concurrently with presenting the second image. <!-- EPO <DP n="37"/>--> </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The method for tracking the target within the series of images of claim 9, wherein: </claim-text><claim-text> presenting, the user-discernable information at least partially based on at least the portion of the data linked with the stored reference template concurrently with presenting the second image comprises: </claim-text><claim-text> use of an augmented reality display; and </claim-text><claim-text> a location of display of the at least the portion of the data linked with the stored reference template on the augmented reality display is affected by a location of the target in the second image. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. The method for tracking the target within the series of images of claim 1 , the method further comprising: </claim-text><claim-text> following locating the target within the second image using the tracking template, locating, by the mobile device, the target within each image of additional images of the series of images using the tracking template. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. The method for tracking the target within the series of images of claim 4, further comprising: </claim-text><claim-text> generating, by the mobile device, pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The method for tracking the target within the series of images of claim 12, further comprising: </claim-text><claim-text> rectifying, by the mobile device, the first image using the pose information; and cropping, by the mobile device, the first image to create the tracking template. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. A computer program residing on a non-transitory processor-readable medium and comprising processor-readable instructions configured to cause a processor to: </claim-text><claim-text> cause a first image to be captured, wherein: </claim-text><claim-text> within the first image is a target; </claim-text><claim-text> the target is an entity to be tracked; <!-- EPO <DP n="38"/>--> cause a second image to be captured; and </claim-text><claim-text> locate the target within the second image using a tracking template created using the first image. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The computer program of claim 14, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> create the tracking template for tracking the target using the first image. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. The computer program of claim 14, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause at least a portion of the first image to be transmitted to a remote server; and receive the tracking template created using the first image from the remote server. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. The computer program of claim 14, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> detect the target within the first image at least partially based on a correspondence between the target and a stored reference template. </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. The computer program of claim 17, wherein the processor-readable instructions configured to cause the processor to detect the target within the first image at least partially based on the correspondence between the target and the stored reference template comprises processor-readable instructions to: </claim-text><claim-text> cause at least a portion of the first image to be transmitted to a remote server; and cause the stored reference template to be received from the remote server. </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The computer program of claim 17, wherein the processor-readable instructions configured to cause the processor to detect the target within the first image at least partially based on the correspondence between the target and the stored reference template comprises processor-readable instructions to: </claim-text><claim-text> cause at least one descriptor based on the first image to be transmitted to a remote server; and </claim-text><claim-text> cause the stored reference template to be received from the remote server. <!-- EPO <DP n="39"/>--> </claim-text></claim><claim id="clm-0020" num="20"><claim-text>20. The computer program of claim 17, wherein the processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template comprise processor-readable instructions configured to cause the processor to: </claim-text><claim-text> compare at least one descriptor of the first image to at least one descriptor of the stored reference template. </claim-text></claim><claim id="clm-0021" num="21"><claim-text>21. The computer program of claim 17, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause, at least partially based on detecting the target within the first image, data linked with the stored reference template to be received; and </claim-text><claim-text> cause user-discernable information at least partially based on at least a portion of the data to be presented. </claim-text></claim><claim id="clm-0022" num="22"><claim-text>22. The computer program of claim 21 , wherein the processor-readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion of the data to be presented comprise processor-readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion the data to be presented concurrently with the second image being presented. </claim-text></claim><claim id="clm-0023" num="23"><claim-text>23. The computer program of claim 22, wherein: </claim-text><claim-text> the processor-readable instructions configured to cause the processor to cause the user-discernable information at least partially based on at least the portion of the data to be presented concurrently with presenting the second image further comprise processor-readable instructions configured to cause the processor to: </claim-text><claim-text> use an augmented reality display; and </claim-text><claim-text> adjust a location of display of the user-discernable information at least partially based on at least the portion of the data on the augmented reality display based on a location of the target in the second image. <!-- EPO <DP n="40"/>--> </claim-text></claim><claim id="clm-0024" num="24"><claim-text>24. The computer program of claim 14, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> following locating the target within the second image using the tracking template, locate the target within each image of a series of images using the tracking template. </claim-text></claim><claim id="clm-0025" num="25"><claim-text>25. The computer program of claim 17, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0026" num="26"><claim-text>26. The computer program of claim 25, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> rectify the first image using the pose information; and </claim-text><claim-text> crop the first image to create the tracking template. </claim-text></claim><claim id="clm-0027" num="27"><claim-text>27. A device for tracking a target within a series of images, the device comprising: </claim-text><claim-text> an image capture device; </claim-text><claim-text> a processor; and </claim-text><claim-text> a memory communicatively coupled with and readable by the processor and having stored therein a series of processor-readable instructions which, when executed by the processor, cause the processor to: </claim-text><claim-text> cause a first image to be captured by the image capture device, wherein: within the first image is the target; and </claim-text><claim-text> the target is an entity to be tracked; </claim-text><claim-text> cause a second image to be captured; </claim-text><claim-text> locate the target within the second image using a tracking template created using the first image. </claim-text></claim><claim id="clm-0028" num="28"><claim-text>28. The device for tracking the target within the series of images of claim 27, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> create the tracking template for tracking the target using the first image. <!-- EPO <DP n="41"/>--> </claim-text></claim><claim id="clm-0029" num="29"><claim-text>29. The device for tracking the target within the series of images of claim 27, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause at least a portion of the first image to be transmitted to a remote server; and receive the tracking template created using the first image from the remote server. </claim-text></claim><claim id="clm-0030" num="30"><claim-text>30. The device for tracking the target within the series of images of claim 27, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> detect the target within the first image at least partially based on a correspondence between the target and a stored reference template. </claim-text></claim><claim id="clm-0031" num="31"><claim-text>31. The device for tracking the target within the series of images of claim 30, wherein the processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template comprise processor- readable instructions configured to cause the processor to: </claim-text><claim-text> cause at least a portion of the first image to be transmitted to a remote server; and </claim-text><claim-text> cause the stored reference template to be received from the remote server. </claim-text></claim><claim id="clm-0032" num="32"><claim-text>32. The device for tracking the target within the series of images of claim 30, wherein the processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template comprise processor- readable instructions configured to cause the processor to: </claim-text><claim-text> cause at least one descriptor based on the first image to be transmitted to a remote server; and </claim-text><claim-text> cause the stored reference template to be received from the remote server. </claim-text></claim><claim id="clm-0033" num="33"><claim-text>33. The device for tracking the target within the series of images of claim 30, wherein the processor-readable instructions configured to cause the processor to detect the target within the first image that corresponds to the stored reference template comprise processor- readable instructions configured to cause the processor to: </claim-text><claim-text> compare at least one descriptor of the first image to at least one descriptor of the stored reference template. <!-- EPO <DP n="42"/>--> </claim-text></claim><claim id="clm-0034" num="34"><claim-text>34. The device for tracking the target within the series of images of claim 30, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause, at least partially based on detecting the target within the first image, data linked with the stored reference template to be received; and </claim-text><claim-text> cause user-discernable information at least partially based on at least a portion of the data to be presented. </claim-text></claim><claim id="clm-0035" num="35"><claim-text>35. The device for tracking the target within the series of images of claim 34, wherein the processor-readable instructions configured to cause the processor to cause the user- discemable information at least partially based on at least the portion of the data to be presented comprise processor-readable instructions configured to cause the processor to cause the user- discemable information at least partially based on at least the portion of the data to be presented concurrently with the second image being presented. </claim-text></claim><claim id="clm-0036" num="36"><claim-text>36. The device for tracking the target within the series of images of claim 35, wherein: </claim-text><claim-text> the processor-readable instructions configured to cause the processor to cause the user-discemable information at least partially based on at least the portion of the data to be presented concurrently with presenting the second image further comprise processor-readable instructions configured to cause the processor to: </claim-text><claim-text> use an augmented reality display; and </claim-text><claim-text> adjust a location of display of the at least the portion of the data on the augmented reality display based on a location of the target in the second image. </claim-text></claim><claim id="clm-0037" num="37"><claim-text>37. The device for tracking the target within the series of images of claim 27, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> following locating the target within the second image captured by the image capture device using the tracking template, locate the target within each image of the series of images captured by the image capture device using the tracking template. </claim-text></claim><claim id="clm-0038" num="38"><claim-text>38. The device for tracking the target within the series of images of claim 30, further comprising processor-readable instructions configured to cause the processor to: <!-- EPO <DP n="43"/>--> generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0039" num="39"><claim-text>39. The device for tracking the target within the series of images of claim 38, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> rectify and crop the first image to create the tracking template. </claim-text></claim><claim id="clm-0040" num="40"><claim-text>40. The device for tracking the target within the series of images of claim 27, wherein the device is selected from a group consisting of: a cellular phone, a tablet computer, and a smartphone. </claim-text></claim><claim id="clm-0041" num="41"><claim-text>41. An apparatus for tracking a target within a series of images, the apparatus comprising: </claim-text><claim-text> means for capturing a first image, wherein: </claim-text><claim-text> within the first image is the target; and </claim-text><claim-text> the target is an entity to be tracked; and </claim-text><claim-text> means for locating the target within a second image captured by the means for capturing the first image using a tracking template created using the first image. </claim-text></claim><claim id="clm-0042" num="42"><claim-text>42. The apparatus for tracking the target within the series of images of claim 41, further comprising : </claim-text><claim-text> means for creating the tracking template for tracking the target using the first image. </claim-text></claim><claim id="clm-0043" num="43"><claim-text>43. The apparatus for tracking the target within the series of images of claim 41, further comprising : </claim-text><claim-text> means for transmitting at least a portion of the first image to a remote server; and means for receiving the tracking template created using the first image from the remote server. </claim-text></claim><claim id="clm-0044" num="44"><claim-text>44. The apparatus for tracking the target within the series of images of claim 41, further comprising : </claim-text><claim-text> means for detecting the target within the first image at least partially based on a correspondence between the target and a stored reference template. <!-- EPO <DP n="44"/>--> </claim-text></claim><claim id="clm-0045" num="45"><claim-text>45. The apparatus for tracking the target within the series of images of claim 44, wherein: </claim-text><claim-text> the means for detecting the target within the first image that corresponds to the stored reference template comprises: </claim-text><claim-text> means for transmitting the first image to a remote server; and </claim-text><claim-text> means for receiving the stored reference template. </claim-text></claim><claim id="clm-0046" num="46"><claim-text>46. The apparatus for tracking the target within the series of images of claim 44, wherein: </claim-text><claim-text> the means for detecting the target within the first image that corresponds to the stored reference template comprises: </claim-text><claim-text> means for transmitting at least one descriptor based on the first image to a remote server; and </claim-text><claim-text> means for receiving the stored reference template. </claim-text></claim><claim id="clm-0047" num="47"><claim-text>47. The apparatus for tracking the target within the series of images of claim 44, wherein: </claim-text><claim-text> the means for detecting the target within the first image that corresponds to the stored reference template comprises: </claim-text><claim-text> means for comparing at least one descriptor of the first image to at least one descriptor of the stored reference template. </claim-text></claim><claim id="clm-0048" num="48"><claim-text>48. The apparatus for tracking the target within the series of images of claim 44, the apparatus further comprising: </claim-text><claim-text> means for receiving, at least partially based on detecting the target within the first image, data linked with the stored reference template; and </claim-text><claim-text> means for presenting user-discernable information at least partially based on at least a portion of the data. </claim-text></claim><claim id="clm-0049" num="49"><claim-text>49. The apparatus for tracking the target within the series of images of claim 48, wherein the means for presenting the user-discernable information at least partially based on at least the portion of the data comprises means for presenting the user-discernable information <!-- EPO <DP n="45"/>--> at least partially based on at least the portion of the data concurrently with presenting the second image. </claim-text></claim><claim id="clm-0050" num="50"><claim-text>50. The apparatus for tracking the target within the series of images of claim 49, wherein: </claim-text><claim-text> the means for presenting user-discernable information at least partially based on at least the portion of the data concurrently with presenting the second image comprises: </claim-text><claim-text> means for using an augmented reality display; and means for adjusting a location of display of the at least the portion of the data on the augmented reality display at least partially based on a location of the target in the second image. </claim-text></claim><claim id="clm-0051" num="51"><claim-text>51. The apparatus for tracking the target within the series of images of claim 41, the apparatus further comprising: </claim-text><claim-text> means for locating the target within each additional image of the series of images captured by the means for capturing the first image using the tracking template. </claim-text></claim><claim id="clm-0052" num="52"><claim-text>52. The apparatus for tracking the target within the series of images of claim 44, further comprising: </claim-text><claim-text> means for generating pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0053" num="53"><claim-text>53. The apparatus for tracking the target within the series of images of claim 52, further comprising: </claim-text><claim-text> means for rectifying the first image using the pose information; and means for cropping the first image to create the tracking template. </claim-text></claim><claim id="clm-0054" num="54"><claim-text>54. A method for tracking a target within a series of images, the method comprising: </claim-text><claim-text> receiving, by a computer system, a first image from a mobile device, wherein: within the first image is the target; and </claim-text><claim-text> the target is an entity to be tracked; <!-- EPO <DP n="46"/>--> creating, by the computer system, a tracking template based on the first image, wherein the tracking template is used for tracking the target in the series of images by the mobile device; and </claim-text><claim-text> transmitting, by the computer system, the tracking template to the mobile device. </claim-text></claim><claim id="clm-0055" num="55"><claim-text>55. The method for tracking the target within the series of images of claim 54, the method further comprising: </claim-text><claim-text> identifying, by the computer system, using the first image, a stored reference template that corresponds to the target. </claim-text></claim><claim id="clm-0056" num="56"><claim-text>56. The method for tracking the target within the series of images of claim 55, the method further comprising: </claim-text><claim-text> transmitting, by the computer system, the stored reference template to the mobile device. </claim-text></claim><claim id="clm-0057" num="57"><claim-text>57. The method for tracking the target within the series of images of claim 55, the method further comprising: </claim-text><claim-text> transmitting, by the computer system, data linked with the stored reference template to the mobile device. </claim-text></claim><claim id="clm-0058" num="58"><claim-text>58. The method for tracking the target within the series of images of claim 55, the method further comprising: </claim-text><claim-text> generating, by the computer system, pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0059" num="59"><claim-text>59. The method for tracking the target within the series of images of claim 58, wherein creating the tracking template comprises: </claim-text><claim-text> rectifying, by the computer system, the first image using the pose information. </claim-text></claim><claim id="clm-0060" num="60"><claim-text>60. The method for tracking the target within the series of images of claim 59, wherein creating the tracking template further comprises: </claim-text><claim-text> cropping, by the computer system, the rectified first image. <!-- EPO <DP n="47"/>--> </claim-text></claim><claim id="clm-0061" num="61"><claim-text>61. A computer program residing on a non-transitory processor-readable medium and comprising processor-readable instructions configured to cause a processor to: </claim-text><claim-text> receive a first image from a mobile device, wherein: </claim-text><claim-text> within the first image is a target; and </claim-text><claim-text> the target is an entity to be tracked; </claim-text><claim-text> create a tracking template based on the first image, wherein the tracking template is used for tracking the target in a series of images by the mobile device; and </claim-text><claim-text> cause the tracking template to be transmitted to the mobile device. </claim-text></claim><claim id="clm-0062" num="62"><claim-text>62. The computer program of claim 61, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> identify, using the first image, a stored reference template that corresponds to the target. </claim-text></claim><claim id="clm-0063" num="63"><claim-text>63. The computer program of claim 62, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause the stored reference template to be transmitted to the mobile device. </claim-text></claim><claim id="clm-0064" num="64"><claim-text>64. The computer program of claim 62, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause data linked with the stored reference template to be transmitted to the mobile device. </claim-text></claim><claim id="clm-0065" num="65"><claim-text>65. The computer program of claim 62, further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0066" num="66"><claim-text>66. The computer program of claim 65, wherein the processor-readable instructions configured to cause the processor to create the tracking template based on the first image comprise processor-readable instructions configured to: </claim-text><claim-text> rectify the first image using the pose information. <!-- EPO <DP n="48"/>--> </claim-text></claim><claim id="clm-0067" num="67"><claim-text>67. The computer program of claim 66, wherein the processor-readable instructions configured to cause the processor to create the tracking template based on the first image comprise processor-readable instructions configured to: </claim-text><claim-text> crop the rectified first image. </claim-text></claim><claim id="clm-0068" num="68"><claim-text>68. A device for tracking a target within a series of images, the device comprising: </claim-text><claim-text> a processor; and </claim-text><claim-text> a memory communicatively coupled with and readable by the processor and having stored therein a series of processor-readable instructions which, when executed by the processor, cause the processor to: </claim-text><claim-text> receive a first image from a mobile device, wherein: </claim-text><claim-text> within the first image is the target; and </claim-text><claim-text> the target is an entity to be tracked; </claim-text><claim-text> create a tracking template based on the first image, wherein the tracking template is used for tracking the target in the series of images by the mobile device; and cause the tracking template to be transmitted to the mobile device. </claim-text></claim><claim id="clm-0069" num="69"><claim-text>69. The device for tracking the target within the series of images of claim 68, the series of processor-readable instructions further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> identify, using the first image, a stored reference template that corresponds to the target. </claim-text></claim><claim id="clm-0070" num="70"><claim-text>70. The device for tracking the target within the series of images of claim 69, the series of processor-readable instructions further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> cause the stored reference template to be transmitted to the mobile device. </claim-text></claim><claim id="clm-0071" num="71"><claim-text>71. The device for tracking the target within the series of images of claim 69, the series of processor-readable instructions further comprising processor-readable instructions configured to cause the processor to: <!-- EPO <DP n="49"/>--> cause data linked with the stored reference template to be transmitted to the mobile device. </claim-text></claim><claim id="clm-0072" num="72"><claim-text>72. The device for tracking the target within the series of images of claim 69, the series of processor-readable instructions further comprising processor-readable instructions configured to cause the processor to: </claim-text><claim-text> generate pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0073" num="73"><claim-text>73. The device for tracking the target within the series of images of claim 72, wherein the processor-readable instructions configured to cause the processor to create the tracking template based on the first image comprise processor-readable instructions configured to: </claim-text><claim-text> rectify the first image using the pose information. </claim-text></claim><claim id="clm-0074" num="74"><claim-text>74. The device for tracking the target within the series of images of claim 73, wherein the processor-readable instructions configured to cause the processor to create the tracking template based on the first image comprise processor-readable instructions configured to: </claim-text><claim-text> crop the rectified first image. </claim-text></claim><claim id="clm-0075" num="75"><claim-text>75. An apparatus for tracking a target within a series of images, the apparatus comprising: </claim-text><claim-text> means for receiving a first image from a mobile device wherein: </claim-text><claim-text> within the first image is the target; and </claim-text><claim-text> the target is an entity to be tracked; </claim-text><claim-text> means for creating a tracking template based on the first image, wherein the tracking template is used for tracking the target in the series of images; and </claim-text><claim-text> means for transmitting the tracking template to the mobile device. </claim-text></claim><claim id="clm-0076" num="76"><claim-text>76. The apparatus for tracking the target within the series of images of claim 75, the apparatus further comprising: <!-- EPO <DP n="50"/>--> means for identifying using the first image, a stored reference template that corresponds to the target. </claim-text></claim><claim id="clm-0077" num="77"><claim-text>77. The apparatus for tracking the target within the series of images of claim 76, the apparatus further comprising: </claim-text><claim-text> means for transmitting the stored reference template to the mobile device. </claim-text></claim><claim id="clm-0078" num="78"><claim-text>78. The apparatus for tracking the target within the series of images of claim 76, the apparatus further comprising: </claim-text><claim-text> means for transmitting data linked with the stored reference template to the mobile device. </claim-text></claim><claim id="clm-0079" num="79"><claim-text>79. The apparatus for tracking the target within the series of images of claim 76, the apparatus further comprising: </claim-text><claim-text> means for generating pose information using the first image, wherein the pose information represents a relationship between the stored reference template and the target. </claim-text></claim><claim id="clm-0080" num="80"><claim-text>80. The apparatus for tracking the target within the series of images of claim 79, wherein the means for creating the tracking template comprises: </claim-text><claim-text> means for rectifying the first image using the pose information. </claim-text></claim><claim id="clm-0081" num="81"><claim-text>81. The apparatus for tracking the target within the series of images of claim 80, wherein the means for creating the tracking template further comprises: </claim-text><claim-text> means for cropping the rectified first image. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
