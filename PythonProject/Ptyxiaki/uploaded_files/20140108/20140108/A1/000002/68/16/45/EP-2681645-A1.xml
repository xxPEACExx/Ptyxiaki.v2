<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681645-A1" country="EP" doc-number="2681645" kind="A1" date="20140108" family-id="43904256" file-reference-id="252638" date-produced="20180823" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586161" ucid="EP-2681645-A1"><document-id><country>EP</country><doc-number>2681645</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12707372-A" is-representative="NO"><document-id mxw-id="PAPP154848353" load-source="docdb" format="epo"><country>EP</country><doc-number>12707372</doc-number><kind>A</kind><date>20120203</date><lang>EN</lang></document-id><document-id mxw-id="PAPP186168102" load-source="docdb" format="original"><country>EP</country><doc-number>12707372.4</doc-number><date>20120203</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140551092" ucid="GB-201103346-A" load-source="docdb"><document-id format="epo"><country>GB</country><doc-number>201103346</doc-number><kind>A</kind><date>20110228</date></document-id></priority-claim><priority-claim mxw-id="PPC140551062" ucid="GB-2012050236-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>GB</country><doc-number>2012050236</doc-number><kind>W</kind><date>20120203</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989321878" load-source="docdb">G06F   3/041       20060101AFI20120920BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1978019276" load-source="docdb" scheme="CPC">G06F2203/04101     20130101 LA20160123BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1978019277" load-source="docdb" scheme="CPC">G06F   3/038       20130101 LI20160123BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1978019278" load-source="docdb" scheme="CPC">G06F   3/03547     20130101 LI20160123BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1978019279" load-source="docdb" scheme="CPC">G06F   3/0304      20130101 LI20160123BHEP        </classification-cpc><classification-cpc mxw-id="PCL2007295988" load-source="docdb" scheme="CPC">G06F   3/033       20130101 FI20140123BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132363116" lang="DE" load-source="patent-office">VERBESSERUNGEN AN ODER IM ZUSAMMENHANG MIT OPTISCHEN NAVIGATIONSVORRICHTUNGEN</invention-title><invention-title mxw-id="PT132363117" lang="EN" load-source="patent-office">IMPROVEMENTS IN OR RELATING TO OPTICAL NAVIGATION DEVICES</invention-title><invention-title mxw-id="PT132363118" lang="FR" load-source="patent-office">AMÉLIORATIONS SUR OU CONCERNANT DES DISPOSITIFS DE NAVIGATION OPTIQUES</invention-title><citations><non-patent-citations><nplcit><text>See references of WO 2012117232A1</text><sources><source mxw-id="PNPL67455822" load-source="docdb" name="SEA"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR919527712" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ST MICROELECTRONICS RES &amp; DEV</last-name><address><country>GB</country></address></addressbook></applicant><applicant mxw-id="PPAR919516835" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>STMICROELECTRONICS (RESEARCH &amp; DEVELOPMENT) LIMITED</last-name></addressbook></applicant><applicant mxw-id="PPAR919017600" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>STMicroelectronics (Research &amp; Development) Limited</last-name><iid>100780440</iid><address><street>Planar House Parkway, Globe Park</street><city>Marlow, Buckinghamshire SL7 1YL</city><country>GB</country></address></addressbook></applicant><applicant mxw-id="PPAR919519600" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>ST MICROELECTRONICS GRENOBLE 2</last-name><address><country>FR</country></address></addressbook></applicant><applicant mxw-id="PPAR919514617" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>STMICROELECTRONICS (GRENOBLE 2) SAS</last-name></addressbook></applicant><applicant mxw-id="PPAR919007264" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>STMicroelectronics (Grenoble 2) SAS</last-name><iid>101142851</iid><address><street>12, Rue Jules Horowitz</street><city>38000 Grenoble</city><country>FR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919513429" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>RAYNOR JEFFREY</last-name><address><country>GB</country></address></addressbook></inventor><inventor mxw-id="PPAR919530811" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>RAYNOR, JEFFREY</last-name></addressbook></inventor><inventor mxw-id="PPAR919009905" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>RAYNOR, JEFFREY</last-name><address><street>Ambridge Hall 17 West Catherine Place</street><city>Edinburgh Lothian EH12 5HZ</city><country>GB</country></address></addressbook></inventor><inventor mxw-id="PPAR919522320" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>MELLOT PASCAL</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR919511792" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>MELLOT, PASCAL</last-name></addressbook></inventor><inventor mxw-id="PPAR919016913" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>MELLOT, PASCAL</last-name><address><street>108 Chemin du Milieu</street><city>38250 Lans En Vercors</city><country>FR</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919019039" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Gray, John James</last-name><suffix>et al</suffix><iid>101144651</iid><address><street>Murgitroyd &amp; Company Scotland House 165-169 Scotland Street</street><city>Glasgow G5 8PL</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="GB-2012050236-W"><document-id><country>GB</country><doc-number>2012050236</doc-number><kind>W</kind><date>20120203</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012117232-A1"><document-id><country>WO</country><doc-number>2012117232</doc-number><kind>A1</kind><date>20120907</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549765401" load-source="docdb">AL</country><country mxw-id="DS549757096" load-source="docdb">AT</country><country mxw-id="DS549765402" load-source="docdb">BE</country><country mxw-id="DS549913568" load-source="docdb">BG</country><country mxw-id="DS549834784" load-source="docdb">CH</country><country mxw-id="DS549757719" load-source="docdb">CY</country><country mxw-id="DS549757101" load-source="docdb">CZ</country><country mxw-id="DS549765403" load-source="docdb">DE</country><country mxw-id="DS549757720" load-source="docdb">DK</country><country mxw-id="DS549757725" load-source="docdb">EE</country><country mxw-id="DS549846032" load-source="docdb">ES</country><country mxw-id="DS549913569" load-source="docdb">FI</country><country mxw-id="DS549834785" load-source="docdb">FR</country><country mxw-id="DS549765404" load-source="docdb">GB</country><country mxw-id="DS549757726" load-source="docdb">GR</country><country mxw-id="DS549765413" load-source="docdb">HR</country><country mxw-id="DS549757102" load-source="docdb">HU</country><country mxw-id="DS549846033" load-source="docdb">IE</country><country mxw-id="DS549757727" load-source="docdb">IS</country><country mxw-id="DS549834790" load-source="docdb">IT</country><country mxw-id="DS549757728" load-source="docdb">LI</country><country mxw-id="DS549913570" load-source="docdb">LT</country><country mxw-id="DS549834151" load-source="docdb">LU</country><country mxw-id="DS549913571" load-source="docdb">LV</country><country mxw-id="DS549913572" load-source="docdb">MC</country><country mxw-id="DS549834152" load-source="docdb">MK</country><country mxw-id="DS549834153" load-source="docdb">MT</country><country mxw-id="DS549834158" load-source="docdb">NL</country><country mxw-id="DS549834791" load-source="docdb">NO</country><country mxw-id="DS549834159" load-source="docdb">PL</country><country mxw-id="DS549846038" load-source="docdb">PT</country><country mxw-id="DS549833751" load-source="docdb">RO</country><country mxw-id="DS549846039" load-source="docdb">RS</country><country mxw-id="DS549834160" load-source="docdb">SE</country><country mxw-id="DS549765414" load-source="docdb">SI</country><country mxw-id="DS549834792" load-source="docdb">SK</country><country mxw-id="DS549834793" load-source="docdb">SM</country><country mxw-id="DS549757737" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA99835699" ref-ucid="WO-2012117232-A1" lang="EN" load-source="patent-office"><p num="0000">An optical navigation device is provided for detecting movement of a pointer, such as a finger, in three dimensions. A sensor obtains images of the pointer which have been illuminated by an illumination source, and an image scaling module determines the difference in size between images acquired by the image sensor to determine the difference in height of the pointer between images.</p></abstract><abstract mxw-id="PA100331506" ref-ucid="WO-2012117232-A1" lang="EN" source="national office" load-source="docdb"><p>An optical navigation device is provided for detecting movement of a pointer, such as a finger, in three dimensions. A sensor obtains images of the pointer which have been illuminated by an illumination source, and an image scaling module determines the difference in size between images acquired by the image sensor to determine the difference in height of the pointer between images.</p></abstract><abstract mxw-id="PA99835700" ref-ucid="WO-2012117232-A1" lang="FR" load-source="patent-office"><p num="0000">L'invention propose un dispositif de navigation optique conçu pour détecter le déplacement d'un pointeur, par exemple un doigt, dans un espace à trois dimensions. Un capteur acquiert des images du pointeur qui ont été éclairées par une source d'éclairage, et un module d'échelonnement d'image détermine la différence de taille entre les images acquises par le capteur d'images pour déterminer la différence de hauteur du pointeur entre les images.</p></abstract><abstract mxw-id="PA100331507" ref-ucid="WO-2012117232-A1" lang="FR" source="national office" load-source="docdb"><p>L'invention propose un dispositif de navigation optique conçu pour détecter le déplacement d'un pointeur, par exemple un doigt, dans un espace à trois dimensions. Un capteur acquiert des images du pointeur qui ont été éclairées par une source d'éclairage, et un module d'échelonnement d'image détermine la différence de taille entre les images acquises par le capteur d'images pour déterminer la différence de hauteur du pointeur entre les images.</p></abstract><description mxw-id="PDES51233050" ref-ucid="WO-2012117232-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> Improvements in or relating to optical navigation devices </p><p id="p0002" num="0002">Description </p><p id="p0003" num="0003">Field of the invention </p><p id="p0004" num="0004">The present invention relates to improvements in or relating to optical navigation devices such as, for example, optical mice being operated in 3D. </p><p id="p0005" num="0005"> Background of the invention </p><p id="p0006" num="0006"> Computer devices are becoming ever smaller and full computing functionality can be found on phones and smart phones and other personal digital assistants (PDA). As the computer devices become smaller so the various features of the computer devices must also become smaller. This includes a requirement for smaller input device for the user to enter inputs into the device. One such input device is an optical navigation device. Many computer devices, large and small, are equipped with optical navigation devices, such as a mouse. </p><p id="p0007" num="0007"> Finger mice are becoming more common on mobile phones, laptops, digital still cameras (DSC) and many other devices. In addition, 3D displays are becoming more popular. Current input or navigation devices (e.g. touch screens, mice and finger mice including Optical Finger </p><p id="p0008" num="0008"> Navigation (OFN), Optical Navigation Mouse (ONM), Optical Joystick (OJ) are typically 2D. Operating systems supporting 3D windows and operations requiring 3D control inputs are currently being developed. Even with a 2D display, a 3D navigation device is useful to recognize a wider range of gestures. </p><p id="p0009" num="0009"> Most finger mice are modifications of a standard PC mouse where a user moves their finger on top of a surface in X and Y directions. A sensor images the finger and uses the images to create X and Y motion vectors 
<!-- EPO <DP n="4"/>-->
 corresponding to the movement of the user's finger. These sensors are unable to detect movement in the Z axis. </p><p id="p0010" num="0010"> A common method to obtain depth information is to use two sensors which are spatially separated as is achieved in the human visual system with two eyes. </p><p id="p0011" num="0011"> Figure 1 shows an example of a proposed fingermouse system with two image sensors. As can be seen in Figure 1 , if the finger is moved vertically (i.e. along the Z axis) a feature on the sensors moves in the image plane. The same feature moves in opposite directions on the two sensors, permitting X and Y movement to be differentiated from Z movement. There are many disadvantages to this technique: </p><p id="p0012" num="0012">• two sensors are required which adds to the cost and space </p><p id="p0013" num="0013"> requirements of the device; </p><p id="p0014" num="0014">• at least two apertures are needed in the device which is generally considered to be unattractive; </p><p id="p0015" num="0015">• the images from the sensors must be compared with one another which requires additional processing; </p><p id="p0016" num="0016">• for high temporal resolution, high frame rates are required, for </p><p id="p0017" num="0017"> example between 1 kHz to 10 kHz is common for optical mice which in turn results in a high bitrate for the image data of typically</p><p id="p0018" num="0018">40Mbps; </p><p id="p0019" num="0019">• the high data rate needs to be supported by both the output </p><p id="p0020" num="0020"> transmitters of the sensors and also the receiver of the image processing unit; · accurately matching the two images, especially with a periodic </p><p id="p0021" num="0021"> structure such as a lateral shift, as found on a fingerprint, is difficult and can produce incorrect triangulation or Z-height information. 
<!-- EPO <DP n="5"/>-->
 Another traditional method of determining the distance is to measure the time it takes light to travel from the emitter such as an LED or VCSEL {Vertical Cavity Surface Emitting Laser} to the object and then back to the sensor. A "SPAD" (Single Photon Avalanche Detector) is suitable for measuring this as it has a high electric field in the detector which allows a photo-generated electron to be accelerated and quickly detected. In addition, the gain of the avalanche means that a digital pulse is produced and can be counted. However, a single SPAD has no spatial resolution and so a single detector cannot determine if the object (i.e. finger) is moving for example along a circle with the SPAD at the centre in the form of a common gesture. </p><p id="p0022" num="0022"> This can be overcome by employing multiple SPAD devices, although the requirement for more apertures in the device housing is unappealing. A possible system would employ 2 SPADs and 1 mouse sensor to extract motion in X, Y and Z axes and would this require 3 apertures. </p><p id="p0023" num="0023"> Objects of the invention </p><p id="p0024" num="0024"> It is an object of the present invention to overcome at least some of the problems associated with the prior art. </p><p id="p0025" num="0025"> It is a further object of the present invention to provide an optical navigation device which operates in 3D. </p><p id="p0026" num="0026"> Summary of the invention </p><p id="p0027" num="0027"> The present invention provides a method and system as set out in the accompanying claims. </p><p id="p0028" num="0028"> According to one aspect of the present invention there is provided an optical navigation device for detecting movement of a pointer, such as a finger, in three dimensions. 
<!-- EPO <DP n="6"/>-->
 Optionally, the optical navigation device further comprising a sensor adapted to obtain images of the pointer which have been illuminated by an illumination source. </p><p id="p0029" num="0029"> Optionally, the optical navigation device, further comprising an image shift module for determining the difference between a first and a second image to thereby determine a motion vector between the first and second image.</p><p id="p0030" num="0030">Optionally, the optical navigation device, further comprising an image scaling module for determining the difference in size between the first and second image to thereby determine the difference in height of the pointer from the first to the second image. </p><p id="p0031" num="0031"> Optionally, the difference is determined by a sum of absolute difference (SAD) process in which image properties are varied to allow the first and second image to be compared with one another. </p><p id="p0032" num="0032"> Optionally, the optical navigation device, further comprising a correlation module for determining a minimum SAD value from the SAD process, which minimum SAD value indicated that the first and second image are a best match, and thereafter deriving the image properties associated with the minimum SAD value. </p><p id="p0033" num="0033"> Optionally, at least one of the image properties associated with the minimum SAD value represents the most likely motion vector between the first and second image. </p><p id="p0034" num="0034"> Optionally, at least one of the image properties associated with the minimum SAD value represents the most likely difference in height of the pointer between the first and second image. </p><p id="p0035" num="0035">Optionally, the optical navigation device, further comprising a gain module for determining and compensating for system performance variations.</p><p id="p0036" num="0036">Optionally, the system performance variations include brightness. </p><p id="p0037" num="0037"> Optionally, the optical navigation device is a mouse. 
<!-- EPO <DP n="7"/>-->
 Optionally, the mouse is a fingermouse. </p><p id="p0038" num="0038"> The present invention offers a number of benefits. The present invention produces an optical sensor which is able to determine motion in 3 dimensions but with only a single aperture. In addition, the present invention provides an efficient method and device for providing mousing functions in 3D, which can easily measure movements in x, y and z directions to thereby determine control functions. Other factors such as ambient light conditions and the distance of the object being imaged from the sensor are also taken into account and compensation applied as appropriate. </p><p id="p0039" num="0039"> Brief description of the drawings </p><p id="p0040" num="0040"> Reference will now be made, by way of example, to the accompanying drawings, in which: </p><p id="p0041" num="0041"> Figure 1 is block diagram of a stereoscopic finger mouse, in accordance with the prior art; </p><p id="p0042" num="0042"> Figure 1 b is a block diagram of a finger mouse, in accordance with an embodiment of the present invention; </p><p id="p0043" num="0043"> Figure 2, is a block diagram of a finger mouse navigation engine, in accordance with an embodiment of the present invention; </p><p id="p0044" num="0044">Figure 3 is a schematic diagram of an image of finger motion, in accordance with an embodiment of the present invention; </p><p id="p0045" num="0045"> Figure 4 is a schematic diagram of a filtered image of finger motion, in accordance with an embodiment of the present invention; </p><p id="p0046" num="0046"> Figure 5 is a schematic diagram of a filtered image of finger motion with shift incorporated, in accordance with an embodiment of the present invention; 
<!-- EPO <DP n="8"/>-->
 Figure 6 is a schematic diagram of an optical path for a 3D finger mouse, in accordance with an embodiment of the present invention; </p><p id="p0047" num="0047"> Figure 7, is a block diagram of a 3D finger mouse navigation engine, in accordance with an embodiment of the present invention; Figure 8, is a table showing sum of absolute differences, in accordance with an embodiment of the present invention; and </p><p id="p0048" num="0048"> Figure 9, is a block diagram of a 3D finger mouse image correlation function, in accordance with an embodiment of the present invention. </p><p id="p0049" num="0049">Detailed description of the preferred embodiments </p><p id="p0050" num="0050">The present invention relates to an optical navigation device such as a mouse of small-scale which is intended to be operated by means of either standard imaging where light is reflected from a finger in order to recognize the movement of a finger on an imaging surface. This type of mouse is herein referred to as a finger mouse. </p><p id="p0051" num="0051">Figure 1 b shows an example of a fingermouse 100. The fingermouse includes a base 102; an imaging element shown generally at 104; an LED 106 and a sensor 108. The top surface 1 10 of the imaging element 104 is a window through which illumination can pass. In addition, the imaging element includes a collimating lens 1 12 between the LED and the imaging element and an imaging lens 1 14 between the imaging element and the sensor. The imaging element further includes two total internal reflection mirror elements 1 16 which direct illumination from the collimating lens to the target, for example the pointer, and then from the target back to the imaging lens. The lower surface 1 18 of the imaging element is </p><p id="p0052" num="0052">substantially flat. This is just one example of an optical mouse and many variations may be implemented without diversifying from the basic principles of operation thereof. 
<!-- EPO <DP n="9"/>-->
 In use, a user may move a pointer over the window 1 10, which may also referred to as the mousing surface. The pointer may be simply the user's finger which passes over the surface. The finger includes fingerprint ridges which can be detected to identify the movement being made. </p><p id="p0053" num="0053">The distance between the window 1 10 and the first LED or sensor in the present embodiment is in the region of 2.5 mm. This distance is the thickness of the navigation device and can vary between 1 mm and 3 mm. Ideally the thickness is not generally greater than 5 mm. The navigation device can be formed from a single piece molding. The molding includes each of the individual optical elements shown in the diagrams. The navigation device could alternatively be made in other appropriate ways with different optical elements which produce the same optical effect. The navigation device may also be made from a number of different elements, rather than a single molding. The technique for forming the navigation device may include techniques other than molding, such as replication, stamping, embossing or machining. </p><p id="p0054" num="0054">The illumination sources are, for example, LEDs which may be of any appropriate type and may generate a source in the "optical" or non-optical ranges. Accordingly, reference to optics and optical are intended to cover wavelengths which are not in the human visible range. The optics which takes the illumination from the source to the imaging surface may be of any appropriate type. </p><p id="p0055" num="0055"> The sensor may be a CMOS sensor having an array of pixels for measuring reflected light at different locations to produce an image. The array is generally formed from a number of pixels forming a grid like array with pixels extending in rows and columns. The invention may equally make use of different types of sensors. There may be various types of pixels / readout methods; for example, a rolling blade or global shutter mechanism; a system of passive pixels or active (3Transistor, 4Transistor) 
<!-- EPO <DP n="10"/>-->
 pixels; a readout with correlated double sampling; or merely double sampling. </p><p id="p0056" num="0056"> The operation of a 2D mouse will now be described with reference to figure 2. Figure 2 shows an overview of a signal path from a sensor image 200. The sensor includes a pixel array (not shown) which is preferably able to take two images per frame. The first is taken with the LED or illumination source off to create a "Black" image and the second with the illumination source on to create an "Exposed" Image. The first image is then subtracted from the second to remove any fixed pattern noise and to perform a reset, also referred to as "kTC" noise. It is possible to compare the images directly, however to ensure even illumination across the image plane application of a filter, such as an "Edge Filter" 202, is applied. This can remove low spatial frequency variations and enhance high spatial frequency variations. With such a filter, any slow or spatial variations of illumination are removed and so the system is more tolerant to uneven illumination across the image plane. Two edge-filtered images 204 and 206 are compared. One image 204 has been recently acquired and the other 206 has been acquired previously. The signal path also includes an image shift 208 and a correlation engine 210, both of which are controlled by control logic 212. The function of these elements will be described below. </p><p id="p0057" num="0057"> There are many different ways to compare the image. One way is to use a previously determined motion vector (Xshift, Yshift) as a first estimate of the motion between the two images. One image is shifted by the image shift using this motion vector and the shifted image is compared to the other non-shifted image. The comparison typically makes use of the Sum of absolute differences (SAD) where the absolute difference between corresponding pixels is added together. This can be expressed as follows: 
<!-- EPO <DP n="11"/>-->
 Correlation_lmage_2 = lmageShift(Filtered_lnnage_2,Xshift,Yshift) SAD=0 </p><p id="p0058" num="0058"> For Y= 0 to YMAX </p><p id="p0059" num="0059"> For X= 0 to XMAX SAD = SAD + ABS (Correlation_lmage_1 (X,Y) </p><p id="p0060" num="0060"> Correlation_lmage_2(X,Y) ) </p><p id="p0061" num="0061"> Next X </p><p id="p0062" num="0062">Next Y </p><p id="p0063" num="0063">The resulting SAD is stored and both the Xshift and Yshift are varied and another SAD is computed. When the calculated value of SAD is at a minimum as determined by the correlation engine, the two images </p><p id="p0064" num="0064">(Correlation_lmage_1 &amp; Correlation_lmage_2) are most similar and the shift applied to the Correlation_lmage_2 is then the correct motion vector between the two images. This then becomes the "winning vector" 214. Figure 3 shows two images: frame N 300 and frame N+1 302. For the purposes of clarity the contrast is enhanced to get black (0% signal) or peak white (100% signal). Furthermore, the image shifts are in shown as integral pixels, although in practice, the image may be shifted by a fraction of a pixel using interpolation of the image. The two images can be filtered as described above with reference to figure 2 to give rise to edge filtered images 400 and 402 as shown in figure 4. Examining the two filtered images 400 and 402 the two images are similar, but there is a shift. This shift can be determined by shifting "Filtered Image - Frame N+1 " in various directions as is shown in Figure 5. The four images in Figure 5 are Filtered Image Frame N+1 with a shift left (500); a shift right (502); a shift up (504) and a shift down (506) respectively. Comparison with each of these shifted images is made with the filtered image of Frame N in figure 4 
<!-- EPO <DP n="12"/>-->
 (400). The closest match in figure 5 is "Filtered Image - Frame N+1 - Shift Left" (500). Thus the movement vector between Frame N (300) and Frame N+1 (302) in figure 3 is one pixel to the left. </p><p id="p0065" num="0065"> As has been seen, the operation of a 2D optical mouse shifts images around and performs correlations to determine movement vectors. Figure 6 shows the optical path for a 3D movement, with a finger 600 placed on top of the mousing surface 602 (typically the surface of the housing) and also with a finger 600' above the mousing surface. From geometrical optics, when the finger is further from the sensor, the ridges and valleys on the fingerprint get smaller on the image plane. This principal can be used in the present invention to monitor the motion of the finger in the Z axis.</p><p id="p0066" num="0066">In contrast to existing optical navigation engines, which operate on size invariant images, the present invention includes a process for scaling the images, such as zooming in or zooming out, before they are correlated. The optical path for the process of the present invention is shown in Figure 7. The optical path is similar to that of figure 2. The sensor image 700 undergoes an edge filtering process 702 in which a first and second edge filtered image 704 and 706 are produced. The first filtered image 704, being the latest filtered image and the second 706, being a previous filtered image. The filtered images then undergo an image scaling process 708 and an image shift process 710, the results from which pass through the correlation engine 712. The control logic 714 then determines the "winning vector" 716 which relates to X, Y movement and the "winning zoom" 718 which relates to movement in the Z direction. </p><p id="p0067" num="0067">The technique employed will now be described in further detail. To determine motion in 3D, an expanded version of the 2D technique is used. A first image is shifted and scaled and then compared to a second. As previously described the comparison may use Sum of absolute differences 
<!-- EPO <DP n="13"/>-->
 (SAD) where the absolute difference between corresponding pixels is summed. This occurs as follows: </p><p id="p0068" num="0068">Function SAD(Correllation_lmage_1 , Correlation_lmage_2, </p><p id="p0069" num="0069">XShift,YShift,ZScaleFactor) </p><p id="p0070" num="0070"> Correlation_lmage_2 = lmageShift(lmageScaling </p><p id="p0071" num="0071">(Filtered_lmage_2,ZScaleFactor), XShift.YShift) </p><p id="p0072" num="0072"> SAD=0 </p><p id="p0073" num="0073"> For Y= 0 to YMAX </p><p id="p0074" num="0074">For X= 0 to XMAX </p><p id="p0075" num="0075"> SAD = SAD + ABS (Correlation_lmage_1 (X,Y) - Correlation_lmage_2(X,Y) ) </p><p id="p0076" num="0076"> Next X </p><p id="p0077" num="0077"> Next Y </p><p id="p0078" num="0078">The resulting SAD is stored and the ZScalefactor, Xshift and Yshift are varied and another SAD is computed. Once the SAD is at a minimum, the two images (Correlation_lmage_1 &amp; Correlation_lmage_2) are most similar and the shift applied to the Correlation_lmage_2 is then </p><p id="p0079" num="0079">representative of the motion vector and the Z scale factor between the two images. A simple algorithm to implement this is shown below: </p><p id="p0080" num="0080">index=0 </p><p id="p0081" num="0081"> For ZScaleFactor = Zoom_Min to Zoom_Max step Zoom_Delta </p><p id="p0082" num="0082"> For XShift = -XShiftMax to +XShiftMax step XShiftDelta </p><p id="p0083" num="0083"> For YShift = -YShiftMax to +YShiftMax step YShiftDelta 
<!-- EPO <DP n="14"/>-->
 SADscore(index) = SAD(Correlation_lmage_1 , </p><p id="p0084" num="0084">Correlation_lmage_2, XShift,YShift,ZScaleFactor) </p><p id="p0085" num="0085"> XShiftArray(index) = XShift </p><p id="p0086" num="0086"> YShiftArray(index) = YShift </p><p id="p0087" num="0087"> ZScaleArray(index) = ZScaleFactor </p><p id="p0088" num="0088"> index = index+1 </p><p id="p0089" num="0089">Next YShift </p><p id="p0090" num="0090">Next XShift </p><p id="p0091" num="0091">Next ZScaleFactor </p><p id="p0092" num="0092">This routine allows all the possible scaling and X and Y shifts to be checked and a resulting SADscore to be determined and stored in an array SADscore. The SADscore is calculated by the function SAD. The scores are then checked to determine the lowest, which equates to the best match. The checking of the scores may be carried out as follows: </p><p id="p0093" num="0093">Numlmages=index-1 # </p><p id="p0094" num="0094"> LowestSAD = SADscore(O) </p><p id="p0095" num="0095"> BestSAD = 0 </p><p id="p0096" num="0096"> For I = 1 to Numlmages </p><p id="p0097" num="0097"> If SADscore(l) &lt; LowestSAD THEN LowestSAD=SADscore(l) ; BestSAD=l </p><p id="p0098" num="0098"> Next I 
<!-- EPO <DP n="15"/>-->
 Having identified the best match (BestSAD), the zoom and shift operations which contain the best match between the reference image and the acquired image are also identified. Hence, XShiftArray(BestSAD), YShiftArray(BestSAD) and ZScaleArray(BestSAD) contain the X, Y, Z movement vectors respectively. </p><p id="p0099" num="0099"> In one embodiment, XShiftMax and YShiftMax may be 2 pixels and XShiftDelta and YShiftDelta may be 1/32 pixel. Thus the number of comparisons for a single zoom factor in this example is: (2*2*32 )<sup>2</sup> = 128<sup>2</sup> = 16,384. In one embodiment, the values in the Z direction may be as follows: Zoom_Min = 0.5, Zoom_Max = 2 and Zoom_Delta = 0.1 giving 15 zoom factors. For the example presented the total number of zoom, shift and correlation searches will be of the order of: 15 16,384 = 246 K operations. The above embodiment is one example of possible </p><p id="p0100" num="0100">conditions, but others are equally relevant depending on the system and application. </p><p id="p0101" num="0101"> The search algorithm described above "covers all" search options </p><p id="p0102" num="0102">(positions and scaling of image) and can require substantial computation effort. As the fingermouse needs to operate at high frame rates (1 KHz to 10 KHz) there is only a relatively short time available to perform the necessary processing. The time scales at these frame rates are of the order of 100 s to 1 ms. In order to reduce the computational effort, the present invention proposes using the previous motion vector (X, Y and Z) as the starting point of any further search. The search then continues by progressing away from the starting point in one or each direction to conduct the search with steps of increasing accuracy. This reduces the area through which the searching progresses and identifies the motion vector with much less computation effort. This may be carried out as described in the following steps. 
<!-- EPO <DP n="16"/>-->
 1 . XO, Y0, ZO = previous motion vector </p><p id="p0103" num="0103"> 2. Set search offsets to be DX1 , DY1 , DZ1 ; where typically DX1 = XO, DY1 = YO and DZO = ZO </p><p id="p0104" num="0104"> 3. Produce SADscores with X0±DX1 ;0, Y0±DY1 ;0, Z0±DZ1 ;0. </p><p id="p0105" num="0105"> Further detail of this is shown by reference to Figure 8, which shows each of the 27 possible SAD runs </p><p id="p0106" num="0106"> 4. Check the SAD results for each of the 27 tests and find the one which has the best correlation or the lowest SADscore. This gives an indication of the best shift to be used for the next iteration. For example, SAD run 16 produces the lowest SAD, hence X1 = X0- DX1 , Y1 = Y1 +DY1 , Z1 = ZO are the best set of coordinates for starting the next pass. </p><p id="p0107" num="0107"> 5. Repeat steps 3 &amp; 4 with the shift values halved, i.e. DX2 = DX1/2, DY2=DY1/2 and DZ2=DZ1/2. </p><p id="p0108" num="0108">As the search is progressing in a repeating cycle, each time the best SADscore, and thus motion vector, is identified and the next repeat produces a more accurate result. Experimentation suggests five repeats of steps 3 and 4 are needed to accurately determine the motion vector. The search distance for each repeat is essentially halved. As such the number of operations, such as zoom, correlation and shift, which occur with the present invention, is significantly reduced when compared with previous systems. The total number of operations to find a match in the above example is 5 27 = 135 operations. This compares favorably with the previous "cover all" algorithm which requires over 245K operations.</p><p id="p0109" num="0109">With the existing finger mice, the user's finger is always on the surface of the module and hence the illumination and brightness seen by the sensor remains substantially constant. However, with the present invention, the 
<!-- EPO <DP n="17"/>-->
 distance between the user's finger and the surface of the fingermouse module varies. As a result the amount of light from the light source in the module will vary. In addition, the amount of light received at the sensor will reduce when the distance between the module surface and the finger increases. This change in intensity of the image could degrade the above described matching algorithm and hence impair tracking. As such the present invention applies a technique to compensate for any change in intensity as shown in Figure 9. </p><p id="p0110" num="0110">As can be seen in Figure 9 an optical path is shown and is essentially the same as that shown in figure 7. As such, like elements have the same reference number and their functions are as described above. Figure 9 further includes a gain stage 900 between the reference image 706 and the image scaling algorithm 708. As the three operations gain 900, image scaling 708 and image shift 710 are linear, it is possible to have different sequence of these operations without any impact to the overall result. The amount of image gain is pre-determined by the control logic, in conjunction with the image scaling factor. If the image scaling is &lt;1 , the image is being shrunk and thus the acquired sensor image is smaller than the reference as a result of the finger pointer being further away. As such, the acquired sensor image will be dimmer than the reference and the gain factor will be &gt;1 . Similarly, if the scaling is &gt;1 , the image is being expanded and thus the acquired sensor image is larger than the reference as a result of the finger or pointer being closer. As a result the sensor image is brighter than the reference and the gain factor will be &lt;1 . </p><p id="p0111" num="0111">The amount of gain may calculated by a simple 1/r<sup>2</sup> function based on basic physics where r is the distance between the illumination source and the object (e.g. finger or pointer). Alternatively, the amount of gain may include a function which accounts for the complete system performance. For example, the complete system performance may vary based the alignment of the light source with the imaging system. In this case the 
<!-- EPO <DP n="18"/>-->
 illumination will vary more than standard 1/r<sup>2</sup> as the object would move away from the centre of the illumination and become darker as it moves away from the sensing module. This would affect performance and could be represented by an appropriate function to compensate for the effect. Other factors may also contribute to the performance and be included in a function to compensate therefore as described above. Other factors may include: the beam profile of the illumination. This will be a peak at the centre of the illumination beam (producing a bright image) but will be lower (and produce a darker image) away from the centre of the beam. The imaging optics will have a finite depth of field and if the finger is away from the optimal focus distance, the sharpness will be reduced. Optionally, there may be a means to sharpen (e.g. increase the high spatial frequency content of) the image to reduce the defocus when the object (finger) is further away from the optimal distance. </p><p id="p0112" num="0112">The navigation device is intended for use in an optical navigation device; however it will be appreciated that the navigation device could be used in any appropriate device, for example fingerprint reader or Lab-on-chip / Bio-Optical sensor systems (which detect chemi-fluorescence for medical and /or bio-testing applications). </p><p id="p0113" num="0113">The optical navigation device may be used in any suitable devices such as a mobile or smart telephone, other personal or communications devices, a computer, a remote controller, access modules for doors and the like, a camera or any other suitable device. </p><p id="p0114" num="0114"> There are many variations of the present invention which will be </p><p id="p0115" num="0115">appreciated by the person skilled in the art and which are included within the scope of the present invention. 
</p></description><claims mxw-id="PCLM44852218" ref-ucid="WO-2012117232-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="19"/>--> Claims </claim-statement><claim id="clm-0001" num="1"><claim-text>1 . An optical navigation device for detecting movement of a pointer, such as a finger, in three dimensions, the optical navigation device comprising a sensor adapted to obtain images of the pointer which have been illuminated by an illumination source and an image scaling module configured to determine the difference in size between a first image acquired by the image sensor and second image acquired by the image sensor to thereby determine the difference in height of the pointer from the first image to the second image. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The optical navigation device of claim 1 , further comprising an image shift module for determining the difference between the first image and the second image to thereby determine a motion vector between the first and second image. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The optical navigation device of claim 1 or claim 2, wherein the difference is determined by a sum of absolute difference (SAD) process in which image properties are varied to allow the first and second image to be compared with one another. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The optical navigation device of claim 3, further comprising a correlation module for determining a minimum SAD value from the SAD process, which minimum SAD value indicated that the first and second image are a best match, and thereafter deriving the image properties associated with the minimum SAD value. <!-- EPO <DP n="20"/>--> </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The optical navigation device of claim 4, wherein at least one of the image properties associated with the minimum SAD value represents the most likely motion vector between the first and second image. </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The optical navigation device of claim 4 or claim 5, wherein at least one of the image properties associated with the minimum SAD value represents the most likely difference in height of the pointer between the first and second image. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The optical navigation device of any preceding claim, further comprising a gain module for determining and compensating for system performance variations. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The optical navigation device of claim 7, wherein the system performance variations include brightness. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. The optical navigation device or any preceding claims, wherein the optical navigation device is a mouse. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The device of claim 9, wherein the mouse is a fingermouse. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>1 1 . An apparatus including the device of any one of claims 1 to 10. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. The apparatus of claim 1 1 , wherein the apparatus is a computer. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The apparatus of claim 1 1 , wherein the apparatus is a phone. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The apparatus of claim 1 1 , wherein the apparatus is a camera. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The apparatus of claim 1 1 , wherein the apparatus is a smart phone. <!-- EPO <DP n="21"/>--> </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. The apparatus of claim 1 1 , wherein the apparatus is a remote controller. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
