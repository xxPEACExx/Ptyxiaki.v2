<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2682898-A1" country="EP" doc-number="2682898" kind="A1" date="20140108" family-id="48747958" file-reference-id="318320" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146584854" ucid="EP-2682898-A1"><document-id><country>EP</country><doc-number>2682898</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13174747-A" is-representative="YES"><document-id mxw-id="PAPP154847046" load-source="docdb" format="epo"><country>EP</country><doc-number>13174747</doc-number><kind>A</kind><date>20130702</date><lang>EN</lang></document-id><document-id mxw-id="PAPP170521196" load-source="docdb" format="original"><country>EP</country><doc-number>13174747.9</doc-number><date>20130702</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140556097" ucid="JP-2012149537-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2012149537</doc-number><kind>A</kind><date>20120703</date></document-id></priority-claim><priority-claim mxw-id="PPC140555122" ucid="JP-2013133576-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2013133576</doc-number><kind>A</kind><date>20130626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989322755" load-source="docdb">G06K   9/00        20060101AFI20130925BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989323577" load-source="docdb">G06T   7/00        20060101ALI20130925BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-2027178748" load-source="docdb" scheme="CPC">G06T2207/20021     20130101 LA20150825BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2027179270" load-source="docdb" scheme="CPC">G06T   7/0002      20130101 LI20150825BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2027179538" load-source="docdb" scheme="CPC">G06T2207/30252     20130101 LA20150825BHEP        </classification-cpc><classification-cpc mxw-id="PCL1991317370" load-source="docdb" scheme="CPC">G06K   9/00791     20130101 FI20140110BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132359195" lang="DE" load-source="patent-office">An Lichtscheibe befestigter Partikeldetektor, Verfahren für an Lichtscheibe befestigten Partikeldetektor und Fahrzeugsystem</invention-title><invention-title mxw-id="PT132359196" lang="EN" load-source="patent-office">Lens-attached matter detector, lens-attached matter detection method, and vehicle system</invention-title><invention-title mxw-id="PT132359197" lang="FR" load-source="patent-office">Détecteur de matières à fixation de lentille, procédé de détection de matière à fixation de lentille et système de véhicule</invention-title><citations><patent-citations><patcit mxw-id="PCIT242942474" load-source="docdb" ucid="DE-10322087-A1"><document-id format="epo"><country>DE</country><doc-number>10322087</doc-number><kind>A1</kind><date>20041202</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942475" load-source="docdb" ucid="JP-2003259358-A"><document-id format="epo"><country>JP</country><doc-number>2003259358</doc-number><kind>A</kind><date>20030912</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942476" load-source="docdb" ucid="US-20060115121-A1"><document-id format="epo"><country>US</country><doc-number>20060115121</doc-number><kind>A1</kind><date>20060601</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942477" load-source="docdb" ucid="US-20070115357-A1"><document-id format="epo"><country>US</country><doc-number>20070115357</doc-number><kind>A1</kind><date>20070524</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242942478" load-source="docdb" ucid="WO-2010038223-A1"><document-id format="epo"><country>WO</country><doc-number>2010038223</doc-number><kind>A1</kind><date>20100408</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit></patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR919546033" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CLARION CO LTD</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR919529219" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CLARION CO., LTD.</last-name></addressbook></applicant><applicant mxw-id="PPAR919014490" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Clarion Co., Ltd.</last-name><iid>101216579</iid><address><street>7-2, Shintoshin Chuo-ku Saitama-shi</street><city>Saitama 330-0081</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919518938" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>IRIE KOTA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919510599" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>IRIE, KOTA</last-name></addressbook></inventor><inventor mxw-id="PPAR919017529" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>IRIE, KOTA</last-name><address><street>c/o Clarion Co., Ltd. 7-2, Shintoshin, Chuo-ku, Saitama-shi,</street><city>Saitama, 330-0081</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919521950" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>MURAMATSU SHOJI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919533655" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>MURAMATSU, SHOJI</last-name></addressbook></inventor><inventor mxw-id="PPAR919014623" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>MURAMATSU, SHOJI</last-name><address><street>c/o Hitachi, Ltd., Intellectual Property Group, 6-1, Marunouchi 1-chome, Chiyoda-ku,</street><city>Tokyo, 100-8220</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919524216" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>KATO KENJI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919523804" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>KATO, KENJI</last-name></addressbook></inventor><inventor mxw-id="PPAR919018499" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>KATO, KENJI</last-name><address><street>c/o Hitachi Advanced Digital, Inc. 292 Yoshida-cho, Totsuka-ku, Yokohama-shi,</street><city>Kanagawa, 244-0817</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919509551" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>KIYOHARA MASAHIRO</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR919532086" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>KIYOHARA, MASAHIRO</last-name></addressbook></inventor><inventor mxw-id="PPAR919016366" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>KIYOHARA, MASAHIRO</last-name><address><street>c/o Hitachi, Ltd., Intellectual Property Group, 6-1, Marunouchi 1-chome, Chiyoda-ku,</street><city>Tokyo, 100-8220</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919013811" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Ahner, Philippe</last-name><suffix>et al</suffix><iid>101021902</iid><address><street>BREVALEX 95 rue d'Amsterdam</street><city>75378 Paris Cedex 8</city><country>FR</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS549907295" load-source="docdb">AL</country><country mxw-id="DS549812221" load-source="docdb">AT</country><country mxw-id="DS549907297" load-source="docdb">BE</country><country mxw-id="DS549741560" load-source="docdb">BG</country><country mxw-id="DS549739929" load-source="docdb">CH</country><country mxw-id="DS549739569" load-source="docdb">CY</country><country mxw-id="DS549812234" load-source="docdb">CZ</country><country mxw-id="DS549907298" load-source="docdb">DE</country><country mxw-id="DS549739570" load-source="docdb">DK</country><country mxw-id="DS549739571" load-source="docdb">EE</country><country mxw-id="DS549811690" load-source="docdb">ES</country><country mxw-id="DS549741581" load-source="docdb">FI</country><country mxw-id="DS549741582" load-source="docdb">FR</country><country mxw-id="DS549907299" load-source="docdb">GB</country><country mxw-id="DS549739572" load-source="docdb">GR</country><country mxw-id="DS549907300" load-source="docdb">HR</country><country mxw-id="DS549812235" load-source="docdb">HU</country><country mxw-id="DS549739930" load-source="docdb">IE</country><country mxw-id="DS549739573" load-source="docdb">IS</country><country mxw-id="DS549741583" load-source="docdb">IT</country><country mxw-id="DS549739574" load-source="docdb">LI</country><country mxw-id="DS549738268" load-source="docdb">LT</country><country mxw-id="DS549739271" load-source="docdb">LU</country><country mxw-id="DS549738269" load-source="docdb">LV</country><country mxw-id="DS549738270" load-source="docdb">MC</country><country mxw-id="DS549739272" load-source="docdb">MK</country><country mxw-id="DS549739273" load-source="docdb">MT</country><country mxw-id="DS549741584" load-source="docdb">NL</country><country mxw-id="DS549812350" load-source="docdb">NO</country><country mxw-id="DS549739931" load-source="docdb">PL</country><country mxw-id="DS549739274" load-source="docdb">PT</country><country mxw-id="DS549741589" load-source="docdb">RO</country><country mxw-id="DS549739275" load-source="docdb">RS</country><country mxw-id="DS549739932" load-source="docdb">SE</country><country mxw-id="DS549812236" load-source="docdb">SI</country><country mxw-id="DS549812351" load-source="docdb">SK</country><country mxw-id="DS549739933" load-source="docdb">SM</country><country mxw-id="DS549738272" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128673026" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A lens-attached matter detector includes an edge extractor configured to create an edge image based on an input image, divide the edge image into a plurality of areas including a plurality of pixels, and extract an area whose edge intensity is a threshold range as an attention area, a brightness distribution extractor configured to obtain a brightness value of the attention area and a brightness value of a circumference area, a brightness change extractor configured to obtain the brightness value of the attention area and the brightness value of the circumference area for a predetermined time interval, and obtain a time series variation in the brightness value of the attention area based on the brightness value of the attention area, and an attached matter determiner configured to determine the presence or absence of attached matter based on the time series variation in the brightness value of the attention area.</p></abstract><abstract mxw-id="PA128737171" lang="EN" source="EPO" load-source="docdb"><p>A lens-attached matter detector includes an edge extractor configured to create an edge image based on an input image, divide the edge image into a plurality of areas including a plurality of pixels, and extract an area whose edge intensity is a threshold range as an attention area, a brightness distribution extractor configured to obtain a brightness value of the attention area and a brightness value of a circumference area, a brightness change extractor configured to obtain the brightness value of the attention area and the brightness value of the circumference area for a predetermined time interval, and obtain a time series variation in the brightness value of the attention area based on the brightness value of the attention area, and an attached matter determiner configured to determine the presence or absence of attached matter based on the time series variation in the brightness value of the attention area.</p></abstract><description mxw-id="PDES63959014" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">BACKGROUND</heading><heading id="h0002">Field of the Invention</heading><p id="p0001" num="0001">The present invention relates to a lens-attached matter detector which detects attached matter on a lens of a camera, for example, a lens-attached matter detection method, and a vehicle system including the lens-attached matter detector.</p><heading id="h0003">Description of the Related Art</heading><p id="p0002" num="0002">A system is known which detects a vehicle or a pedestrian existing in a blind area behind a vehicle by an in-vehicle camera when changing a lane, so as to draw a driver's attention by an alarm or an indication light. A system is also known which detects a white line on a road by a camera, so as to draw a driver's attention by an alarm or an indication light when a driver drifts from a lane. Further, a system which sounds a red alert by detecting a vehicle getting closer from behind and a system which assists parking by detecting a parking frame are also known. Hereinafter, an in-vehicle system using an image recognition technique is referred to as an image-sensing application.</p><p id="p0003" num="0003">A camera for use in such a system may be provided outside a vehicle, and is used while the vehicle is running. For this reason, a stain such as splash of dirt attaches onto a lens surface. When such a stain is prominent, the detection accuracy of a subject by a camera is deteriorated. For this reason, the performance<!-- EPO <DP n="2"> --> of an image-sensing application such as a system which detects a vehicle or a pedestrian, or a system which detects a white line may be affected. Thus, when a driver is aware of a stain, such a stain is eliminated from a lens by spraying air or cleaning liquid.</p><p id="p0004" num="0004">However, a driver is sometimes not aware of a stain. For this reason, a stain detector, which automatically detects a stain on a lens, so as to encourage cleaning by informing a driver of the generation of the stain or automatically clean the stain, has been developed (refer to, for example, <patcit id="pcit0001" dnum="JP2003259358A"><text>JP 2003-259358A</text></patcit>). In the stain detector described in <patcit id="pcit0002" dnum="JP2003259358A"><text>JP 2003-259358A</text></patcit>, a concentration value is obtained for each of two images shot by a camera in different timings, and a difference of these values is extracted to be integrated, so as to obtain an integrated image. In such an integrated image, an area having a predetermined concentration value or below, namely, an area without having a change over time is determined as an area to which a stain is attached. When this stain-attached area is an area for use in an image process by a camera, it is determined that a lens is stained, and a stain-attached signal is sent to a driver or a cleaner.</p><p id="p0005" num="0005">However, in the stain detector described in <patcit id="pcit0003" dnum="JP2003259358A"><text>JP 2003-259358A</text></patcit>, a landscape or an object to be shot (for example, long guide rail, parapet, or side walk) having less change over time has an increased concentration value of an integrated image with a difference, causing false-determination as a stain. Therefore, the development of a highly accurate technique which can detect only attached matter on a lens such as a stain is requested.<!-- EPO <DP n="3"> --></p><heading id="h0004">SUMMARY</heading><p id="p0006" num="0006">The present invention has been made in view of the above circumferences, and an object of the present invention is to provide a lens-attached matter detector, lens-attached matter detection method capable of detecting only lens-attached matter such as dirt, dust or waterdrops with a high accuracy, and a vehicle system having the lens-attached matter detector.</p><p id="p0007" num="0007">To attain the above object, one embodiment of the present invention provides a lens-attached matter detector, including: an edge extractor configured to create an edge image based on an input image from an imager having a lens, divide the edge image into a plurality of areas including a plurality of pixels, and extract an area whose edge intensity is a threshold range as an attention area; a brightness distribution extractor configured to obtain a brightness value of the attention area and a brightness value of a circumference area; a brightness change extractor configured to obtain the brightness value of the attention area and the brightness value of the circumference area obtained by the brightness distribution extractor for a predetermined time interval, and obtain a time series variation in the brightness value of the attention area based on the brightness value of the attention area for the predetermined time interval; and an attached matter determiner configured to determine the presence or absence of attached matter based on the time series variation in the brightness value of the attention area.</p><p id="p0008" num="0008">Moreover, one embodiment of the present invention provides a lens-attached matter detection method which is executed by the above lens-attached<!-- EPO <DP n="4"> --> matter detector, including an edge extraction process of creating an edge image based on an input image, dividing the edge image into a plurality of areas including a plurality of pixels, and extracting an area whose edge intensity is a threshold range as an attention area; a brightness value distribution extraction process of obtaining a brightness value of the attention area and a brightness value of a circumference area of the attention area; a brightness value extraction process of obtaining the brightness value of the attention area and the brightness value of the circumference area obtained by the brightness value distribution extraction process for a predetermined time interval, and obtaining a time series variation in the brightness value of the attention area based on the brightness value of the attention area for the predetermined time interval; and an attached matter determination process of determining the presence or absence of attached matter based on the time series variation in the brightness value of the attention area.</p><p id="p0009" num="0009">Furthermore, one embodiment of the present invention provides a vehicle system, including the above lens-attached matter detector; an imager provided in a vehicle, having a lens, and configured to image a circumference of the vehicle; and at least one application configured to operate based on detection information of the lens-attached matter detected by the lens-attached matter detector relative to the input image shot by the imager.</p><heading id="h0005">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0010" num="0010">The accompanying drawings are included to provide further understanding of the invention, and are incorporated in and constitute a part of this specification.<!-- EPO <DP n="5"> --> The drawings illustrate embodiments of the invention and, together with the specification, serve to explain the principle of the invention.
<ul><li><figref idrefs="f0001">FIG. 1</figref> is a schematic view illustrating a vehicle equipped with a vehicle system including a lens-attached matter detector according to first and second embodiments of the present invention.</li><li><figref idrefs="f0002">FIG. 2</figref> is a block diagram illustrating the configuration of the vehicle system including the lens-attached matter detector according to the first and second embodiments of the present invention.</li><li><figref idrefs="f0003">FIGS. 3A, 3B</figref> are views describing a step of blocking a process target image, <figref idrefs="f0003">FIG. 3A</figref> illustrates areas for use in vehicle detection and lane detection in an input image, and <figref idrefs="f0003">FIG. 3B</figref> illustrates blocked areas including these areas.</li><li><figref idrefs="f0004">FIG. 4</figref> is a view describing a relationship between an input image (a) input to an edge extractor and an edge image (b) output from the edge extractor.</li><li><figref idrefs="f0005">FIG. 5</figref> is a view describing a step of obtaining a brightness average of process blocks.</li><li><figref idrefs="f0006">FIGS. 6A, 6B</figref> are views describing a step of counting a bright circumference block, <figref idrefs="f0006">FIG. 6A</figref> illustrates an attention block and its circumference blocks, and <figref idrefs="f0006">FIG. 6B</figref> illustrates circumference blocks which are targets for the counting step in the second embodiment.</li><li><figref idrefs="f0007">FIG. 7</figref> is a view describing a step of counting the number of weak edges, and also is a schematic view illustrating a block having a weak edge.</li><li><figref idrefs="f0007">FIG. 8</figref> is a view describing accumulation of brightness value information in chronological order, and illustrating an image of record which is recorded in a memory.</li><li><figref idrefs="f0008">FIGS. 9A, 9B, 9C</figref> are views describing a mask process for a lane, <figref idrefs="f0008">FIG. 9A</figref><!-- EPO <DP n="6"> --> illustrates a schematic view of an edge filter for a left area, <figref idrefs="f0008">FIG. 9B</figref> illustrates a schematic view of an edge filter for a right area, and <figref idrefs="f0008">FIG. 9C</figref> illustrates an image of a lane by real view.</li><li><figref idrefs="f0009">FIG. 10</figref> is a view describing a mask process for a lane, and illustrating a step of generating an edge image by applying a filter in each of the right and left areas.</li><li><figref idrefs="f0010">FIG. 11</figref> is a view describing a mask process for a track, and illustrating an input image (a) having a track, an image (b) in which the input image is divided into a plurality of frames, and the edges are detected, and a track mask image (c) generated by a mask processor, and an output image.</li><li><figref idrefs="f0011">FIGS. 12A, 12B</figref> are views describing a mask process for a light source, <figref idrefs="f0011">FIG. 12A</figref> illustrates an input image having a west sun, and <figref idrefs="f0011">FIG. 12B</figref> illustrates a light source mask image generated by the mask processor.</li><li><figref idrefs="f0012">FIG. 13</figref> is a view describing a mask process for an own vehicle shadow, and illustrating an input image (a) having an own vehicle shadow and an own vehicle shadow mask image (b) generated by the mask processor.</li><li><figref idrefs="f0013">FIG. 14</figref> is a view describing a mask process for a vehicle body, and illustrating an input image (a) having a vehicle body and a vehicle body mask image (b) generated by the mask processor.</li><li><figref idrefs="f0014">FIG. 15</figref> is a schematic view illustrating a weak edge image before the mask process and a weak edge image after the mask process.</li><li><figref idrefs="f0015">FIG. 16</figref> is as flowchart illustrating flow of a lens-attached matter detection process in the first embodiment.</li><li><figref idrefs="f0016">FIG. 17</figref> is a flowchart illustrating flow of a process area-obtaining process.</li><li><figref idrefs="f0016">FIG. 18</figref> is a flowchart illustrating flow of an edge extraction process.</li><li><figref idrefs="f0017">FIG. 19</figref> is a flowchart illustrating brightness distribution extraction process.<!-- EPO <DP n="7"> --></li><li><figref idrefs="f0017">FIG. 20</figref> is a flowchart illustrating flow of an attached matter determination process.</li><li><figref idrefs="f0018">FIG. 21</figref> is a flowchart illustrating flow of a lens-attached matter detection process in the second embodiment.</li></ul></p><heading id="h0006">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading><p id="p0011" num="0011">Hereinafter, an embodiment of a vehicle system including a lens-attached matter detector according to the present invention will be described with reference to the drawings.</p><heading id="h0007">First Embodiment</heading><p id="p0012" num="0012"><figref idrefs="f0001">FIG. 1</figref> is a view illustrating a running vehicle 1 equipped with a vehicle system including a lens-attached matter detector according to the first embodiment. A camera 20 is provided in the back of the vehicle 1. The camera 20 has a shooting angle of view w which can shoot back over a wide range, so that the camera 20 can shoot a blind area of a driver. A vehicle detection application or the like is performed by using the camera 20. When the vehicle 1 runs in a traffic lane L<sub>2</sub> of a three-lane road 2 having lanes L<sub>1</sub>-L<sub>3</sub>, and changes a traffic lane from the traffic lane L<sub>2</sub> to the traffic lane L<sub>1</sub>, another vehicle 3 in the blind area in the back of the vehicle 1 is detected. The vehicle detection application which detects another vehicle 3 starts to beep or calls attention to a driver by lighting an indication light.</p><p id="p0013" num="0013">In this embodiment, the lens-attached matter detector is used for the camera 20 provided in the back of the vehicle 1, but the present invention is not limited<!-- EPO <DP n="8"> --> thereto. When a camera is provided in the front or the side of a vehicle, any lens-attached matter of these cameras can be detected. The lens-attached matter detector can be configured to inform which lens has attached matter.</p><p id="p0014" num="0014">A vehicle system 10 according to the first embodiment will be described with reference to <figref idrefs="f0002">FIG. 2</figref>. The vehicle system 10 according to the first embodiment is installed in the vehicle 1, and includes the camera (imager) 20 which images an image at the back of the own vehicle 1, a lens-attached matter detector 30 which detects lens-attached matter, a cleaning function (cleaner) 51 which automatically cleans based on the result detected by the lens-attached matter detector 30, an alarm generator 53 which informs a driver of lens-attached matter by alarm, a display 54 which informs a driver a lens-attached matter by a lamp or character display, an image-sensing application 52 such as vehicle detection or lane detection, and a memory 60 in which a program or data for use in each process is stored.</p><p id="p0015" num="0015">The camera 20 includes a lens 21, an imaging element 22 which converts an image imaged by the lens 21 into analogue electric signals, and a gain adjuster 23 which adjusts a gain of the image obtained by the imaging element 22. The memory 60 can be a hard disk or an external memory which stores programs or data, or can be a memory which temporarily stores data such as RAM or ROM.</p><p id="p0016" num="0016">The lens-attached matter detector 30 includes an image processor 31 which accumulates image information by executing various processes to the image input from the camera 20, an attached matter determiner 32 which determines the presence or absence of lens-attached matter based on the information from the<!-- EPO <DP n="9"> --> image processor 31, a vehicle information obtainer 33 which obtains vehicle information such as a vehicle speed, an output information generator 34 which generates output information to another processor based the presence or absence of lens-attached matter, and a memory 60 which stores the image information obtained by the image processor 31, the determination result by the attached matter determiner 32 or the like. In the lens-attached matter detector 30 of the first embodiment, the memory 60 provided in the vehicle system 10 is also used as a memory. The lens-attached matter detector 30 or the image-sensing application 52 can be a program which is executed by a computer having a CPU, memory, I/O, timer or the like. In the lens-attached matter detector 30, the lens-attached matter detection process is programmed, and a repetition process is executed at a predetermined cycle.</p><p id="p0017" num="0017">As illustrated by the solid line in <figref idrefs="f0002">FIG. 2</figref>, the image processor 31 of the first embodiment includes a process area obtainer 35 which obtains the input image from the camera 20, and sets a process area to divide the process area into a plurality of blocks, an edge extractor 36 which extracts an area having a weak edge intensity from the input image, a brightness distribution extractor 37 which extracts a brightness distribution by obtaining a brightness value of the area having a weak edge intensity and the area therearound, and a brightness change extractor 38 which obtains a time series variation in a brightness value based on the brightness value accumulated through time.</p><p id="p0018" num="0018">A lens-attached matter detection process in the lens-attached matter detector 30 according to the first embodiment will be described with reference to<!-- EPO <DP n="10"> --> <figref idrefs="f0003 f0004 f0005 f0006 f0007">FIGS. 3-8</figref> describing processes and the flowcharts of <figref idrefs="f0015 f0016 f0017">FIGS. 16-20</figref>. In this case, processing steps for detecting dirt as lens-attached matter will be described.</p><p id="p0019" num="0019">As illustrated in the flowchart in <figref idrefs="f0015">FIG. 16</figref>, the lens-attached matter detection process of the first embodiment includes a vehicle information-obtaining process (step S10), vehicle speed determination process (step S20), process area-obtaining process (step S30), edge extraction process (step S40), brightness distribution extraction process (step S50), process time determination process (step S60), brightness change extraction process (step S70), attached matter determination process (step S80), and output information generation process (step S90). These processes are controlled by a controller (CPU).</p><heading id="h0008">(Vehicle Information-Obtaining Process and Vehicle Speed Determination Process)</heading><p id="p0020" num="0020">The vehicle information-obtaining process (Step S10) and the vehicle speed determination process (Step S20) are executed by the vehicle information obtainer 33. The vehicle speed obtainer 33 obtains speed information sent from a speed sensor or the like (Step S10). Next, it is determined whether or not the vehicle speed is a predetermined value or more (for example, 1 km / h or more) (Step S20). When it is determined that the vehicle speed is a predetermined value or more (YES), the subroutine proceeds to the next process area-obtaining process (Step S30). When it is determined that the vehicle speed is less than 10 km / h (NO), the subroutine proceeds to the output information generation process (Step S90) after skipping the processes in Steps S30-S80. After that, the whole lens-attached matter detection process is completed. More particularly, the attachment of the dirt easily occurs while the vehicle 2 is running. For this reason, the attachment of the dirt hardly<!-- EPO <DP n="11"> --> occurs while the vehicle 2 is not running or is slowly running, in which case it is determined that it is not necessary to execute the lens-attached matter detection process.</p><p id="p0021" num="0021">In the first embodiment, although the criteria for determining the continuation or completion of the lens-attached matter detection process is one predetermined value as described above, the present invention is not limited thereto. For example, the start and the completion can be determined based on different predetermined speeds, or the engine startup or the shift change can be used as the determination criteria. Moreover, the continuation or the completion of the lens-attached matter detection process can be determined based on vehicle information of a weather condition (for example, wet day) or a road condition (off-road running) which easily occur attachment of dirt.</p><heading id="h0009">(Process Area-Obtaining Process)</heading><p id="p0022" num="0022">The process area-obtaining process (Step S30) is executed by the process area obtainer 35. The details of the process area-obtaining process will be described with reference to the flowchart in <figref idrefs="f0016">FIG. 17</figref>. The process area obtainer 35 firstly obtains the image input from the camera 20, and reduces the image (Step S31). However, the present invention is not limited thereto, and the image can be used without being reduced. However, the process speed as well as the memory capacity for image information or the like can be reduced by using such a reduced image in the subsequent processes.</p><p id="p0023" num="0023">Next, the process area obtainer 35 sets a process target area from a reduced<!-- EPO <DP n="12"> --> monochrome image (Step S32). The process target area can be the entire input image, but an area including the process area of the image-sensing application 52 for use in this embodiment (for example, a process area B in vehicle detection, process area L in traffic lane detection and area which determines the execution of the automatic cleaning in the cleaner 51) can be the process target area 102 in the input image 100 in this embodiment as illustrated in <figref idrefs="f0003">FIGS. 3A, 3B</figref>. The accuracy of the image-sensing application as well as the process efficiency of the lens-attached matter detection process can be improved by detecting the lens attachment in the position including at least the process area of various functions or the image-sensing application 52.</p><p id="p0024" num="0024">Then, this process target area is divided into a plurality of blocks 201 (Step S33) as illustrated in <figref idrefs="f0003">FIG. 3B</figref>. The subsequent processes are performed in each block, so that the subsequent processes can be effectively executed compared to a case which executes the subsequent processes in each pixel. In this embodiment, the size of each block 201 is set in a size of desired dirt to be detected or below. By this setting, only dirt can be securely and effectively detected. The block information of each divided block 201 such as coordinates is stored in the memory 60 in accordance with a block number.</p><heading id="h0010">(Edge Extraction Process)</heading><p id="p0025" num="0025">The edge extraction process (Step S40) is executed by the edge extractor 36. The details of the edge extraction process will be described with reference to the flowchart in <figref idrefs="f0016">FIG. 18</figref>. As illustrated in <figref idrefs="f0016">FIG. 18</figref>, the edge extractor 36 reduces the input image reduced by the process area-obtaining process (Step S30), and obtains a<!-- EPO <DP n="13"> --> black and white gray-scaled image (Step S41). The effective process can be executed by reducing the image as just described. Next, the edge extractor 36 executes the edge extraction of the reduced input image (Step S42). This edge extraction can be executed by using a known technique. A threshold process is executed to edge intensity by using the extracted edge image to extract only an edge required for this process, and then a binarization process is executed (Step S43). In the threshold process, an edge image having only an edge (weak edge) whose intensity p is within a predetermined range, for example, is generated. <figref idrefs="f0004">FIG. 4</figref> illustrates a view illustrating an edge image 103 generated from the input image 100 by the edge extraction process. Dirt portions are extracted from the input image (a) in <figref idrefs="f0004">FIG. 4</figref> as weak edges as illustrated in the edge image (b) in <figref idrefs="f0004">FIG. 4</figref></p><p id="p0026" num="0026">In addition, the dirt when running an off road differs from the dirt when running an on road in a concentration or a tone, or in a degree of weak edge intensity. The edge intensity may differ depending on a type of an attached material. For this reason, a plurality of thresholds is prepared according to such road conditions, other running conditions, types of attached materials, attachment conditions or the like, and it may be determined which threshold is used when executing the lens-attached matter detection.</p><p id="p0027" num="0027">Next, a noise elimination process (Step S44) which eliminates noise presenting in the generated weak edge image is executed. In this embodiment, the following edges are defined as noise.
<ol><li>(a) An edge in a position different from an edge position of a previously extracted edge image.<!-- EPO <DP n="14"> --></li><li>(b) An edge whose area is a predetermined value or below.</li></ol></p><p id="p0028" num="0028">At first, the noise of the above (a) is eliminated by obtaining AND of the edge image extracted in Step S43 and the previously extracted edge image. This is because the edge which is desired to be extracted by the lens-attached matter detection process of the present embodiment is an edge of dirt attached to a lens, and such dirt attached to the lens exists in the same position for a predetermined time, so that a momentarily extracted edge may be noise. In addition, the previously extracted image indicates an edge image obtained by the previously executed edge extraction process. This process is to compare the edge image extracted by the current edge extraction process with the edge image extracted by the previous process because this process is repeated multiple times within a predetermined time. However, in the first process, there is no previous edge image, thus, the elimination of the noise (a) can be skipped.</p><p id="p0029" num="0029">Next, the above edge (b) whose area is a predetermined value or below is eliminated as noise. The edge of the dirt attached to the lens is assumed to be extracted as a block, so that such an independent small edge is determined as not being dirt. By executing the above noise elimination, the lens-attached matter can be detected with high accuracy.</p><heading id="h0011">(Brightness Distribution Extraction Process)</heading><p id="p0030" num="0030">The brightness distribution extraction process (Step S50) is executed by the brightness distribution extractor 37. The details of the brightness distribution extraction process will be described with reference to the flowchart in <figref idrefs="f0017">FIG. 19</figref> and<!-- EPO <DP n="15"> --> <figref idrefs="f0005">FIGS. 5</figref>, <figref idrefs="f0006">6A, 6B</figref>. At first, in the brightness distribution extractor 37, as illustrated in <figref idrefs="f0005">FIG. 5</figref>, an average brightness value I<sub>ave</sub> of a brightness value I of pixels in each block set and divided by the setting process for the process target area is calculated by the following expression (1) (Step S51). In the following expression (1), u, v denote x, y coordinates in a block, N, M denote the number of pixels in the x direction (horizontal) and in the y direction (vertical) in a block, n, m denote a position (relative coordinates) of the x direction (horizontal) and y direction (vertical) of a pixel in a block, and n<sub>min</sub>, m<sub>min</sub> denote the coordinates of the first pixel in a block. In addition, the brightness value uses a gray-scaled image before binarization.</p><p id="p0031" num="0031">[Expression 1] <maths id="math0001" num="(1)"><math display="block"><mi mathvariant="italic">Iave</mi><mfenced separators=""><mi>u</mi><mo>⁢</mo><mi>v</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi mathvariant="normal">N</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>M</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mi>I</mi><mo>⁢</mo><mfenced separators=""><msub><mi>n</mi><mi>min</mi></msub><mo>+</mo><mi>n</mi><mo>,</mo><msub><mi>m</mi><mi>min</mi></msub><mo>+</mo><mi>m</mi></mfenced></math><img id="ib0001" file="imgb0001.tif" wi="139" he="21" img-content="math" img-format="tif"/></maths></p><p id="p0032" num="0032">Next, in the brightness distribution extraction process, an attention block and blocks around the attention block (hereinafter referred to as a circumference block) are selected (Step S52) based on the average brightness value of each block calculated by the above equation (1). A block 201a illustrated by a thick line in <figref idrefs="f0006">FIG. 6A</figref> is the attention block. This attention block 201a is selected from blocks 201 having a low average brightness value. Namely, the brightness value of the area where dirt is attached is likely to be smaller than the average brightness value of the area where dirt is not attached. For this reason, in the lens-attached matter detection process of the present embodiment, this tendency is used as a feature of dirt, and the attachment of dirt is determined in the subsequent attached matter determination process.<!-- EPO <DP n="16"> --></p><p id="p0033" num="0033">The circumference blocks 201b located in the outer circumferences of the blocks 201 adjacent to the attention block 201a in the outer circumference of the attention block 201a are selected as the circumference blocks 201b. This is because the dirt often attaches not only to one block but also to the adjacent blocks, so that it is considered that there is no difference in an average brightness value between the attention block 201a and the blocks adjacent to the attention block. Therefore, the blocks outside the adjacent blocks are selected as the circumference blocks 201b. In addition, the present invention is not limited thereto. When the attachment area of the attached material is small, the blocks 201 adjacent to the attention block 201a can be selected as the circumference blocks 201b. When the attachment area of the attached material is large, the blocks away from the attention block by several blocks can be selected as the circumference blocks 201b.</p><p id="p0034" num="0034">As described above, after selecting the attention block 201a and the circumference blocks 201b, the number of the circumference blocks 201b having an average brightness value higher than that of the attention block 201a is counted (Step S53). In this case, the number of circumference blocks is counted by using the gray-scaled image before binarization. Next, the ratio of the bright circumference block 201b (the number of bright circumference blocks / the total number of circumference blocks) is calculated (Step S54). As a result, regarding a block having dirt (attention block 201a), the ratio of the number of bright circumference blocks having a high average brightness value is increased.</p><p id="p0035" num="0035">Next, the weak edge is counted (Step S55) by using the edge image extracted<!-- EPO <DP n="17"> --> by the edge extraction process (step S40). The counting of this weak edge is executed by using the image after binarization. The dirt attached to the lens is likely to be blurred without being focused and present as a block of weak edges. For this reason, in the lens-attached matter detection process of the present invention, the number of weak edges is counted in each block. <figref idrefs="f0007">FIG. 7</figref> illustrates an image of weak edges. In this block, the weak edges are present in a portion surrounded by the inside white line. The number of the counted weak edges is stored in the memory 60.</p><heading id="h0012">(Process Time Determination Process)</heading><p id="p0036" num="0036">After the above process is completed to one input image, the process time determination process (Step S60) in <figref idrefs="f0015">FIG. 16</figref> is executed by the controller. When it is determined that a predetermined time has passed, the subroutine proceeds to a next brightness change extraction process (Step S70). When a predetermined time has not passed, the process area-obtaining process (Step S30), edge extraction process (Step S40) and brightness distribution extraction process (Step S50) are repeated. By repeating Steps S30-S50 multiple times within a predetermined time, information such as the average brightness value, the ratio of bright circumference blocks and the number of counted weak edges are accumulated in the memory 60 in chronological order. In this embodiment, the process is executed twenty times at intervals of one second to accumulate the information. In addition, this time can be freely set according to the vehicle information such as a type or vehicle speed or other conditions. For example, during off-road running on a rainy day, attachment of dirt occurs very often. Accordingly, it is possible to detect dirt in a short period of time whereas smooth warning is required. For this reason, it is preferable to set a short<!-- EPO <DP n="18"> --> period of time. On the other hand, during off-road running on a sunny day, attachment of dirt hardly occurs. Accordingly, it is preferable to set a long period of time because it is preferable to accumulate information for a long period of time in order to achieve highly accurate detection.</p><heading id="h0013">(Brightness Change Extraction Process)</heading><p id="p0037" num="0037">The brightness change extraction process (Step S70) is executed by the brightness change extractor 38. The details of the brightness change extraction process will be described with referent to <figref idrefs="f0007">FIG. 8</figref>. The dirt attached to the lens hardly moves over time, and the permeability of the dirt is low. For this reason, a variation in the brightness values in the time direction within the range is reduced. In order to find out a change in pixel values in the time direction, the average brightness value (representative brightness value of block) for a predetermined time is accumulated in the memory 60. In other words, the average brightness value obtained in the brightness distribution extraction process of Step S50 is accumulated in the memory 60 with respect to each block. <figref idrefs="f0007">FIG. 8</figref> illustrates an image of record of the average brightness value of each block at regular time intervals.</p><p id="p0038" num="0038">The representative brightness value E with respect to each block is calculated by using the following expression (2) based on the accumulated average brightness value. In the following equation (2), I<sub>ave</sub> denotes an average brightness value of a block, i denotes a block number, and N denotes a predetermined time (process time).</p><p id="p0039" num="0039">[Expression 2]<!-- EPO <DP n="19"> --> <maths id="math0002" num="(2)"><math display="block"><mi>E</mi><mfenced><mi>i</mi></mfenced><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi mathvariant="normal">N</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mi mathvariant="italic">Iave</mi><mfenced><mi>i</mi></mfenced></math><img id="ib0002" file="imgb0002.tif" wi="73" he="24" img-content="math" img-format="tif"/></maths></p><p id="p0040" num="0040">Next, the variance V in the time direction is calculated with respect to each block by using the following expression (3) based on the average brightness value with respect to each block calculated by the above expression (2). Namely, the variance in the time direction is calculated by using the representative brightness value for one cycle previously processed and accumulated in a predetermined time. The variance V of the representative brightness value in the time direction is calculated with respect to each block. In the following expression (3), I<sub>ave</sub> denotes an average brightness value of a block, i denotes a block number, and N denotes a predetermined time (process time).</p><p id="p0041" num="0041">[Equation 3] <maths id="math0003" num="(3)"><math display="block"><mi>V</mi><mfenced><mi>i</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi mathvariant="normal">N</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><msup><mfenced separators=""><mi mathvariant="italic">Iave</mi><mfenced><mi>i</mi></mfenced><mo>-</mo><mi>E</mi><mfenced><mi>i</mi></mfenced></mfenced><mn>2</mn></msup></math><img id="ib0003" file="imgb0003.tif" wi="99" he="19" img-content="math" img-format="tif"/></maths></p><heading id="h0014">(Attached Matter Determination Process)</heading><p id="p0042" num="0042">Upon completion of the process for the accumulated information, next, the attached matter determination process (Step S80) is executed by the adhered matter determiner 32. In this embodiment, since dirt is detected, the attached matter is limited to dirt in this process. However, the attached matter is not limited to dirt in the present invention as a dirt determination target, and dirt can be read as each attached matter, for example, a waterdrop determination target.</p><p id="p0043" num="0043"><!-- EPO <DP n="20"> --> The details of the attached matter determination process will be described with reference to the flowchart in <figref idrefs="f0017">FIG. 20</figref>. At first, the score of the dirt determination target is calculated with respect to each block (Step S81) based on the following information accumulated in the memory 60.
<ol><li>(a) The number of counted weak edges</li><li>(b) Ratio of the number of bright circumference blocks in circumference blocks</li><li>(c) Variance of an average brightness value with respect to each process block</li></ol></p><p id="p0044" num="0044">More particularly, in the attention block in which the number of counted weak edges is less than a threshold, the score of the dirt determination target is not counted due to a low attachment rate of dirt. When the ratio of the number of bright circumference blocks in the circumference blocks is higher than the threshold, the adding rate of the score is increased. Moreover, when the variance of the average brightness value with respect to each block is within a predetermined range, the adding rate of the score is increased.</p><p id="p0045" num="0045">Next, the dirt determination is executed (Step S82) based on the following information. The dirt is determined when any of the following conditions is satisfied.
<ol><li>(a) Score of dirt determination target is a threshold or more</li><li>(b) Variance value with respect to each process block is a threshold or below</li></ol></p><p id="p0046" num="0046">In the present embodiment, after determining dirt, the output information is<!-- EPO <DP n="21"> --> generated in the after-described output information generation process, so as to send the dirt information to the cleaner 51 or the image-sensing application 52 such as vehicle detection or lane detection. In the dirt determination, the following information is calculated as determination results for use in the output information generation process. The calculation result is stored in the memory 60.
<ol><li>(a) Dirt determination result with respect to each process block (attachment of dirt / no attachment of dirt)</li><li>(b) Dirt area on process block (unit: block)</li></ol></p><p id="p0047" num="0047">Next, the following dirt attachment rates are calculated based on the dirt determination result (Step S83). In addition, the following information is an example, and arbitrary information can be calculated by the image-sensing application (for example, moving vehicle detection or parking frame recognition) using dirt attachment information or a way of dealing with dirt.
<ol><li>(a) Dirt attachment rate in process area in lane detection</li><li>(b) Dirt attachment rate in process area in vehicle detection</li><li>(c) Dirt attachment rate in process block of automatic cleaning execution determination area</li></ol></p><heading id="h0015">(Output Information Generation Process)</heading><p id="p0048" num="0048">The output information generation process (Step S90) is executed by the output information generator 34. In this case, the output information for sending to another application or device is generated based on various information calculated in the attached matter determination process. In addition, when the process determined as NO in the vehicle speed determination process (Step S20) is skipped,<!-- EPO <DP n="22"> --> the output information (for example, clear information) indicating that the lens-attached matter detection process is performed is output.</p><p id="p0049" num="0049">For example, the cleaner 51 performs the cleaning process for a lens based on the output information from the lens-attached matter detector 30. The alarm generator 53 and the display 54 draw a driver's attention by generating an alarm and displaying a warning lamp and a warning character. Moreover, in the image-sensing application 52 such as vehicle detection, traffic lane detection or the like, the output information is used for the determination information of each process. A process to be executed by sending information to any of these devices or applications is arbitrary and appropriately selected depending on the configurations of the vehicle system 10.</p><p id="p0050" num="0050">As described above, in the first embodiment, the presence or absence of the attachment of the dirt can be detected with high accuracy. Consequently, for example, dirt on a lens can be effectively informed to a driver, automatic cleaner or the like so as to smoothly eliminate lens-attached matter, and an image-sensing application such as vehicle detection using a camera can be preferably performed so as to enable high-performance vehicle running. Since attached matter is detected by both of a brightness value and variation with time, an imaging object except lens-attached matter such as a landscape, guide rail, parapet, or sidewalk is eliminated from the attention area, so that highly accurate detection can be achieved. As a result, it becomes unnecessary to inform a driver about dirt in an undesirable manner or to perform automatic cleaning, so that effective preferable vehicle running can be effectively performed.<!-- EPO <DP n="23"> --></p><p id="p0051" num="0051">In addition, in the first embodiment, in the edge extraction process or the brightness distribution process, each extraction data is stored and accumulated in the memory 60, and a statistic process is executed in the brightness change extraction process after the elapse of a certain period of time. However, the present invention is not limited thereto, the brightness change extraction process can store the data extracted by the edge extraction process or the brightness distribution process. Moreover, the brightness change extraction process can monitor a time with a timer, and execute the statistic process after the elapse of a certain period of time.</p><heading id="h0016">Second Embodiment</heading><p id="p0052" num="0052">A vehicle system including a lens-attached matter detector according to the second embodiment will be described. The configurations of the vehicle system in the second embodiment are the same as those of the vehicle system in the first embodiment except that the vehicle system in the second embodiment includes a mask processor which controls false detection. Therefore, the configurations of the vehicle system of the second embodiment will be described with reference to <figref idrefs="f0002">FIG. 2</figref>, and the same reference numbers are applied to the configurations which are the same as those in the first embodiment; thus, the detailed description thereof will be omitted.</p><p id="p0053" num="0053">As illustrated in <figref idrefs="f0002">FIG. 2</figref>, the vehicle system 10 including a lens-attached matter detector of the second embodiment includes the camera (imaging device) 20, lens-attached matter detector 30, cleaner 51, image-sensing application 52, alarm generator 53, display 54, and memory 60.<!-- EPO <DP n="24"> --></p><p id="p0054" num="0054">The image processor 31 of the second embodiment includes the process area obtainer 35, edge extractor 36, brightness distribution extractor 37, brightness change detector 38, and a mask processor 39 as illustrated by the solid line and the dotted line in <figref idrefs="f0002">FIG. 2</figref>.</p><p id="p0055" num="0055">The lens-attached matter detection process in the lens-attached matter detector 30 according to the second embodiment will be described with reference to the drawings. In this embodiment, dirt is detected as lens-attached matter. The same description and the same flowcharts are used for the processes similar to those in the first embodiment.</p><p id="p0056" num="0056">As illustrated in the flowchart in <figref idrefs="f0018">FIG. 21</figref>, the lens-attached matter detection process of the second embodiment includes a various vehicle information-obtaining process (Step S110), process prosecution determination process (Step S120), process area-obtaining process (Step S130), edge extraction process (Step S140), mask process (Step S150), brightness distribution extraction process (Step S160), process time determination process (Step S170), brightness change extraction process (Step S180), attached matter determination process (Step S190), and output information generation process (Step S200). These processes are controlled by the controller (CPU).</p><heading id="h0017">(Various Vehicle Information-Obtaining Process)</heading><p id="p0057" num="0057">In the various vehicle information-obtaining process (Step S110), at first, various vehicle information such as vehicle speed information, on-road or off-road<!-- EPO <DP n="25"> --> information, or day-and-night information is obtained. Weather information such as sunny, rain or snow can be obtained. The on-road or off-road information can be obtained by any device. It can be determined based on the image of the camera 20 or the GPS information, for example, or can be determined based on car navigation information. The day-and-night information is determined based on the gain adjustment value by the gain adjuster 23 relative to the image obtained by the imaging element 22 of the camera 20. The increase in the gain adjustment value relative to the image indicates that a dark image is obtained, which can be determined as night-time. On the other hand, the decrease in the gain adjustment value indicates that a bright image is obtained, which can be determined as day-time. The vehicle speed information is obtained from the signal information from the vehicle speed sensor. The weather information can be determined based on rainfall by using a driving signal of a windshield wiper or can be obtained from car navigation information including a communication function, or the like.</p><heading id="h0018">(Process Prosecution Determination Process)</heading><p id="p0058" num="0058">In the process prosecution determination process (Step S120), it is determined whether or not the lens-attached matter detection process is prosecuted based on the obtained various vehicle information. In the day-and-night determination, when it is determined as a daytime based on the gain adjustment, the cleaning determination is performed. When it is determined as night-time or the like in addition to day-time, the processes in Steps S130-S190 are skipped, and the subroutine proceeds to the output information generation process (Step S200), and then, the whole processes are completed.</p><p id="p0059" num="0059"><!-- EPO <DP n="26"> --> In the second embodiment, the lens-attached matter detection process is activated with the engine startup or the running start of the vehicle 1 as a trigger, and the prosecution, completion, and interruption of the above-described lens-attached matter detection process are determined based on day and night and the cleaning condition. However, the present invention is not limited thereto. Any condition can be a determination target according to the running condition or the like. For example, as described in the first embodiment, the determination can be made based on the vehicle speed. The determination can be made based on an on road or an off road, and the present process can be activated in an off road, for example.</p><heading id="h0019">(Process Area-Obtaining Process)</heading><p id="p0060" num="0060">The process area-obtaining process (Step S130) is performed by the process area obtainer 35, and the process target area is set from the input image from the camera 20 so as to be blocked. In this embodiment, an area including the process area in the vehicle detection, the process area in the lane detection and the automatic cleaning determination area is set as a process target area. Since the outline of the process area-obtaining process is similar to that of the first embodiment, the detailed description thereof will be omitted.</p><heading id="h0020">(Edge Extraction Process)</heading><p id="p0061" num="0061">The edge extraction process (Step S140) is performed by the edge extractor 36 and the mask processor 39. The edge extraction process in the present embodiment is a process similar to that in the first embodiment except that the edge extractor 36 extracts an edge in the edge extraction (step S42) by using an image in which a lane marker (hereinafter, referred to as a lane) is masked by the mask<!-- EPO <DP n="27"> --> processor 39. Therefore, the detailed description for the same processes is omitted, and the mask process will be only described in this embodiment. An image of a lane indicated on a road is sharply shot by the camera 20, so that a weak edge is hardly detected in the border with the road. However, when the lane is unclear or tainted, the edge portion of the lane may be blurred, and the weak edge may be sometimes detected, so that it becomes difficult to discriminate the weak edge of the lane from the weak edge of the lens-attached matter. For this reason, the mask process for the lane is executed for improving the detection accuracy of the lens-attached matter.</p><p id="p0062" num="0062">The mask process for the lane will be described with reference to <figref idrefs="f0008 f0009">FIGS. 9A-10</figref>. <figref idrefs="f0008">FIG. 9C</figref> illustrates the image of the lane on the shot image (real view) of the back of the vehicle. In order to control the edge extraction of symmetrical lanes L, R generating from a disappearance point, the image is divided into half right and left, and a filter for the left half image and a filter for the right half image are applied. A left area edge filter (edge filter whose brightness is increased from the upper right to the lower left) illustrated in <figref idrefs="f0008">FIG. 9A</figref> is a filter for avoiding the extraction of the edge of the lane in the L-portion in <figref idrefs="f0008">FIG. 9C</figref>. A right area edge filter (edge filter whose brightness is increased from the upper left to the lower right) illustrated in <figref idrefs="f0008">FIG. 9B</figref> is a filter for avoiding the extraction of the edge of the lane in the R-portion in <figref idrefs="f0008">FIG. 9C</figref>. The edge filters are for decreasing noise edges by increasing a size of a filter.</p><p id="p0063" num="0063">As illustrated in <figref idrefs="f0009">FIG. 10</figref>, after controlling the lane by applying the right area filter and the left area filter to the input image divided right and left, the edge of the right area and the edge of the left area are extracted. By combining the edge images independently obtained in the right area and the left area, the edge image is<!-- EPO <DP n="28"> --> generated. In addition, the mask for the lane can be performed when executing the mask process for the after-described light source or the like. However, by masking the lane with a process which extracts the edge image, the edge extraction process can be improved and the speed of the edge extraction process can be improved, and these processes can be improved because it becomes unnecessary to count an unnecessary edge in the subsequent processes.</p><p id="p0064" num="0064">Next, the threshold process and the binarization process are executed based on the weak edge image generated above to execute the noise elimination (refer to Steps S43, S44 in <figref idrefs="f0016">FIG. 18</figref>). These processes are similar to those in the first embodiment; thus, the detailed description thereof will be omitted.</p><heading id="h0021">(Mask Process)</heading><p id="p0065" num="0065">The mask process (Step S150) is performed by the mask processor 39. In this embodiment, the mask process for a track and the mask process for a light source are executed. The details of the mask process will be described with reference to <figref idrefs="f0010 f0011">FIGS. 11-12B</figref>.</p><p id="p0066" num="0066">The various processes for producing a track mask image illustrated in <figref idrefs="f0010">FIG. 11</figref> will be described. Tracks generated on a road surface include a snow track or a rain track. In an off-road, a track due to a tire track on a road surface is generated. The contours of these tracks are sometimes extracted as weak edges, and it may be difficult to distinguish the weak edges of the counters of the tracks from the weak edge of the dirt. For this reason, the weak edge extraction may be affected (noise), so that the area including the tracks is masked in order to effectively and highly<!-- EPO <DP n="29"> --> accurately extract the weak edge. In addition, the tracks are likely to be generated during off-road running on a snow day or a rain day. Therefore, during on-road running on a clear day which hardly generates tracks, the track image process and the track mask process can be skipped. Depending on the difference of these conditions, the contrasting density of a track is changed, so that the parameter for use in the generation process of the mask image can be adjusted in accordance with the difference in the conditions.</p><p id="p0067" num="0067">In the track detection, the contour position of the track is specified in the input image illustrated in (a) of <figref idrefs="f0010">FIG. 11</figref>. In order to specify the contour position of the track, the input images for a predetermined time are compared, and the area where the weak edges are temporarily and spatially continued in the vehicle traveling direction is detected. The process target area is divided into a plurality of small frames, and the pixels which detect the edges two frames in a row are displayed by white as illustrated in (b) of <figref idrefs="f0010">FIG. 11</figref>. A line in which the edges are continued in the Z axis direction on the world coordinate (in this case, the X axis is the vehicle width direction, the Y axis is the vertical direction, and the Z axis is the vehicle traveling direction) is extracted, and this line is set as the mask target area. The mask image obtained by this process is illustrated in (c) of <figref idrefs="f0010">FIG. 11</figref>. The details of these processes will be described below.</p><p id="p0068" num="0068">At first, the process target area is divided into a plurality of small frames as illustrated in (b) of <figref idrefs="f0010">FIG. 11</figref>, and the presence or absence of the edge in the frame is confirmed, and counted. Moreover, the projection in which each frame is arranged on the Z axis and the X axis on the world coordinate is obtained. In this case, the<!-- EPO <DP n="30"> --> presence or absence of the edge (0/1) not the number of edges is projected.</p><p id="p0069" num="0069">Next, the presence or absence of the track is determined with respect to each line based on the above tract detection data. The track determination is performed according to the following procedures by using the number of areas having an edge projected on the X axis.</p><heading id="h0022">(A) Track Determination</heading><p id="p0070" num="0070">In this determination, the determination differs between the outer edge portion of the process target area and the portion except the outer edge portion. In addition, the outer edge portion is defined as a predetermined area of the left end side and a predetermined area of the right end side in the X-axis projection. In the portion except the outer edge portion, it is determined that the line includes the track when satisfying the following conditions (a), (b). In the outer edge portion, it is determined that the line includes the track when satisfying the following condition (c).
<ol><li>(a) The number of areas having an edge in a line is threshold 1 or more.</li><li>(b) Two lines each on the right and left sides in a line include a line in which the number of areas including an edge in a line is less than threshold 2.</li><li>(c) The number of areas including an edge in a line is threshold 3 or more.</li></ol></p><heading id="h0023">(B) Track Continuation Determination</heading><p id="p0071" num="0071">In the determination of the above (A), in a case that no track is determined (in a case that the conditions of the above (A) are not satisfied), and a track is recently detected, the track determination of the line is continued when satisfying<!-- EPO <DP n="31"> --> both of the following conditions (d), (e). A period for continuing a track is twice (one second), and the continuation period is reset when the track determination is continued by this process.
<ul><li>(d) The number of areas having an edge in a line is 1/2 or more of threshold 1.</li><li>(e) Two lines each on the right and left sides in lines include a line in which the number of areas having an edge is less than 1/2 of threshold 2.</li></ul></p><p id="p0072" num="0072">An image which masks a line determined as including a track is produced based on the track detection result. The mask image is produced in accordance with the following procedures. The track mask image illustrated in (c) of <figref idrefs="f0010">FIG. 11</figref> is produced in accordance with the following procedures.
<ol><li>(a) The output image is filled with white.</li><li>(b) When there is a line determined as the presence of a track, that area is blacked out.</li></ol></p><p id="p0073" num="0073">Next, the various processes for masking an area including a light source will be described. When a light source such as sun exits at the back of the own vehicle, the weak edge of the light source is detected in the boarder of the road surface reflection area. This weak edge due to the light source is hardly distinguished from the weak edge of the dirt, and may affect the performance of the lens-attached matter detection process. The mask process for the light source is executed for controlling such a negative influence. <figref idrefs="f0011">FIG. 12A</figref> illustrates the image of the light source and the road surface reflection. <figref idrefs="f0011">FIG. 12B</figref> illustrates the light source mask image in which the light source is masked. A procedure for producing the light source mask image relative to such an input image is described as follows.<!-- EPO <DP n="32"> --></p><p id="p0074" num="0074">The following information is used in this embodiment as information of a light source area.
<ol><li>(a) West sun determination result</li><li>(b) Diffusion and reflection area<br/>
Since the information of the above (a), (b) is an area including a predetermined brightness value or more, the light source area can be obtained by binarizing the input image with a predetermined brightness value. In addition, the information of the light source area is not limited thereto, and other information can be used. As described above, in the process block which overlaps with the light source area, the score of the dirt determination target is cleared. Namely, when executing the after-described attached matter determination process, the process block which overlaps with the light source area is set aside for counting, so that the area (light source area) except the dirt is prevented from being counted.</li></ol></p><p id="p0075" num="0075">As described above, in the present embodiment, the mask process is executed for the area including a track and the area including a light source. However, the present invention is not limited thereto, and any target which can be noise can be masked for effectively and highly accurately extracting an edge. Another example of a mask target includes own vehicle shadow illustrated in <figref idrefs="f0011">FIGS. 12A, 12B</figref> and a vehicle body illustrated in <figref idrefs="f0012">FIG. 13</figref>. The own vehicle shadow is masked because the weak edge is detected by the contour of the own vehicle shadow, and such detection affects the detection of the weak edge of the dirt. For this reason, in this process, as illustrated in <figref idrefs="f0012">FIG. 13</figref>, a mask image for masking the contour of the own vehicle shadow is created. Moreover, the own vehicle shadow generated<!-- EPO <DP n="33"> --> in the input image is not fixed, and the projected image generated on the image differs according to a time or a direction. In this case, the mask image is created by accumulating the input images for a predetermined time, and specifying the own vehicle shadow from the portion without having a change over time. The own vehicle shadow can be specified based on the shape of the projected image or the previously prepared sample image of the own vehicle shadow. There may be a case in which the contour of the shadow changes in accordance with intensity of light depending on weather, or the shadow is not obtained on a cloudy day or the like. For this reason, the process of creating the mask image of the own vehicle shadow can be changed or can be skipped based on the weather information.</p><p id="p0076" num="0076">In contrast to this, since the image of the vehicle body is fixed based on the attachment position of the camera 20 or the like, it becomes unnecessary to execute a time-dependent process, and the mask image illustrated in <figref idrefs="f0013">FIG. 14</figref> may be prepared in advance. The image of the vehicle body can be obtained from the input image as an initial process upon the activation of the lens-attached matter detection process, and the mask image can be created. As described above, the mask process for a lane can be executed in this step. <figref idrefs="f0014">FIG. 15</figref> illustrates the weak edge image before the mask process and the weak edge image after the mask process. <figref idrefs="f0014">FIG. 15</figref> illustrates the images after the mask process by creating the track-masked image, own vehicle shadow-masked image and vehicle body-masked image.</p><heading id="h0024">(Brightness Distribution Extraction Process)</heading><p id="p0077" num="0077">The brightness distribution extraction process (Step S160) is executed by the brightness distribution extractor 37. The brightness distribution extraction process<!-- EPO <DP n="34"> --> in the second embodiment is similar to that in the first embodiment except that the process is not executed for the masked block. Consequently, the detailed description of the similar processes is omitted, and the process different from that in the first embodiment will be described below.</p><p id="p0078" num="0078">In the first embodiment, when selecting the attention block and the circumference blocks (Step S52), all of the blocks located in the outer circumference of the blocks adjacent to the attention blocks are selected as the circumference blocks for process targets. However, in the second embodiment, as illustrated in <figref idrefs="f0006">FIG. 6B</figref>, the circumference blocks 201b are selected from the process target blocks 201c. Thus, the dotted line portions are not selected as the circumference blocks 201b. These process target blocks 201c are portions which are not masked by the mask process (Step S150). Namely, the masked portions are not set as process target blocks. Owing to such a process, a dark block except dirt is prevented from being counted, while the number of process target blocks is reduced. Accordingly, the process speed can be improved. In the process which counts a weak edge (Step S55), since the weak edge of the masked portion is not counted, the process speed can be also improved.</p><heading id="h0025">(Process Time Determination Process)</heading><p id="p0079" num="0079">In the process time determination process (Step S170) according to the second embodiment, the time interval for executing each process and the accumulation of information is used depending on an on road or an off road in the determination of the elapse of a predetermined time. Namely, in an off road, the attachment of the dirt easily occurs, so that the presence or absence of the dirt can<!-- EPO <DP n="35"> --> be detected by the information accumulated for a short period of time. On the other hand, in an on road, the attachment of the dirt hardly occurs compared to an off road, so that a long time interval is set, and the determination is executed by accumulating the information for a long period of time. In this embodiment, the time interval for accumulating the information is set depending on an on road or an off road because the presence or absence of the dirt is detected. However, the present invention is not limited thereto. In a case of detecting waterdrops, for example, the time interval can be adjusted depending on weather. The time interval for detecting the presence or absence of the dirt can be adjusted depending on not only a road condition but also weather.</p><heading id="h0026">(Brightness Change Extraction Process)</heading><p id="p0080" num="0080">The brightness change extraction process (Step S180) is executed by the brightness change extractor 38. In the brightness change extraction process in the second embodiment, the process similar to that in the first embodiment is executed. Thus, the detailed description thereof will be omitted. This process is not executed for the masked block, so that the process speed can be improved.</p><p id="p0081" num="0081">After the accumulated information is sufficiently obtained, the attached matter determination process is executed by the attached matter determiner 32 (Step S190). The attached matter determination process in the second embodiment is similar to the process in the first embodiment except that the masked block is not counted. Thus, the description for the similar processes is omitted, and the process different from that in the first embodiment will be described below.</p><p id="p0082" num="0082"><!-- EPO <DP n="36"> --> In the calculation of the score of the dirt determination target with respect to each block (Step S81), the block masked by the above-described each mask process is not set as a count target for each process. Namely, as described above, the process block which overlaps with the light source area, the process block which overlaps with the track, the process block which overlaps with the own vehicle shadow and the process block which overlaps with the own vehicle body are set aside for counting a dirt determination target.</p><heading id="h0027">(Output Information Generation Process)</heading><p id="p0083" num="0083">The output information generation process (Step S200) is executed by the output information generator 34. In the output information generation process in the second embodiment, the process similar to that in the first embodiment is executed. Thus, the detailed description thereof will be omitted.</p><p id="p0084" num="0084">As described above, in the second embodiment, even when a road surface condition (for example, lane marker, track, own vehicle shadow) or illumination environment (for example, light source such as sun light or vehicle light), which is likely to be false-recognized as dirt, occurs, the mask process which eliminates an imaging target except lens-attached matter from the attention block is executed, so that the presence or absence of the attachment of the dirt can be detected with a high accuracy. By eliminating the mask area from the dirt determination process target, the whole lens-attached matter detection process can be effectively executed at a high speed.</p><p id="p0085" num="0085">In the first and second embodiments, an example which detects dirt is<!-- EPO <DP n="37"> --> described. However, the present invention is not limited to the detection of the dirt. The present invention can be used for a detector or a detection process for another lens-attached matter such as waterdrops, dust, paint or bird dropping. For example, when waterdrops such as rain or fog attach to a lens, a problem, for example, a strain image or miss-focusing occurs, so that the operation of the image-sensing application such as vehicle detection or lane detection may be affected. For this reason, it becomes necessary to draw driver's attention by detecting the attachment of the waterdrops to blow out the attachment by air, for example.</p><p id="p0086" num="0086">In each embodiment, since the dirt is detected, the presence of the attachment of the dirt is determined when the average brightness value of the attention block is lower than that of the circumference block in the brightness distribution extraction process and the change in the average brightness value is small in the brightness change extraction process. More specifically, the brightness is decreased because the dirt has a low light permeability and is brackish. On the other hand, the waterdrops have a high light permeability and brightness which is substantially similar to that in the circumference, but the brightness of the waterdrops is likely to be changed over time. Moreover, the weak edge is extracted in the outer circumference of the waterdrops when the waterdrops attach to a lens. However, the degree of the edge intensity may differ from that of the dirt. Therefore, the attachment of the waterdrops can be determined when the average brightness value of the attention block is equal to that of the circumference block or a change in the average brightness value is large in the brightness change extraction process in a case that the weak edge is detected.</p><p id="p0087" num="0087"><!-- EPO <DP n="38"> --> Consequently, in the detection of the attachment of waterdrops, it is preferable to use a threshold suitable for the waterdrops when extracting the weak edge by the edge extraction process in the first and second embodiments. When comparing the average brightness value of the attention block with that of the circumference block by the brightness change extraction process, the determination is based on whether or not the brightness is the same or whether or not the change in the brightness by the brightness change extraction process is large. As described above, it is preferable to configure the lens-attached matter detection process with the logic which executes a process according to a type of attached matter. Moreover, it is preferable to configure the vehicle system which executes the most suitable process according to a type of attached matter.</p><p id="p0088" num="0088">According to the above-described lens-attached matter detector, only lens-attached matter such as dirt, dust, or waterdrops, which is attached to a lens of a camera, for example, can be detected with a high accuracy. Therefore, for example, a stain on a lens can be smoothly informed to a driver or an automatic cleaning system, so that lens-attached matter can be smoothly eliminated. Moreover, the imaging sensing application such as vehicle detection using a camera can be preferably performed. Furthermore, only lens-attached matter can be detected with a high accuracy without detecting attached matter except lens-attached matter because the lens-attached matter is detected based on the comparison between the brightness value of the attention area and the brightness value of the circumference area. Consequently, the process efficiency can be improved without unnecessarily informing dirt to a driver or unnecessarily performing automatic cleaning.</p><p id="p0089" num="0089"><!-- EPO <DP n="39"> --> As described above, the lens-attached matter detector may include the mask processor which eliminates from the attention area an area except lens-attached matter in the area whose edge intensity is within a threshold range extracted by the edge extractor. With this configuration, when a road surface condition (for example, lane marker, track, or own vehicle shadow) or an illumination environment (light source such as sun light or vehicle light), which is likely to be false-detected as dirt, in which a time series variation in a brightness value is small, and the brightness value of the attention area is smaller than the brightness value of the circumference area in the area whose edge intensity is within the threshold range occurs, an imaging object except lens-attached matter can be eliminated from the attention area. Therefore, the determination of the presence or absence of the lens-attached matter by the lens-attached matter determiner can be executed with a high accuracy.</p><p id="p0090" num="0090">Moreover, according to the embodiments of the present invention, the lens-attached matter detector and the lens-attached matter detection method capable of detecting only lens-attached matter such as dirt, dust, or waterdrops which is attached to a lens of a camera, for example, and the vehicle system including such a lens-attached matter detector can be provided.</p><p id="p0091" num="0091">Although the embodiments of the present invention have been described above, the present invention is not limited thereto. It should be appreciated that variations may be made in the embodiments described by persons skilled in the art without departing from the scope of the present invention.</p></description><claims mxw-id="PCLM56982035" lang="EN" load-source="patent-office"><!-- EPO <DP n="40"> --><claim id="c-en-0001" num="0001"><claim-text>A lens-attached matter detector, comprising:
<claim-text>an edge extractor (36) configured to create an edge image based on an input image from an imager (20) having a lens (21), divide the edge image into a plurality of areas including a plurality of pixels, and extract an area whose edge intensity is a threshold range as an attention area;</claim-text>
<claim-text>a brightness distribution extractor (37) configured to obtain a brightness value of the attention area and a brightness value of a circumference area;</claim-text>
<claim-text>a brightness change extractor (38) configured to obtain the brightness value of the attention area and the brightness value of the circumference area obtained by the brightness distribution extractor for a predetermined time interval, and obtain a time series variation in the brightness value of the attention area based on the brightness value of the attention area for the predetermined time interval; and</claim-text>
<claim-text>an attached matter determiner (32) configured to determine the presence or absence of attached matter based on the time series variation in the brightness value of the attention area.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The lens-attached matter detector according to Claim 1, further comprising a mask processor (39) configured to eliminate from the attention area the area except lens-attached matter in the area whose edge intensity is the threshold range extracted by the edge extractor.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The lens-attached matter detector according to Claim 2, wherein the mask processor is configured to eliminate from the attention area an area including an edge extending in a direction toward a disappearance point of the input image.<!-- EPO <DP n="41"> --></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The lens-attached matter detector according to claim 2 or claim 3, wherein the mask processor is configured to eliminate from the attention area an area including a track on a road surface, which is generated on the input image.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The lens-attached matter detector according to any one of Claims 2-4, wherein the mask processor is configured to eliminate from the attention area an area including a shadow of an attached object provided with the imager, which is generated on the input image.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The lens-attached matter detector according to any one of Claims 1-5, wherein the mask processor is configured to eliminate from the attention area an area including an image of a light source, which is generated on the input image.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The lens-attached matter detector according to Claim 1, wherein when the brightness value of the attention area obtained by the brightness distribution extractor is smaller than a predetermined value by being compared with the circumferential area, and the time series variation in the brightness value obtained by the brightness change extractor is smaller than a predetermined value, the attached matter determiner is configured to determine that attached matter having a low permeability is attached to the attention area.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The lens-attached matter detector according to Claim 1, wherein when the brightness value of the attention area obtained by the brightness distribution extractor is larger than a predetermined value by being compared with the circumferential area, and the time series variation in the brightness value obtained by<!-- EPO <DP n="42"> --> the brightness change extractor is larger than a predetermined value, the attached matter determiner is configured to determine that attached matter having a high permeability is attached to the attention area.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>A lens-attached matter detection method which is executed by the lens-attached matter detector according to any one of Claims 1-8, comprising<br/>
an edge extraction process (S40) of creating an edge image based on an input image, dividing the edge image into a plurality of areas including a plurality of pixels, and extracting an area whose edge intensity is a threshold range as an attention area;<br/>
a brightness value distribution extraction process (S50) of obtaining a brightness value of the attention area and a brightness value of a circumference area of the attention area;<br/>
a brightness value extraction process of obtaining the brightness value of the attention area and the brightness value of the circumference area obtained by the brightness value distribution extraction process for a predetermined time interval, and obtaining a time series variation in the brightness value of the attention area based on the brightness value of the attention area for the predetermined time interval; and<br/>
an attached matter determination process (S80) of determining the presence or absence of attached matter based on the time series variation in the brightness value of the attention area.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>The lens-attached matter detection method according to Claim 9, further comprising a vehicle information-obtaining process of obtaining vehicle information including at least vehicle speed information, wherein<br/>
<!-- EPO <DP n="43"> -->all of the processes are completed when the vehicle information-obtaining process determines that a vehicle speed is a threshold or below based on the vehicle speed information.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The lens-attached matter detection method according to Claim 9 or Claim 10, wherein all of the processes are completed when a gain adjustment value of the input image is a threshold or more.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>A vehicle system, comprising<br/>
a lens-attached matter detector according to any one of Claims 1-8;<br/>
an imager provided in a vehicle, having a lens, and configured to image a circumference of the vehicle; and<br/>
at least one application (52) configured to operate based on detection information of the lens-attached matter detected by the lens-attached matter detector relative to the input image shot by the imager.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>The vehicle system according to Claim 12, wherein the lens-attached matter detector is configured to detect the lens-attached matter relative to only an area for use in the application in the input image shot by the lens.</claim-text></claim></claims><drawings mxw-id="PDW16670429" load-source="patent-office"><!-- EPO <DP n="44"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="143" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="45"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="46"> --><figure id="f0003" num="3A,3B"><img id="if0003" file="imgf0003.tif" wi="153" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="47"> --><figure id="f0004" num="4(a),4(b)"><img id="if0004" file="imgf0004.tif" wi="155" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="48"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="165" he="143" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="49"> --><figure id="f0006" num="6A,6B"><img id="if0006" file="imgf0006.tif" wi="157" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="50"> --><figure id="f0007" num="7,8"><img id="if0007" file="imgf0007.tif" wi="141" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="51"> --><figure id="f0008" num="9A,9B,9C"><img id="if0008" file="imgf0008.tif" wi="145" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="52"> --><figure id="f0009" num="10"><img id="if0009" file="imgf0009.tif" wi="156" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="53"> --><figure id="f0010" num="11A,11B,11C"><img id="if0010" file="imgf0010.tif" wi="150" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="54"> --><figure id="f0011" num="12A,12B"><img id="if0011" file="imgf0011.tif" wi="162" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="55"> --><figure id="f0012" num="13(a),13(b)"><img id="if0012" file="imgf0012.tif" wi="152" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="56"> --><figure id="f0013" num="14(a),14(b)"><img id="if0013" file="imgf0013.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="57"> --><figure id="f0014" num="15"><img id="if0014" file="imgf0014.tif" wi="153" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="58"> --><figure id="f0015" num="16"><img id="if0015" file="imgf0015.tif" wi="119" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="59"> --><figure id="f0016" num="17,18"><img id="if0016" file="imgf0016.tif" wi="119" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="60"> --><figure id="f0017" num="19,20"><img id="if0017" file="imgf0017.tif" wi="110" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="61"> --><figure id="f0018" num="21"><img id="if0018" file="imgf0018.tif" wi="130" he="233" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="161" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
