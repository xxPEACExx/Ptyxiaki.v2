<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681138-A1" country="EP" doc-number="2681138" kind="A1" date="20140108" family-id="43976208" file-reference-id="252638" date-produced="20180823" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586618" ucid="EP-2681138-A1"><document-id><country>EP</country><doc-number>2681138</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12710542-A" is-representative="NO"><document-id mxw-id="PAPP154848810" load-source="docdb" format="epo"><country>EP</country><doc-number>12710542</doc-number><kind>A</kind><date>20120220</date><lang>IT</lang></document-id><document-id mxw-id="PAPP199048064" load-source="docdb" format="original"><country>EP</country><doc-number>12710542.7</doc-number><date>20120220</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140548859" ucid="IB-2012000324-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>IB</country><doc-number>2012000324</doc-number><kind>W</kind><date>20120220</date></document-id></priority-claim><priority-claim mxw-id="PPC140551778" ucid="IT-MI20110307-A" load-source="docdb"><document-id format="epo"><country>IT</country><doc-number>MI20110307</doc-number><kind>A</kind><date>20110228</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1989321292" load-source="docdb">G06K   7/10        20060101ALI20120920BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1989323225" load-source="docdb">B65G  47/48        20060101AFI20120920BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1962104968" load-source="docdb" scheme="CPC">G06K   7/10861     20130101 LI20160308BHEP        </classification-cpc><classification-cpc mxw-id="PCL2007296045" load-source="docdb" scheme="CPC">G06K   9/00624     20130101 FI20140123BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132364487" lang="DE" load-source="patent-office">VERFAHREN ZUR OPTISCHEN IDENTIFIZIERUNG VON OBJEKTEN IN BEWEGUNG</invention-title><invention-title mxw-id="PT132364488" lang="EN" load-source="patent-office">METHOD FOR THE OPTICAL IDENTIFICATION OF OBJECTS IN MOTION</invention-title><invention-title mxw-id="PT132364489" lang="FR" load-source="patent-office">PROCÉDÉ POUR L'IDENTIFICATION OPTIQUE D'OBJETS EN DÉPLACEMENT</invention-title><citations><non-patent-citations><nplcit><text>See references of WO 2012117283A1</text><sources><source mxw-id="PNPL67456061" load-source="docdb" name="SEA"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR991279528" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>Datalogic IP Tech Srl</last-name><address><country>IT</country></address></addressbook></applicant><applicant mxw-id="PPAR919505824" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>DATALOGIC IP TECH S.R.L.</last-name></addressbook></applicant><applicant mxw-id="PPAR919011786" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Datalogic IP TECH S.r.l.</last-name><iid>101310981</iid><address><street>Via San Vitalino, 13</street><city>40012 Lippo di Calderara di Reno (BO)</city><country>IT</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919513259" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>FIORINI DANIELE</last-name><address><country>IT</country></address></addressbook></inventor><inventor mxw-id="PPAR919528256" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>FIORINI, DANIELE</last-name></addressbook></inventor><inventor mxw-id="PPAR919010115" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>FIORINI, DANIELE</last-name><address><street>Via Pietro Micca 6</street><city>I-40033 Casalecchio di Reno (Bologna)</city><country>IT</country></address></addressbook></inventor><inventor mxw-id="PPAR919523803" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>OWEN RICKY DARRELL</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919536028" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>OWEN, Ricky, Darrell</last-name></addressbook></inventor><inventor mxw-id="PPAR919014739" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>OWEN, Ricky, Darrell</last-name><address><street>2377 Hempfling Road</street><city>Morning View KY 41063</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919528682" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>PELLEGRINO LUIGI</last-name><address><country>IT</country></address></addressbook></inventor><inventor mxw-id="PPAR919504027" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>PELLEGRINO, LUIGI</last-name></addressbook></inventor><inventor mxw-id="PPAR919010471" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>PELLEGRINO, LUIGI</last-name><address><street>Via XXIV Maggio 10</street><city>I-73048 Nardò (Lecce)</city><country>IT</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919009920" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Castiglia, Paolo</last-name><suffix>et al</suffix><iid>100048396</iid><address><street>Porta, Checcacci &amp; Associati S.p.A. Via Trebbia, 20</street><city>20135 Milano</city><country>IT</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="IB-2012000324-W"><document-id><country>IB</country><doc-number>2012000324</doc-number><kind>W</kind><date>20120220</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012117283-A1"><document-id><country>WO</country><doc-number>2012117283</doc-number><kind>A1</kind><date>20120907</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549855771" load-source="docdb">AL</country><country mxw-id="DS549841478" load-source="docdb">AT</country><country mxw-id="DS549855772" load-source="docdb">BE</country><country mxw-id="DS549915741" load-source="docdb">BG</country><country mxw-id="DS549773817" load-source="docdb">CH</country><country mxw-id="DS549842335" load-source="docdb">CY</country><country mxw-id="DS549841479" load-source="docdb">CZ</country><country mxw-id="DS549855773" load-source="docdb">DE</country><country mxw-id="DS549842336" load-source="docdb">DK</country><country mxw-id="DS549842337" load-source="docdb">EE</country><country mxw-id="DS549763015" load-source="docdb">ES</country><country mxw-id="DS549915742" load-source="docdb">FI</country><country mxw-id="DS549915743" load-source="docdb">FR</country><country mxw-id="DS549855778" load-source="docdb">GB</country><country mxw-id="DS549842346" load-source="docdb">GR</country><country mxw-id="DS549855779" load-source="docdb">HR</country><country mxw-id="DS549841480" load-source="docdb">HU</country><country mxw-id="DS549773818" load-source="docdb">IE</country><country mxw-id="DS549855780" load-source="docdb">IS</country><country mxw-id="DS549915744" load-source="docdb">IT</country><country mxw-id="DS549842347" load-source="docdb">LI</country><country mxw-id="DS549842010" load-source="docdb">LT</country><country mxw-id="DS549841481" load-source="docdb">LU</country><country mxw-id="DS549842011" load-source="docdb">LV</country><country mxw-id="DS549842012" load-source="docdb">MC</country><country mxw-id="DS549762806" load-source="docdb">MK</country><country mxw-id="DS549762807" load-source="docdb">MT</country><country mxw-id="DS549841486" load-source="docdb">NL</country><country mxw-id="DS549855781" load-source="docdb">NO</country><country mxw-id="DS549842348" load-source="docdb">PL</country><country mxw-id="DS549842013" load-source="docdb">PT</country><country mxw-id="DS549841487" load-source="docdb">RO</country><country mxw-id="DS549842018" load-source="docdb">RS</country><country mxw-id="DS549841488" load-source="docdb">SE</country><country mxw-id="DS549842019" load-source="docdb">SI</country><country mxw-id="DS549855806" load-source="docdb">SK</country><country mxw-id="DS549842349" load-source="docdb">SM</country><country mxw-id="DS549762808" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA99836551" ref-ucid="WO-2012117283-A1" lang="EN" load-source="patent-office"><p num="0000">The method of the invention comprises the steps of acquiring, by at least one camera (2) having a predetermined magnification ratio, at least one image containing at least one coded information (30) and at least one object (3) and properly associating said coded information with said at least one object (3) on the basis of the position of said coded information (30) and of said at least one object (3) along an advancing direction (A) with respect to a fixed reference system. The above- mentioned position of the coded information is determined starting from the position of the coded information (30) within the image acquired by the camera (2) and on the basis of the distance of the coded information (30) from the camera (2). Such a distance is in turn determined on the basis of the magnification factor of a reference physical dimension detected at a surface of the object (3) or of the coded information (30)</p></abstract><abstract mxw-id="PA100332323" ref-ucid="WO-2012117283-A1" lang="EN" source="national office" load-source="docdb"><p>The method of the invention comprises the steps of acquiring, by at least one camera (2) having a predetermined magnification ratio, at least one image containing at least one coded information (30) and at least one object (3) and properly associating said coded information with said at least one object (3) on the basis of the position of said coded information (30) and of said at least one object (3) along an advancing direction (A) with respect to a fixed reference system. The above- mentioned position of the coded information is determined starting from the position of the coded information (30) within the image acquired by the camera (2) and on the basis of the distance of the coded information (30) from the camera (2). Such a distance is in turn determined on the basis of the magnification factor of a reference physical dimension detected at a surface of the object (3) or of the coded information (30)</p></abstract><abstract mxw-id="PA99836552" ref-ucid="WO-2012117283-A1" lang="FR" load-source="patent-office"><p num="0000">L'invention porte sur un procédé, qui comprend les étapes consistant à acquérir, par au moins un appareil de prise de vues (2) ayant un taux de grossissement prédéterminé, au moins une image contenant au moins une information codée (30) et au moins un objet (3), et à associer correctement ladite information codée audit ou auxdits objets (3) sur la base de la position de ladite information codée (30) et dudit ou desdits objets (3) le long d'une direction d'avance (A) par rapport à un système de référence fixe. La position ci-dessus mentionnée de l'information codée est déterminée à partir de la position de l'information codée (30) à l'intérieur de l'image acquise par l'appareil de prise de vues (2) et sur la base de la distance de l'information codée (30) à partir de la caméra (2). Cette distance est elle-même déterminée sur la base du coefficient de grossissement d'une dimension physique de référence détectée à une surface de l'objet (3) ou de l'information codée (30).</p></abstract><abstract mxw-id="PA100332324" ref-ucid="WO-2012117283-A1" lang="FR" source="national office" load-source="docdb"><p>L'invention porte sur un procédé, qui comprend les étapes consistant à acquérir, par au moins un appareil de prise de vues (2) ayant un taux de grossissement prédéterminé, au moins une image contenant au moins une information codée (30) et au moins un objet (3), et à associer correctement ladite information codée audit ou auxdits objets (3) sur la base de la position de ladite information codée (30) et dudit ou desdits objets (3) le long d'une direction d'avance (A) par rapport à un système de référence fixe. La position ci-dessus mentionnée de l'information codée est déterminée à partir de la position de l'information codée (30) à l'intérieur de l'image acquise par l'appareil de prise de vues (2) et sur la base de la distance de l'information codée (30) à partir de la caméra (2). Cette distance est elle-même déterminée sur la base du coefficient de grossissement d'une dimension physique de référence détectée à une surface de l'objet (3) ou de l'information codée (30).</p></abstract><description mxw-id="PDES51233343" ref-ucid="WO-2012117283-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001"> Method for the optical identification of objects in motion </p><p id="p0002" num="0002">DESCRIPTION </p><p id="p0003" num="0003">The present invention relates to a method for the optical identification of objects in motion. </p><p id="p0004" num="0004">Throughout the following description and the following claims, the expression: "optical identification" is used to indicate the acquisition and reading of coded information of an object (for example distance, volume, overall dimensions, or object identification data) for example through the acquisition and processing of a light signal diffused by the same object. The term: "coded information" is preferably used to indicate the whole identification data contained in an optical code. The term: "optical code" is used to indicate any graphical representation having the function of storing said coded information. </p><p id="p0005" num="0005">A particular example of optical code consists of the linear or two-dimensional codes, wherein the information is coded through suitable combinations of elements with a predetermined shape, for example squared, rectangular or hexagonal, of dark colour (usually black) separated by clear elements (spaces, usually white) , such as barcodes, stacked codes, two-dimensional codes in general, colour codes, etc. The term "optical code" further comprises, more generally, also other graphical patterns with information coding function, including clear printed characters (letters, numbers, etc.) and special patterns (such as stamps, logos, signatures, fingerprints, etc.). The term "optical code" also comprises graphical representations which are detectable not only in the field of visible light but also in the range of wavelengths comprised between infrared and ultraviolet. </p><p id="p0006" num="0006">For the sake of making the following explanation easier, explicit reference to linear and two-dimensional codes shall be made hereinafter. 
<!-- EPO <DP n="3"/>-->
Systems for conveying and sorting packs, luggage and more in general, objects, are commonly used in the field of transports and logistics. In these systems, the objects are placed on a conveyor belt in motion and sorted on the basis of the reading of an optical code printed on a label associated with each object. </p><p id="p0007" num="0007">In the past, when there were only linear codes, reading the optical codes was carried out by scanning a laser light beam emitted by a dedicated laser reader onto the optical code . </p><p id="p0008" num="0008">With the coming of two-dimensional codes, the use of digital cameras typically using CCD/CMOS sensors has become widespread. Such cameras allow a greater usage flexibility. Indeed, they are capable of reading both traditional linear codes and two-dimensional codes, as well as other types of codes, besides offering additional functions such as OCR (optical character recognition) . </p><p id="p0009" num="0009">A problem in object conveying and sorting systems is that of distinguishing objects that may even be very close to each other, so as to associate each object with the content of the corresponding optical code. </p><p id="p0010" num="0010">In those systems using laser readers, the problem of the correct association between object and respective code is solved for example by a system of the type described in EP 0 851 376 and EP 1 363 228. In such system, the laser reader that performs the scanning for reading the optical code also measures the distance and the angular position of the optical code within the scanning, thus providing the polar coordinates of the optical code with respect to the reader. The position of the optical code in the space with respect to a fixed reference system is obtained from these data, the position of the reader with respect to such a fixed reference system being known. A photocell barrier or other outside sensor provides an object presence signal. The temporal advancing of the object with respect to the 
<!-- EPO <DP n="4"/>-->
signal of entry within the field of view of the laser reader is obtained from a further signal provided by an encoder associated with the conveyor belt. The position of the object along the advancing direction is obtained from these data. The association of the code with the respective object is made on the basis of the comparison between the positions of the optical code and of the objects along said advancing direction. </p><p id="p0011" num="0011">The Applicant has noted that the problem of the proper association code - object occurs in a particularly critical manner in systems using digital cameras. Indeed, digital cameras have a two-dimensional field of view that is typically much wider than that of laser readers. Accordingly, a condition wherein there are multiple objects and multiple optical codes within an image acquired by a digital camera may frequently occur. </p><p id="p0012" num="0012">The Applicant has noted that in systems that use digital cameras situations may occur in which, in order to carry out the proper association between optical code and respective object, it is necessary to have an information about the distance at which the objects are with respect to the cameras. </p><p id="p0013" num="0013">In order to better understand this aspect, let us consider for example the situation shown in the annexed figures 1A and IB. </p><p id="p0014" num="0014">Such figures schematize a case wherein a camera T detects the presence of two objects K and K-l within the field of view V thereof and reads an optical code C at the inlet end of the field of view V with reference to the advancing direction A of objects K and K-l. Figure 1A shows a possible situation wherein object K has a greater height than that of object K-l and such as to at least partly hide object K-l right at the inlet end of the field of view V. Figure IB, on the other hand, shows a possible situation wherein object K has such a height as not to hide object K- 
<!-- EPO <DP n="5"/>-->
1. The comparison between the two figures shows that it is necessary to have an information about the height of objects K and K-1 for determining which one of the two objects K and K-1 should be associated with the optical code C. Indeed, in the case of figure 1A, the optical code C should be associated with object K since even if object K-1 had an optical code at the inlet end of the field of view V, it would not be visible by camera T as it would be hidden by object K. On the contrary, in the case of figure IB, the optical code C shall be associated with object K-1. </p><p id="p0015" num="0015">The Applicant has found that a very simple and effective way for implementing a technique for measuring the distance in a system that uses digital cameras is to use a height detector (such as for example the one used in EP 0 851 376 and EP 1 363 228), comprising for example a photocell barrier . </p><p id="p0016" num="0016">An exemplary embodiment of such a system is shown in figure</p><p id="p0017" num="0017">2. This system, globally indicated with reference numeral 10, comprises a conveyor belt 1 which moves a plurality of objects 3 along a direction A with respect to a camera 2, each object 3 being provided with an optical code (non visible) . Camera 2 frames a field of view 4 shaped as a pyramid. A processing unit 5, which is connected to the camera 2, is capable of decoding the optical codes associated with objects 3 when these are within the field of view 4. The processing unit 5 further receives a signal of arrival of objects 3 and the respective height from a presence/height sensor 6 arranged upstream of the camera 2 with reference to the feeding direction A of objects 3. Based also on the knowledge of the feeding speed, which is substantially deemed as being constant or measured in real time by an encoder 7 associated with the conveyor belt 1, the processing unit 5 is capable of synchronising the image acquisition by camera 2 with the instant at which each detected object 3 travels through the field of view 4. It is therefore possible to associate an optical code having a 
<!-- EPO <DP n="6"/>-->
predetermxned position along the feeding direction A at a certain instant with an object 3 having the same position at the same instant, such association being made on the basis of the height signal provided by the presence/height sensor 6. </p><p id="p0018" num="0018">Throughout the present description and following claims, the term "height" is used to refer to the distance from the camera through which the presence of objects and/or coded information is detected. The term "height" shall therefore comprise both a distance along a vertical direction (or along a direction having at least one component in the vertical direction) when said camera is arranged above the conveyor belt on which the objects are arranged so that the vertical projection thereof falls right onto the conveyor belt, and a distance along a horizontal direction (or a direction having at least one component in the horizontal direction) when said camera is arranged on the side of, in front of or behind the conveyor belt or above the latter but laterally displaced, forward or backward, so that the vertical projection thereof does not fall onto the conveyor belt. For this reason, the term "height" and the more generic term "distance" shall be used without distinction. </p><p id="p0019" num="0019">The information about object presence and object height may be provided by specific sensors (respectively presence sensor and height sensor) or by the height sensor only, which is in fact capable of also acting as a presence sensor . </p><p id="p0020" num="0020">In an alternative embodiment to that described above, the distance information is provided by a<sup>'</sup> distance measuring device integrated with the camera and connected thereto at a geometrically defined position. In this case it is possible to use, for example, a laser reader of the type described in EP 0 851 376 and EP 1 363 228. As an alternative, a laser pointer may be used, the laser pointer being of the static beam type capable of measuring the flight time or the phase shift between the emitted and the 
<!-- EPO <DP n="7"/>-->
reflected laser signal, such as for example the laser distance measuring device S80 marketed by Datalogic</p><p id="p0021" num="0021">Automation </p><p id="p0022" num="0022">According to a further alternative embodiment, the distance information is provided by a light pattern projector, for example a laser projector capable of producing a structured light beam, for example a pair of converging or diverging light figures or lines. The object distance may be obtained from the distance between said two lines or figures, said distance being measured in pixels on the image acquired by the camera, and by using a suitable look-up table (obtained empirically or starting from a formula that corresponds to the implementation of the geometrical model of the deformation to which the light beam is subjected) , stored into the processing unit. This operation may be carried out at the same time as the image acquisition by the camera. </p><p id="p0023" num="0023">The projector may also be a single one for multiple cameras, provided that the position and the shape of the pattern projected at the various distances are known for the field of view of each camera, for example by using said look-up table. </p><p id="p0024" num="0024">A projector of the type described above may also allow the detection of information about shape, position and footprint of the objects travelling through the field of view directly onto the image acquired, by analysing the deformation of a dedicated reference reticule. </p><p id="p0025" num="0025">A further alternative embodiment provides for the use of a stereo camera, that is a particular type of camera provided with two or more lenses and a separate image sensor for each lens. Such camera is capable of simulating the human binocular vision and thus capturing three-dimensional images (this process is known as stereo photography) . Stereo cameras may actually be used for producing stereoscopic views or three-dimensional images for movies or for producing images containing the object distance 
<!-- EPO <DP n="8"/>-->
information (range imaging) . </p><p id="p0026" num="0026">The Applicant has noted that while all the solutions described above are suitable for allowing a proper code - object association, they provide for the distance information to be obtained through the use of an additional device to be associated with the camera, or to be mounted within the camera. This clearly involves an increase in costs and a complication of the installation operations. </p><p id="p0027" num="0027">The Applicant has therefore considered the problem of finding further solutions adapted to implement, in systems that use digital cameras, a distance measurement technigue that would not require the use of additional devices, so as to carry out a proper and effective code - object association without an increase in costs and/or installation burdens. </p><p id="p0028" num="0028">The Applicant has found that a solution to the above problem could be provided by the use of a TOF camera (where TOF is the acronym of "time-of-flight") having such an optical resolution as to allow a reliable reading of the optical codes. In fact, TOF cameras are capable of providing an object distance information on the basis of the time elapsed between the emission of a light pulse and the reception of the signal backscattered by the object. </p><p id="p0029" num="0029">However, the Applicant has noted that to date, TOF cameras do not allow a reliable reading of optical codes due to the reduced optical resolution thereof. Such solution will therefore be able to be actuated only when TOF cameras with optical resolutions sufficient for the purpose will be available . Another solution found by the Applicant provides for the application of a logo of known dimensions on a conveyor belt and the application of an identical logo on the objects. From the comparison of the dimensions of the two logos in the images acquired by the camera with the actual 
<!-- EPO <DP n="9"/>-->
dimensions of said logos it is possible to deduce the distance between object and camera, without in this case requiring the magnification ratio of the camera to be known . The Applicant wanted to find a further way for solving the problems related to the implementation, in systems that use digital cameras, of a distance measurement technique which should not require the use of additional devices, and found, as a further valid solution, the invention described hereinafter. </p><p id="p0030" num="0030">Such invention relates to a method for the optical identification of objects in motion, comprising the following steps: </p><p id="p0031" num="0031"> - acquiring, by at least one camera having, at least during such an acquisition, a respective predetermined magnification ratio, at least one image of a predetermined detection area, said at least one image containing at least one portion of at least one object travelling through the detection area along a predetermined advancing direction; - determining the position of said at least one portion of said at least one object along the predetermined advancing direction with respect to a predetermined reference system;</p><p id="p0032" num="0032">- detecting, in said at least one image or in at least one subsequent image containing said at least one portion of at least one object, at least one coded information travelling through the detection area along the predetermined advancing direction, said detection being carried out, respectively, by said at least one camera or by a distinct camera having a predetermined relative position with respect to said at least one camera and, at least during the acquisition of said at least one image or of said at least one subsequent image, a respective predetermined magnification ratio; </p><p id="p0033" num="0033"> - reading said at least one coded information; </p><p id="p0034" num="0034">- determining the position of at least one portion of said at least one coded information along the predetermined 
<!-- EPO <DP n="10"/>-->
advancing direction within the image; </p><p id="p0035" num="0035"> - detecting, by said at least one camera or said at least one distinct camera, at least one reference physical dimension belonging to at least one surface portion of said at least one object or of said at least one coded information; </p><p id="p0036" num="0036"> - determining the distance of said at least one surface portion from said at least one camera and, if provided, from said at least one distinct camera on the basis of said at least one reference physical dimension and of the magnification ratio of the camera by which said at least one reference physical dimension has been detected; </p><p id="p0037" num="0037"> - determining the position of said at least one portion of coded information along the predetermined advancing direction with respect to the predetermined reference system on the basis of said distance and of the position of said at least one portion of said at least one coded information along the predetermined advancing direction within the image; </p><p id="p0038" num="0038">- associating said at least one coded information with a respective object travelling through the detection area when said at least one coded information is detected, said association being made on the basis of said position of said at least one portion of said at least one object with respect to the predetermined reference system and of said position of said at least one portion of said at least one coded information with respect to the predetermined reference system. </p><p id="p0039" num="0039">Advantageously, the Applicant has found that the method of the present invention allows carrying out, in a system for the optical identification of objects in motion through digital cameras, the correct code - object association on the basis of a distance reading that only results from the analysis of the image acquired by the camera, that is without the need of using additional devices other than a conventional camera. 
<!-- EPO <DP n="11"/>-->
In particular, referring for simplicity of explanation to the case in which the optical identification system comprises a single camera, the Applicant has noted that, one reference physical dimension being known or determined (as described hereinafter) on a surface of the object, or the coded information, , and the magnification ratio of the imaging sensor mounted in the camera being known, it is possible to determine the distance of said surface from said camera. By using this distance information in combination with the one relating to the position of the coded information along the advancing direction of the objects within the image acquired by said camera, it is possible to determine the position of the coded information along the advancing direction with respect to a fixed reference system. It is therefore possible to proceed with assigning said coded information to the object that, at the time of the acquisition of the image containing the coded information by the camera, is in a corresponding position. </p><p id="p0040" num="0040">If the optical identification system comprises more than one camera, once the relative position of these cameras in the fixed reference system and the respective magnification ratios are known, it is possible to determine the distance of the object from all the above cameras through the measurement technique described above. </p><p id="p0041" num="0041">According to the present invention, the image by which the presence of the coded information is detected may also differ from the image by which the object travelling is detected. Moreover, the camera by which the image containing the coded information is acquired may also be different from the one by which the image containing the travelling object is acquired. </p><p id="p0042" num="0042">If the detection of the travelling object and of the coded information is carried out through distinct cameras, all these cameras have a respective magnification ratio that may be constant or variable over the time, in this latter case it being sufficient to know the extent of said 
<!-- EPO <DP n="12"/>-->
magnification ratio at the time of the acquisition of the respective image. It is therefore possible to determine the position of what framed by the camera with respect to said fixed reference system. </p><p id="p0043" num="0043">If the magnification ratio of the camera (s) is variable, as in the case of an autofocus/zoom system, the extent of such magnification ratio is determined by a proper processing unit of the identification system discussed herein (such processing unit being incorporated or not within the camera but in any case associated with the latter) on the basis of the focusing position of the autofocus/zoom system at the time of the acquisition. </p><p id="p0044" num="0044">According to the present invention, if the objects that travel through the detection area are arranged so as to be spaced from each other along said advancing direction, said reference physical dimension may be defined without distinction on the object or on the coded information. In this case, therefore, the determination of the object distance from the camera does not necessarily require the previous detection of a coded information. </p><p id="p0045" num="0045">On the other hand, if the objects are at least partly arranged side by side with reference to said advancing direction, said reference physical dimension is defined on the coded information. </p><p id="p0046" num="0046">The detection of said reference physical dimension may also take place before the object or coded information enters the detection area framed by the camera (s) of the identification system discussed herein. </p><p id="p0047" num="0047">In a first preferred embodiment of the method of the present invention, which may be carried out if the objects that travel through the detection area are arranged so as to be spaced from each other with reference to said advancing direction, the step of detecting said at least one reference physical dimension comprises the steps of: 
<!-- EPO <DP n="13"/>-->
- determining the displacement of said at least one object along said advancing direction; </p><p id="p0048" num="0048"> - determining the maximum size of said at least one object along said advancing direction on the basis of said displacement . </p><p id="p0049" num="0049">In this case, therefore, the reference physical dimension is directly taken on the object and advantageously corresponds exactly to the maximum size of the object along the advancing direction. </p><p id="p0050" num="0050">If the advancing speed of the object is constant, said displacement may be determined using a presence sensor (for example a photocell) . In that case, such displacement corresponds to the time interval between the moment at which the sensor indicates the beginning of the object entry within an observation area and the moment at which the presence sensor indicates the end of the object entry within said observation area. Such observation area may precede, coincide or be at least partly overlapped to the detection area framed by the camera (s) of the optical identification system discussed herein. </p><p id="p0051" num="0051">If the advancing speed of the object is not constant, said displacement may be determined using, besides a presence sensor, an encoder provided with a proper incremental counter. In that case, the number of unit steps of the encoder is counted by the incremental counter starting from the moment at which the presence sensor indicates the passage of the object. Each counter increase in fact corresponds to a physical displacement of the object along the advancing direction. </p><p id="p0052" num="0052">Preferably, the step of determining the maximum size of said at least one object along said advancing direction comprises the following steps: </p><p id="p0053" num="0053"> - determining the position of a first significant contrast variation along said advancing direction within said at least one image; 
<!-- EPO <DP n="14"/>-->
- determining the number of pixels of said at least one image of said at least one camera occupied by said at least one object on the basis of the position of said first significant contrast variation. </p><p id="p0054" num="0054">Advantageously, the position of said first significant contrast variation may be determined on the basis of the object entry signal provided by said presence sensor, optionally combined with the signal provided by said encoder (if the object advancing speed is not constant) . </p><p id="p0055" num="0055">In any case, once the number of pixels of the image occupied by the object has been determined, it is possible to determine, through the magnification ratio of the camera, the distance at which the object is with respect to the camera itself. </p><p id="p0056" num="0056">More preferably, the images are acquired so that they entirely contain said at least one object. In that case, the step of determining the maximum size of said at least one object along said advancing direction comprises the following steps: </p><p id="p0057" num="0057"> - determining the position of a first significant contrast variation and of a last significant contrast variation along said advancing direction within said at least one image ; </p><p id="p0058" num="0058"> - determining the number of pixels of said at least one image occupied by said at least one object on the basis of the positions of said first significant contrast variation and said last significant contrast variation. </p><p id="p0059" num="0059">In this way, the determination of the maximum size of the object along the advancing direction is simpler and more immediate to be carried out. </p><p id="p0060" num="0060">In a different embodiment of the method of the present invention, said at least one reference physical dimension is defined by a known dimension of a logo or different graphical element applied onto the object. 
<!-- EPO <DP n="15"/>-->
In a second preferred embodiment of the method of the present invention, which does not necessarily require the objects that travel through the detection area to be arranged so as to be spaced from each other with reference to the advancing direction, the detection of said at least one reference physical dimension comprises the steps of determining the physical dimension of said at least one coded information along at least one characteristic direction, said at least one coded information having a predetermined optical resolution. </p><p id="p0061" num="0061">In this case, therefore, the reference physical dimension is taken on the coded information_and advantageously corresponds to the length of the coded information along at least one predetermined direction. </p><p id="p0062" num="0062">Throughout the present description and the following claims, the expression "predetermined optical resolution" is used to indicate an optical resolution of known extent given by a single value corresponding to an expected resolution or by a range of values within such a tolerance range as to cause a positioning error of the coded information smaller than the minimum admissible distance between two adjacent objects. </p><p id="p0063" num="0063">As shall appear more clearly from the following description, in preferred embodiments of the method of the present invention said coded information is an optical code having a predetermined optical resolution and/or belonging to a predetermined symbology. In that case, said characteristic direction is the direction along which the elements (or information characters) of the optical code follow one another. </p><p id="p0064" num="0064">Advantageously, the Applicant has observed that by previously setting the value (or range of values) of said optical resolution and the magnification ratio of the imaging sensor mounted in the camera (s) it is possible to determine the distance of the coded information from the 
<!-- EPO <DP n="16"/>-->
camera(s) on the basis of the comparison between the actual known dimensions and the dimensions in the image acquired by the camera (s) . </p><p id="p0065" num="0065">Preferably, said predetermined optical resolution, if variable, varies by ± 15%, more preferably by ± 5% with respect to a predetermined reference value. </p><p id="p0066" num="0066">Preferably, the step of determining the physical dimension of said at least one coded information comprises the following steps: </p><p id="p0067" num="0067"> - determining the position of a first significant contrast variation along said at least one characteristic direction;</p><p id="p0068" num="0068">- determining the number of pixels of said at least one image of said at least one camera or of said at least one distinct camera occupied by said coded information starting from the position of said first significant contrast variation and on the basis of said predetermined optical resolution. </p><p id="p0069" num="0069">Advantageously, by counting the number of pixels of the image occupied by said coded information it is possible to determine, through the magnification ratio of the camera, the distance at which the coded information is with respect to the camera itself. </p><p id="p0070" num="0070">More preferably, the images are acquired so that they entirely contain said at least one coded information. In that case, the step of determining the physical dimension of said at least one coded information preferably comprises the following steps: </p><p id="p0071" num="0071"> - determining the position of a first significant contrast variation and of a last significant contrast variation along said at least one characteristic direction; </p><p id="p0072" num="0072"> - determining the number of pixels of said at least one image of said at least one camera or of said at least one distinct camera occupied by said coded information starting from the positions of said first significant contrast variation and of said last significant contrast variation 
<!-- EPO <DP n="17"/>-->
and on the basis of said predetermined optical resolution. </p><p id="p0073" num="0073">In this way, the determination of the physical dimension of the coded information along the advancing direction is simpler and more immediate to be carried out. </p><p id="p0074" num="0074">Preferably, the step of determining the physical dimension of said at least one coded information comprises the step of measuring at least one element of said optical code along said at least one characteristic direction. </p><p id="p0075" num="0075">In particular, if said at least one coded information is a linear optical code (that is, a code whose elements extend along a single direction) , the step of determining a physical dimension of said at least one coded information comprises the step of determining the dimension of said optical code along said single direction. On the other hand, if said at least one coded information is an optical code whose elements extend along at least two predetermined orthogonal directions (for example a two-dimensional code<sup>'</sup> or other types of code) , the step of determining a physical dimension of said at least one coded information comprises the step of determining the dimension of said optical code along at least one of said at least two predetermined orthogonal directions. </p><p id="p0076" num="0076">Advantageously, the Applicant has noted that the types and the optical resolutions of the optical codes used in the object sorting systems are known and in any case limited. Therefore, the physical dimension of the optical code along a predetermined direction (in the case of linear codes) or along two predetermined orthogonal directions (in the case of two-dimensional codes or other types of code) is substantially constant (if the number of elements of the optical code is constant) or in any case a function of said number of information characters (if such number is variable) . The physical dimensions of the optical code therefore are either known in advance (if the number of elements of the optical code is constant) or they may be 
<!-- EPO <DP n="18"/>-->
determined upon reading of the same optical code (if the number of elements of the optical code is variable) . It is therefore possible to determine the physical dimension of the code along the advancing direction through the measurement of at least one element of such optical code along a predetermined characteristic direction. </p><p id="p0077" num="0077">In any case, an analysis of the type described above is especially advantageous since the physical dimensions of the optical code or part thereof do not depend on the printing quality and can therefore be previously set on the system. </p><p id="p0078" num="0078">If the reference physical dimension is taken on the optical code, the above-mentioned contrast variations are defined by the first and/or last element of the code along a predetermined direction or by the opposite sides of an optional frame inside of which the code elements are. If said frame exists, it may be detected and measured once the coded information has been read and the characteristic direction of the optical code has been determined (through a suitable software) . The physical dimensions of the optical code may therefore be obtained from the distance between the first element and the last element of the optical code along a respective direction, or . from the distance between the two quiet zones often provided before the first element and after the last element of the optical code along a predetermined direction, or also from the dimension of one element in a direction orthogonal to the development direction of the code. </p><p id="p0079" num="0079">Throughout the present description and the following claims, the expression "quiet zone" is used to indicate a white portion of predetermined width arranged before the beginning and after the end of the sequence of code elements and usually extended starting from such end elements up to the edge of the frame that includes said sequence of elements. 
<!-- EPO <DP n="19"/>-->
Preferably, said optical code belongs to a predetermined symbology and said at least one reference physical dimension is determined on the basis of said predetermined symbology . </p><p id="p0080" num="0080">The Applicant has advantageously observed that the number of characters of an optical code may be uniquely given, or reduced to a limited number of possibilities, by the particular type of symbology used. Once the symbology has been known, it is therefore possible to determine the physical dimension of the optical code. </p><p id="p0081" num="0081">In preferred embodiments of the present invention, the step of determining the position of the first significant contrast variation along said at least one predetermined direction is repeated more times along parallel paths. Such operation is advantageous for validating the results obtained. </p><p id="p0082" num="0082">Said step preferably includes, moreover, the step of travelling through said at least one predetermined direction along two opposite directions. </p><p id="p0083" num="0083">Preferably, the step of determining the position of said at least one portion of said at least one object comprises the steps of: </p><p id="p0084" num="0084"> - detecting the instant at which said at least one object travels through a predetermined detection position; </p><p id="p0085" num="0085"> - determining the displacement of said at least one object along the advancing direction. </p><p id="p0086" num="0086">Preferably, the step of determining the displacement of said at least one object comprises the steps of: </p><p id="p0087" num="0087"> - comparing the position of said at least one portion of said at least one image in at least two acquisitions; </p><p id="p0088" num="0088"> - calculating the displacement of said at least one object on the basis of said comparison. </p><p id="p0089" num="0089">The two above-mentioned acquisitions need not necessarily be consecutive. 
<!-- EPO <DP n="20"/>-->
As an alternative, a conventional encoder associated with the conveyor belt may be used, as in the system shown in figure 2. Of course, in those cases where the conveyor belt moves at a constant speed, it is not necessary to determine the movement of the object along the advancing direction, since such displacement is known in advance. </p><p id="p0090" num="0090">In a preferred embodiment thereof, the method of the present invention further comprises the step of detecting the distance of said at least one object by a specific distance measuring device distinct from the camera. </p><p id="p0091" num="0091">Such solution is particularly advantageous in those cases when there is uncertainty in the code-object association due to the fact that the coded information is in the proximity of the object edges and/or objects arranged very close to each other travel through the field of view of the camera (such as for example in the case shown in figures 1A and IB) . In that case, the code-object association is made on the basis of the comparison between the object distance information coming from said distance measuring device and the distance information of the coded information with respect to the camera. Said distance measuring device therefore acts as an auxiliary device for reducing the uncertainty in the code-object association. </p><p id="p0092" num="0092">In particularly preferred embodiments thereof, the method of the present invention further comprises the following steps : </p><p id="p0093" num="0093"> - determining the position of said at least one object along two orthogonal directions within said at least one image; </p><p id="p0094" num="0094"> - determining the footprint of said at least one object in a plane with respect to said predetermined reference system on the basis of the position of said at least one object along said two orthogonal directions and of said distance. </p><p id="p0095" num="0095">Even more preferably, the method of the present invention further comprises the step of determining the volume of 
<!-- EPO <DP n="21"/>-->
said at least one object on the basis of said footprint and of said distance. </p><p id="p0096" num="0096">In this way, advantageously, all the object dimensions are determined and thus, the position of the object in the fixed reference system is uniquely determined. Accordingly, once the position of each camera with respect to the other ones and to the fixed reference system is known, also the relative position between object and any other cameras of the optical identification system is uniquely determined. Each camera may therefore communicate the information acquired to the other cameras, thus allowing the problem of the code-object association to be solved in any case, even in those cases in which this problem cannot be solved with a single camera and requires the intervention of the other cameras . </p><p id="p0097" num="0097">In a particularly preferred embodiment of the present invention, at least two distinct cameras are used, wherein at least one camera is positioned so as to frame said at least one object from above the object and at least another camera is positioned so as to frame said at least one object from one of the sides thereof. Preferably, at least one of said cameras is positioned on at least one side of the conveyor belt on which the objects are arranged, so as to frame at least one between the right and left side faces of the objects. However, alternative embodiments are not excluded wherein said camera or at least one further camera is positioned in front or back position with respect to said conveyor belt, so as to frame at least one between the front and back faces of the objects. </p><p id="p0098" num="0098">In particularly preferred embodiments of the present invention, said at least one camera and, if present, said at least one distinct camera, is arranged so that its optical axis is perpendicular to said advancing direction. </p><p id="p0099" num="0099">In that case, preferably, said at least one image is acquired by said at least one camera when said at least one 
<!-- EPO <DP n="22"/>-->
object is at said optical axis. </p><p id="p0100" num="0100">Advantageously, the choice of the particular position described above for the image acquisition allows reducing the possibilities of error in the code-objects association in those situations in which the objects are arranged as shown in figures 1A and IB, thanks to the minimisation of any relative darkening effect among the objects, and to the maximisation of the probability that the framed object is the one of interest. Moreover, any possible error caused by perspective deformations is prevented. </p><p id="p0101" num="0101">Preferably, the method of the present invention further comprises the steps of: </p><p id="p0102" num="0102"> - determining the distance of a plurality of points of said at least one surface portion; </p><p id="p0103" num="0103"> - determining a possible rotation angle between the optical axis of said at least one camera and the surface portion on the basis of the distance of said plurality of points. </p><p id="p0104" num="0104">It is therefore possible to determine the aforementioned reference physical dimension also in the presence of objects or coded information which are rotated about the optical axis of the camera and with the camera arranged at the side, front or back of the conveyor belt, in those cases wherein the coded information is on a face of the object other than the upper or lower one. </p><p id="p0105" num="0105">Preferably, the distance of said plurality of points is determined on the basis of the analysis of a perspective deformation of said at least one coded information. </p><p id="p0106" num="0106">In this way it is possible to determine the aforementioned reference physical dimension also in the presence of coded information arranged in a rotated position on one of the object faces. </p><p id="p0107" num="0107">More preferably, said optional angle of rotation is determined on the basis of the analysis of said footprint. 
<!-- EPO <DP n="23"/>-->
Further characteristics and advantages of the present invention will appear more clearly from the following detailed description of a preferred embodiment thereof, made by way of a non-limiting example with reference to the attached drawings. In such drawings: </p><p id="p0108" num="0108"> - figure 3 is a schematic view of one of the preferred embodiments of an object identification system that carries out the method of the present invention; </p><p id="p0109" num="0109"> - figures 4a-4d show various exemplary applications of a technique carried out in one of the embodiments of the method of the present invention; </p><p id="p0110" num="0110"> - figure 5a is a schematic top view of a possible operating configuration of the system that carries out the method of the present invention; </p><p id="p0111" num="0111"> - figure 5b shows an exemplary application of a further technique carried out in one of the embodiments of the method of the present invention; </p><p id="p0112" num="0112"> - figure 6a is a schematic top view of a further possible operating configuration of the system that carries out the method of the present invention; </p><p id="p0113" num="0113"> - figure 6b shows an image of the coded information that may be detected by the system of figure 6a; </p><p id="p0114" num="0114"> - figure 7a is a schematic side view of a further possible operating configuration of the system that carries out the method of the present invention; </p><p id="p0115" num="0115"> - figure 7b shows an image of the coded information that may be detected by the system of figure 7a; </p><p id="p0116" num="0116"> - figure 8a is a schematic top view of a further possible operating configuration of the system that carries out the method of the present invention; </p><p id="p0117" num="0117"> - figure 8b shows an image of the coded information that may be detected by the system of <sup>'</sup> figure 8a. </p><p id="p0118" num="0118">In figure 3, reference numeral 10 indicates an object identification system that carries out a first preferred embodiment of the method of the present invention. </p><p id="p0119" num="0119">System 10 comprises a conveyor belt 1 which moves a 
<!-- EPO <DP n="24"/>-->
plurality of objects 3 along an advancing direction A with respect to a digital camera 2, each object being provided with an optical code (not visible) on the top face thereof, such an optical code having a predetermined optical resolution . </p><p id="p0120" num="0120">The camera 2 is arranged in a predetermined position with respect to the conveyor belt 1. In particular, it is arranged above the conveyor belt 1 and frames a detection area 4 shaped as a pyramid through which objects 3 travel. </p><p id="p0121" num="0121">Preferably, as shown, the camera 2 is arranged with its optical axis perpendicular to conveyor belt 1 and acquires images when objects 3 travel at said optical axis. </p><p id="p0122" num="0122">As shown, at least some of objects 3 are at least partly arranged side by side with reference to said advancing direction A. </p><p id="p0123" num="0123">A processing unit 5, connected to the camera 2, is capable of decoding the optical codes associated with objects 3 when these are within the detection area 4. Such unit may also be incorporated into the camera 2. </p><p id="p0124" num="0124">The camera 2 comprises an imaging sensor (not visible) having a predetermined magnification ratio and a predetermined optical resolution. Camera 2 may also incorporate a variable magnification system whose magnification ratio at the time of the acquisition is known, as described above. </p><p id="p0125" num="0125">The predetermined magnification ratio and the predetermined optical resolution are previously set on the processing unit 5, along with a reference physical dimension taken on the surface of the optical code and linked to its resolution and/or to the type of symbology used for the optical coding of the optical code. Such dimension may be a single value corresponding to an expected resolution or a limited range of values. 
<!-- EPO <DP n="25"/>-->
Through the presence of an entry sensor 8 arranged in the proximity of conveyor belt 1 upstream of the detection area 4 with reference to the advancing direction A, camera 2 detects the entry of a plurality of objects 3 within the detection area 4 and, on the basis of the knowledge of the advancing speed of the conveyor belt 1 (known in advance if such speed is constant, or determined by the use of an encoder if such speed is not constant) , it allows the processing unit 5 to synchronise the repeated acquisition of image sequences by the camera 2 with the instant of entry of each object 3 within the detection area 4. </p><p id="p0126" num="0126">The processing unit 5 is further capable of determining, through the camera 2, the position of each optical code and of each object 3 in a reference plane parallel to the top surface of the conveyor belt 1. </p><p id="p0127" num="0127">As shall be better described hereinafter, according to the present invention, knowing the optical resolution of the optical codes, the symbology they belong to and the magnification ratio of the imaging sensor of the camera 2, from the analysis of a single image acquired by the camera 2 the processing unit 5 is capable of determining the distance of the optical codes from the camera 2. </p><p id="p0128" num="0128">Combining this distance information with the information relating to the position of each optical code in said reference plane, the processing unit 5 is capable of determining the position of the optical code along the advancing direction A with respect to a predetermined reference system. </p><p id="p0129" num="0129">Such position is compared with that of objects 3 so as to associate the optical code read with that specific object 3 which, at the time of the image acquisition by the camera 2, has a corresponding position. </p><p id="p0130" num="0130">The position of objects 3 along the advancing direction A is known once the instant at which each object travels at a 
<!-- EPO <DP n="26"/>-->
predetermined detection position (which may precede the detection area 4 or be within the latter) has been detected and when the advancing speed of conveyor belt 1 is known. </p><p id="p0131" num="0131">Such speed may be deemed as constant (and thus known in advance) or, if it is variable, measured in real time, as described above. </p><p id="p0132" num="0132">In the embodiment shown in figure 3, the measurement of the advancing speed of the objects (and thus of the temporal position thereof) is directly carried out by the processing unit 5 through the camera 2 and on the basis of the analysis of the displacement of any article framed (for example the optical code or an object 3) in two subsequent acquisitions . </p><p id="p0133" num="0133">In particular, this measurement may be obtained by analysing only a portion of the image each time acquired, so as to reduce the processing times. This is possible thanks to the capability of some sensors of acquiring and transmitting a signal generated by just a part of the total pixels of the image acquired, for example only a line parallel to the scanning direction, or only a window of reduced dimensions (this capability is known by the term: windowing, or sensor windowing) . </p><p id="p0134" num="0134">In a preferred embodiment of the present invention, the information that links the displacement of the image portion analysed to the advancing speed of conveyor belt 1 may be obtained by analysing the position of at least one portion of an element of the optical code (for example the bars and/or spaces of a barcode) within the image. </p><p id="p0135" num="0135">As an alternative, any sign or element of any geometrical shape arranged on the face itself of the object 3 or on the conveyor belt 1 may be used, provided that the dimensions thereof are known in advance. </p><p id="p0136" num="0136">Alternatively, once the distance has been determined, the displacement speed of a front (contrast variation) may be 
<!-- EPO <DP n="27"/>-->
measured, which identifies the edge of the object or of the optical code (or of another optional sign or element) on a sequence of images acquired by camera 2. This measurement may be made in real time, correcting image after image, the detected speed value. </p><p id="p0137" num="0137">The technique for measuring the distance of the optical code according to the present invention shall now be described with particular reference to a linear optical code and to figures 4a-4d, wherein reference numeral 30 indicates a barcode. What described hereinafter applies likewise to codes of different types too, such as for example a two-dimensional code, etc. </p><p id="p0138" num="0138">Code 30 comprises a plurality of elements (all indicated with B and, in the specific case illustrated herein, defined by bars and spaces) that follow one another along a predetermined horizontal direction 0. </p><p id="p0139" num="0139">Unlike figures 4a and 4b, code 30 of figures 4c and 4d comprises a frame 31 that defines a quiet zone 32a on the left of the first element Bl and a quiet zone 32b on the right of the last element Bn. </p><p id="p0140" num="0140">According to the present invention and as already anticipated above, the measurement of the distance of code 30 from camera 2 is carried out using the code 30 itself, once its optical resolution, magnification ratio of the imaging sensor mounted in camera 2 (such magnification ratio being known at least at the time of the acquisition of the relevant image) and optionally the symbology the optical code belongs to, are known. If the optical resolution is variable, it preferably varies by ± 15%, more preferably by + 5% with respect to a predetermined reference value. </p><p id="p0141" num="0141">The distance measurement in particular envisages an analysis of code 30 through a plurality of parallel linear paths, horizontal or vertical, searching for the position 
<!-- EPO <DP n="28"/>-->
of the first significant contrast variation. </p><p id="p0142" num="0142">In the specific example shown in figure 4a, a first horizontal path Ola is made from left to right and preferably also an opposite path Olb from right to left. The first significant contrast variation will in this case be detected at the first element Bl and at the last element Bn, respectively. </p><p id="p0143" num="0143">As an alternative, as shown in the specific example shown in figure 4b, a first vertical path 02a is made from the top to the bottom and preferably also an opposite path 02b from the bottom to the top. The first significant contrast variation will in this case be detected at any element B of code 30, respectively. In this case, it is important to repeat the analysis by making multiple vertical paths until a bar of code 30 is intercepted. </p><p id="p0144" num="0144">As an alternative, as shown in the specific example shown in figure 4c, a first horizontal path 03a is made from left to right and preferably also an opposite path 03b from right to left. The first significant contrast variation will in this case be detected at a vertical side of frame 31 and at the opposite vertical side of said frame, respectively. </p><p id="p0145" num="0145">As an alternative, as shown in the specific example shown in figure 4d, a first path 04a is made from the top to the bottom and preferably also an opposite path 04b from the bottom to the top. The first significant contrast variation will in this case be respectively detected at a horizontal side of frame 31 and of the opposite horizontal side of said frame. Once the position of the first significant contrast variation in at least one of the two opposite paths has been detected according to one of the methods shown in figures 4a-4d, it is possible to calculate the number of image pixels occupied by code 30. This data represents the 
<!-- EPO <DP n="29"/>-->
dimension of code 30 in terms of pixels at distance D in the image plane (hereinafter also "Pixels occupied at D") . </p><p id="p0146" num="0146">The geometrical distance D of code 30 from camera 2 may be obtained applying the formula: </p><p id="p0147" num="0147">Pixel occupied at Dmax/Sensor Resolution </p><p id="p0148" num="0148"> D= Code-camera distance = <sup>*</sup>Dmax </p><p id="p0149" num="0149"> Pixel occupied at D/ Sensor Resolution wherein Dmax is. the distance between camera 2 and conveyor belt 1 (previously set in the processing unit 5) . </p><p id="p0150" num="0150">Knowing the optical resolution of the particular type of sensor mounted in camera 2, camera 2 frames at distance Dmax a rectangle with sides LA and LB, wherein the dimensions of sides LA and LB are known. The ratio between the number of pixels occupied at distance Dmax from the code and the sensor resolution may therefore be obtained from ratio So/LA or Sv/LB (depending on whether code 30 is being analysed along a horizontal or a vertical direction) , where So is the horizontal dimension of code 30 and Sv is the vertical dimension of code 30. Dimensions So and Sv are previously set in the processing unit 5 since substantially constant or obtainable on the basis of the number of elements B of the code upon reading the optical code (if such number is variable) , knowing also the symbology used and/or measuring the dimension of the single element of the code along the direction in which the various elements of the code itself follow one another. </p><p id="p0151" num="0151">In the case of two-dimensional codes, or more in general of codes that extend along two parallel directions, the technique described above may be repeated along at least one horizontal direction or at least one vertical direction . </p><p id="p0152" num="0152">In order to validate the results obtained with the search technique described above it is preferable to repeat said 
<!-- EPO <DP n="30"/>-->
technique by a plurality of horizontal and/or vertical parallel paths. </p><p id="p0153" num="0153">A different embodiment of the method of the present description is described hereinafter with reference to figure 5, which as an alternative to the embodiments described above may be used if objects 3 are arranged on conveyor belt 1 so as to be spaced from each other with reference to the advancing direction A. </p><p id="p0154" num="0154">Such embodiment differs from the one described above with reference to figure 3 in that the determination of the distance of each object 3 from the camera does not necessarily require the prior detection of a coded information. In fact, the distance may be determined starting from the maximum size of object 3 along the advancing direction A. Such maximum size is determined on the basis of the displacement of object 3 along the advancing direction A, as described in detail hereinafter. </p><p id="p0155" num="0155">As shown, also in this case the object identification system 10 comprises a conveyor belt 1 on which objects 3 are arranged, at least one camera (not visible) , the presence sensor 8 and the encoder 7. Of course, if the advancing speed of conveyor belt 1 is constant, using the encoder 7 is not required. </p><p id="p0156" num="0156">As in the embodiment shown in figure 3, also in this case for simplicity of explanation it is considered that the camera is arranged above the conveyor belt 1. </p><p id="p0157" num="0157">The encoder 7 and the presence sensor 8, if provided, are both connected to a motion control unit (not shown) provided with an incremental counter. Such motion control unit may be incorporated in the camera or be a unit separate from the latter. </p><p id="p0158" num="0158">The determination of the displacement of objects 3 along the advancing direction A occurs through a suitable detection of the signals provided by the presence sensor 8 
<!-- EPO <DP n="31"/>-->
and by the encoder 7, as described hereinafter. </p><p id="p0159" num="0159">In the specific example illustrated in figure 5a, three rectangular objects 3 respectively identified with K, K-l and K-2 are shown. Object K is aligned with the advancing direction A and is shown in the position in which, during the displacement thereof along the advancing direction A, it has reached the presence sensor 8, which has been activated in advance. At an instant, which may be coincident with or subsequent to the instant of reaching the sensor 8, the camera detects a significant contrast variation and stores the value of the incremental counter of the encoder 7 at this instant (first value) . When the back of object K has passed the presence sensor 8, at an instant, which may be coincident with or subsequent to the instant of passing the sensor 8, the camera detects a new significant contrast variation and stores the value of the incremental counter at this instant (second value) . </p><p id="p0160" num="0160">The difference between the second value and the first value represents the differential of the encoder 7, which depends on the maximum size of object 3 along the advancing direction A, which in the specific case of object K corresponds to its length (indicated with I in the figure) . In particular, said maximum size may be determined by multiplying the differential of the encoder 7 by the unit pitch of the encoder. </p><p id="p0161" num="0161">The same steps described above may be applied to objects K- 1 and K-2. In these cases, the maximum size of the two objects determined as described above does not represent the length of the respective objects but the projection of the respective lengths along the advancing direction A (respectively indicated with I' and I' ' ) . </p><p id="p0162" num="0162">Proceeding as described above it is possible to assign to each object 3 a respective maximum size, which is suitably stored. 
<!-- EPO <DP n="32"/>-->
As shown in figure 5a, the displacement of objects 3 (and thus the maximum size thereof) is preferably determined at a presence sensor 8 which is arranged in an observation area that precedes the detection area 4 framed by the camera. Alternative embodiments are in any case envisaged wherein said observation area coincides or is at least partly overlapped to the detection area 4. </p><p id="p0163" num="0163">After a predetermined number of unit encoder steps, each object 3 reaches the detection area 4. The camera preferably acquires the image when object 3 is at the optical axis of the camera. The first significant contrast variation and, preferably, the last significant contrast variation along the advancing direction A are identified through image processing techniques. One of above-mentioned techniques provides for tracing a plurality of parallel lines 05b all perpendicular to the advancing direction A, as shown in figure 5b and measuring the number of pixels in the image that space the two extreme parallel lines. The overall size in pixels (indicated with I in figure 5b) of object 3 in the advancing direction A is thus determined. </p><p id="p0164" num="0164">By comparing the maximum size determined beforehand on the basis of the signals provided by the presence sensor 8 and by the encoder 7 with the size in pixels it is possible to determine the distance of object 3 from the camera. </p><p id="p0165" num="0165">As an alternative, the aforementioned reference physical dimension is a logo or a different graphic element of known dimension associated with object 3. </p><p id="p0166" num="0166">In all the embodiments described above, considering that the contrast between object 3 and conveyor belt 1 highlights the position, in the image acquired by the camera, of object 3 along two orthogonal directions, it is possible to determine the footprint in pixels of object 3 and, knowing the distance of object 3 from the camera, the footprint of object 3 in the fixed reference system. Based on this footprint and on the distance of the object from 
<!-- EPO <DP n="33"/>-->
the camera, it is possible to determine the volume of object 3. </p><p id="p0167" num="0167">The techniques described above may be applied both to the cases where camera 2 frames objects 3 from the top (as shown in figure 3) , and where camera 2 is arranged laterally (as shown in figure 6a and 8a) , frontally (as shown in figure 7a), or at the back with respect to objects 3, in order to capture optical codes arranged on any one of the object surfaces. In the case of a reading carried out on the bottom surface, the distance of the optical code from the camera is uniquely determined and corresponds to the distance between camera and conveyor belt. </p><p id="p0168" num="0168">The techniques described above, moreover, are applicable to the cases in which the optical codes lies on a plane not perfectly perpendicular to the optical axis of camera 2, as shown in figure 6a, 7a and 8a. In these cases, the analysis of the optical code at multiple points allows detecting whether object 3 is or not rotated and on which side. </p><p id="p0169" num="0169">Let us consider for example a reading of a code arranged on a side face of an object 3 with reference to an advancing direction A that goes from right to left (figure 6a) . Using one of the techniques described above it is possible to determine the distance of a plurality of points of said side face from camera 2. If, as shown in figure 6b, it is found that the physical dimension of the optical code 30 detected along a vertical path on the left is greater than the physical dimension detected along a vertical path on the right, object 3 will then be rotated with the front side towards camera 2. Indeed, the more camera 2 is approached, the more the area framed by the same decreases and the number of pixels occupied increases proportionally. This occurs provided that the top horizontal measurement coincides with the bottom horizontal measurement, that is, the plane on which the code lies is perpendicular to the conveyor belt. 
<!-- EPO <DP n="34"/>-->
In this case, the distance of the optical code 30 may be calculated by making an arithmetical average between the left vertical measurement and the right vertical measurement . </p><p id="p0170" num="0170">Likewise it is possible to calculate the code distance from the camera 2 when object 3 is not rotated with respect to the camera 2 and the code lies on a surface perpendicular to the advancing direction A (figure 7a) . In this case, the distance of code 30 may be calculated by making an arithmetical average between the top horizontal measurement and the bottom horizontal measurement (figure 7b) . </p><p id="p0171" num="0171">On the other hand, if code 30 is rotated in the plane on which it lies (figures 8a and 8b), that is, the sides thereof are not parallel or perpendicular to the advancing direction A, the techniques described above allow determining the orientation of code 30, and optionally rotating the image in the plane so as to return to one of the previous cases. </p><p id="p0172" num="0172">In general, once the reference physical dimension has been determined in one of the ways described above, from the analysis of the footprint in pixels of object 3 (such footprint being optionally determined through a proper perspective deformation of object 3) and from the position thereof within the image it is possible, through even more complex geometrical transformations than a simple rotation, to obtain the orientation of object 3 with respect to the camera, and accordingly the overall distance thereof. </p><p id="p0173" num="0173">There may be situations in which there is uncertainty in the code-object association due to the presence of coded information in the proximity of the edges of object 3 and/or objects 3 very close to one another. </p><p id="p0174" num="0174">In that case, preferably, the distance measurement techniques described above are combined with a distance signal detected by a proper distance measuring device 
<!-- EPO <DP n="35"/>-->
arranged in the proximity of the conveyor belt 1 (such as for example the height measuring device 6 of figure 2) . </p><p id="p0175" num="0175">In the practice, the code-object association is made in this case on the basis of the comparison between the height information of objects 3 coming from said distance measuring device and the measurement of the distance of the read code from the camera 2 . If the distance measuring device signals an object 3 with height Al whose passage begins at Til and ends at T12, and camera 2, knowing the travelling time of a particular object 3 within the detection area 4, detects one or more codes at the same height within the corresponding zone of the detection area 4, then these codes are assigned to that object 3. If no codes are read in the presence of an "object presence" signal, this activates a "non identified object" signal, which may generate a particular action by the system. </p><p id="p0176" num="0176">As an alternative, all the codes contained in the image are read and, at the same time, all the "object entry" signals coming from the presence sensor are collected, after which a probabilistic code-to-object association is made using the highest amount of information available for such association to be as correct as possible. </p><p id="p0177" num="0177">In the preferred embodiments of system 10 described above, it is used at least one camera arranged above the conveyor belt 1 (as in figure 3) and at least one camera arranged on the side of the conveyor belt 1 (as in figure 6a and 8a) . A camera may also be provided in front of the conveyor belt 1 (as in figure 7a) . The relative position of such cameras and the position of each one of them in the fixed reference system shall be known and can therefore be previously set in the system. The above-mentioned cameras may in this way exchange the information acquired so as to obtain a sufficient amount of information to proceed with a correct association between code and object. </p><p id="p0178" num="0178">It is clear that a man skilled in the art may make further 
<!-- EPO <DP n="36"/>-->
changes and modifications to the invention described hereinbefore in order to meet specific and contingent application requirements, these changes and modifications in any case falling within the scope of protection defined by the following claims. 
</p></description><claims mxw-id="PCLM44852515" ref-ucid="WO-2012117283-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="37"/>-->- 36 - CLAIMS </claim-statement><claim id="clm-0001" num="1"><claim-text>1. Method for the optical identification of objects in motion, comprising the following steps: </claim-text><claim-text> - acquiring, by at least one camera (2) having, at least during such an acquisition, a respective predetermined magnification ratio, at least one image of a predetermined detection area (4), said at least one image containing at least one portion of at least one object (3) travelling through the detection area (4) along a predetermined advancing direction (A) ; </claim-text><claim-text> - determining the position of said at least one portion of said at least one object (3) along the predetermined advancing direction (A) with respect to a predetermined reference system; </claim-text><claim-text> - detecting, in said at least one image or in at least one subsequent image containing said at least one portion of at least one object (3), at least one coded information (30) travelling through the detection area (4) along the predetermined advancing direction (A) , said detection being carried out, respectively, by said at least one camera (2) or by a distinct camera having a predetermined relative position with respect to said at least one camera (2) and, at least during the acquisition of said at least one image or of said at least one subsequent image, a respective predetermined magnification ratio; </claim-text><claim-text> - reading said at least one coded information (30) ; </claim-text><claim-text> - determining the position of at least one portion of said at least one coded information (30) along the predetermined advancing direction (A) within the image; </claim-text><claim-text> - detecting, by said at least one camera (2) or said at least one distinct camera, at least one reference physical dimension belonging to at least one surface portion of said at least one object (3) or of said at least one coded information (30); </claim-text><claim-text> - determining the distance of said at least one surface portion from said at least one camera and, if provided, from said at least one distinct camera on the basis of said <!-- EPO <DP n="38"/>--> -37- at least one reference physical dimension and of the magnification ratio of the camera by which said at least one reference physical dimension has been detected; </claim-text><claim-text> - determining the position of said at least one portion of coded information (30) along the predetermined advancing direction (A) with respect to the predetermined reference system on the basis of said distance and of the position of said at least one portion of said at least one coded information (30) along the predetermined advancing direction (A) within the image; </claim-text><claim-text> - associating said at least one coded information (30) with a respective object (3) travelling through the detection area (4) when said at least one coded information (30) is detected, said association being made on the basis of said position of said at least one portion of said at least one object (3) with respect to the predetermined reference system and of said position of said at least one portion of said at least one coded information (30) with respect to the predetermined reference system. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. Method according to claim 1, wherein the step of detecting at least one reference physical dimension comprises the steps of: </claim-text><claim-text> - determining the displacement of said at least one object (3) along said advancing direction (A); </claim-text><claim-text>- determining the maximum size of said at least one object (3) along said advancing direction (A) on the basis of said displacement . </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. Method according to claim 2, wherein the step of determining the maximum size of said at least one object (3) along said advancing direction (A) comprises the following steps: </claim-text><claim-text> - determining the position of a first significant contrast variation along said advancing direction (A) within said at least one image; </claim-text><claim-text>- determining the number of pixels of said at least one image of said at least one camera (2) occupied by said at <!-- EPO <DP n="39"/>--> -38- least one object (3) on the basis of the position of said first significant contrast variation. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. Method according to claim 2, wherein said at least one image entirely contains said at least one object (3) and the step of determining the maximum size of said at least one object (3) along said advancing direction (A) comprises the following steps: </claim-text><claim-text> - determining the position of a first significant contrast variation and of a last significant contrast variation along said advancing direction (A) within said at least one image; </claim-text><claim-text> - determining the number of pixels of said at least one image occupied by said at least one object (3) on the basis of the positions of said first significant contrast variation and said last significant contrast variation. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. Method according to claim 1, wherein said at least one reference physical dimension is defined by a known dimension of a logo or different graphical element applied onto said at least one object (3) . </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. Method according to claim 1, wherein said at least one coded information (30) has a predetermined optical resolution and wherein the step of detecting at least one reference physical dimension comprises the step of determining the physical dimension of said at least one coded information (30) along at least one characteristic direction . </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. Method according to claim 6, wherein the step of determining the physical dimension of said at least one coded information (30) comprises the following steps: </claim-text><claim-text>- determining the position of a first significant contrast variation along said at least one characteristic direction;</claim-text><claim-text>- determining the number of pixels of said at least one image of said at least one camera (2) or of said at least one distinct camera occupied by said coded information (30) starting from the position of said first significant <!-- EPO <DP n="40"/>--> contrast variation and on the basis of said predetermined optical resolution. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. Method according to claim 6 or 7, wherein said at least one coded information (30) is a linear optical code and the step of determining the physical dimension of said at least one coded information (30) comprises the step of measuring at least one element of said optical code along said at least one characteristic direction. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. Method according to claim 6 or 7, wherein said at least one coded information (30) is an optical code whose content extends along at least two predetermined orthogonal directions and the step of determining the physical dimension of said at least one coded information (30) comprises the step of determining the dimensions of said optical code along at least one of said at least two predetermined orthogonal directions. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. Method according to claim 8 or 9, wherein said optical code belongs to a predetermined symbology and said at least one reference physical dimension is determined on the basis of said predetermined symbology. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. Method according to any one of the previous claims, wherein the step of determining the position of said at least one portion of said at least one object (3) comprises the steps of: </claim-text><claim-text> - detecting the instant at which said at least one object (3) travels through a predetermined detection position; </claim-text><claim-text> - determining the displacement of said at least one object (3) along said advancing direction (A). </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. Method according to claim 11, wherein the step of determining the displacement of said at least one object (3) comprises the steps of: </claim-text><claim-text> - comparing the position of said at least one portion of said at least one image in at least two acquisitions; </claim-text><claim-text> - calculating the displacement of said at least one object <!-- EPO <DP n="41"/>--> -40- </claim-text><claim-text>(3) on the basis of said comparison. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. Method according to any of the previous claims, further comprising the step of detecting the distance of said at least one object (3) by a specific distance measuring device. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. Method according to any of the previous claims, further comprising the following steps: </claim-text><claim-text> - determining the position of said at least one object (3) along two orthogonal directions within said at least one image; </claim-text><claim-text> - determining the footprint of said at least one object (3) in a plane with respect to said predetermined reference system on the basis of the position of said at least one object (3) along said two orthogonal directions and of said distance. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. Method according to claim 14, further comprising the step of determining the volume of said at least one object (3) on the basis of said footprint and of said distance. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. Method according to any of the previous claims, wherein said at least one camera (2) is arranged so as to frame said at least one object (3) from above the object (3) and said at least one distinct camera is arranged so as to frame said at least one object (3) from any of its sides. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. Method according to any of the previous claims, wherein said at least one camera (2) and, if provided, said at least one distinct camera, is arranged so that its optical axis is orthogonal to said advancing direction (A) . </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. Method according to claim 17, wherein said at least one image is acquired by said at least one camera (2) when said at least one object (3) is at said optical axis. </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. Method according to claim 18, comprising the steps of:</claim-text><claim-text>- determining the distance of a plurality of points of said at least one surface portion; <!-- EPO <DP n="42"/>--> - determining a possible rotation angle between the optical axis of said at least one camera (2) and the surface portion on the basis of the distance of said plurality of points. </claim-text></claim><claim id="clm-0020" num="20"><claim-text>20. Method according to claim 19, wherein the distance of said plurality of points is determined on the basis of the analysis of a perspective deformation of said at least one coded information (30) . </claim-text></claim><claim id="clm-0021" num="21"><claim-text>21. Method according to claim 19 or 20 when depending on claim 14 or 15, wherein said possible rotation angle is determined on the basis of the analysis of said footprint. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
