<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2681638-A1" country="EP" doc-number="2681638" kind="A1" date="20140108" family-id="46758253" file-reference-id="257354" date-produced="20180823" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146586168" ucid="EP-2681638-A1"><document-id><country>EP</country><doc-number>2681638</doc-number><kind>A1</kind><date>20140108</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12752342-A" is-representative="NO"><document-id mxw-id="PAPP154848360" load-source="docdb" format="epo"><country>EP</country><doc-number>12752342</doc-number><kind>A</kind><date>20120228</date><lang>EN</lang></document-id><document-id mxw-id="PAPP210132551" load-source="docdb" format="original"><country>EP</country><doc-number>12752342.1</doc-number><date>20120228</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140548214" ucid="US-201161447698-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201161447698</doc-number><kind>P</kind><date>20110228</date></document-id></priority-claim><priority-claim mxw-id="PPC140549319" ucid="US-201161470481-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201161470481</doc-number><kind>P</kind><date>20110331</date></document-id></priority-claim><priority-claim mxw-id="PPC140556987" ucid="US-2012000111-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2012000111</doc-number><kind>W</kind><date>20120228</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1924006742" load-source="docdb">G06Q  30/02        20120101AFI20160627BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1924011397" load-source="docdb">G06T  19/00        20110101ALI20160627BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1848629630" load-source="docdb" scheme="CPC">G06K   9/4604      20130101 LI20170202BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848631006" load-source="docdb" scheme="CPC">G06F   3/017       20130101 LI20170202BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848631283" load-source="docdb" scheme="CPC">G06T2210/16        20130101 LA20170204BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848631795" load-source="docdb" scheme="CPC">G06Q  30/02        20130101 LI20170204BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848632251" load-source="docdb" scheme="CPC">G06F   3/005       20130101 LI20170202BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848632626" load-source="docdb" scheme="CPC">G06K   9/00369     20130101 LI20170202BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848632820" load-source="docdb" scheme="CPC">G06T2219/2016      20130101 LA20170202BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848636012" load-source="docdb" scheme="CPC">G06T  19/006       20130101 FI20170204BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1848636887" load-source="docdb" scheme="CPC">G06T  19/20        20130101 LI20170202BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132363137" lang="DE" load-source="patent-office">VIRTUELLE ECHTZEITREFLEXION</invention-title><invention-title mxw-id="PT132363138" lang="EN" load-source="patent-office">REAL-TIME VIRTUAL REFLECTION</invention-title><invention-title mxw-id="PT132363139" lang="FR" load-source="patent-office">RÉFLEXION VIRTUELLE EN TEMPS RÉEL</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR919522828" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>FACECAKE MARKETING TECHNOLOGIES INC</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR919534846" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>FACECAKE MARKETING TECHNOLOGIES, INC.</last-name></addressbook></applicant><applicant mxw-id="PPAR919006185" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Facecake Marketing Technologies, Inc.</last-name><iid>101411090</iid><address><street>22287 Mulholland Highway, Suite 97</street><city>Calabasas, CA 91302</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR919540163" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SMITH LINDA</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919535167" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SMITH, LINDA</last-name></addressbook></inventor><inventor mxw-id="PPAR919016694" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SMITH, LINDA</last-name><address><street>4603 Deseret Drive</street><city>Woodland Hills, CA 91364</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919535604" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>GRAFF CLAYTON</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919535234" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>GRAFF, Clayton</last-name></addressbook></inventor><inventor mxw-id="PPAR919005920" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>GRAFF, Clayton</last-name><address><street>21520 Burbank Blvd., Apt. 121</street><city>Woodland Hills, CA 91367</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919511941" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>LU DARREN</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR919528513" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>LU, Darren</last-name></addressbook></inventor><inventor mxw-id="PPAR919015746" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>LU, Darren</last-name><address><street>60 Cormorant Circle</street><city>Newport Beach, CA 92660</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR919008747" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>EIP</last-name><iid>101401446</iid><address><street>Fairfax House 15 Fulwood Place</street><city>London WC1V 6HU</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2012000111-W"><document-id><country>US</country><doc-number>2012000111</doc-number><kind>W</kind><date>20120228</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012118560-A1"><document-id><country>WO</country><doc-number>2012118560</doc-number><kind>A1</kind><date>20120907</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS549757820" load-source="docdb">AL</country><country mxw-id="DS549834215" load-source="docdb">AT</country><country mxw-id="DS549757825" load-source="docdb">BE</country><country mxw-id="DS549913598" load-source="docdb">BG</country><country mxw-id="DS549765532" load-source="docdb">CH</country><country mxw-id="DS549833866" load-source="docdb">CY</country><country mxw-id="DS549833867" load-source="docdb">CZ</country><country mxw-id="DS549757168" load-source="docdb">DE</country><country mxw-id="DS549757826" load-source="docdb">DK</country><country mxw-id="DS549757827" load-source="docdb">EE</country><country mxw-id="DS549846115" load-source="docdb">ES</country><country mxw-id="DS549913599" load-source="docdb">FI</country><country mxw-id="DS549913600" load-source="docdb">FR</country><country mxw-id="DS549757177" load-source="docdb">GB</country><country mxw-id="DS549757828" load-source="docdb">GR</country><country mxw-id="DS549757178" load-source="docdb">HR</country><country mxw-id="DS549833868" load-source="docdb">HU</country><country mxw-id="DS549765537" load-source="docdb">IE</country><country mxw-id="DS549757833" load-source="docdb">IS</country><country mxw-id="DS549913601" load-source="docdb">IT</country><country mxw-id="DS549757834" load-source="docdb">LI</country><country mxw-id="DS549757179" load-source="docdb">LT</country><country mxw-id="DS549834216" load-source="docdb">LU</country><country mxw-id="DS549913602" load-source="docdb">LV</country><country mxw-id="DS549757180" load-source="docdb">MC</country><country mxw-id="DS549834217" load-source="docdb">MK</country><country mxw-id="DS549835122" load-source="docdb">MT</country><country mxw-id="DS549835123" load-source="docdb">NL</country><country mxw-id="DS549833869" load-source="docdb">NO</country><country mxw-id="DS549835124" load-source="docdb">PL</country><country mxw-id="DS549757835" load-source="docdb">PT</country><country mxw-id="DS549834866" load-source="docdb">RO</country><country mxw-id="DS549757836" load-source="docdb">RS</country><country mxw-id="DS549835125" load-source="docdb">SE</country><country mxw-id="DS549757189" load-source="docdb">SI</country><country mxw-id="DS549833874" load-source="docdb">SK</country><country mxw-id="DS549833875" load-source="docdb">SM</country><country mxw-id="DS549846116" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA99830658" ref-ucid="WO-2012118560-A1" lang="EN" load-source="patent-office"><p num="0000">Techniques are provided for a real-time virtual reflection and for video interaction with virtual objects in a system. According to one embodiment, a user's image is captured by a video camera, and is outputted to a visual display in reverse, providing a mirror-image feedback to the user's movements. Through the camera interface, the system interprets user's gestures to manipulate virtual objects in real-time, in conjunction with the user's movements. In some embodiments, a user simulates trying on apparel and accessories according to the techniques set forth.</p></abstract><abstract mxw-id="PA100327484" ref-ucid="WO-2012118560-A1" lang="EN" source="national office" load-source="docdb"><p>Techniques are provided for a real-time virtual reflection and for video interaction with virtual objects in a system. According to one embodiment, a user's image is captured by a video camera, and is outputted to a visual display in reverse, providing a mirror-image feedback to the user's movements. Through the camera interface, the system interprets user's gestures to manipulate virtual objects in real-time, in conjunction with the user's movements. In some embodiments, a user simulates trying on apparel and accessories according to the techniques set forth.</p></abstract><abstract mxw-id="PA99830659" ref-ucid="WO-2012118560-A1" lang="FR" load-source="patent-office"><p num="0000">Cette invention se rapporte à des techniques de réflexion virtuelle en temps réel et d'interaction vidéo avec des objets virtuels dans un système. Selon un mode de réalisation, l'image d'un utilisateur est acquise par une caméra vidéo, et est délivrée en sortie à un affichage visuel à l'envers, en fournissant une rétroaction d'image en miroir aux mouvements de l'utilisateur. Grâce à l'interface de la caméra, le système interprète les gestes de l'utilisateur de façon à manipuler des objets virtuels en temps réel, conjointement avec les mouvements de l'utilisateur. Dans certains modes de réalisation, un utilisateur simule l'essayage de vêtements et d'accessoires selon les techniques présentées.</p></abstract><abstract mxw-id="PA100327485" ref-ucid="WO-2012118560-A1" lang="FR" source="national office" load-source="docdb"><p>Cette invention se rapporte à des techniques de réflexion virtuelle en temps réel et d'interaction vidéo avec des objets virtuels dans un système. Selon un mode de réalisation, l'image d'un utilisateur est acquise par une caméra vidéo, et est délivrée en sortie à un affichage visuel à l'envers, en fournissant une rétroaction d'image en miroir aux mouvements de l'utilisateur. Grâce à l'interface de la caméra, le système interprète les gestes de l'utilisateur de façon à manipuler des objets virtuels en temps réel, conjointement avec les mouvements de l'utilisateur. Dans certains modes de réalisation, un utilisateur simule l'essayage de vêtements et d'accessoires selon les techniques présentées.</p></abstract><description mxw-id="PDES51232338" ref-ucid="WO-2012118560-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> REAL-TIME VIRTUAL REFLECTION </p><p id="p0002" num="0002">CROSS-REFERENCE TO RELATED APPLICATION </p><p id="p0003" num="0003"> [0001] This application claims the benefit of provisional Application No. 61/447,698, filed February 28, 201 1, and provisional Application No. 61/470,481 , filed March 31 , 2011. </p><p id="p0004" num="0004">FIELD OF THE INVENTION </p><p id="p0005" num="0005"> [0002] The present invention generally relates to manipulation of captured visual data. BACKGROUND OF THE INVENTION </p><p id="p0006" num="0006"> [0003] Try-On systems typically allow users to simulate being in a certain environment or having a certain appearance by taking a photograph of a part of the user, and merging the image with other images to generate a new image with the simulation. According to one approach, a photograph of a face of a user is received as input into the system, and the user's face is merged with other images to generate an image that simulates the user's likeness being in another environment, wearing different apparel, or having different features. For example, the user may specify the system to generate an image of the user wearing a hat, having different hair color, and wearing particular clothing, using a photograph of a user that is received as input. </p><p id="p0007" num="0007"> [0004] The approaches described in this section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section. </p><p id="p0008" num="0008">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0009" num="0009"> [0005] The present embodiments of the invention are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings and in which like reference numerals refer to similar elements and in which: 
<!-- EPO <DP n="4"/>-->
 [0006] FIG. 1 is a diagram that illustrates a user interacting with the system to generate a real-time virtual reflection of the user virtually wearing a purse, according to one embodiment of the invention. </p><p id="p0010" num="0010"> [0007] FIG. 2 is a diagram that shows an example of a user's interaction with the realtime virtual reflection system according to one embodiment of the invention. </p><p id="p0011" num="0011">[0008] FIG. 3 is a flow diagram illustrating a computer-implemented process for video interaction with virtual objects according to one embodiment of the invention. </p><p id="p0012" num="0012">[0009] FIG. 4 is a diagram showing an example of an output frame overlaid with icons, with user interaction according to one embodiment of the invention. </p><p id="p0013" num="0013">[0010] FIG. 5 is a flow diagram illustrating a process for applying a virtual object to an output stream of frames based on a concurrently captured stream of frames, wherein a visual data capture device captures video frames and depth cloud data, according to one embodiment of the invention. </p><p id="p0014" num="0014"> [0011] FIG. 6 is a diagram showing an image of a virtual object in three rotations, according to one embodiment of the invention. </p><p id="p0015" num="0015"> [0012] FIG. 7 illustrates the system performing a side-by-side comparison between one virtual object applied to one output stream, and another virtual object applied to another output stream, according to some embodiments of the invention. </p><p id="p0016" num="0016"> [0013] FIG. 8 is a flow diagram illustrating a process for applying a virtual object to an output stream of frames based on a concurrently captured stream of frames without using depth cloud data, according to one embodiment of the invention. </p><p id="p0017" num="0017"> [0014] FIG. 9 is a flow diagram for executing a function based on a gesture according to one embodiment of the invention. </p><p id="p0018" num="0018"> [0015] FIG. 10 illustrates the system changing user features in the stream of frames according to one embodiment of the invention </p><p id="p0019" num="0019"> [0016] FIG. 1 1 is a diagram illustrating a virtual closet according to one embodiment of the invention. 
<!-- EPO <DP n="5"/>-->
 [0017] FIG. 12 is a diagram illustrating one example of a computer system on which some embodiments of the invention are implemented. 
<!-- EPO <DP n="6"/>-->
</p><p id="p0020" num="0020">DETAILED DESCRIPTION </p><p id="p0021" num="0021"> [0018] In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present embodiments of the invention. It will be apparent, however, that the present embodiments of the invention may be practiced without these specific details. In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the present embodiments of the invention. </p><p id="p0022" num="0022"> [0019] Techniques are provided to create a real-time virtual reflection of a user that shows the user interacting with virtual objects. </p><p id="p0023" num="0023"> [0020] FIG. 1 shows one embodiment of the real-time virtual reflection system 100 in which a user 101 stands in front of visual data capture device 1 10 and visual display 120. According to the embodiment, the visual data capture device 1 10 continually captures sequential images of the user, and sends the data to a computer system for processing as a real-time video image that is displayed on visual display 120. The computer processing system receives selection input from user 101 that indicates which virtual object to display with the user's video image. In this embodiment, user 101 has selected a virtual purse 130 to wear in the virtual reflection 140 of user 101. </p><p id="p0024" num="0024"> [0021] As shown in FIG. 1 , user 101 is moving her body in space as if holding purse in her right arm. The visual data capture device 1 10 captures user 101 's movements in real time, and a computer processing system couples the captured data with a video representation 130 of a purse. In this embodiment, the virtual purse 130 is persistently coupled to the virtual reflection 140 such that the virtual display 120 shows the virtual purse 130 moving with the user 101 's movements in real time. Thus, the virtual reflection 140 appears as if it is a reflection of the purse is being worn by the user in real-time. </p><p id="p0025" num="0025">[0022] The visual data capture device 1 10, according to one embodiment of the invention, includes a depth camera system that is able to capture depth data from a scene. In other embodiments, the visual data capture device 1 10 includes a 3-D camera that includes 
<!-- EPO <DP n="7"/>-->
 two or more physically separated lenses that captures the scene at different angles in order to obtain stereo visual data that may be used to generate depth information. In one embodiment of the invention, a visual data capture device and computer processing system such as the ones described in U.S. Patent App. Nos. 1 1/899,542, and 12/522, 171 , are used to capture and process the necessary visual data to render virtual reflection 140 on visual display 120. In other embodiments, visual data capture device 1 10 may include a camera that includes only one aperture coupled with at least one lens for capturing visual data. </p><p id="p0026" num="0026"> [0023] In this embodiment, visual display 120 may be part of an audiovisual device, such as a television, a monitor, a high-definition television, a screen onto which an image is projected from a video projector, or any such device that may provide visual data to a user. </p><p id="p0027" num="0027">[0024] FIG. 2 shows an example of a user's interaction with the real-time virtual reflection system 100 according to one embodiment of the invention. This figure shows one example of a user's interaction with real-time virtual reflection system 100 as a series of three still captures of the video images shown in visual display 120 from three moments in time. At moment 210, the visual data capture device 1 10 captures the scene of a user posing with one arm extended, and displays virtual user reflection object 21 1 in visual display 120. </p><p id="p0028" num="0028">[0025] According to this embodiment, a computer system receives the visual data from the visual data capture device 1 10, and interprets the gesture of the user's arm extension parsed from the visual data. In response to the visual data at moment 210, the computer system activates background image 221 and purse selections 222 to be displayed in visual display 120, as shown in moment 220. Next, the visual data capture device 1 10 captures a scene of the user's hand in a grabbing gesture, and displays virtual user reflection object 223 with background image 221 and purse selections 222. The computer system receives the visual data, and based on the grabbing gesture, selects purse image object 224 to couple with virtual user reflection object 223. </p><p id="p0029" num="0029"> [0026] After the selection of purse image object 224 is made, purse image object 224 is persistently coupled to virtual user reflection object 223, as shown a moment 230, which 
<!-- EPO <DP n="8"/>-->
 allows for a user to move while maintaining the effect of virtual user reflection object 223 appearing to hold onto purse image object 224. </p><p id="p0030" num="0030"> [0027] Although the embodiments described herein show a purse object as the virtual object that is selected and integrated into the virtual reflection, other virtual objects can be used in a like manner in other embodiments of the invention, including other apparel items and non-apparel items. In other embodiments, the apparel items that are selected may conform to the shape of the user's virtual body, such as a shirt, gloves, socks, shoes, trousers, skirt, or dress. In still other embodiments, the items include, but are not limited to other objects such as glasses, sunglasses, colored contacts, necklaces, scarves, stud, hoop, and dangling earrings and body rings, watches, finger rings, hair accessories, hairstyles. Non- apparel items include animals, snow, fantastical objects, sports equipment, and any object that can be made as an image or images by photography, animation, or other graphic- generating techniques. </p><p id="p0031" num="0031"> [0028] FIG. 3 is a flow diagram illustrating a computer-implemented process for video interaction with virtual objects according to at least one embodiment of the invention. At step 301 , a stream of frames is received from a visual data capture device. At step 303, occurring concurrently with step 301 , the stream of frames is output as a signal for a visual display, such as visual display 120. In an embodiment, the stream of frames are output in reverse. In the embodiment, there is minimal lag time between when a frame is received and when the frame in reverse is output. Thus, at step 303, a user captured by the video stream is presented with mirror-image feedback of his or her image in an output video stream. In other embodiments, the output stream is not in reverse, but is in an original or other orientation. Through the use of mirrors or other optical devices, the mirror-image feedback can be achieved without reversing the output stream. In other embodiments, the output stream is the same orientation as the input stream, and the mirror-image feedback is not provided. In still other embodiments, the stream of frames is received and stored for later output as an output stream of frames. In such embodiments, the system receives a request for any user for 
<!-- EPO <DP n="9"/>-->
 applying to the stream of frames after the time of the capture of the video, including a user different from the user pictured in the video. </p><p id="p0032" num="0032"> [0029] At step 305, a request for applying a virtual object to the stream of frames is received. In one embodiment, the request is triggered by a gesture or command by a user identified by the system, such as the arm extension and grabbing gestures described above with reference to FIG. 2. In one embodiment, as shown in FIG. 4 which illustrates one output frame 401 in an example embodiment of the invention, the output video stream is overlaid with icons 403 - 409, with which the user 402 interacts by making one or more virtual tapping gestures 411 that are captured by the visual data capture device as positioned at same frame location as the icon, and interpreted by the system as a selection of the icon. In the embodiment shown in FIG. 4, the system receives a request from the user 402 to apply a necklace to user's reverse image in the output stream of frames. </p><p id="p0033" num="0033"> [0030] In other embodiments, the request is triggered by the system detecting a user's image in the stream of frames, not caused by any intentional user command. In one example embodiment, the system detects a user's image moving into frames of a stream of frames being captured and received. The presence of a user's image in one or more frames triggers the request at step 305. Other automatic request triggers include but are not limited to detecting a user moving through and stopping to look at the user's output image on a video display, detecting a user's image moving through the frame in a particular orientation, such as a predominantly forward-facing orientation facing the visual data capture device, or other features analyzed from the images in the stream of frames. In some embodiments, such image analyses are performed using computer vision techniques. </p><p id="p0034" num="0034"> [0031] At step 307, the system processes the request for applying the virtual object to the stream of frames. Processing the request includes determining an appropriate position for applying the virtual object to the output image frame. Techniques for processing of the request according to some embodiments of the invention are presented in further detail below with reference to FIGS. 5 and 8. 
<!-- EPO <DP n="10"/>-->
 [0032] At step 309, the system outputs the reverse stream of frames with the virtual object applied. In some embodiments, the virtual object is persistently coupled to a particular user's feature in the image, for example, the user's right arm, such that the virtual display appears to show the virtual object moving with the user's movements in real time. Thus, the output stream of frames, when viewed, looks as if it is a reflection of the virtual object being worn by the user in real-time. In other embodiments, the virtual object is persistently coupled until a gesture indicates a change in coupling to another of the user's features. For example, in some embodiments, while the virtual object is coupled to the right arm, a system detects a gesture by a left hand on the virtual object, and the virtual object is changed to couple with the left arm and hand. In such embodiments, the user can shift the virtual object from arm to arm, or body part to body part. </p><p id="p0035" num="0035"> [0033] While the steps in the flow diagrams described herein are shown as a sequential series of steps, it is understood that some steps may be concurrent, the order of the steps may be different, there may be a substantial gap of time elapsed between steps, and certain steps may be skipped and steps not shown may be added to implement the process of video interaction with virtual objects as shown and described with respect to FIGS. 1 , 2 and 4. </p><p id="p0036" num="0036">[0034] FIG. 5 is a flow diagram illustrating a process for applying a virtual object to an output stream of frames based on a concurrently captured stream of frames, wherein a visual data capture device captures video frames and depth cloud data, according to one </p><p id="p0037" num="0037">embodiment of the invention. At step 501 , the system receives a stream of video frames from a visual data capture device. At step 503, the system receives depth cloud data for video frames from the visual data capture device. </p><p id="p0038" num="0038"> [0035] At step 505, image feature points such as image feature points or facial points for the user for a video frame of the stream of video frames is determined using the depth cloud data. At step 507, one or more angles of rotation for the user is determined based on the image feature points. In some embodiments, roll, pitch, and yaw is determined for each 
<!-- EPO <DP n="11"/>-->
 identified user feature. For example, the roll, pitch and yaw for a user's head, arm, torso, leg, hands, or feet is determined to determine a rotation angle for the user feature. </p><p id="p0039" num="0039">[0036] At step 509, one or more of images stored for a virtual object by the system are identified. At step 51 1 , from among the stored images for the virtual object, the system determines an image to apply to the video frame based on one of the user's angles of rotation. In one embodiment, each of the plurality of images of the virtual object depicts the virtual object at a different yaw, roll and pitch. For example, as illustrated in FIG. 6, an image of a virtual headband at a first rotation 601 is used as the user's head is turned to the right in a first frame, an image at a second rotation 603 is used as the user's head is facing forward in a second frame, and an image at a third rotation 605 is used as the user's head is turned to the left in a third frame. According to some embodiments, only one image is stored for a virtual object. For such embodiments, the same image is identified to be applied regardless of the user's angle of rotation. </p><p id="p0040" num="0040"> [0037] At step 513, position values are determined based on image feature points for applying the image of a particular rotation to the frame. At step 515, image size values are determined based on depth cloud data for applying the image of a particular rotation to the frame. For example, a larger image size value is determined for image feature points with a shorter depth than for image feature points for a longer depth. </p><p id="p0041" num="0041"> [0038] At step 517, the image of a particular rotation is modified based on the image size values at a desired position. In some embodiments, certain virtual objects are associated with a particular user feature, and the position values of the user feature are determined for applying the virtual object. With reference to FIG. 6, the virtual headband is associated with a user's head at a particular relative position. In the embodiment shown in FIG. 6, in one example of executing step 517, the image of the headband at a particular rotation is modified based on the image size values for the user's head. In some embodiments, the image of a particular rotation is skewed or warped to correspond to different image size values for different positions, allowing for the virtual object to fit and conform over curves of the user. 
<!-- EPO <DP n="12"/>-->
 [0039] At step 519, the modified image of a particular rotation is applied to the frame based on position values. In some embodiments, a desired position is based on gestures received from the user. For example, with reference the embodiment shown in FIG. 1 , a system detects a gesture of putting a left hand on the virtual purse 130. The gesture will cause the virtual object on the next frame to be applied to position values determined for the hand instead of the left shoulder. </p><p id="p0042" num="0042"> REAL-TIME COMPARISON </p><p id="p0043" num="0043">[0040] FIG. 7 illustrates the system performing a side-by-side comparison between one virtual object applied to one output stream, and another virtual object applied to another output stream, according to some embodiments of the invention. For example, the system receives a command to perform a side-by-side comparison. The system duplicates the output stream of frames into two or more streams. In some embodiments, the output streams are positioned such that user's virtual reflection appears on both sides of a split screen, reflecting the user's movement in real-time. FIG. 7 shows user 701 duplicated into two output streams. One side shows the user with necklace 703 in a short version. One side shows the user with necklace 705 in a long version. </p><p id="p0044" num="0044"> [0041] FIG. 8 is a flow diagram illustrating a process for applying a virtual object to an output stream of frames based on a concurrently captured stream of frames without using depth cloud data, according to one embodiment of the invention. At step 801 , a stream of video frames is received from a visual data capture device. At step 803, the system analyzes the image in the video frame using computer vision techniques. At step 805, based on the analysis, image feature points for the user are determined for a frame from a video stream. </p><p id="p0045" num="0045">[0042] At step 807, one or more of user's angles of rotation are determined. In some embodiments, roll, pitch, and yaw is determined for each identified user feature. For example, the roll, pitch and yaw for a user's head, arm, torso, leg, hands, or feet is determined to determine a rotation angle for the user feature. 
<!-- EPO <DP n="13"/>-->
 [0043] At step 809, one or more stored images for a virtual object is identified. At step 81 1 , from among the stored images for the virtual object, the system determines an image for the virtual object in particular rotation to apply to the video frame based on one of the user's angles of rotation. In one embodiment, each of the plurality of images of the virtual object depicts the virtual object at a different yaw, roll and pitch. </p><p id="p0046" num="0046"> [0044] At step 813, position values are determined for applying the image for the virtual object of a particular rotation based on image feature points. At step 815, image size values for applying the virtual object to the video frame are determined based on image feature points and the video frame. At step 817, the image of the virtual object is modified based on the image size values. At step 819, the modified image of the virtual object is applied to the video frame based on position values. Additional techniques and variations for some embodiments as described with reference to FIG. 5 are available for use with the process of FIG. 8 where applicable. </p><p id="p0047" num="0047"> GESTURES </p><p id="p0048" num="0048"> [0045] In some embodiments, gestures by the user that are detected by the system, and are interpreted by the system as commands corresponding to executing particular functions. A flowchart according to one embodiment of the invention is shown at FIG. 9 for executing a function based on a gesture. At step 901 , the system detects a gesture by a user. At step 903, the system determines which function is associated with the gesture. At step 905, the system executes the function based on the gesture. In some embodiments of the invention, a gesture is detected by determining movement of a user feature based on transposing of interrelated image feature points from frame to frame. For example, a tapping gesture on an icon, as previously described with reference to FIG. 4, is associated with a selection command of the icon. The tapping gesture on the icon as determined to apply a necklace to the user's reverse image in the output stream of frames. </p><p id="p0049" num="0049"> [0046] In another example, a hand-wave that is captured in the video stream as movement across most of the frame is associated with a clearing function for removing all 
<!-- EPO <DP n="14"/>-->
 overlaid virtual objects from the output stream. In some embodiments, the gesture of touching a virtual object causes the system to switch among the available variations of the virtual objects, or to switch back and forth between two variations of the virtual objects. Further, the size and shape of virtual objects can be modified and manipulated by gestures. For example, a grabbing gesture on a necklace is a command corresponding to the function of lengthening the necklace. </p><p id="p0050" num="0050"> ENVIRONMENT SIMULATION </p><p id="p0051" num="0051">[0047] In addition to applying virtual object onto the users image, in some embodiments of the invention, the system may mask out the background in each frame of the stream of frames around the user and replace it with another background to make it appear that the user is in a different environment and location. The background may be a static scene or a moving background stored in the system, or retrieved by the system for use from a repository of backgrounds. In some embodiments, the background is selected to complement the virtual object or objects being worn by the user. Furthermore, foreground elements may be virtually added to each frame to simulate weather or other objects around the user. Examples include snow or leaves falling around the user. Using the techniques described in FIGS. 1 - 8, the user may interact with these virtual elements. For example, virtual snow and virtual leaves are applied to the output stream of frames to show the objects collecting on the user's head and shoulders as they would in real life. Further details for this technique are set forth in copending Applic. No. 12/714,518, which is incorporated by reference as if fully set forth herein. In other embodiments, for example, a fairy tale scene from "Snow White" is applied to an output stream of frames. Background objects or moving scenes include animals, which can be applied to a user feature to depict, for example, birds landing on the user's hand. </p><p id="p0052" num="0052"> COLOR CHANGES </p><p id="p0053" num="0053"> [0048] FIG. 10 illustrates the system changing user features in the stream of frames according to one embodiment of the invention, and showing a comparison between the variations while capturing and outputting the user's movements in real-time using techniques 
<!-- EPO <DP n="15"/>-->
 virtual objects applied as described above with reference to at least FIGS. 3, 5 and 8. </p><p id="p0054" num="0054">According to one embodiment, the system detects user features based on image feature points and computer vision techniques. The system receives a command from user to change the color of a user feature. As illustrated in FIG. 10, the system applied four hair colors, and is showing a side-by-side comparison of each of the four applied hair colors. Other features whose color may be changed include facial features such as lip color and eye color. </p><p id="p0055" num="0055"> SHAPE CHANGES </p><p id="p0056" num="0056"> [0049] According to one embodiment, the system detects user features based on image feature points and computer vision techniques. The system receives a request to change the shape of the user feature. In some embodiments, changes to one or more user features in the output stream of frames in real time include by are not limited to adding or subtracting size of body parts, changing facial features, and changing height. In one embodiment, once the changes are in place, the changes persist, and techniques for applying virtual objects, such as those described in FIGS. 3, 5 and 8, can be used with the changed user features. </p><p id="p0057" num="0057"> CLOTHING PHYSICS </p><p id="p0058" num="0058">[0050] When the user applies virtual objects, the software may add animated motion to the applied image to enhance the realism of the product being applied. The software may apply moving transformation to simulate the movement of clothing and fabric and have it respond appropriately based on the users movements. This motion may also be used to highlight a promotional product being apply such a hairstyle moving "in the wind" to focus the user's attention on that product. </p><p id="p0059" num="0059"> VIRTUAL CLOSET </p><p id="p0060" num="0060"> [0051] As shown in FIG. 1 1 , according to some embodiments of the invention, a virtual closet (more specifically a personalized interactive virtual closet) lets the user collect and save the virtual objects available in the system to the virtual closet. Virtual objects are stored on centralized remote servers and are accessible by the user whenever she logs in with a user account when using the system. The virtual objects correspond to items that the user owns in 
<!-- EPO <DP n="16"/>-->
 the real world, owns virtually in digital form only, or does not own but wishes to own at a later date. Items may be added to the virtual closet by saving them while using the system, saving them from other interactions (e.g. adding from a retailer's web site) to use in the system later, or as a recommendation from a retailer, marketer, or manufacturer as a marketing opportunity. The virtual items saved in the virtual closet may be shared with and amongst the user's friends and family to review or try on themselves. The virtual closet can be decorated with virtual goods and designed by the user with the user favorites given premium position for try-on or viewing again. </p><p id="p0061" num="0061"> SOCIAL SHARING </p><p id="p0062" num="0062"> [0052] According to one embodiment of the invention, multiple users may be able to view the visual display 120 at other visual displays connected over any computer network. For example, other virtual displays include one or more web browser displays at location that is remote from the on-camera user's location. </p><p id="p0063" num="0063"> [0053] According to one embodiment, two such systems may be communicatively connected to allow two users who are simultaneously in two different virtual user reflection simulation sessions to interact with each other through the system. </p><p id="p0064" num="0064"> [0054] According to one embodiment of the invention, the background display 221 can be chosen by the user, and modified by the user or automatically, at any time during a virtual user reflection simulation session. </p><p id="p0065" num="0065"> [0055] According to one embodiment of the invention, the set of apparel objects that are offered to a user for selection are provided by third-party vendors on a real-time basis, based on the user's previous selections. In other embodiments, multiple auxiliary users who are viewing the virtual user reflection simulation session may cause other objects to be offered to the on-camera user. 
<!-- EPO <DP n="17"/>-->
 HARDWARE OVERVIEW </p><p id="p0066" num="0066">[0056] FIG. 12 is a block diagram that illustrates a computer system 1200 upon which an embodiment of the invention may be implemented. Computer system 1200 includes a bus 1202 or other communication mechanism for communicating information, and a processor 1204 coupled with bus 1202 for processing information. Computer system 1200 also includes a main memory 1206, such as a random access memory (RAM) or other dynamic storage device, coupled to bus 1202 for storing information and instructions to be executed by processor 1204. Main memory 1206 also may be used for storing temporary variables or other intermediate information during execution of instructions to be executed by processor 1204. Computer system 1200 further includes a read only memory (ROM) 1208 or other static storage device coupled to bus 1202 for storing static information and instructions for processor 1204. A storage device 1210, such as a magnetic disk or optical disk, is provided and coupled to bus 1202 for storing information and instructions. </p><p id="p0067" num="0067"> [0057] Computer system 1200 may be coupled via bus 1202 to a display 1212, such as a cathode ray tube (CRT) or liquid crystal display (LCD), for displaying information to a computer user. An input device 1214, including alphanumeric and other keys, is coupled to bus 1202 for communicating information and command selections to processor 1204. </p><p id="p0068" num="0068">Another type of user input device is cursor control 1216, such as a mouse, a trackball, or cursor direction keys for communicating direction information and command selections to processor 1204 and for controlling cursor movement on display 1212. This input device typically has two degrees of freedom in two axes, a first axis (e.g., x) and a second axis (e.g., y), that allows the device to specify positions in a plane. Another type of input device includes a video camera, a depth camera, or a 3D camera. Another type of input device includes a gesture-based input device, such as the Microsoft XBOX Kinect. </p><p id="p0069" num="0069">[0058] The invention is related to the use of computer system 1200 for implementing the techniques described herein. According to one embodiment of the invention, those techniques are performed by computer system 1200 in response to processor 1204 executing 
<!-- EPO <DP n="18"/>-->
 one or more sequences of one or more instructions contained in main memory 1206. Such instructions may be read into main memory 1206 from another machine-readable medium, such as storage device 1210. Execution of the sequences of instructions contained in main memory 1206 causes processor 1204 to perform the process steps described herein. In alternative embodiments, hard-wired circuitry may be used in place of or in combination with software instructions to implement the invention. Thus, embodiments of the invention are not limited to any specific combination of hardware circuitry and software. In further embodiments, multiple computer systems 1200 are operatively coupled to implement the embodiments in a distributed system. </p><p id="p0070" num="0070"> [0059] The term "machine-readable medium" as used herein refers to any medium that participates in providing data that causes a machine to operation in a specific fashion. In an embodiment implemented using computer system 1200, various machine-readable media are involved, for example, in providing instructions to processor 1204 for execution. Such a medium may take many forms, including but not limited to storage media and transmission media. Storage media includes both non-volatile media and volatile media. Non-volatile media includes, for example, optical or magnetic disks, such as storage device 1210. Volatile media includes dynamic memory, such as main memory 1206. Transmission media includes coaxial cables, copper wire and fiber optics, including the wires that comprise bus 1202. Transmission media can also take the form of acoustic or light waves, such as those generated during radio-wave and infra-red data communications. All such media must be tangible to enable the instructions carried by the media to be detected by a physical mechanism that reads the instructions into a machine. </p><p id="p0071" num="0071"> [0060] Common forms of machine-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, or any other magnetic medium, a CD-ROM, any other optical medium, punchcards, papertape, any other physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM, any other memory chip or cartridge, a carrier wave as described hereinafter, or any other medium from which a computer can read. 
<!-- EPO <DP n="19"/>-->
 [0061] Various forms of machine-readable media may be involved in carrying one or more sequences of one or more instructions to processor 1204 for execution. For example, the instructions may initially be carried on a magnetic disk of a remote computer. The remote computer can load the instructions into its dynamic memory and send the instructions over a telephone line using a modem. A modem local to computer system 1200 can receive the data on the telephone line and use an infra-red transmitter to convert the data to an infrared signal. An infra-red detector can receive the data carried in the infra-red signal and appropriate circuitry can place the data on bus 1202. Bus 1202 carries the data to main memory 1206, from which processor 1204 retrieves and executes the instructions. The instructions received by main memory 1206 may optionally be stored on storage device 1210 either before or after execution by processor 1204. </p><p id="p0072" num="0072"> [0062] Computer system 1200 also includes a communication interface 1218 coupled to bus 1202. Communication interface 1218 provides a two-way data communication coupling to a network link 1220 that is connected to a local network 1222. For example, </p><p id="p0073" num="0073">communication interface 1218 may be an integrated services digital network (ISDN) card or other internet connection device, or a modem to provide a data communication connection to a corresponding type of telephone line. As another example, communication interface 1218 may be a local area network (LAN) card to provide a data communication connection to a compatible LAN. Wireless network links may also be implemented. In any such </p><p id="p0074" num="0074">implementation, communication interface 1218 sends and receives electrical, electromagnetic or optical signals that carry digital data streams representing various types of information. </p><p id="p0075" num="0075">[0063] Network link 1220 typically provides data communication through one or more networks to other data devices. For example, network link 1220 may provide a connection through local network 1222 to a host computer 1224 or to data equipment operated by an Internet Service Provider (ISP) 1226. ISP 1226 in turn provides data communication services through the world wide packet data communication network now commonly referred to as the Internet 1228. Local network 1222 and Internet 1228 both use electrical, 
<!-- EPO <DP n="20"/>-->
 electromagnetic or optical signals that carry digital data streams. The signals through the various networks and the signals on network link 1220 and through communication interface 1218, which carry the digital data to and from computer system 1200, are exemplary forms of carrier waves transporting the information. </p><p id="p0076" num="0076"> [0064] Computer system 1200 can send messages and receive data, including program code, through the network(s), network link 1220 and communication interface 1218. In the Internet example, a server 1210 might transmit a requested code for an application program through Internet 1228, ISP 1226, local network 1222 and communication interface 1218. </p><p id="p0077" num="0077">[0065] The received code may be executed by processor 1204 as it is received, and/or stored in storage device 1210, or other non-volatile storage for later execution. In this manner, computer system 1200 may obtain application code in the form of a carrier wave. </p><p id="p0078" num="0078">[0066] In the foregoing specification, embodiments of the invention have been described with reference to numerous specific details that may vary from implementation to implementation. Thus, the sole and exclusive indicator of what is the invention, and is intended by the applicants to be the invention, is the set of claims that issue from this application, in the specific form in which such claims issue, including any subsequent correction. Any definitions expressly set forth herein for terms contained in such claims shall govern the meaning of such terms as used in the claims. Hence, no limitation, element, property, feature, advantage or attribute that is not expressly recited in a claim should limit the scope of such claim in any way. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. 
</p></description><claims mxw-id="PCLM44851474" ref-ucid="WO-2012118560-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="21"/>-->CLAIMS What is claimed is: </claim-statement><claim id="clm-0001" num="1"><claim-text>1. A computer-implemented method for video interaction with virtual objects, the method comprising the steps of: </claim-text><claim-text> receiving, by a computer, a stream of frames from a visual data capture device of a user in movement; </claim-text><claim-text> outputting a stream of frames as video signal for processing by a visual display device based on the received stream of frames; </claim-text><claim-text> while receiving and outputting, </claim-text><claim-text> receiving a request for applying a virtual object to a stream of frames; processing the request for applying the virtual object to one or more </claim-text><claim-text> frames of the stream of frames; and </claim-text><claim-text> outputting modified stream of frames with the virtual object applied to the one or more frames. </claim-text></claim><claim id="clm-0002" num="2"><claim-text> 2. A computer-implemented method of claim 1 , wherein the processing further comprising the steps of: </claim-text><claim-text> for each frame in the stream of frames, </claim-text><claim-text> determining image feature points for a user in a current output frame; identifying one or more images stored in a storage medium for the virtual object; determining a first image of the virtual object to apply to the current output frame; determining a position for applying the virtual object to the current output frame; and </claim-text><claim-text> applying the first image of the virtual object to the current output frame. <!-- EPO <DP n="22"/>--> </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. A computer-implemented method of claim 2, further comprising determining one or more of user's angles of rotation in the first frame based on image feature points; and the determining of the first image is based on a particular angle of rotation. </claim-text></claim><claim id="clm-0004" num="4"><claim-text> 4. A computer-implemented method of claim 2, wherein the determining a position further comprises determining position values for applying image of virtual object to the current output frame based on the image feature points. </claim-text></claim><claim id="clm-0005" num="5"><claim-text> 5. A computer-implemented method of claim 1 , wherein the virtual object applied to one or more frames is persistently coupled to a user feature. </claim-text></claim><claim id="clm-0006" num="6"><claim-text> 6. A computer-implemented method of claim 5, wherein the user features includes a body part of the user. </claim-text></claim><claim id="clm-0007" num="7"><claim-text> 7. A computer-implemented method of claim 5, wherein the user feature is determined from a stream of video using computer vision techniques. </claim-text></claim><claim id="clm-0008" num="8"><claim-text> 8. A computer-implemented method of claim 1 , wherein each of frames of the output stream of frames is the reverse image of a received frame. </claim-text></claim><claim id="clm-0009" num="9"><claim-text> 9. A computer-implemented method of claim 8, the each of the reversed frames of the output stream of frames is outputted with minimal time elapsed between the receiving and the outputting. </claim-text></claim><claim id="clm-0010" num="10"><claim-text> 10. A computer-implemented method of claim 1 , wherein the output stream of frames comprises a virtual reflection of the user while in motion. </claim-text></claim><claim id="clm-0011" num="11"><claim-text> 1 1. A system for video interaction with virtual objects, said system comprising: </claim-text><claim-text> one or more processors; and </claim-text><claim-text> a computer-readable storage medium carrying one or more sequences of instructions, which when executed by said one or more processors implement a method for video interaction with virtual objects said method comprising: </claim-text><claim-text> receiving, by a computer, a stream of frames from a visual data capture device of a user in movement; <!-- EPO <DP n="23"/>--> outputting a stream of frames as video signal for processing by a visual display device based on the received stream of frames; </claim-text><claim-text> while receiving and outputting, </claim-text><claim-text> receiving a request for applying a virtual object to a stream of frames; processing the request for applying the virtual object to one or more </claim-text><claim-text> frames of the stream of frames; and </claim-text><claim-text> outputting modified stream of frames with the virtual object applied to the one or more frames. </claim-text></claim><claim id="clm-0012" num="12"><claim-text> 12. A system of claim 1 1 , wherein the processing further comprising the steps of: </claim-text><claim-text> for each frame in the stream of frames, </claim-text><claim-text> determining image feature points for a user in a current output frame; identifying one or more images stored in a storage medium for the virtual object; determining a first image of the virtual object to apply to the current output frame; determining a position for applying the virtual object to the current output frame; and </claim-text><claim-text> applying the first image of the virtual object to the current output frame. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. A system of claim 12, further comprising determining one or more of user's angles of rotation in the first frame based on the image feature points; and the determining of the first image is based on a particular angle of rotation. </claim-text></claim><claim id="clm-0014" num="14"><claim-text> 14. A system of claim 12, wherein the determining a position further comprises determining position values for applying image of virtual object to the current output frame based on the image feature points. </claim-text></claim><claim id="clm-0015" num="15"><claim-text> 15. A system of claim 1 1 , wherein the virtual object applied to one or more frames is persistently coupled to a user feature. </claim-text></claim><claim id="clm-0016" num="16"><claim-text> 16. A system of claim 15, wherein the user features includes a body part of the user. <!-- EPO <DP n="24"/>--> </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. A system of claim 15, wherein the user feature is determined from a stream of video using computer vision techniques. </claim-text></claim><claim id="clm-0018" num="18"><claim-text> 18. A system of claim 1 1 , wherein each of frames of the output stream of frames is the reverse image of a received frame. </claim-text></claim><claim id="clm-0019" num="19"><claim-text> 19. A system of claim 18, the each of the reversed frames of the output stream of frames is outputted with minimal time elapsed between the receiving and the outputting. </claim-text></claim><claim id="clm-0020" num="20"><claim-text> 20. A system of claim 1 1 , wherein the output stream of frames comprises a virtual reflection of the user while in motion. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
