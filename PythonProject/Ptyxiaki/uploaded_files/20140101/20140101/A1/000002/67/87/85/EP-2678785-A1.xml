<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2678785-A1" country="EP" doc-number="2678785" kind="A1" date="20140101" family-id="46721103" file-reference-id="301915" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146551284" ucid="EP-2678785-A1"><document-id><country>EP</country><doc-number>2678785</doc-number><kind>A1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-11859417-A" is-representative="NO"><document-id mxw-id="PAPP154825207" load-source="docdb" format="epo"><country>EP</country><doc-number>11859417</doc-number><kind>A</kind><date>20110224</date><lang>EN</lang></document-id><document-id mxw-id="PAPP182926871" load-source="docdb" format="original"><country>EP</country><doc-number>11859417.5</doc-number><date>20110224</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140451623" ucid="SE-2011050212-W" linkage-type="A" load-source="docdb"><document-id format="epo"><country>SE</country><doc-number>2011050212</doc-number><kind>W</kind><date>20110224</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1916233245" load-source="docdb">G06F  17/30        20060101AFI20160712BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-2023455790" load-source="docdb" scheme="CPC">G06F  17/30038     20130101 LI20150909BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988121852" load-source="docdb" scheme="CPC">G06F  17/30598     20130101 FI20131127BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132186336" lang="DE" load-source="patent-office">MEDIENKLASSIFIKATIONSVERFAHREN UND -SERVER</invention-title><invention-title mxw-id="PT132186337" lang="EN" load-source="patent-office">METHOD AND SERVER FOR MEDIA CLASSIFICATION</invention-title><invention-title mxw-id="PT132186338" lang="FR" load-source="patent-office">PROCÉDÉ ET SERVEUR DE CLASSIFICATION DE DONNÉES MULTIMÉDIA</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR918143952" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ERICSSON TELEFON AB L M</last-name><address><country>SE</country></address></addressbook></applicant><applicant mxw-id="PPAR918138725" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>TELEFONAKTIEBOLAGET L M ERICSSON (PUBL)</last-name></addressbook></applicant><applicant mxw-id="PPAR918992948" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Telefonaktiebolaget L M Ericsson (PUBL)</last-name><iid>101131063</iid><address><city>164 83 Stockholm</city><country>SE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918156785" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>GEORGAKIS APOSTOLOS</last-name><address><country>SE</country></address></addressbook></inventor><inventor mxw-id="PPAR918154179" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>GEORGAKIS, APOSTOLOS</last-name></addressbook></inventor><inventor mxw-id="PPAR918993294" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>GEORGAKIS, APOSTOLOS</last-name><address><street>Kolsnarsvägen 21</street><city>S-12051 Stockholm</city><country>SE</country></address></addressbook></inventor><inventor mxw-id="PPAR918149812" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>SOEDERBERG JOAKIM</last-name><address><country>SE</country></address></addressbook></inventor><inventor mxw-id="PPAR918140318" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>SOEDERBERG, JOAKIM</last-name></addressbook></inventor><inventor mxw-id="PPAR918982217" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>Söderberg, Joakim</last-name><address><street>Himlabacken 9a</street><city>S-17078 Solna</city><country>SE</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR918990559" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Egrelius, Fredrik</last-name><suffix>et al</suffix><iid>101307911</iid><address><street>Ericsson AB Patent Unit Kista Device, Service &amp; Media Torshamnsgatan 21-23</street><city>164 80 Stockholm</city><country>SE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="SE-2011050212-W"><document-id><country>SE</country><doc-number>2011050212</doc-number><kind>W</kind><date>20110224</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012115550-A1"><document-id><country>WO</country><doc-number>2012115550</doc-number><kind>A1</kind><date>20120830</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS548929450" load-source="docdb">AL</country><country mxw-id="DS548816125" load-source="docdb">AT</country><country mxw-id="DS548929451" load-source="docdb">BE</country><country mxw-id="DS548930737" load-source="docdb">BG</country><country mxw-id="DS548934769" load-source="docdb">CH</country><country mxw-id="DS548865702" load-source="docdb">CY</country><country mxw-id="DS548816126" load-source="docdb">CZ</country><country mxw-id="DS548837169" load-source="docdb">DE</country><country mxw-id="DS548865703" load-source="docdb">DK</country><country mxw-id="DS548865704" load-source="docdb">EE</country><country mxw-id="DS548927107" load-source="docdb">ES</country><country mxw-id="DS548930738" load-source="docdb">FI</country><country mxw-id="DS548930739" load-source="docdb">FR</country><country mxw-id="DS548929452" load-source="docdb">GB</country><country mxw-id="DS548865705" load-source="docdb">GR</country><country mxw-id="DS548929453" load-source="docdb">HR</country><country mxw-id="DS548816127" load-source="docdb">HU</country><country mxw-id="DS548934770" load-source="docdb">IE</country><country mxw-id="DS548929458" load-source="docdb">IS</country><country mxw-id="DS548930740" load-source="docdb">IT</country><country mxw-id="DS548865706" load-source="docdb">LI</country><country mxw-id="DS548837174" load-source="docdb">LT</country><country mxw-id="DS548816128" load-source="docdb">LU</country><country mxw-id="DS548837175" load-source="docdb">LV</country><country mxw-id="DS548837176" load-source="docdb">MC</country><country mxw-id="DS548914168" load-source="docdb">MK</country><country mxw-id="DS548914169" load-source="docdb">MT</country><country mxw-id="DS548929459" load-source="docdb">NL</country><country mxw-id="DS548930741" load-source="docdb">NO</country><country mxw-id="DS548929460" load-source="docdb">PL</country><country mxw-id="DS548934771" load-source="docdb">PT</country><country mxw-id="DS548929461" load-source="docdb">RO</country><country mxw-id="DS548934772" load-source="docdb">RS</country><country mxw-id="DS548929466" load-source="docdb">SE</country><country mxw-id="DS548934773" load-source="docdb">SI</country><country mxw-id="DS548837177" load-source="docdb">SK</country><country mxw-id="DS548837182" load-source="docdb">SM</country><country mxw-id="DS548865707" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA99620524" ref-ucid="WO-2012115550-A1" lang="EN" load-source="patent-office"><p num="0000">The embodiments of the present invention relates to a method and system for classifying media. The classification is achieved by using annotation ontolgies and by associating bottom level concepts of the annotation ontology tree with explanatory representation data of a selected representation domain and then comparing the explanatory representation data with transformation of the media in the selected representation domain. In this way tags can be generated which corresponds to bottom level concepts of the ontology tree which corresponds to explanatory representation data which can be found in the transformed media.</p></abstract><abstract mxw-id="PA99820183" ref-ucid="WO-2012115550-A1" lang="EN" source="national office" load-source="docdb"><p>The embodiments of the present invention relates to a method and system for classifying media. The classification is achieved by using annotation ontolgies and by associating bottom level concepts of the annotation ontology tree with explanatory representation data of a selected representation domain and then comparing the explanatory representation data with transformation of the media in the selected representation domain. In this way tags can be generated which corresponds to bottom level concepts of the ontology tree which corresponds to explanatory representation data which can be found in the transformed media.</p></abstract><abstract mxw-id="PA99620525" ref-ucid="WO-2012115550-A1" lang="FR" load-source="patent-office"><p num="0000">Les modes de réalisation de la présente invention concernent un procédé et un système de classification de données multimédia. La classification est effectuée en utilisant des ontologies d'annotation et en associant des concepts de niveau inférieur de l'arbre d'ontologies d'annotation à des données de représentation explicatives d'un domaine de représentation sélectionné puis en comparant les données de représentation explicatives à la transformation des données multimédia dans le domaine de représentation sélectionné. Il est ainsi possible de générer des balises qui correspondent aux concepts de niveau inférieur de l'arbre d'ontologies correspondant aux données de représentation explicatives présentes dans les données multimédia transformées.</p></abstract><abstract mxw-id="PA99820184" ref-ucid="WO-2012115550-A1" lang="FR" source="national office" load-source="docdb"><p>Les modes de réalisation de la présente invention concernent un procédé et un système de classification de données multimédia. La classification est effectuée en utilisant des ontologies d'annotation et en associant des concepts de niveau inférieur de l'arbre d'ontologies d'annotation à des données de représentation explicatives d'un domaine de représentation sélectionné puis en comparant les données de représentation explicatives à la transformation des données multimédia dans le domaine de représentation sélectionné. Il est ainsi possible de générer des balises qui correspondent aux concepts de niveau inférieur de l'arbre d'ontologies correspondant aux données de représentation explicatives présentes dans les données multimédia transformées.</p></abstract><description mxw-id="PDES50930667" ref-ucid="WO-2012115550-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001"> Method and server for media classification </p><p id="p0002" num="0002">Background </p><p id="p0003" num="0003"> Mankind has generated tremendous amounts of digital data, e.g. from cameras, microphones, scientific equipments etc. Out of this wealth of digital data some data is useless to be processed and/ or stored whereas other data is of high importance. Digital data such as images, video, audio and text are a part of our collective and individualistic identity and are often used as building blocks for new knowledge, experiences, products, business models, etc. This means that certain data like personal photos can be used as starting points for collective or individualistic applications. </p><p id="p0004" num="0004">Due to the exponential growth of the availability of digital sensor equipment such as digital cameras, the Internet, mobile phones, etc, the amount of information deemed as important for preservation for either the society in general and each individual person has surpassed all the limitations of human memory, the cataloging systems and even indexing schemes that were in place the last 200 years. The sheer volume of recorded data makes it impossible to locate and retrieve past data unless they are somehow annotated. </p><p id="p0005" num="0005">The last passage has dire consequences for society and individuals. Existing and new knowledge will be forgotten or rendered useless because there is no economical way for cataloging, organizing and searching it. </p><p id="p0006" num="0006">The above situation resulted in the emergence of numerous semi-automatic and automatic solutions for media annotation. In such approaches an "intelligent" system tries to substitute a human indexer in assigning annotation tags in objects exemplified by books, photos, mp3s, etc. The success rate of such a scheme depends on the initial assumptions made for the underlying data, the system's scalability capabilities and the quality of the annotation libraries or dictionaries i.e. the actual tags that are used to annotate the data. 
<!-- EPO <DP n="3"/>-->
 Some annotation systems use ontologies, which are formal representations of knowledge as sets of concepts within a specific domain along with the relationships between those concepts. An ontology denotes a taxonomy with a set of inference rules and can be seen as a class hierarchy from abstract to more specific objects. Figure 1 provides such a taxonomy. </p><p id="p0007" num="0007">The following are examples of such systems: </p><p id="p0008" num="0008">US20100030552A1 uses ontologies to describe real world entities and the relationship between tags by determining properties associated with tags and domains, using linguistic analysis. </p><p id="p0009" num="0009">US20100004923A1 describes a method for ontology-based clustering of process models e.g. manufacturing process in organization. The method involves a distance calculation unit for calculating a distance matrix, and partitioning models into set of clusters based on calculated matrix. </p><p id="p0010" num="0010">US20080168070A1 , presents a classification method for use over Internet, involving evaluation of multimedia artifacts (e.g. photographs) using selected classifiers to determine classification (tags). The semantic tagging is enhanced by applying only classifiers from selected ontologies based on scoring. </p><p id="p0011" num="0011">JP2008131 170A defines an apparatus for generating knowledge metadata for use in choosing multimedia content. It specifies a generation unit that generates knowledge metadata relevant to a user, based on ontology with respect to information resource after storing new concept in the ontology. </p><p id="p0012" num="0012">The examples above elaborate on media classification involving ontologies in some way, but none of them presents a solution on how to connect specialized concepts in an ontology to numeric measurable observations in the media domain. 
<!-- EPO <DP n="4"/>-->
 Summary </p><p id="p0013" num="0013">The embodiments of the present invention provides an automatic method and arrangement for assisting users with annotating different types of media objects such as images, video, audio, etc using enhanced structured annotations. </p><p id="p0014" num="0014">This is achieved by associating bottom level concepts of the annotation ontology tree with explanatory representation data of a selected </p><p id="p0015" num="0015">representation domain and then comparing the explanatory representation data with transformation of the media in the selected representation domain. In this way tags can be generated which correspond to bottom level concepts of the ontology tree which correspond to explanatory </p><p id="p0016" num="0016">representation data which can be found in the transformed media. </p><p id="p0017" num="0017">According to a first aspect of embodiments of the present invention a method for a media classification is provided. The media is classified and stored hierarchical according to at least one annotation ontology tree associated with a respective media class. In the method, at least one annotation ontology tree associated with a respective media class is retrieved. A representation domain to be used for the classification is selected and bottom level concepts of the annotation ontology tree are associated with explanatory representation data of the selected </p><p id="p0018" num="0018">representation domain. A transformation of the media to be classified into the selected representation domain is received and the media according to the retrieved annotation ontology tree is classified by comparing the explanatory representation data with the transformed media to be classified. </p><p id="p0019" num="0019">According to a second aspect of embodiments of the present invention, a media classification system is provided. The media is classified and stored hierarchical according to at least one annotation ontology tree associated 
<!-- EPO <DP n="5"/>-->
 with a respective media class. The system is configured to retrieve at least one annotation ontology tree associated with a respective media class. The system is further configured to select a representation domain to be used for the classification and to associate bottom level concepts of the annotation ontology tree with explanatory representation data of the selected </p><p id="p0020" num="0020">representation domain. The system is also configured to receive a </p><p id="p0021" num="0021">transformation of the media to be classified into the selected representation domain, and to classify the media according to the retrieved annotation ontology tree by comparing the explanatory representation data with the transformed media to be classified. </p><p id="p0022" num="0022">The proposed solution may generate tags that are spatially and temporally aligned with the objects in the media under consideration. That is, the proposed tags can be linked with specific spatial or temporal portions of the media and not just the media at its entire. Moreover, a feedback channel may be provided which assures to the continue training or improvement of the system's performance. </p><p id="p0023" num="0023">Further advantages with embodiments are that it is easy to organize photos and other media using enhanced ontologies. A further advantage is the embodiments bridge the gap between ontological concepts and observable descriptors in the signal processing domain, enabling improved media annotation systems such as video and photo tagging. A yet further advantage with embodiments is that a learning mechanism that adjusts over time to reflect the user preferences. </p><p id="p0024" num="0024">Brief Description of the drawings </p><p id="p0025" num="0025">Figure 1 exemplifies an ontology tree which can be used in the </p><p id="p0026" num="0026">embodiments of the present invention. </p><p id="p0027" num="0027">Figure 2 exemplifies different explanatory representation data in different representation domains. 
<!-- EPO <DP n="6"/>-->
 Figures 3-5 are flowcharts of methods according to embodiments of the present invention. </p><p id="p0028" num="0028"> Figure 6 illustrates schematically a server and a user device according to embodiments of the present invention. </p><p id="p0029" num="0029">Detailed description </p><p id="p0030" num="0030">The proposed solution according to embodiments of the present invention utilizes structured ontologies. As mentioned above, ontologies are formal knowledge representation of concepts from specific domains along with the relationships between these concepts. An ontology has a formal structure with top, middle and bottom level classes, where a class is a concept or "type-of object or thing. An example of an ontology tree is illustrated in figure 1, where, top, middle and bottom level concepts are shown. It should be noted that the terms ontology and annotation ontology are used interchangeably within this specification. </p><p id="p0031" num="0031">According to embodiments of the present invention each bottom level concept of an annotation ontology tree is associated with explanatory representation data in a selected representation domain. Examples of explanatory representations are images if the media to be classified are photos, descriptive audio segments if the media to be classified are audio etc. 
<!-- EPO <DP n="7"/>-->
</p><p id="p0032" num="0032">However, according to embodiments, the explanatory representations are not limited to be a representation of a representation domain which is a human perceivable domain. An example of a representation domain which is not a human perceivable domain is the transform domain, wherein such explanatory representations may be in the Discrete Cosine Transform (DCT) , the Fast Fourier Transform (FFT), the Wavelets transform etc. Other examples of representation domains which are not in the human perceivable domain are the statistical domain with histograms, moments (variance, skewness, kurtosis, etc), etc. By using the different representation domains, it is possible to generalize knowledge from low level features to higher level concepts in various situations. </p><p id="p0033" num="0033">If the ontology would relate to apples or fruits at a higher level and a bottom level concept is Red Delicious having a structure as below, examples of explanatory representations are illustrated in figure 2. </p><p id="p0034" num="0034">Fruit (top level) </p><p id="p0035" num="0035"> Apple (middle level) </p><p id="p0036" num="0036">Red Delicious (bottom level) </p><p id="p0037" num="0037">Golden Delicious (bottom level) </p><p id="p0038" num="0038">Red Rome (bottom level) </p><p id="p0039" num="0039">Granny Smith (bottom level), </p><p id="p0040" num="0040"> The explanatory representations of a Red Delicious are an actual photo, Y channel of the photo, the histogram for the Y channel, the FFT and the Radon transform of it in transform domains. The Y channel, is the luminance channel in the YUV data format, which is equivalent to the RGB data format for storing images. 
<!-- EPO <DP n="8"/>-->
</p><p id="p0041" num="0041">These are only some examples of explanatory representations and </p><p id="p0042" num="0042">representation domains. Different ontologies may be associated with different explanatory representations depending on the media to be classified. That implies that suitable explanatory representations when the media to be classified are photos may be images, the Y channel etc as in the case with Red Delicious as disclosed above. While if the media to be classified is audio, the explanatory representations may be low-level signal properties, Mel- frequency cepstral coefficients (MFCC), psychoacoustic features (roughness, loudness, etc) </p><p id="p0043" num="0043">According to embodiments of the present invention, the media to be classified is transformed to a representation domain. The transformed media is then compared with explanatory representations of the same </p><p id="p0044" num="0044">representation domain. In this way the media can be classified when an explanatory representation in an annotation ontology is found that corresponds to the transformed media. The bottom level concept of the annotation ontology tree which is associated with the explanatory </p><p id="p0045" num="0045">representation which corresponds to the transformed media can be applied as a tag to the media. </p><p id="p0046" num="0046">As illustrated in the flowchart of figure 3, a method for a media </p><p id="p0047" num="0047">classification is provided. The media can be exemplified by image, video and audio. The media is classified and stored hierarchical according to at least one annotation ontology tree associated with a respective media class. </p><p id="p0048" num="0048">According to embodiments where the method is implemented in a server or a user device, the media to be classified is received 401. At least one </p><p id="p0049" num="0049">annotation ontology tree associated with a respective media class is selected and a representation domain to be used for the classification is selected 403. Further, the bottom level concepts of the annotation ontology tree are associated 404 with explanatory representation data of the selected representation domain. 
<!-- EPO <DP n="9"/>-->
 A transformation of the media to be classified into the selected </p><p id="p0050" num="0050">representation domain is received is step 405. In some embodiments when the method is performed by a server or a user device, this step implies that the media is transformed 405a. Then the media is classified 406 according to the retrieved annotation ontology tree by comparing the explanatory representation data with the transformed media to be classified. </p><p id="p0051" num="0051">Hence, the method may either be performed in a server or in a user device. If the method is performed in the user device, the user device may download the selected annotation ontologies from a central server. </p><p id="p0052" num="0052">If the method is performed in the server, the user device sends the media to be classified to the server. The server may have access to user preferences associated with the user which may be used in the classification procedure as explained further below. The server has typically access to different annotation ontology dictionaries. </p><p id="p0053" num="0053">In a further embodiment, the method is performed by both the user device and the server. Thus, the user device receives the media to be classified and transforms the media to the selected representation domain and sends the transformed media to the server. Information regarding which </p><p id="p0054" num="0054">representation domain the user device should transform the media to may be obtained from the server. In this further embodiment, the server retrieves 402 at least one annotation ontology tree associated with a respective media class, selects a representation domain to be used for the classification, associates 404 bottom level concepts of the annotation ontology tree with explanatory representation data of the selected representation domain, receives 405 a transformation of the media to be classified into the selected representation domain, and classifies 406 the media according to the retrieved annotation ontology tree by comparing the explanatory </p><p id="p0055" num="0055">representation data with the transformed media to be classified. 
<!-- EPO <DP n="10"/>-->
 According to an embodiment, the media is classified by identifying 406a transformed media which corresponds to the explanatory representation data. Tags are then generated 406b to the identified transformed media wherein the tags correspond to the bottom level concept, or a level above the bottom level concept, associated with the explanatory representation data. That means that the generated tags may indicate a higher level in the ontology tree than the bottom level concept. The generated tags are applied to the media to be classified accordingly. </p><p id="p0056" num="0056">How the generated tags are applied may depend on user input. E.g. more than one tag may be generated for one object of the media or the entire media, then the user may select which of the generated tag that should be applied. For example, if the object is a child in front of a tree, the tags indicating the tree and the child may be generated and the user can then select by an input operation which tag(s) should be applied. </p><p id="p0057" num="0057">According to a further embodiment, the application of the generated tags is dependent on user input. The server or the user's client that generates the tags may apply tags according to a user profile which defines the user preferences. For example the user may have a special interest in sports, which implies that tags related to sports should be prioritized to tags in other categories. </p><p id="p0058" num="0058">The server or the user device may also transcode the explanatory </p><p id="p0059" num="0059">representation data into a different domain. That is, if the ontology does not contain the domain representation that is required for a specific task then it is very simply for the explanatory data to be transcoded into different domains. 
<!-- EPO <DP n="11"/>-->
 The embodiments exploit the existing knowledge from the enhanced ontologies to annotate media objects. Further, the embodiments can also adjust the weights by learning the preferences of the user. These preferences are revealed to the classification device, which may be a server or user device, when the user selects some of the proposed tags as the actual tags for the media under consideration. </p><p id="p0060" num="0060">The following example illustrates how tags may be generated according to one embodiment. </p><p id="p0061" num="0061">A user wants to use ontologies regarding birds, outdoor scenes and nature. The user selects them from a list presented to him by his camera-phone. The user might have to pay a fee for some of these ontologies. </p><p id="p0062" num="0062">Hence the camera-phone acquires knowledge domain specific ontologies regarding birds, outdoor scenes and nature. These ontologies may be downloaded from sources that give them out for free or for a price. This step can be automatic or it might require user input. </p><p id="p0063" num="0063">The camera-phone decides which representation domain to use, e.g. wavelet transform and shift transform. This may be done automatically by the camera-phone. </p><p id="p0064" num="0064">Now the user takes an outdoor photo. The photo is transformed using both wavelets and shift transform i.e. the selected representation domains are wavelets and shift transform. </p><p id="p0065" num="0065">Low level features are extracted from the transformed media (i.e. the photo in this case) . Features of the transformed media that match the explanatory representation data from the ontologies are extracted and the rest is discarded. 
<!-- EPO <DP n="12"/>-->
 An ontology dictionary is created which contains all the explanatory representation data which is associated with the bottom level concepts of the annotation ontology tree which the classification system in the camera- phone can identify. The dictionary is used as a lookup table. If a feature from the photo is not in the dictionary then it is thrown away. </p><p id="p0066" num="0066">An analysis is performed on the rest of the features. The analysis can come in the form of cluster analysis (k-means, HMM, etc), mixture modeling (GMM, Bayesian modeling, etc), statistical analysis (parametric and non- parametric), etc. The outcome of the above analysis is principal features, that is, the most prominent or important features. These features are the transformed media. Thus the analysis comprises a comparison of the transformed media and the explanatory data of associated with the bottom level concepts of the ontology tree. The bottom level concepts of the ontology tree corresponding to the prominent features of the photo can be used as tags which the user can choose from. Examples of tags presented to the user are birch, leporinum, altostratus, etc since the photo was taken outdoors and contains some trees and portions of the sky. </p><p id="p0067" num="0067">It is also possible to generalize the tags by progressing from the bottom level concepts in the ontology to middle or even top level concepts and use these as tags instead for the bottom level concepts which are associated with the explanatory data corresponding to the transformed media. 
<!-- EPO <DP n="13"/>-->
</p><p id="p0068" num="0068">Moreover, due to the fact that the features are spatially and temporally distributed over the media for example image, the generated tags can also be spatio-temporally distributed on that media. For example, a generated tag may indicate two different objects in a special relationship to each other e.g. a book beside a pencil in an image, or a sky in top of an image, or a bird appearing before a tree in a video clip. Or, they can be aggregated as global tags for the media at its entity, i.e. one single tag can be applied to the entire image . </p><p id="p0069" num="0069">As mentioned above, the classification system can learn the user's preference and act accordingly. </p><p id="p0070" num="0070">Thus the classification system generates tags and suggests tags to the user for a given media (spatio-temporal or not). For example the user may choose from "sky" or "tree" if there is a photo of trees and the sky in the top. </p><p id="p0071" num="0071">The user selects some of the tags and annotates the media. In the example above, the user selects tree as the tag to annotate the photo. </p><p id="p0072" num="0072">The selected tag(s) is (are) feedback to the tag generation mechanism. Tags selected by the user indicate that these tags, in the future, should have higher priority whereas rejected tags should have lower priority. Accordingly, if the user has chosen tree as in the example above, tree will be generated as tag in the next photo with trees and the sky. These user preferences help the learning mechanism to readjust the weights with which it evaluates the importance of each feature. </p><p id="p0073" num="0073">Since the tag "trees" was selected by Alice this means that the features relevant to them should be more important to her than other features on the image. A feature weighting mechanism is employed here when learning Alice's preferences. 
<!-- EPO <DP n="14"/>-->
 User input can come in an a priori or an a posteriori manner. In the a priori manner the user has already annotated an object(media). The classification system generates tags as described above and then the user provided tags are used to assess the correctness or not of the newly generated tags. In the a posteriori manner the user selects some of the tags that the classification system has generated and this process signals the user's preference towards the most suitable tags for the given object. </p><p id="p0074" num="0074">This implies that the user can annotate new photos when she shoots them or old photos that she had annotated in the past. For old photos she simply inputs them in the system and the system learns the user's preferences in a similar manner. </p><p id="p0075" num="0075">Turning now to figure 6 illustrating a media classification system 600, wherein the media 650 is classified and stored hierarchical according to at least one annotation ontology tree associated with a respective media class. The system 600 is configured to retrieve at least one annotation ontology tree associated with a respective media class. The at least one annotation ontology tree may be retrieved from a database 640. The system 600 is further configured to select a representation domain to be used for the classification and to associate bottom level concepts of the annotation ontology tree with explanatory representation data of the selected </p><p id="p0076" num="0076">representation domain. The system 600 is also configured to receive a transformation of the media 650 to be classified into the selected </p><p id="p0077" num="0077">representation domain. If the system is implemented in the user device, the system is configured to receive the media and to perform the transformation. If the system 600 is implemented in a server, the system is configured to either receive the transformed media or to receive the media and to transform the received media. The system 600 is configured to classify the media 650 according to the retrieved annotation ontology tree by comparing the explanatory representation data with the transformed media to be classified. 
<!-- EPO <DP n="15"/>-->
 According to an embodiment, the media classification system is further configured to identify transformed media corresponding to the explanatory representation data and to generate tags corresponding to the bottom level concept associated with the explanatory representation data to the identified transformed media or to generate tags corresponding to a level higher than the bottom level concept associated with the explanatory representation data to the identified transformed media. </p><p id="p0078" num="0078">Hence the classification system may either be implemented in a server or in a user device 630. Figure 6 illustrates schematically when the system 600 is implemented in a server and when the server receives the media to be classified 650, which implies that the classification system is configured to perform the transformation to the selected representation domain. The functionalities of the system may be realized by a processor 610 configured to execute computer program products which are stored in a memory 620 associated with the processor. The memory 620 may also store user profile information and other information relating to user preferences. 
</p></description><claims mxw-id="PCLM44726603" ref-ucid="WO-2012115550-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="16"/>-->CLAIMS </claim-statement><claim id="clm-0001" num="1"><claim-text>1. A method for a media classification, wherein media is classified and stored hierarchical according to at least one annotation ontology tree associated with a respective media class, the method comprises: </claim-text><claim-text> -retrieving (402) at least one annotation ontology tree associated with a respective media class, </claim-text><claim-text> -selecting (403) a representation domain to be used for the classification, -associating (404) bottom level concepts of the annotation ontology tree with explanatory representation data of the selected representation domain,</claim-text><claim-text>-receiving (405) a transformation (405) of the media to be classified into the selected representation domain, and </claim-text><claim-text> -classification (406) the media according to the retrieved annotation ontology tree by comparing the explanatory representation data with the transformed media to be classified. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The method according to claim 1 , wherein the method comprises the further step of: </claim-text><claim-text> -receiving (401) media to be classified. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The method according to claim 2, wherein the receiving (405) the transformation comprises: </claim-text><claim-text> -transforming (405a) the media to be classified into the selected </claim-text><claim-text>representation domain. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The method according to any of claims 1-3, wherein the classification further comprises: </claim-text><claim-text> -identifying (406a) transformed media corresponding to the explanatory representation data, </claim-text><claim-text>-generating (406b) tags corresponding to the bottom level concept associated with the explanatory representation data to the identified transformed media, and </claim-text><claim-text> -applying (406c) the generated tags to media to be classified. <!-- EPO <DP n="17"/>--> </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method according to any of claims 1-3, wherein the classification further comprises: </claim-text><claim-text> -identifying (406a) transformed media corresponding to the explanatory representation data, </claim-text><claim-text> -generating (406b) tags corresponding to a concept level above the bottom level concept associated with the explanatory representation data to the identified transformed media, and </claim-text><claim-text> -applying (406c) the generated tags to media to be classified. </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The method according to any of claims 1-5, wherein the application of the generated tags is dependent on user input. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method according to any of claims 1-5, wherein the application of the generated tags is dependent on user preferences. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8 The method according to any of claims 1-7, wherein the media is any of an image, video and audio. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. The method according to any of claims 1-6, wherein the representation domain is a transform domain. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The method according to any of claims 1-6, wherein the representation domain is a human perceivable domain. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>1 1. The method according to claim 7, wherein the transform domain is any of discrete cosine transform, fast fourier transform, wavelets, and histogram. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. A media classification system (600), wherein media is classified and stored hierarchical according to at least one annotation ontology tree associated with a respective media class, the system is configured to retrieve at least one annotation ontology tree associated with a respective media class, to select a representation domain to be used for the classification, to <!-- EPO <DP n="18"/>--> associate bottom level concepts of the annotation ontology tree with explanatory representation data of the selected representation domain, to receive a transformation of the media to be classified into the selected representation domain, and to classify the media according to the retrieved annotation ontology tree by comparing the explanatory representation data with the transformed media to be classified. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The media classification system according to claim 12 , wherein the media classification system is further configured to receive media to be classified. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The media classification system according to claim 13, wherein the media classification system is further configured to transform the media to be classified into the selected representation domain. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The media classification system according to any of claims 12-14, wherein the media classification system is further configured to identify transformed media corresponding to the explanatory representation data, to generate tags corresponding to the bottom level concept associated with the explanatory representation data to the identified transformed media, and to apply the generated tags to media to be classified. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. The media classification system according to any of claims 12-14, wherein the media classification system is further configured to identify transformed media corresponding to the explanatory representation data, to generate tags corresponding to a concept level above the bottom level concept associated with the explanatory representation data to the identified transformed media, and to apply the generated tags to media to be classified. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. The media classification system according to any of claims 12-16, wherein the application of the generated tags is dependent on user input. <!-- EPO <DP n="19"/>--> </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. The media classification system according to any of claims 12-16, wherein the application of the generated tags is dependent on user preferences. </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The media classification system according to any of claims 12-18, wherein the media is any of an image, video and audio. </claim-text></claim><claim id="clm-0020" num="20"><claim-text>20. The media classification system according to any of claims 12-19, wherein the representation domain is a transform domain. </claim-text></claim><claim id="clm-0021" num="21"><claim-text>21. The media classification system according to any of claims 12-19, wherein the representation domain is a human perceivable domain. </claim-text></claim><claim id="clm-0022" num="22"><claim-text>22. The media classification system according to claim 20, wherein the transform domain is any of discrete cosine transform, fast fourier transform, wavelets, and histogram. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
