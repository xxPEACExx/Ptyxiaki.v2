<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2680189-A1" country="EP" doc-number="2680189" kind="A1" date="20140101" family-id="46548354" file-reference-id="305010" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146549416" ucid="EP-2680189-A1"><document-id><country>EP</country><doc-number>2680189</doc-number><kind>A1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12305740-A" is-representative="YES"><document-id mxw-id="PAPP154823339" load-source="docdb" format="epo"><country>EP</country><doc-number>12305740</doc-number><kind>A</kind><date>20120626</date><lang>EN</lang></document-id><document-id mxw-id="PAPP255122606" load-source="docdb" format="original"><country>EP</country><doc-number>12305740.8</doc-number><date>20120626</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140454645" ucid="EP-12305740-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>12305740</doc-number><kind>A</kind><date>20120626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988095619" load-source="docdb">G06K   9/62        20060101ALI20121217BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1988098687" load-source="docdb">G06K   9/00        20060101AFI20121217BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1988918650" load-source="docdb" scheme="CPC">G06K   9/6256      20130101 LI20140103BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988923000" load-source="docdb" scheme="CPC">G06K   9/00744     20130101 FI20140103BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132180732" lang="DE" load-source="patent-office">Verfahren und System zur Erzeugung von Multimedia-Deskriptoren</invention-title><invention-title mxw-id="PT132180733" lang="EN" load-source="patent-office">Method and system for generating multimedia descriptors</invention-title><invention-title mxw-id="PT132180734" lang="FR" load-source="patent-office">Procédé et système de génération de descripteurs multimédia</invention-title><citations><patent-citations><patcit mxw-id="PCIT242652440" load-source="docdb" ucid="US-20100008547-A1"><document-id format="epo"><country>US</country><doc-number>20100008547</doc-number><kind>A1</kind><date>20100114</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242652441" load-source="docdb" ucid="US-20110135166-A1"><document-id format="epo"><country>US</country><doc-number>20110135166</doc-number><kind>A1</kind><date>20110609</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242652439" load-source="docdb" ucid="WO-2000016243-A1"><document-id format="epo"><country>WO</country><doc-number>2000016243</doc-number><kind>A1</kind><date>20000323</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>None</text><sources><source mxw-id="PNPL45131059" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR918167916" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ALCATEL LUCENT</last-name><address><country>FR</country></address></addressbook></applicant><applicant mxw-id="PPAR918135892" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>ALCATEL LUCENT</last-name></addressbook></applicant><applicant mxw-id="PPAR918978929" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Alcatel-Lucent</last-name><iid>101311164</iid><address><street>3, Avenue Octave Gréard</street><city>75007 Paris</city><country>FR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918157923" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>NAMBOODIRI VINAY</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918156830" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>NAMBOODIRI, Vinay</last-name></addressbook></inventor><inventor mxw-id="PPAR918987321" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>NAMBOODIRI, Vinay</last-name><address><street>Paul Lebrunstraat 26 bus 201</street><city>3000 Leuven</city><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918148907" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>TYTGAT DONNY</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918142643" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>Tytgat, Donny</last-name></addressbook></inventor><inventor mxw-id="PPAR918994218" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>Tytgat, Donny</last-name><address><street>Karperstraat, 100</street><city>9000 Gent</city><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918134745" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>LIEVENS SAMMY</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918170423" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>Lievens, Sammy</last-name></addressbook></inventor><inventor mxw-id="PPAR918983613" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>Lievens, Sammy</last-name><address><street>Leeuweriklaan, 7</street><city>2930 Brasschaat</city><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918150070" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>SIX ERWIN</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918167499" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>SIX, ERWIN</last-name></addressbook></inventor><inventor mxw-id="PPAR918990217" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>SIX, ERWIN</last-name><address><street>Centrumwijk, 47</street><city>9270 Kalken</city><country>BE</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR918988010" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>ALU Antw Patent Attorneys</last-name><iid>101179751</iid><address><street>Intellectual Property and Standards Copernicuslaan 50</street><city>2018 Antwerp</city><country>BE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS548804101" load-source="docdb">AL</country><country mxw-id="DS548840989" load-source="docdb">AT</country><country mxw-id="DS548804107" load-source="docdb">BE</country><country mxw-id="DS548892005" load-source="docdb">BG</country><country mxw-id="DS548868551" load-source="docdb">CH</country><country mxw-id="DS548860943" load-source="docdb">CY</country><country mxw-id="DS548840990" load-source="docdb">CZ</country><country mxw-id="DS548804108" load-source="docdb">DE</country><country mxw-id="DS548860944" load-source="docdb">DK</country><country mxw-id="DS548860945" load-source="docdb">EE</country><country mxw-id="DS548862171" load-source="docdb">ES</country><country mxw-id="DS548892006" load-source="docdb">FI</country><country mxw-id="DS548892007" load-source="docdb">FR</country><country mxw-id="DS548804109" load-source="docdb">GB</country><country mxw-id="DS548860946" load-source="docdb">GR</country><country mxw-id="DS548804118" load-source="docdb">HR</country><country mxw-id="DS548840991" load-source="docdb">HU</country><country mxw-id="DS548868552" load-source="docdb">IE</country><country mxw-id="DS548804119" load-source="docdb">IS</country><country mxw-id="DS548892008" load-source="docdb">IT</country><country mxw-id="DS548860947" load-source="docdb">LI</country><country mxw-id="DS548879641" load-source="docdb">LT</country><country mxw-id="DS548840992" load-source="docdb">LU</country><country mxw-id="DS548879642" load-source="docdb">LV</country><country mxw-id="DS548879643" load-source="docdb">MC</country><country mxw-id="DS548804806" load-source="docdb">MK</country><country mxw-id="DS548804807" load-source="docdb">MT</country><country mxw-id="DS548840993" load-source="docdb">NL</country><country mxw-id="DS548804120" load-source="docdb">NO</country><country mxw-id="DS548860948" load-source="docdb">PL</country><country mxw-id="DS548879645" load-source="docdb">PT</country><country mxw-id="DS548840994" load-source="docdb">RO</country><country mxw-id="DS548879646" load-source="docdb">RS</country><country mxw-id="DS548860949" load-source="docdb">SE</country><country mxw-id="DS548879647" load-source="docdb">SI</country><country mxw-id="DS548804121" load-source="docdb">SK</country><country mxw-id="DS548860950" load-source="docdb">SM</country><country mxw-id="DS548804808" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128670085" lang="EN" load-source="patent-office"><p id="pa01" num="0001">Method for generating multimedia descriptors from input multimedia content, said method includes a training procedure for generating a learned model in an iterative procedure based on received training media content, and from a plurality of localization identifiers provided by a plurality of detectors, said method further including the steps of generating said multimedia descriptors together with associated localization identifiers, based on said learned model, and a step of providing the generated multimedia descriptors together with the associated localization identifiers within the media as an output.
<img id="iaf01" file="imgaf001.tif" wi="129" he="101" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA128499458" lang="EN" source="EPO" load-source="docdb"><p>Method for generating multimedia descriptors from input multimedia content, said method includes a training procedure for generating a learned model in an iterative procedure based on received training media content, and from a plurality of localization identifiers provided by a plurality of detectors, said method further including the steps of generating said multimedia descriptors together with associated localization identifiers, based on said learned model, and a step of providing the generated multimedia descriptors together with the associated localization identifiers within the media as an output.</p></abstract><description mxw-id="PDES63955515" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">The present invention relates to a method and a system for generating multimedia descriptors from multimedia input files.</p><p id="p0002" num="0002">An object of the present invention is therefore to provide a method and a tool for generating multimedia descriptors from input multimedia content, which can be done in real time and delivers more precise and reliable metadata descriptors than the present systems. In addition it is the aim to provide also localization identifiers such as bounding boxes for indicating within the input media , where the detected objects, as identified by their descriptors, are exactly located.</p><p id="p0003" num="0003">According to the invention, this object is achieved by providing a method for generating multimedia descriptors from input multimedia content, said method includes a training procedure for generating a learned model in an iterative procedure based on received training media content, and from a plurality of localization identifiers provided by a plurality of detectors, said method further including the steps of generating said multimedia descriptors together with associated localization identifiers, based on said learned model, and a step of providing the generated multimedia descriptors together with the associated localization identifiers within the media as an output.</p><p id="p0004" num="0004">In this way, the iterative learning aspect combined with auxiliary detector information enables to generate more precise descriptors as well as bounding boxes in comparison to the known methods</p><p id="p0005" num="0005">In an embodiment the method further comprises the steps of generating low level training features and descriptors from said training media content such that said low level training features and descriptors are used by said plurality of detectors during said training procedure.</p><p id="p0006" num="0006">In another variant said multimedia descriptors with said associated localization identifiers are further determined from localization identifiers provided by said plurality of other detectors.<!-- EPO <DP n="2"> --></p><p id="p0007" num="0007">Another embodiment further comprises a step of generating low level features and descriptors from said input multimedia content, which are further used by said plurality of detectors and during the generation of said localization identifiers.</p><p id="p0008" num="0008">The present invention relates as well to a system for implementing such a method and to a computer program for implementing the steps of the method.</p><p id="p0009" num="0009">The above and other objects and features of the invention will become more apparent and the invention itself will be best understood by referring to the following description of an embodiment taken in conjunction with the accompanying drawings wherein:
<ul><li><figref idrefs="f0001">Fig. 1</figref> shows a high level architecture of the present method</li></ul></p><p id="p0010" num="0010">The following merely illustrates the principles of the invention. It will thus be appreciated that those skilled in the art will be able to devise various arrangements that, although not explicitly described or shown herein, embody the principles of the invention and are included within its spirit and scope. Furthermore, all examples and conditional language recited herein are principally intended expressly to be only for pedagogical purposes to aid the reader in understanding the principles of the invention and the concepts contributed by the inventor(s) to furthering the art, and are to be construed as being without limitation to such specifically recited examples and conditions. Moreover, all statements herein reciting principles, aspects, and embodiments of the invention, as well as specific examples thereof, are intended to encompass both structural and functional equivalents thereof. Additionally, it is intended that such equivalents include both currently known equivalents as well as equivalents developed in the future, i.e., any elements developed that perform the same function, regardless of structure.</p><p id="p0011" num="0011">The present invention concerns a method for generating media descriptors from multimedia data such as audio, voice, video, images, graphics, 3D models, etc. <figref idrefs="f0001">Fig. 1</figref> shows the basic building blocks of the method and system<!-- EPO <DP n="3"> --> whereby input media comprising raw audio/video/speech... input data, e.g. video and audio data under the form of frames, are provided as input data for the method or to a system for implementing the method.</p><p id="p0012" num="0012">The method comprises two main stages : a first stage, performed by a first module, being a training module as indicated in <figref idrefs="f0001">fig. 1</figref>, aims to learn an accurate model of the object class, also named tag, to be detected/identified in a next stage, performed by a second module, denoted "tagging module" in <figref idrefs="f0001">fig. 1</figref>. The determination of such an accurate model helps to automatically identify and detect the objects from the multimedia input. Furthermore, by means of an accurate object model, the determination of the bounding boxes, for indicating within the media, e.g. thus within the images themselves, where the object is located, will also be much facilitated.</p><p id="p0013" num="0013">The input multimedia used for the training are dedicated training multimedia, known beforehand to contain objects of which the model is to be learned in the training module. The training multimedia data furthermore contains tag or descriptor information with respect to these objects.</p><p id="p0014" num="0014">The training module itself is built up from different sub-modules, as shown in the figure. However in other embodiments this delineation into different sub-modules is not present, and the complete method, or several steps in combination may be performed by one single processor, or a combination of several processors working together.</p><p id="p0015" num="0015">Returning back to the figure, the training module comprises a first sub-module, denoted LLDE, which aims to extract low level descriptors from the input media. Together with the determination of these low level descriptors, which may just be an indication of a gradient in the color level of a group of pixels, low level features are also determined, which may also just relate to a histogram of this gradient for the aforementioned example. These are provided to several other detectors, which can e.g. comprise a hand, face, generic object, person, car,... detector.<!-- EPO <DP n="4"> --></p><p id="p0016" num="0016">These other detectors are able to determine bounding boxes, from the low level features and descriptors, which bounding boxes are an example of localization identifiers for indicating within the images/video frames, where exactly the objects detected by these other detectors are located. They may for instance provide bounding boxes around a detected hand, another bounding box around a detected face .</p><p id="p0017" num="0017">These low level features and descriptors, provided by the LLDE module, are as well provided to an initialization module, denoted INIT, which also receives from these other detectors e.g. face and hand detectors, their bounding boxes. From all this information the initialization module will then be able to determine an initial bounding box e.g. by selecting a bounding box associated to the most likely object detected by one of the other detectors. This initially determined bounding box is provided to a model learning block, denoted ML, which also receives the training input media, the low level features and descriptors, the input descriptor or the object to be detected, e.g. a person, as well as, via a feedback loop from a next sub-module, a re-estimated bounding box. At the onset of the method, this re-estimated bounding box is not yet ready, so at this stage no signal is provided at this input. The model learning block is adapted to extract and derive from the input media, from the initial bounding box and from the low level descriptors and features a model of a person with respect to the aforementioned example, which is to be used in the next stage of the method. During the training phase however, several estimates of the model are determined in an iterative process, and these estimates are next provided to another sub-module denoted LPI, being a latent parameter interference module, adapted to determine from the model estimate, as well as from the detector bounding boxes, an updated bounding box, which is provided back to the model learning module. The LPI module also checks whether convergence is present for several estimates of the model, and, if so, determines that the iterations are to be stopped, and that the estimated model determined so far is then of sufficient quality for being provided to the next stage. This final<!-- EPO <DP n="5"> --> estimated model is denoted M, and its provision together with its descriptor (a person for the aforementioned example) to the next module, being the tagging module, is depicted by means of the thick black arrow.</p><p id="p0018" num="0018">This second module, will then receive the real-life media, which are generally different from the training input media used during the training phase. Also in this module an initial low level feature and descriptor extractor sub-module can be present, having a similar functionality as the one described for the training module. In some embodiments just one LLDE module can be present, which may be shared between the training and tagging module.</p><p id="p0019" num="0019">The thus determined low level features and descriptors are now provided to a latent variable estimator sub-module, denoted LVE. Its function is to determine a bounding box around the identified features/object as a bounding box is an example of a latent variable. Such a bounding box is then determined based on the earlier learned model M, and based upon the information from the other detectors, which also received the low level features and descriptors, in a similar way as in the training module. The other detectors therefore provide their bounding boxes also to the LVE, which is able to determine therefrom, from the learned model, and from the low level features and descriptors a more accurate value of the bounding box. Its function is similar to the LPI sub-module of the training module such that the learned model as well as the other detector information is used to estimate the maximum likelihood of class bounding boxes.</p><p id="p0020" num="0020">This predicted bounding box is then fed to another sub-module, denoted MBP, which is able to determine therefrom and from the learned model, and even more accurate bounding box, and to integrate it into the media. The tag or descriptor itself was provided together with the model information M from ML. By considering the bounding box provided by LVE, and by checking whether different scales of the model fit with this, it can be determined whether the model (a person) is present in the input media. In other implementations, LVE and MBP can be performed by one single module, and the scaling can be done on the low<!-- EPO <DP n="6"> --> level features and descriptors, for determining whether there is a match with the input model. In case the model is not found in the input media, this information can also be provided at the output of the system. In case the to be detected object is present, MBP will provide the media with a tagged bounding box to an output of the system. In case the object to look for is not present in the media, this may be output as well.</p><p id="p0021" num="0021">While the principles of the invention have been described above in connection with specific apparatus, it is to be clearly understood that this description is made only by way of example and not as a limitation on the scope of the invention, as defined in the appended claims.</p></description><claims mxw-id="PCLM56976433" lang="EN" load-source="patent-office"><!-- EPO <DP n="7"> --><claim id="c-en-0001" num="0001"><claim-text>Method for generating multimedia descriptors from input multimedia content, said method includes a training procedure for generating a learned model in an iterative procedure based on received training media content, and from a plurality of localization identifiers provided by a plurality of detectors, said method further including the steps of generating said multimedia descriptors together with associated localization identifiers, based on said learned model, and a step of providing the generated multimedia descriptors together with the associated localization identifiers within the media as an output.</claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>method according to claim 1 further comprising the steps of generating low level training features and descriptors from said training media content such that said low level training features and descriptors are used by said plurality of detectors during said training procedure.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>Method according to claim 1 or 2 wherein said multimedia descriptors with said associated localization identifiers are further determined from localization identifiers provided by said plurality of other detectors.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>Method according to claim 3 further comprising a step of generating low level features and descriptors from said input multimedia content, which are further used by said plurality of detectors and during the generation of said localization identifiers.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>System for generating multimedia descriptors from input multimedia content, said system including a training module for generating a learned model in an iterative procedure based on received training media content, and from a plurality of localization identifiers provided by a plurality of detectors comprised within said system, said system further being adapted to generate said multimedia descriptors together with associated localization<!-- EPO <DP n="8"> --> identifiers, based on said learned model, and to provide the generated multimedia descriptors together with the associated localization identifiers within the media as an output.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>System according to claim 5 further being adapted to generate low level training features and descriptors from said training media content such that said low level training features and descriptors are used by said plurality of detectors within said training module.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>System according to claim 5 or 6 further being adapted to determine said multimedia descriptors with said associated localization identifiers are from localization identifiers determined and provided by said plurality of other detectors.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>System according to claim 7 further being adapted to generate low level features and descriptors from said input multimedia content, and wherein said plurality of detectors are further adapted to use said low level features and descriptors during the generation of said localization identifiers.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>A computer program product adapted to perform the method according to any of the claims 1 to 4.</claim-text></claim></claims><drawings mxw-id="PDW16667213" load-source="patent-office"><!-- EPO <DP n="9"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="125" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="159" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="158" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
