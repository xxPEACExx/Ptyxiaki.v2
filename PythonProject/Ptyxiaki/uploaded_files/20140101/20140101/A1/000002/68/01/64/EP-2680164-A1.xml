<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2680164-A1" country="EP" doc-number="2680164" kind="A1" date="20140101" family-id="46545716" file-reference-id="205009" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146549466" ucid="EP-2680164-A1"><document-id><country>EP</country><doc-number>2680164</doc-number><kind>A1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12360049-A" is-representative="YES"><document-id mxw-id="PAPP154823389" load-source="docdb" format="epo"><country>EP</country><doc-number>12360049</doc-number><kind>A</kind><date>20120628</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140447185" ucid="EP-12360049-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>12360049</doc-number><kind>A</kind><date>20120628</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988106179" load-source="docdb">G06F  17/30        20060101AFI20121109BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-2135387784" load-source="docdb" scheme="CPC">H04N   9/8205      20130101 LI20141118BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988922266" load-source="docdb" scheme="CPC">G06F  17/30784     20130101 LI20140103BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988924064" load-source="docdb" scheme="CPC">G06F  17/30047     20130101 FI20140103BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132180882" lang="DE" load-source="patent-office">Interaktion mit Inhalten</invention-title><invention-title mxw-id="PT132180883" lang="EN" load-source="patent-office">Content data interaction</invention-title><invention-title mxw-id="PT132180884" lang="FR" load-source="patent-office">Interaction de données de contenu</invention-title><citations><patent-citations><patcit mxw-id="PCIT242652512" load-source="docdb" ucid="US-20090103887-A1"><document-id format="epo"><country>US</country><doc-number>20090103887</doc-number><kind>A1</kind><date>20090423</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT242652513" load-source="docdb" ucid="US-20120045093-A1"><document-id format="epo"><country>US</country><doc-number>20120045093</doc-number><kind>A1</kind><date>20120223</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>Anonymous:  "Microsoft demos face recognition in video", , 9 March 2011 (2011-03-09), pages 1-1, XP55043234, Retrieved from the Internet: URL:http://blogs.technet.com/b/next/archive/2011/03/09/microsoft-demos-face-recognition-in-video.aspx [retrieved on 2012-11-06]</text><sources><source mxw-id="PNPL45131095" load-source="docdb" name="SEA" category="I"/></sources></nplcit><nplcit><text>None</text><sources><source mxw-id="PNPL45131096" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR918164447" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ALCATEL LUCENT</last-name><address><country>FR</country></address></addressbook></applicant><applicant mxw-id="PPAR918134807" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>ALCATEL LUCENT</last-name></addressbook></applicant><applicant mxw-id="PPAR918984864" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Alcatel-Lucent</last-name><iid>101311164</iid><address><street>3, Avenue Octave Gréard</street><city>75007 Paris</city><country>FR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918134116" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SHALLIKER STEPHAN J</last-name><address><country>GB</country></address></addressbook></inventor><inventor mxw-id="PPAR918171951" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SHALLIKER, STEPHAN J.</last-name></addressbook></inventor><inventor mxw-id="PPAR918987447" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SHALLIKER, STEPHAN J.</last-name><address><street>64 Sherford Rd. Elburton</street><city>Plymouth, Devon PL9 8BW</city><country>GB</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR918989757" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Sarup, David Alexander</last-name><suffix>et al</suffix><iid>100824209</iid><address><street>Alcatel-Lucent Telecom Ltd Intellectual Property Business Group Christchurch Way Greenwich</street><city>London SE10 0AG</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS548867323" load-source="docdb">AL</country><country mxw-id="DS548842553" load-source="docdb">AT</country><country mxw-id="DS548867325" load-source="docdb">BE</country><country mxw-id="DS548896148" load-source="docdb">BG</country><country mxw-id="DS548870294" load-source="docdb">CH</country><country mxw-id="DS548863900" load-source="docdb">CY</country><country mxw-id="DS548842554" load-source="docdb">CZ</country><country mxw-id="DS548867326" load-source="docdb">DE</country><country mxw-id="DS548863901" load-source="docdb">DK</country><country mxw-id="DS548863902" load-source="docdb">EE</country><country mxw-id="DS548805176" load-source="docdb">ES</country><country mxw-id="DS548896149" load-source="docdb">FI</country><country mxw-id="DS548896166" load-source="docdb">FR</country><country mxw-id="DS548867327" load-source="docdb">GB</country><country mxw-id="DS548863903" load-source="docdb">GR</country><country mxw-id="DS548867328" load-source="docdb">HR</country><country mxw-id="DS548842555" load-source="docdb">HU</country><country mxw-id="DS548870295" load-source="docdb">IE</country><country mxw-id="DS548867329" load-source="docdb">IS</country><country mxw-id="DS548896167" load-source="docdb">IT</country><country mxw-id="DS548863904" load-source="docdb">LI</country><country mxw-id="DS548805126" load-source="docdb">LT</country><country mxw-id="DS548842556" load-source="docdb">LU</country><country mxw-id="DS548805127" load-source="docdb">LV</country><country mxw-id="DS548805128" load-source="docdb">MC</country><country mxw-id="DS548882576" load-source="docdb">MK</country><country mxw-id="DS548882577" load-source="docdb">MT</country><country mxw-id="DS548842557" load-source="docdb">NL</country><country mxw-id="DS548867330" load-source="docdb">NO</country><country mxw-id="DS548863905" load-source="docdb">PL</country><country mxw-id="DS548805130" load-source="docdb">PT</country><country mxw-id="DS548842558" load-source="docdb">RO</country><country mxw-id="DS548805131" load-source="docdb">RS</country><country mxw-id="DS548863906" load-source="docdb">SE</country><country mxw-id="DS548805132" load-source="docdb">SI</country><country mxw-id="DS548867331" load-source="docdb">SK</country><country mxw-id="DS548863907" load-source="docdb">SM</country><country mxw-id="DS548882578" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128670135" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A method of providing interaction options to a user consuming content data, a content delivery apparatus operable to provide interaction options to a user consuming content data and a computer program product are disclosed. The method comprising the steps of: receiving an indication that the user requested interaction options for a class of features within the content data being consumed by the user; recognising instances of the class of features within the content data being consumed by the user; and identifying the instances to the user together with at least one interaction option associated with the instances. This approach enables the interactivity options or metadata to be generated dynamically as the content is being consumed, rather than relying on this information being pre-generated and then having to be associated with each occurrence within the content data. This significantly reduces the burden on the content provider. This also enables interactivity options or metadata to be generated for existing content data for which no such information was pre-generated. This also enables the interaction options or other metadata to evolve based on the user's requirements.
<img id="iaf01" file="imgaf001.tif" wi="134" he="97" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA128499508" lang="EN" source="EPO" load-source="docdb"><p>A method of providing interaction options to a user consuming content data, a content delivery apparatus operable to provide interaction options to a user consuming content data and a computer program product are disclosed. The method comprising the steps of: receiving an indication that the user requested interaction options for a class of features within the content data being consumed by the user; recognising instances of the class of features within the content data being consumed by the user; and identifying the instances to the user together with at least one interaction option associated with the instances. This approach enables the interactivity options or metadata to be generated dynamically as the content is being consumed, rather than relying on this information being pre-generated and then having to be associated with each occurrence within the content data. This significantly reduces the burden on the content provider. This also enables interactivity options or metadata to be generated for existing content data for which no such information was pre-generated. This also enables the interaction options or other metadata to evolve based on the user's requirements.</p></abstract><description mxw-id="PDES63955565" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><u>FIELD OF THE INVENTION</u></heading><p id="p0001" num="0001">The present invention relates to a method of providing interaction options to a user consuming content data, a content delivery apparatus operable to provide interaction options to a user consuming content data and a computer program product.</p><heading id="h0002"><u>BACKGROUND</u></heading><p id="p0002" num="0002">Content data, such as video, audio, image or other data is typically produced to be consumed or experienced by users. Traditionally, users consumed that content data in a generally passive manner. With the advent of interactive services, the demands and expectations of users when consuming or experiencing content data has increased and there is an expectation on the part of users that it is possible to interact with the content data being consumed.</p><p id="p0003" num="0003">Although techniques exist to enable users to interact with content data, they each have their own shortcomings.</p><p id="p0004" num="0004">Accordingly, it is desired to provide an improved technique for enabling users to interact with content data.</p><heading id="h0003"><u>SUMMARY</u></heading><p id="p0005" num="0005">According to a first aspect, there is provided a method of providing interaction options to a user consuming content data, the method comprising the steps of: receiving an indication that the user requested interaction options for a class of features within the content data being consumed by the user; recognising instances of the class of features within the content data being consumed by the user; and identifying the instances to the user together with at least one interaction option associated with the instances.</p><p id="p0006" num="0006">The first aspect recognizes that a problem with existing techniques is that metadata needs to be pre-generated to enable interaction options such as the presentation of additional information relating to the content data being consumed and/or contextual navigation options to be displayed. For much existing content data, no such metadata exists and would need to be generated. For newly produced content data, pre-generating such metadata is time-consuming. In either event, the metadata is generally incomplete or can become inappropriate with time. This is because a judgment needs to be made on what metadata to include and this judgment can vary over time. Hence,<!-- EPO <DP n="2"> --> it is difficult to determine in advance what metadata needs to be generated which would be appropriate to the user. Also, that metadata would typically need to be associated with all the relevant parts of the content data being consumed by the user. Again, this is extremely time-consuming.</p><p id="p0007" num="0007">Accordingly, a method of providing interactivity to a user consuming content data is provided. The method may comprise the step of receiving an indication that the user has requested information or interactivity options for a class or type of features or objects within the content data being consumed or experienced by the user. The method may also comprise the step of recognizing instances all occurrences of the class or types of features or objects within the content data being consumed or experienced by the user. The method may also comprise the step of identifying the instances or examples of the class or type of features or objects within the content being consumed by the user together with one or more interaction options associated with those instances. This approach enables the interactivity options or metadata to be generated dynamically in real time (or near real time) as the content is being consumed, rather than relying on this information being pre-generated and then having to be associated with each occurrence within the content data. This significantly reduces the burden on the content provider. This also enables interactivity options or metadata to be generated for existing content data for which no such information was pre-generated. This also enables the interaction options or other metadata to evolve based on the user's requirements. In addition, the interactivity options or metadata may be generated for later use as content is made available or re-generated periodically to ensure that the interactivity options or metadata remains relevant. These interactivity options or metadata may then be stored or cached to reduce the real time processing burden, particularly where multiple requests for the same content data are made.</p><p id="p0008" num="0008">In one embodiment, the content data comprises at least one of audio, video and an image. Accordingly, the content data may comprise any media data or multimedia data.</p><p id="p0009" num="0009">In one embodiment, the content data comprises a stream of content data and the method comprises the step of pausing the stream of content data. It will be appreciated that the stream of content data need not be paused although pausing the content stream ensures that the item of interest to the user remains on the screen.<!-- EPO <DP n="3"> --></p><p id="p0010" num="0010">In one embodiment, the step of recognising comprises recognising instances of the class of features within at least a paused frame of the stream of content data. Accordingly, those instances or examples of the class or type of features or object within the content data may be recognized.</p><p id="p0011" num="0011">In one embodiment, the class of features comprises a geographical location recognisable from the content data. It will be appreciated that the geographical location may be the actual location that the content data was recorded. This may be derivable from landmark buildings or a recognizable view or panorama.</p><p id="p0012" num="0012">In one embodiment, the class of features comprises items recorded by the content data.</p><p id="p0013" num="0013">In one embodiment, the items comprise at least one of characters, people and products.</p><p id="p0014" num="0014">In one embodiment, the step of recognising comprises recognising using at least one of image and audio recognition on the content data. Accordingly, appropriate recognition may be performed on the content data in order to identify the instances of the objects recorded by the content data.</p><p id="p0015" num="0015">In one embodiment, the step of recognising comprises referencing a relevance associated with items recognised and dismissing those items recognised having a relevance below a relevance threshold. Accordingly, the only those items which are likely to be instances of the class of features are recognized as such. This reduces the risk of incorrect recognition.</p><p id="p0016" num="0016">In one embodiment, the step of recognising comprises identifying whether interaction options are associated with items recognised and dismissing those items recognised having no interaction options. Accordingly, only instances which have interaction options are identified to the user. For example, if an object is identified but there are no interaction options for that object, then the object may not be distinguished to the user. This avoids distinguishing objects for which no interactivity options exist.</p><p id="p0017" num="0017">In one embodiment, the step of recognising comprises providing information relating the content data to a server which performs the recognition and provides an indication of the instances together with an indication of the at least one interaction option associated with the instances. Accordingly, a server may be provided which undertakes the recognition, identifies the instances and/or provides the interaction options<!-- EPO <DP n="4"> --> associated with those instances. It will be appreciated that this reduces the processing burden of any user equipment or apparatus.</p><p id="p0018" num="0018">In one embodiment, the information relating the content data comprises at least one of the content data and an identifier identifying the content data. Accordingly, the content data itself or a reference to the content data may be provided to the server. Providing a reference to the content data may reduce the resources required to support communication with the server.</p><p id="p0019" num="0019">In one embodiment, the step of indicating comprises distinguishing the instances for the user. Accordingly, the recognized instances may be identified to the user. This provides an indication to the user of those instances which have been recognized.</p><p id="p0020" num="0020">In one embodiment, the step of distinguishing comprises at least one of highlighting, outlining and surrounding the instances within the content data.</p><p id="p0021" num="0021">In one embodiment, the step of indicating interaction options comprises displaying information associated with each of the instances. Accordingly, the information retrieved relating to the recognized instance may be displayed.</p><p id="p0022" num="0022">In one embodiment, the step of indicating interaction options comprises displaying at least one link to additional interaction options. Accordingly, one or more links for navigation options to additional information, content and/or interaction options may be provided. This enables the user to interact with further content.</p><p id="p0023" num="0023">According to a second aspect, there is provided a content delivery apparatus operable to provide interaction options to a user consuming content data, the apparatus comprising: logic operable to receive an indication that the user requested interaction options for a class of features within the content data being consumed by the user; recognition logic operable to recognise instances of the class of features within the content data being consumed by the user; and identification logic operable to identify the instances to the user together with at least one interaction option associated with the instances.</p><p id="p0024" num="0024">In one embodiment, the content data comprises at least one of audio, video and an image.<!-- EPO <DP n="5"> --></p><p id="p0025" num="0025">In one embodiment, the content data comprises a stream of content data and the logic is operable to pause the stream of content data.</p><p id="p0026" num="0026">In one embodiment, the recognition logic is operable to recognise instances of the class of features within at least a paused frame of the stream of content data.</p><p id="p0027" num="0027">In one embodiment, the class of features comprises a geographical location recognisable from the content data.</p><p id="p0028" num="0028">In one embodiment, the class of features comprises items recorded by the content data.</p><p id="p0029" num="0029">In one embodiment, the items comprise at least one of characters, people and products.</p><p id="p0030" num="0030">In one embodiment, the recognition logic is operable to recognise using at least one of image and audio recognition on the content data.</p><p id="p0031" num="0031">In one embodiment, the recognition logic is operable to referencing a relevance associated with items recognised and to dismiss those items recognised having a relevance below a relevance threshold.</p><p id="p0032" num="0032">In one embodiment, the recognition logic is operable to identify whether interaction options are associated with items recognised and to dismiss those items recognised having no interaction options.</p><p id="p0033" num="0033">In one embodiment, the recognition logic is operable to provide information relating the content data to a server which performs the recognition and provides an indication of the instances together with an indication of the at least one interaction option associated with the instances.</p><p id="p0034" num="0034">In one embodiment, the information relating the content data comprises at least one of the content data and an identifier identifying the content data.</p><p id="p0035" num="0035">In one embodiment, the identification logic is operable to distinguish the instances for the user.<!-- EPO <DP n="6"> --></p><p id="p0036" num="0036">In one embodiment, the identification logic is operable to distinguish by performing at least one of highlighting, outlining and surrounding the instances within the content data.</p><p id="p0037" num="0037">In one embodiment, the identification logic is operable to display information associated with each of the instances.</p><p id="p0038" num="0038">In one embodiment, the identification logic is operable to display at least one link to additional interaction options.</p><p id="p0039" num="0039">According to a third aspect, there is provided a computer program product operable, when executed on a computer, to perform the method steps of the first aspect.</p><p id="p0040" num="0040">Further particular and preferred aspects are set out in the accompanying independent and dependent claims. Features of the dependent claims may be combined with features of the independent claims as appropriate, and in combinations other than those explicitly set out in the claims.</p><p id="p0041" num="0041">Where an apparatus feature is described as being operable to provide a function, it will be appreciated that this includes an apparatus feature which provides that function or which is adapted or configured to provide that function.</p><heading id="h0004"><u>BRIEF DESCRIPTION OF THE DRAWINGS</u></heading><p id="p0042" num="0042">Embodiments of the present invention will now be described further, with reference to the accompanying drawings, in which:
<ul><li><figref idrefs="f0001">Figure 1</figref> illustrates an example user interaction according to one embodiment; and</li><li><figref idrefs="f0002">Figure 2</figref> illustrates an example network configuration according to one embodiment.</li></ul></p><heading id="h0005"><u>DESCRIPTION OF THE EMBODIMENTS</u></heading><heading id="h0006"><u>Overview</u></heading><p id="p0043" num="0043">Before discussing the embodiments in any more detail, first an overview will be provided. As mentioned above, embodiments provide the ability for users to obtain additional information or metadata and to interact with content data which they are experiencing or consuming, even when that content data has not been specifically associated with metadata providing that information. Instead, metadata is generated by analysing the content data and performing recognition techniques on that content data to identify automatically items or features recorded in the content data. Data<!-- EPO <DP n="7"> --> sources, such as search engines, are then interrogated to identify what the items or features recognized within the content data are likely to be and this information is then utilized to provide additional information regarding those items, as well as further options relating to those items which are available to the user.</p><p id="p0044" num="0044">This approach enables this additional metadata to be generated when required without needing the content data producer to invest resources creating the metadata. Likewise, any distributor of the content data also does not need to invest resources pre-generating such metadata. Instead, the metadata can be generated when required. This also avoids generating metadata which may only be relevant at the time that the metadata was created. In addition, metadata can be generated for historical content data produced prior to any interactivity being envisaged.</p><p id="p0045" num="0045">It will be appreciated that metadata is typically generated based on the perception of the creator of that metadata of what metadata is relevant to include or not. Relevance is something that may vary with time and so metadata which is relevant now may have been omitted at the time that any pre-generated metadata was created. For example, a particular location, person or object may not be considered to be relevant and so may not be included in metadata associated with content data when that metadata is pre-generated. However, a location, person or object may become particularly significant and their existence within particular content data may be unknown unless the metadata for that content data is regenerated. By constantly regenerating metadata for the content data as it is consumed, the metadata is generated can remain relevant and up-to-date.</p><heading id="h0007"><u>Example user interaction</u></heading><p id="p0046" num="0046"><figref idrefs="f0001">Figure 1</figref> illustrates an example user interaction according to one embodiment. In this example, the user is consuming video content. However, it will be appreciated that the same technique can apply to other content such as audio content or even image content. In this example, the user is viewing a video which is provided by the set top box 10 from a content provider and displays on a display apparatus 20.</p><p id="p0047" num="0047">The user watches the video content and wishes to find out further information relating to the image or audio being provided by the display apparatus 20. In this example, the user presses a pause button on the controller 30, which instructs the set-top box 10 to pause the content. The user identifies that he wishes to know the names of the actors being displayed, however, it will be appreciated that the user may indicate other classes<!-- EPO <DP n="8"> --> of types of features of objects for which additional information is required. For example, the user may indicate that he wishes additional information on objects, sounds or other items recorded in the content data, or even the geographical location where the content was recorded. Accordingly, in this example, the set-top box 10 manages the generation of metadata associated with the actors currently being displayed.</p><p id="p0048" num="0048">The actors which are identified are then highlighted 40a, 40b on the display apparatus 20. In this example, Brad Pitt and John Wayne have been identified as the actors shown on the display apparatus 20. The user may then either continue with the video or may choose one of the highlighted actors 40a, 40b, which prompts the display of additional information 50 which includes, in this example, biographical information 50a and links to additional information 50b. In this example, links are provided to the movies within which that actor appears, however, it will be appreciated that other links or possible user interactions may be provided. If these additional links are selected, then the user may watch those movies, may bookmark them for future viewing or may perform some other interaction.</p><p id="p0049" num="0049">As mentioned above, although in this example the class of items to be identified within the content is the actors currently shown on the display apparatus 20, it will be appreciated that other classes of features within the content may be classified. For example, the user may request that objects such as branded products or other identifiable items are to be highlighted. Likewise, the user may request that the geographical location indicating where the content was recorded be identified. This information, together with any navigational links, may then be displayed on the display apparatus 20.</p><p id="p0050" num="0050">Also, although in this example the content is paused, it will be appreciated that this need not be necessary and that the content may continue to be played with the recognizable items continuing to be highlighted as they appear on screen.</p><p id="p0051" num="0051">It will be appreciated that although a conventional set top box 10, display screen 20 and controller 30 is illustrated, it will be appreciated that these items may be distributed in a different manner or may be combined into a single device such as, for example, a tablet.</p><heading id="h0008"><u>Example network configuration</u></heading><!-- EPO <DP n="9"> --><p id="p0052" num="0052"><figref idrefs="f0002">Figure 2</figref> shows an example network configuration according to one embodiment. In this example, the set-top box 10, display screen 20 and controller 30 arrangement is similar to that described above. Also, in a similar manner to that described above, the exact configuration of these items may vary or they may be incorporated into a single device such as a tablet.</p><p id="p0053" num="0053">As described above, at step S1, the user indicates that they wish additional information associated with the video content data being consumed to be provided. The user is receiving the content data from the TV middleware server 80 provided by a broadcaster. However, it will be appreciated that the user may also receive the content data from elsewhere such as from the internet or from a broadcaster using other delivery media. In this example, the video content data is paused.</p><p id="p0054" num="0054">At step S2, the set-top box 10 sends a copy of the paused video frame via the Internet 60 to a metadata server 70. Although in this example the set-top box provides a video frame to the metadata server 70 via the internet 60, it will be appreciated that the set-top box 10 may instead simply provide an indication identifying the paused frame and the frame may instead be provided by the TV middleware server 80.</p><p id="p0055" num="0055">The metadata server 70 then identifies the class of feature to be identified within the content data provided.</p><p id="p0056" num="0056">At step S3, the metadata server 70 then uses a detection engine 80 which is used to detect instances of that class of features within the content data. In this example, the detection engine 80 is a face detection engine. The detection engine 80 performs image recognition and discards potentially identified faces which fail to achieve a detection probability threshold. It will be appreciated that other detection engines may be provided which perform other than image detection such as audio detection.</p><p id="p0057" num="0057">At step S4, the detection engine 80 returns the location of the faces within the content data to the metadata server 70. It will be appreciated that different detection engines 80 may be provided for detecting different classes of features or a single detection engine 80 may be provided which can detect each different class of features.</p><p id="p0058" num="0058">The metadata server 70 then uses this location information to extract the faces from the content data.<!-- EPO <DP n="10"> --></p><p id="p0059" num="0059">At step S5, the faces are provided to an identification engine 90.</p><p id="p0060" num="0060">The identification engine 90 then, at step S6, sends a search request to an image search engine 100 to identify possible matching images.</p><p id="p0061" num="0061">At step S7, the image search engine 100 then returns information associated with matching images to the identification engine 90.</p><p id="p0062" num="0062">At step S8, this information is then passed back to the metadata server 70. Accordingly, the metadata server 70 now has information on the location of detected items within the content data, as well as an indication of what those detected items may be.</p><p id="p0063" num="0063">In order to obtain the biographical and link information, two main approaches are envisaged both of which utilize a catalogue 120 which contains biographical or other information related to identified instances within the content data, together with appropriate further interaction options.</p><p id="p0064" num="0064">In an option designated as "Option 1", the catalogue 120 is maintained by the broadcaster providing the TV middleware server 80.</p><p id="p0065" num="0065">The broadcaster provides information, such as biographical information, relating to instances of features that may occur within their content, together with any appropriate links or other user interaction options. For example, a broadcaster may provide biographical or other information on an actor who features within content that they provide, together with a link to other films that the broadcaster has within their portfolio which also features that actor.</p><p id="p0066" num="0066">It will be appreciated that although the broadcaster may provide this information or metadata within the catalogue 120, there is no requirement on the broadcaster to link this metadata to every instance of that actor appearing within any of its content (which is clearly a resource-intensive process), but instead this information is available to the metadata server 70 and may be utilized whenever that actor is identified within the content data. Likewise, it will be appreciated that the broadcaster may maintain a list of products for which they may receive payment for providing additional information and the opportunity for those products to be purchased, this may also be maintained within the catalogue 120. Similarly, the broadcaster may maintain a list of identifiable<!-- EPO <DP n="11"> --> geographical locations and again information may be provided to the user when those locations are recognized following a location request by a user.</p><p id="p0067" num="0067">In an option designated as "Option 2", the catalogue 120 is instead maintained by the provider of the metadata server 70 and so a direct request can be made by the metadata server 70 to the catalogue 120.</p><p id="p0068" num="0068">This information and interaction options are collated by the metadata server 70 and provided, at step S9, to the set top box 10 for display on the display 20. The set top box 10 then indicates to the user on the display 20 those recognised instances of the class of features. It will be appreciated that any of a variety of different techniques may be used to distinguish the recognised features such as highlighting, outlining and surrounding those features on the display 20. The user may then indicate whether they require any additional information to be displayed and may utilise any interaction options.</p><p id="p0069" num="0069">It will be appreciated that in order to reduce processing requirements, the metadata generator 70 may initially only indicate the recognised instances of features to the user and defer the generation and provision of further metadata until the user indicates that they desire that information.</p><p id="p0070" num="0070">With either option, if no information is returned by the catalogue 120, then even if an item is recognized within the content data, then this item may not be highlighted to the user. It will be appreciated that this helps to reduce the number of items that may be highlighted to the user to a manageable level.</p><p id="p0071" num="0071">In a further embodiment, either no catalogue 120 is provided or recognized items which are not within the catalogue 120 are still identified to the user and information and links are provided based on search results received using a search engine accessing the Internet 60. The catalogue 120 may then be updated, particularly if the user indicates that the information provided is relevant. This again increases the flexibility by enabling additional information and links to be provided automatically without needing any metadata to be pre-generated.</p><p id="p0072" num="0072">In another embodiment, the user may identify a particular item within the content data and the identification engine 90 is used to identify the item. Again, should no information exists within the catalogue 120 then a search engine may perform an Internet search to provide any information and associated links that can be displayed to<!-- EPO <DP n="12"> --> the user. Again, the catalogue 120 may then be updated, particularly if the user indicates that the information provided is relevant.</p><p id="p0073" num="0073">In another embodiment, the functions of the metadata server 70, the detection engine 80 and the identification engine 90 may are provided by the set top box 10 or another associated item of user apparatus.</p><p id="p0074" num="0074">Accordingly, it can be seen that embodiments enable real time actor identification from video. In particular, when a user is watching television there is often conversation about who is that actor, what have else have they been in and the like. Embodiments provide a real time solution with an application that, when activated, pauses the television screen, highlights the faces of the actors on the screen and identifies them. A navigational model allows the user to select an identified actor on the screen and get more information. This information could include biography information, current news and a list a list of other media that they have been in. This could include movies and other video content that is also available on the television service, content published on the public internet and other media such as books, compact discs, digital versatile discs available for retail sale. The user can then use this list to mark content for future purchase, show an interest to a recommendation engine, navigate directly to the content or purchase the content for online or retail delivery.</p><p id="p0075" num="0075">In addition, embodiments can also provide:
<ul><li>Real time/Near real time identification of the location, landmarks and the like linking onwards to information about the location. Navigate may be provided to other content set in the same location or to retail purchases of pictures or holidays;</li><li>Real time/Near real time identification and highlighting of branded fashion items such as handbags, dresses and the like. This could be linked to product placement metadata;</li><li>Real time/Near Real time identification of cars (or other retail items) and linkage through to advertising micro sites (set of interactive or linear advertising material about the product).</li></ul></p><p id="p0076" num="0076">Embodiments provide an approach which:
<ol><li>1) Highlights the actors on the screen;</li><li>2) Identifies the names of specific actors in real time;</li><li>3) Presents an onward navigational model relating to the actors for purchasing or watching further content.</li></ol><!-- EPO <DP n="13"> --></p><p id="p0077" num="0077">Embodiments pause the TV screen and identify the current video frame that is being shown. To then access the video frame and submit it for processing:
<ol><li>1) The video frame the end subscriber is watching will be identified and made ready for processing;</li><li>2) Software would first identify where the actors are on the screen. This could use face detection software;</li><li>3) The identified actors would then be submitted to software that could identify them. This would be achieved using image recognition software, and if necessary metadata associated with the content to narrow down the search.</li><li>4) The metadata on the identified actors would then be assembled;</li><li>5) The area on the screen to be highlighted would be established;</li><li>6) The local software on the set top box would then highlight the designated area on the screen and put up the actors names;</li><li>7) The navigational system for choosing between the actors for further information, navigation and purchase opportunities would be put into place;</li><li>8) The user gets the opportunity to navigate for more information or restart the video. On restart the highlights and actors information is removed from the screen.</li></ol></p><p id="p0078" num="0078">Embodiments provide an application built for digital TV middleware such as Microsoft Mediaroom (trade mark) or MiViewTV (trade mark). The video frame identification software, image recognition software and meta data aggregation software would be built agnostic to the individual TV solution. The front end application visualization of the concept would be specific to the TV system targeted.</p><p id="p0079" num="0079">Embodiments allow users to identify actors from their on-screen image and then purchase other contents with those actors in. This could generate revenue through onward sales of content, a monthly or one time usage fee or be provided free of charge in order to reduce churn in the service</p><p id="p0080" num="0080">A person of skill in the art would readily recognise that steps of various above-described methods can be performed by programmed computers. Herein, some embodiments are also intended to cover program storage devices, e.g., digital data storage media, which are machine or computer readable and encode machine-executable or computer-executable programs of instructions, wherein said instructions perform some or all of the steps of said above-described methods. The program storage devices may be, e.g., digital memories, magnetic storage media such as a magnetic disks and magnetic tapes,<!-- EPO <DP n="14"> --> hard drives, or optically readable digital data storage media. The embodiments are also intended to cover computers programmed to perform said steps of the above-described methods.</p><p id="p0081" num="0081">The functions of the various elements shown in the Figures, including any functional blocks labelled as "processors" or "logic", may be provided through the use of dedicated hardware as well as hardware capable of executing software in association with appropriate software. When provided by a processor, the functions may be provided by a single dedicated processor, by a single shared processor, or by a plurality of individual processors, some of which may be shared. Moreover, explicit use of the term "processor" or "controller" or "logic" should not be construed to refer exclusively to hardware capable of executing software, and may implicitly include, without limitation, digital signal processor (DSP) hardware, network processor, application specific integrated circuit (ASIC), field programmable gate array (FPGA), read only memory (ROM) for storing software, random access memory (RAM), and non volatile storage. Other hardware, conventional and/or custom, may also be included. Similarly, any switches shown in the Figures are conceptual only. Their function may be carried out through the operation of program logic, through dedicated logic, through the interaction of program control and dedicated logic, or even manually, the particular technique being selectable by the implementer as more specifically understood from the context.</p><p id="p0082" num="0082">It should be appreciated by those skilled in the art that any block diagrams herein represent conceptual views of illustrative circuitry embodying the principles of the invention. Similarly, it will be appreciated that any flow charts, flow diagrams, state transition diagrams, pseudo code, and the like represent various processes which may be substantially represented in computer readable medium and so executed by a computer or processor, whether or not such computer or processor is explicitly shown.</p><p id="p0083" num="0083">The description and drawings merely illustrate the principles of the invention. It will thus be appreciated that those skilled in the art will be able to devise various arrangements that, although not explicitly described or shown herein, embody the principles of the invention and are included within its spirit and scope. Furthermore, all examples recited herein are principally intended expressly to be only for pedagogical purposes to aid the reader in understanding the principles of the invention and the concepts contributed by the inventor(s) to furthering the art, and are to be construed as being without limitation to such specifically recited examples and conditions.<!-- EPO <DP n="15"> --></p><p id="p0084" num="0084">Moreover, all statements herein reciting principles, aspects, and embodiments of the invention, as well as specific examples thereof, are intended to encompass equivalents thereof.</p></description><claims mxw-id="PCLM56976483" lang="EN" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-en-0001" num="0001"><claim-text>A method of providing interaction options to a user consuming content data, said method comprising the steps of:
<claim-text>receiving an indication that said user requested interaction options for a class of features within said content data being consumed by said user;</claim-text>
<claim-text>recognising instances of said class of features within said content data being consumed by said user; and</claim-text>
<claim-text>identifying said instances to said user together with at least one interaction option associated with said instances.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The method of claim 1, wherein said content data comprises a stream of content data and said method comprises the step of pausing said stream of content data.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The method of claim 1 or 2, wherein said step of recognising comprises recognising instances of said class of features within at least a paused frame of said stream of content data.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The method of any preceding claim, wherein said class of features comprises items recorded by said content data.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The method of any preceding claim, wherein said step of recognising comprises recognising using at least one of image and audio recognition on said content data.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The method of any preceding claim, wherein said step of recognising comprises referencing a relevance associated with items recognised and dismissing those items recognised having a relevance below a relevance threshold.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The method of any preceding claim, wherein said step of recognising comprises identifying whether interaction options are associated with items recognised and dismissing those items recognised having no interaction options.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The method of any preceding claim, wherein said step of recognising comprises providing information relating said content data to a server which performs said recognition and provides an indication of said instances together with an indication of said at least one interaction option associated with said instances.<!-- EPO <DP n="17"> --></claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>The method of claim 8, wherein said information relating said content data comprises at least one of said content data and an identifier identifying said content data.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>The method of any preceding claim, wherein said step of indicating comprises distinguishing said instances for said user.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The method of any preceding claim, wherein said step of distinguishing comprises at least one of highlighting, outlining and surrounding said instances within said content data.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>The method of any preceding claim, wherein said step of indicating interaction options comprises displaying information associated with each of said instances.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>The method of any preceding claim, wherein said step of indicating interaction options comprises displaying at least one link to additional interaction options.</claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>A content delivery apparatus operable to provide interaction options to a user consuming content data, said apparatus comprising:
<claim-text>logic operable to receive an indication that said user requested interaction options for a class of features within said content data being consumed by said user;</claim-text>
<claim-text>recognition logic operable to recognise instances of said class of features within said content data being consumed by said user; and</claim-text>
<claim-text>identification logic operable to identify said instances to said user together with at least one interaction option associated with said instances.</claim-text></claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>A computer program product operable, when executed on a computer, to perform the method steps of any one of claims 1 to 13.</claim-text></claim></claims><drawings mxw-id="PDW16667263" load-source="patent-office"><!-- EPO <DP n="18"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="150" he="204" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="19"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="147" he="233" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="161" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="155" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
