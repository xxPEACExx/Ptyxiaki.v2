<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2680097-A1" country="EP" doc-number="2680097" kind="A1" date="20140101" family-id="48741028" file-reference-id="318319" date-produced="20180823" status="corrected" lang="DE"><bibliographic-data><publication-reference fvid="146549974" ucid="EP-2680097-A1"><document-id><country>EP</country><doc-number>2680097</doc-number><kind>A1</kind><date>20140101</date><lang>DE</lang></document-id></publication-reference><application-reference ucid="EP-13401056-A" is-representative="YES"><document-id mxw-id="PAPP154823897" load-source="docdb" format="epo"><country>EP</country><doc-number>13401056</doc-number><kind>A</kind><date>20130604</date><lang>DE</lang></document-id><document-id mxw-id="PAPP216528180" load-source="docdb" format="original"><country>EP</country><doc-number>13401056.0</doc-number><date>20130604</date><lang>DE</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140454220" ucid="DE-102012105608-A" load-source="docdb"><document-id format="epo"><country>DE</country><doc-number>102012105608</doc-number><kind>A</kind><date>20120627</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988127971" load-source="docdb">G05D   1/00        20060101AFI20131018BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1988110319" load-source="docdb" scheme="CPC">G05D   1/0016      20130101 FI20131014BHEP        </classification-cpc><classification-cpc mxw-id="PCL2073963114" load-source="docdb" scheme="CPC">G05D2201/0203      20130101 LA20140506BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132182406" lang="DE" load-source="patent-office">Selbstfahrendes Reinigungsgerät und Verfahren zur Bedienung eines selbstfahrenden Reinigungsgeräts</invention-title><invention-title mxw-id="PT132182407" lang="EN" load-source="patent-office">Self-propelled cleaning device and method for operating the same</invention-title><invention-title mxw-id="PT132182408" lang="FR" load-source="patent-office">Robot de nettoyage et procédé de commande associé</invention-title><citations><patent-citations><patcit mxw-id="PCIT417584266" load-source="docdb" ucid="DE-102004060853-A1"><document-id format="epo"><country>DE</country><doc-number>102004060853</doc-number><kind>A1</kind><date>20051208</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT417583834" load-source="docdb" ucid="DE-102011050357-A1"><document-id format="epo"><country>DE</country><doc-number>102011050357</doc-number><kind>A1</kind><date>20120216</date></document-id><sources><source name="APP" created-by-npl="N"/><source name="SEA" category="XDAYI" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT417577148" load-source="docdb" ucid="US-20010020837-A1"><document-id format="epo"><country>US</country><doc-number>20010020837</doc-number><kind>A1</kind><date>20010913</date></document-id><sources><source name="SEA" category="YA" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT417577017" load-source="docdb" ucid="US-20020181773-A1"><document-id format="epo"><country>US</country><doc-number>20020181773</doc-number><kind>A1</kind><date>20021205</date></document-id><sources><source name="SEA" category="Y" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT417580567" load-source="docdb" ucid="US-20100066676-A1"><document-id format="epo"><country>US</country><doc-number>20100066676</doc-number><kind>A1</kind><date>20100318</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT417580252" load-source="docdb" ucid="US-20110288684-A1"><document-id format="epo"><country>US</country><doc-number>20110288684</doc-number><kind>A1</kind><date>20111124</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>TAKUYA SHIRAISHI ET AL: "Operation improvement of indoor robot by gesture recognition", MODELING, SIMULATION AND APPLIED OPTIMIZATION (ICMSAO), 2011 4TH INTERNATIONAL CONFERENCE ON, IEEE, 19 April 2011 (2011-04-19), pages 1 - 4, XP031876780, ISBN: 978-1-4577-0003-3, DOI: 10.1109/ICMSAO.2011.5775539</text><sources><source mxw-id="PNPL64905043" load-source="docdb" name="SEA" category="A"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR918147239" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>MIELE &amp; CIE</last-name><address><country>DE</country></address></addressbook></applicant><applicant mxw-id="PPAR918139588" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>MIELE &amp; CIE. KG</last-name></addressbook></applicant><applicant mxw-id="PPAR918989619" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Miele &amp; Cie. KG</last-name><iid>100178438</iid><address><street>Carl-Miele-Straße 29</street><city>33332 Gütersloh</city><country>DE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918157675" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>BERTRAM ANDRE</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918143812" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>BERTRAM, ANDRE</last-name></addressbook></inventor><inventor mxw-id="PPAR918980762" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>BERTRAM, ANDRE</last-name><address><street>Berkensiek 24</street><city>33739 Bielefeld</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918155681" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>BUHL DAVID</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918134776" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>Buhl, David</last-name></addressbook></inventor><inventor mxw-id="PPAR918985555" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>Buhl, David</last-name><address><street>Voltmannstr. 236 a</street><city>33613 Bielefeld</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918152929" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>DOERING SEBASTIAN</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918164958" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>DOERING, SEBASTIAN</last-name></addressbook></inventor><inventor mxw-id="PPAR918991233" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>Döring, Sebastian</last-name><address><street>Roonstraße 42</street><city>33615 Bielefeld</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918150329" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>ENNEN GUENTHER</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918142301" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>ENNEN, GUENTHER</last-name></addressbook></inventor><inventor mxw-id="PPAR918981208" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>ENNEN, Günther</last-name><address><street>Ringstr. 59</street><city>32130 Enger</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918172657" load-source="docdb" sequence="5" format="epo"><addressbook><last-name>KARA SEYFETTIN</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918153997" load-source="docdb" sequence="5" format="intermediate"><addressbook><last-name>KARA, SEYFETTIN</last-name></addressbook></inventor><inventor mxw-id="PPAR918981415" load-source="patent-office" sequence="5" format="original"><addressbook><last-name>KARA, SEYFETTIN</last-name><address><street>Wallenbrücker Straße 10</street><city>32139 Spenge</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918133637" load-source="docdb" sequence="6" format="epo"><addressbook><last-name>ROTH MARTIN</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918132168" load-source="docdb" sequence="6" format="intermediate"><addressbook><last-name>ROTH, MARTIN</last-name></addressbook></inventor><inventor mxw-id="PPAR918988245" load-source="patent-office" sequence="6" format="original"><addressbook><last-name>ROTH, MARTIN</last-name><address><street>Detmolder Str. 17</street><city>33604 Bielefeld</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918164219" load-source="docdb" sequence="7" format="epo"><addressbook><last-name>WOLF CORNELIUS</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918151145" load-source="docdb" sequence="7" format="intermediate"><addressbook><last-name>WOLF, CORNELIUS</last-name></addressbook></inventor><inventor mxw-id="PPAR918994287" load-source="patent-office" sequence="7" format="original"><addressbook><last-name>WOLF, CORNELIUS</last-name><address><street>Am Waldschlößchen 58</street><city>33739 Bielefeld</city><country>DE</country></address></addressbook></inventor></inventors></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS548904487" load-source="docdb">AL</country><country mxw-id="DS548888958" load-source="docdb">AT</country><country mxw-id="DS548808628" load-source="docdb">BE</country><country mxw-id="DS548896910" load-source="docdb">BG</country><country mxw-id="DS548819011" load-source="docdb">CH</country><country mxw-id="DS548808629" load-source="docdb">CY</country><country mxw-id="DS548888959" load-source="docdb">CZ</country><country mxw-id="DS548904489" load-source="docdb">DE</country><country mxw-id="DS548808630" load-source="docdb">DK</country><country mxw-id="DS548808631" load-source="docdb">EE</country><country mxw-id="DS548899577" load-source="docdb">ES</country><country mxw-id="DS548896911" load-source="docdb">FI</country><country mxw-id="DS548819012" load-source="docdb">FR</country><country mxw-id="DS548904494" load-source="docdb">GB</country><country mxw-id="DS548808632" load-source="docdb">GR</country><country mxw-id="DS548904495" load-source="docdb">HR</country><country mxw-id="DS548888960" load-source="docdb">HU</country><country mxw-id="DS548819013" load-source="docdb">IE</country><country mxw-id="DS548808633" load-source="docdb">IS</country><country mxw-id="DS548896912" load-source="docdb">IT</country><country mxw-id="DS548808634" load-source="docdb">LI</country><country mxw-id="DS548896913" load-source="docdb">LT</country><country mxw-id="DS548891833" load-source="docdb">LU</country><country mxw-id="DS548896926" load-source="docdb">LV</country><country mxw-id="DS548896927" load-source="docdb">MC</country><country mxw-id="DS548891834" load-source="docdb">MK</country><country mxw-id="DS548891835" load-source="docdb">MT</country><country mxw-id="DS548899578" load-source="docdb">NL</country><country mxw-id="DS548852561" load-source="docdb">NO</country><country mxw-id="DS548899579" load-source="docdb">PL</country><country mxw-id="DS548891836" load-source="docdb">PT</country><country mxw-id="DS548819014" load-source="docdb">RO</country><country mxw-id="DS548891837" load-source="docdb">RS</country><country mxw-id="DS548899580" load-source="docdb">SE</country><country mxw-id="DS548888961" load-source="docdb">SI</country><country mxw-id="DS548852602" load-source="docdb">SK</country><country mxw-id="DS548852603" load-source="docdb">SM</country><country mxw-id="DS548896928" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA128670642" lang="DE" load-source="patent-office"><p id="pa01" num="0001">Die Erfindung betrifft ein selbstfahrendes Reinigungsgerät zur automatisierten Reinigung von Flächen mit mindestens einem optischen Sensor zur Erfassung einer Umgebung des Reinigungsgeräts. Das selbstfahrendes Reinigungsgerät zeichnet sich dadurch aus, dass es eine Steuervorrichtung aufweist, die dazu eingerichtet ist, Signale von dem optischen Sensor auszuwerten, Gesten eines Benutzers zu erkennen und das Reinigungsgerät anhand von erkannten Gesten zu steuern.<br/>
Die Erfindung betrifft weiter ein Verfahren zur Bedienung eines selbstfahrenden Reinigungsgeräts.
<img id="iaf01" file="imgaf001.tif" wi="109" he="91" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA128500015" lang="DE" source="EPO" load-source="docdb"><p>Die Erfindung betrifft ein selbstfahrendes Reinigungsgerät zur automatisierten Reinigung von Flächen mit mindestens einem optischen Sensor zur Erfassung einer Umgebung des Reinigungsgeräts. Das selbstfahrendes Reinigungsgerät zeichnet sich dadurch aus, dass es eine Steuervorrichtung aufweist, die dazu eingerichtet ist, Signale von dem optischen Sensor auszuwerten, Gesten eines Benutzers zu erkennen und das Reinigungsgerät anhand von erkannten Gesten zu steuern. 
Die Erfindung betrifft weiter ein Verfahren zur Bedienung eines selbstfahrenden Reinigungsgeräts.</p></abstract><abstract mxw-id="PA133635135" lang="EN" source="transcript" load-source="docdb"><p>The self-propelled cleaning device has an optical sensor (6) for detecting an environment of the cleaning device, where a control device (7) is arranged to analyze signals from the optical sensor for detecting gestures of a user (9) and for controlling the cleaning device by using recognized gestures. The optical sensor is arranged on an upper side of the cleaning device and is designed as an imaging camera with a wide field of view. An optical projection device projects a predetermined pattern of light in the vicinity of the cleaning device. An independent claim is included for a method for operating a self-propelled cleaning device.</p></abstract><description mxw-id="PDES63956072" lang="DE" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">Die Erfindung betrifft ein selbstfahrendes Reinigungsgerät, insbesondere einen Saugroboter, zur automatisierten Reinigung von Flächen mit mindestens einem optischen Sensor zur Erfassung einer Umgebung des Reinigungsgeräts. Die Erfindung betrifft weiterhin ein Verfahren zur Bedienung eines selbstfahrenden Reinigungsgeräts.</p><p id="p0002" num="0002">Derartige selbstfahrende Reinigungsgeräte dienen der automatisierten Reinigung von Flächen, ohne dass sie von einem Benutzer geschoben oder geführt werden müssen. Es ist bekannt, solche Reinigungsgeräte als Staubsauger auszubilden, die dann üblicherweise als Saugroboter bezeichnet werden. Weiter sind selbstfahrende Reinigungsgeräte z.B. zum Wischen von glatten Bodenbelägen bekannt.</p><p id="p0003" num="0003">Üblicherweise weist ein derartiges Reinigungsgerät einen oder mehrere Sensoren auf, um die Bewegung des Reinigungsgeräts über die zu reinigende Fläche zu kontrollieren, beispielsweise um einen Zusammenstoß mit Hindernissen zu vermeiden. Solche Sensoren können beispielsweise Tast- oder Ultraschallsensoren oder auch optische Sensoren sein. Nur in einem Nahbereich arbeitende Sensoren dienen meist der Vermeidung von Zusammenstößen mit Hindernissen, wohingegen Sensoren mit einer größeren Erfassungsreichweite auch zur Planung einer koordinierten Bewegung des Geräts über die Fläche eingesetzt werden, um beispielsweise sicherzustellen, dass die gesamte Fläche gereinigt wird.</p><p id="p0004" num="0004">Die Druckschrift <patcit id="pcit0001" dnum="DE102004060853A1"><text>DE 10 2004 060 853 A1</text></patcit> beschreibt beispielsweise einen Saugroboter, der zwei bildaufnehmende Kameras als optische Sensoren zur Steuerung der automatischen Bewegung über die zu reinigende Fläche aufweist. Eine Bedienung des Saugroboters erfolgt über am Gerät vorgesehene Tasten oder über eine Funk mit dem Saugroboter in Verbindung stehenden Fernbedienung. Aufgrund der eigenständigen Bewegung des Saugroboters im Raum ist eine Bedienung über am Gerät angeordnete Tasten im Betrieb unter Umständen schwierig. Bei einer Fernbedienung besteht das Problem, dass diese schnell verlegt werden kann oder im entscheidenden Moment nicht zur Hand ist. Zudem ist eine Eingabe von komplexen Eingaben, beispielsweise zur Definition von zu reinigenden Flächen, über eine Tasteneingabe mühsam.</p><p id="p0005" num="0005">Aus der <patcit id="pcit0002" dnum="US20110288684A1"><text>US 2011/0288684 A1</text></patcit> ist ein Roboter bekannt, der mehrere um das Gerät verteilte Sensoren aufweist, die in der Lage sind Gesten einer Person zu erfassen und auf diese im<!-- EPO <DP n="2"> --> Rahmen von Fahrbefehlen zu reagieren. Nachteil an dieser Lösung ist, dass der Benutzer von einem der Sensoren erfasst werden muss, d.h. dass sich der Benutzer zu dem Roboter in Position bringen muss.</p><p id="p0006" num="0006">Die <patcit id="pcit0003" dnum="DE102011050357A1"><text>DE 10 2011 050 357 A1</text></patcit> offenbart außerdem ein selbstständig verfahrbares Gerät zur Bodenreinigung, welches mittels einer Kamera Bewegungsmuster eines Benutzers erfasst, durch Auswertesoftware analysiert und in Form von Befehlen an das Gerät weiterleitet. Auch in der hier beschriebenen Lösung liegen die oben genannten Nachteile vor.</p><p id="p0007" num="0007">Der Erfindung stellt sich somit das Problem, ein selbstfahrendes Reinigungsgerät zu schaffen, das auf einfache und intuitive Weise jederzeit bequem bedient werden kann. Es ist eine weitere Aufgabe der vorliegenden Erfindung, ein Verfahren zur Bedienung eines derartigen Reinigungsgeräts anzugeben.</p><p id="p0008" num="0008">Erfindungsgemäß wird dieses Problem durch ein selbstfahrendes Reinigungsgerät mit den Merkmalen des Patentanspruchs 1 und ein Verfahren zur Bedienung eines selbstfahrenden Reinigungsgeräts mit den Merkmalen des Patentanspruchs 8 gelöst.</p><p id="p0009" num="0009">Erfindungsgemäß ist ein selbstfahrendes Reinigungsgerät der eingangsgenannten Art dazu eingerichtet, Signale von dem optischen Sensor auszuwerten, Gesten eines Benutzers zu erkennen und das Reinigungsgerät anhand von erkannten Gesten zu steuern, wobei der optische Sensor auf einer Oberseite des Reinigungsgeräts angeordnet und als eine bildgebende Kamera mit einem weiten Gesichtsfeld ausgebildet ist. Für eine Kamera mit einem weiten Gesichtsfeld kommt insbesondere die Verwendung eines sog. Fisheye-Objektivs in Frage, aber auch andere omnidirektionale Kameras sind denkbar.</p><p id="p0010" num="0010">Besonders der optische Sensor an der Oberseite des Reinigungsgeräts ermöglicht jederzeit eine Steuerung über Gesten. Der Erfassungsbereich ist besonders groß, wenn der Sensor als bildgebende Kamera mit einem weiten Gesichtsfeld ausgebildet ist. Hierdurch kann auf weitere Sensoren zur Erfassung der Gesten verzichtet werden. Außerdem muss der Benutzer nicht eine bestimmte Position in Bezug auf den Sensor einnehmen, um in dessen Erfassungsbereich zu sein. Das Reinigungsgerät kann so von allen Seiten Befehle annehmen unabhängig von der Position des Benutzers in Bezug auf das Reinigungsgerät.</p><p id="p0011" num="0011">Als besonders vorteilhaft hat sich eine bildgebende Kamera als optischen Sensor erwiesen, mit einem weiten Gesichtsfeld von über 160 Grad, vorzugsweise über 180 Grad. Diese weiten Gesichtsfelder sind vor allem durch extreme Weitwinkelobjektive, wie beispielsweise Fisheye-Objektive, an der Kamera erreichbar. Mit einem Gesichtsfeld, also einem Öffnungswinkel von 185° ergibt sich beispielsweise relativ zum Horizont ein vertikaler Öffnungswinkel von -2,5 Grad bis 90 Grad. Durch die großen Sichtbereiche dieser Systeme<!-- EPO <DP n="3"> --> kann die Zahl der notwendigen optischen Sensoren stark, vorzugsweise auf einen Sensor, reduziert werden.</p><p id="p0012" num="0012">Vorteilhafterweise ist der optische Sensor fest auf der Oberseite des Reinigungsgerätes angeordnet. Dies erleichtert sowohl die Gesten- als auch die Positions- und Hinderniserkennung, da Bewegungen des Sensors selbst nicht aufwändig aus den erfassten Informationen herausgerechnet werden müssen. Die Bewegung des Sensors setzt hier vielmehr auch immer eine Bewegung des Reinigungsgeräts voraus. Außerdem ist eine feste Anordnung des optischen Sensors, also eine feste Lagerung in Bezug auf ein Gehäuse des Reinigungsgerätes besonders einfach realisierbar.</p><p id="p0013" num="0013">In entsprechender Weise weist ein erfindungsgemäßes Verfahren zur Bedienung eines selbstfahrenden Reinigungsgerät, insbesondere eines Saugroboters, wobei mindestens eine Kamera als optischer Sensor zur Aufnahme von Bildern einer Umgebung des Reinigungsgeräts vorgesehen ist, und wobei der Schritt des Auswertens eine Extraktion von Merkmalen wie Kanten oder Mustern aus den Bildern umfasst, die folgenden Schritten auf: Es werden Signale mindestens eines am Reinigungsgerät angeordneten optischen Sensors zur Erkennung von Gesten eines Benutzers ausgewertet und erkannte Gesten in Anweisungen zur Steuerung des Reinigungsgeräts umgesetzt. Das Reinigungsgerät wird dann gemäß den Anweisungen gesteuert.</p><p id="p0014" num="0014">Eine Steuerung über Gesten ist intuitiv und kann jederzeit ohne notwendige zusätzliche Geräte wie Fernbedienungen eingesetzt werden. Die Akzeptanz von selbstreinigenden Reinigungsgeräten kann aufgrund der intuitiven einfachen Steuerung auch bei technisch weniger affinen Anwendern gesteigert werden. Die Gestensteuerung ermöglicht zudem eine nahezu barrierefreie Bedienung, die im Gegensatz zu einer Bedienung direkt an einem unter Umständen bewegten Reinigungsgerät auch von körperlich eingeschränkten Benutzern möglich ist.</p><p id="p0015" num="0015">In einer vorteilhaften Ausgestaltung des Reinigungsgeräts ist vorgesehen, die Signale des mindestens einen optischen Sensors zusätzlich zur Positions- und Hinderniserkennung auszuwerten. Auf diese Weise kann ein optischer Sensor, beispielsweise eine oder mehrere Kameras, sowohl zur Kontrolle der automatischen Bewegung über die zu reinigende Fläche als auch zur Bedienung des Reinigungsgeräts eingesetzt werden. Diese Mehrfachnutzung des optischen Sensors führt vorteilhaft zu einer Material- und damit Kostenersparnis.</p><p id="p0016" num="0016">In einer weiteren vorteilhaften Ausgestaltung des Reinigungsgeräts sowie des Verfahrens zur Bedienung ist ein akustischer Sensor vorgesehen, dessen Signale ausgewertet werden, wobei akustische Befehle eines Benutzers erkannt werden. Die akustischen Befehle können<!-- EPO <DP n="4"> --> ergänzend zu der Gestensteuerung zur Bedienung des Reinigungsgeräts herangezogen werden. In einer bevorzugten Ausgestaltung kann vorgesehen sein, über die akustischen Befehle die Gestensteuerung des Reinigungsgeräts zu kontrollieren und insbesondere zu aktivieren und/oder zu deaktivieren. Auch in der zwischenmenschlichen Kommunikation werden akustische und optische Kommunikationskanäle ergänzend benutzt. Ein entsprechend auf akustische und optische Signale reagierendes Bedienverfahren für das Reinigungsgerät ist damit besonders intuitiv.</p><p id="p0017" num="0017">Weitere vorteilhafte Ausgestaltungen und Weiterbildungen der Erfindung ergeben sich aus den Unteransprüchen.</p><p id="p0018" num="0018">Ausführungsbeispiele der Erfindung sind in den nachfolgenden Zeichnungen rein schematisch dargestellt und werden im Folgenden näher beschrieben. Es zeigt:
<dl id="dl0001"><dt>Fig. 1</dt><dd>ein erstes Ausführungsbeispiel eines selbstfahrenden Reinigungsgeräts und</dd><dt>Fig. 2</dt><dd>ein zweites Ausführungsbeispiel eines selbstfahrenden Reinigungsgeräts.</dd></dl></p><p id="p0019" num="0019"><figref idrefs="f0001">Fig. 1</figref> zeigt als Beispiel eines selbstfahrenden Reinigungsgeräts einen Saugroboter 1 in einer schematischen Darstellung. Der Saugroboter 1 weist ein Antriebssystem auf, von dem in der Figur Antriebsräder 2 sowie ein Stützrad 3 dargestellt werden. Es sind beispielsweise zwei Antriebsräder 2, eins auf jeder Seite des Saugroboters 1 vorgesehen, die unabhängig voneinander über hier nicht wiedergegebene Antriebsmotoren angetrieben werden. Das Stützrad 3 ist entweder verschwenkbar oder als eine in alle Richtungen drehbare Kugel ausgebildet. Bei voneinander unabhängiger Ansteuerung der Drehrichtungen und Drehgeschwindigkeiten der Antriebsräder 2 kann der Saugroboter 1 beliebige Bewegungen auf einer zu reinigenden Fläche ausführen.</p><p id="p0020" num="0020">Weiterhin ist ein Saugbereich 4 in der <figref idrefs="f0001">Fig. 1</figref> angedeutet, in dem nicht dargestellte Saugdüsen angeordnet sind, die beispielsweise in bekannter Weise mit einem Filtersystem, z.B. einem Staubsaugerbeutel, mit einem Gebläsemotor zusammen wirken. Unterstützend ist in dem dargestellten Ausführungsbeispiel im Saugbereich 4 eine rotierende Bürste 5 angeordnet. Der Saugbereich 4 und die rotierende Bürste 5 repräsentieren die Reinigungseinrichtungen des Saugroboters 1. Weitere für den Betrieb des Saugroboters 1 vorhandene Elemente, beispielsweise eine wiederaufladbare Batterie zur Stromversorgung, Ladeanschlüsse für die Batterie oder eine Entnahmemöglichkeit für ein Staubsammelorgan, beispielsweise ein Staubsaugerbeutel sind in der <figref idrefs="f0001">Fig. 1</figref> aus Gründen der Übersichtlichkeit nicht wiedergegeben.</p><p id="p0021" num="0021">Der Saugroboter 1 weist eine Steuervorrichtung 7 auf, die mit dem Antriebssystem sowie den Reinigungseinrichtungen zu deren Steuerung verbunden ist. Weiter ist ein optischer Sensor 6<!-- EPO <DP n="5"> --> auf einer Oberseite des Saugroboters 1 angeordnet, der im dargestellten Ausführungsbeispiel als eine bildgebende Kamera mit einem weiten Gesichtsfeld ausgebildet ist. Ein derartiges breites Gesichtsfeld kann z. B. durch die Verwendung eines sog. Fisheye-Objektivs erzielt werden. Zur einfacheren Darstellung wird der optische Sensor 6 im Folgenden als Kamera 6 bezeichnet.</p><p id="p0022" num="0022">Im Betrieb des Saugroboters 1 erfasst die Kamera 6 fortlaufend das Umfeld des Saugroboters 1 und sendet eine Folge aufgenommener Bilder als Signale an die Steuereinheit 7. Diese ist dazu eingerichtet, die Bildfolge der Kamera 6 auszuwerten und aus den Bildern Gesten eines Benutzers 9, der in der <figref idrefs="f0001">Fig. 1</figref> als eine Hand symbolisiert ist, zu erkennen.</p><p id="p0023" num="0023">Erkannte Gesten werden dann von der Steuereinheit 7 in Anweisung zur Steuerung des Saugroboters 1 umgesetzt. Die Umsetzung in Steueranweisungen für den Saugroboter 1 kann dabei anhand von vorgegebenen hinterlegten Tabellen in der Steuereinrichtung 7 erfolgen. Beispielsweise kann ein Zeigen des Benutzers 9 auf eine bestimmte Stelle im Raum als eine Anweisung umgesetzt werden, dass der Roboter diese Stelle im Raum anfährt. Als eine weitere Geste kann beispielsweise ein Hochhalten der geöffneten Hand als eine Anweisung umgesetzt werden, den Reinigungsvorgang zu beschleunigen oder zu verlangsamen, um die Reinigungsqualität zu beeinflussen. Als eine Geste ist dabei nicht nur eine statische Handstellung zu verstehen, sondern auch ein Bewegungsmuster, das von Benutzer 9 ausgeführt wird. So kann z.B. ein Zeigen auf einen bestimmten Bereich durch Abfahren der Grenzen des Bereichs als eine Anweisung umgesetzt werden, vom Saugroboter 1 diesen Bereich zu definieren, um ihn entweder zu reinigen oder von der Reinigung auszunehmen.</p><p id="p0024" num="0024">Zur Erkennung der Gesten anhand der Bilder der Kamera 6 kann die Steuereinrichtung 7 eine Extraktion von ausgezeichneten Merkmalen der übermittelten Bilder, beispielsweise von Kanten oder Mustern, umfassen. Insbesondere in Verbindung mit einer Bewegungsanalyse, d. h. dem Nachverfolgen extrahierter Merkmale in aufeinanderfolgende Bildsequenzen, kann so eine Gestenerkennung auf einfache Weise mit nur einer bildgebenden Kamera durchgeführt werden. Unterstützend kann der Saugroboter 1 eine Projektionseinrichtung aufweisen, die ein Lichtmuster in die Umgebung des Saugroboters 1 projiziert. Ein solches Lichtmuster kann beispielsweise ein Punkt- Strich - oder Gittermuster sein, das z.B. durch einen Laser erzeugt wird. Es kann Infrarot- oder sichtbares Licht eingesetzt werden. Eine Analyse der Abbildung des Gittermusters auf die Umgebung ermöglicht dann eine Extraktion von dreidimensionalen Informationen aus den zweidimensionalen Bildern, die die Kamera 6 bereitstellt.<!-- EPO <DP n="6"> --></p><p id="p0025" num="0025">In alternativen Ausgestaltungen des Saugroboters 1 kann vorgesehen sein, als optischen Sensor 6 mindestens voneinander beabstandete Kameras einzusetzen. Mithilfe von zwei Kameras können stereoskopische Verfahren zur Extraktion dreidimensionaler Informationen eingesetzt werden. Dreidimensionale Informationen erweitern den Erkennungsbereich von Gesten, da beispielsweise ein auf eine Kamera hin gerichteter Fingerzeig in einer zweidimensionalen Darstellung nur schwer identifiziert werden kann. In einer weiteren alternativen Ausgestaltung können zur Ermittlung von dreidimensionalen Informationen Licht-Laufzeitmessungen eingesetzt werden. Derartige Verfahren sind als TOF (Time-Of-Flight)-Verfahren bekannt.</p><p id="p0026" num="0026">Die Kamera 6 kann exklusiv nur im Rahmen der Gestenerkennung eingesetzt werden. Es ist jedoch auch möglich und im Sinne einer Material- und Kosteneinsparung vorteilhaft, die von der Kamera 6 bereitgestellten Bilder auch zu weiteren Zwecken einzusetzen, insbesondere zur Hindernis- und Positionserkennung.</p><p id="p0027" num="0027"><figref idrefs="f0002">Fig. 2</figref> zeigt ein weiteres Ausführungsbeispiel eines Saugroboters 1 als selbstfahrendes Reinigungsgerät. Gleiche Bezugszeichen kennzeichnen in dieser Figur gleiche oder gleichwirkende Elemente wie beim Ausführungsbeispiel der <figref idrefs="f0001">Fig. 1</figref>.</p><p id="p0028" num="0028">Gegenüber dem Ausführungsbeispiel 1 ist der in <figref idrefs="f0002">Fig. 2</figref> dargestellte Saugroboter 1 um ein Mikrofon als akustischen Sensor 8 ergänzt. Der einfacheren Darstellung halber wird der akustische Sensor 8 im Folgenden auch als Mikrofon 8 bezeichnet. Bezüglich der Antriebseinheit und der Reinigungseinrichtungen wird auf die Beschreibungen zum ersten Ausführungsbeispiel verwiesen.</p><p id="p0029" num="0029">Beim Saugroboter der <figref idrefs="f0002">Fig. 2</figref> werden von dem Mikrofon 8 aufgenommene Signale der Steuervorrichtung 7 zugeführt und dort ausgewertet. Die Auswertung umfasst beispielsweise eine Unterdrückung von Hintergrundgeräuschen und eine Erkennung von akustischen Befehlen des Benutzers 9. Derartige Befehle können markante Geräusche wie beispielsweise Händeklatschen oder eine Folge von Händeklatschen sein, aber auch gesprochene Befehle (Spracherkennung).</p><p id="p0030" num="0030">Es kann zum einen vorgesehen sein, erkannte akustische Befehle zur übergeordneten Kontrolle der Gestensteuerung des Reinigungsgerätes einzusetzen. Es kann beispielsweise vorgesehen sein, die Gestensteuerung durch akustische Befehle, z.B. Händeklatschen, zu aktivieren und/oder zu deaktivieren. Auf diese Weise kann verhindert werden, dass der Saugroboter 1 nicht für ihn bestimmte Handbewegungen des Benutzers 9 als Gesten auffasst.<!-- EPO <DP n="7"> --></p><p id="p0031" num="0031">Zum anderen kann die Steuerung über akustische Befehle ergänzend zur Gestensteuerung eingesetzt werden. Beispielsweise kann das Anzeigen eines Bereichs der zu reinigenden Fläche über eine Geste erfolgen, während die Art der dort auszuführenden Tätigkeit des Saugroboters 1 akustisch über Sprache gesteuert wird. Bei gleicher Geste kann dann ein gesprochener Zusatz wie "hier reinigen" oder "hier gründlicher reinigen" oder "diesen Bereich auslassen" dazu dienen, verschiedene Betriebsweisen des Saugroboters 1 festzulegen. Besonders vorteilhaft ist eine Sprachsteuerung wenn normal gesprochene Sätze anstelle von fest vorgegebenen Befehlen von der Steuervorrichtung 7 erkannt und interpretiert werden können.</p><p id="p0032" num="0032">Die Kombination aus akustischer Steuerung und Gestensteuerung kann zudem eingesetzt werden, um die Gestensteuerung lernfähig auszugestalten. In einer derartigen Weiterbildung des Saugroboters 1 ist die Umsetzung erkannter Gesten in Anweisung zur Steuerung des Saugroboters 1 nicht zwingend fest vorgegeben, sondern kann anhand von ergänzenden Sprachbefehlen individuell definiert werden.<!-- EPO <DP n="8"> --></p><heading id="h0001">Bezugszeichenliste</heading><p id="p0033" num="0033"><dl id="dl0002"><dt>1</dt><dd>Saugroboter</dd><dt>2</dt><dd>Antriebsrad</dd><dt>3</dt><dd>Stützrad</dd><dt>4</dt><dd>Saugbereich</dd><dt>5</dt><dd>Bürste</dd><dt>6</dt><dd>Optischer Sensor (Kamera)</dd><dt>7</dt><dd>Steuervorrichtung</dd><dt>8</dt><dd>Akustischer Sensor (Mikrofon)</dd><dt>9</dt><dd>Benutzer</dd></dl></p></description><claims mxw-id="PCLM56976996" lang="DE" load-source="patent-office"><!-- EPO <DP n="9"> --><claim id="c-de-0001" num="0001"><claim-text>Selbstfahrendes Reinigungsgerät zur automatisierten Reinigung von Flächen, aufweisend mindestens einen optischen Sensor (6) zur Erfassung einer Umgebung des Reinigungsgeräts, <b>dadurch gekennzeichnet, dass</b> das Reinigungsgerät eine Steuervorrichtung (7) aufweist, die dazu eingerichtet ist, Signale von dem optischen Sensor (6) auszuwerten, Gesten eines Benutzers (9) zu erkennen und das Reinigungsgerät anhand von erkannten Gesten zu steuern, wobei der optische Sensor (6) auf einer Oberseite des Reinigungsgeräts angeordnet und als eine bildgebende Kamera mit einem weiten Gesichtsfeld ausgebildet ist.</claim-text></claim><claim id="c-de-0002" num="0002"><claim-text>Reinigungsgerät nach Anspruch 1, bei dem die bildgebende Kamera einem Gesichtsfeld von über 160 Grad, vorzugsweise über 180 Grad aufweist</claim-text></claim><claim id="c-de-0003" num="0003"><claim-text>Reinigungsgerät nach Anspruch 1 oder 2, bei dem das Reinigungsgerät eine Vorrichtung zur Positions- und Hinderniserkennung aufweist, die dazu eingerichtet ist, die Signale des mindestens einen optischen Sensors (6) zur Positions- und Hinderniserkennung auszuwerten.</claim-text></claim><claim id="c-de-0004" num="0004"><claim-text>Reinigungsgerät nach Anspruch 3, bei dem das Reinigungsgerät eine optische Projektionseinrichtung zur Projektion eines vorgegebenen Lichtmusters in der Umgebung des Reinigungsgeräts aufweist.</claim-text></claim><claim id="c-de-0005" num="0005"><claim-text>Reinigungsgerät nach Anspruch 3, bei dem das Reinigungsgerät eine Lichtquelle zur Abgabe von Lichtpulsen in die Umgebung des Reinigungsgeräts aufweist, und bei dem die Steuervorrichtung (7) zur Durchführung einer Laufzeitmessung für von der Lichtquelle abgegebenes, reflektiertes und von dem mindestens einen optischen Sensor (6) empfangenes Licht eingerichtet ist.</claim-text></claim><claim id="c-de-0006" num="0006"><claim-text>Reinigungsgerät nach einem der Ansprüche 1 bis 5, weiter aufweisend einen mit der Steuervorrichtung (7) verbundenen akustischen Sensor (8), wobei die Steuervorrichtung (7) dazu eingerichtet ist, Signale von dem akustischen Sensor (8) auszuwerten, akustische Befehle eines Benutzers zu erkennen und das Reinigungsgerät zusätzlich und/oder ergänzend zu den erkannten Gesten anhand erkannter akustischer Befehle zu steuern.</claim-text></claim><claim id="c-de-0007" num="0007"><claim-text>Reinigungsgerät nach einem der Ansprüche 1 bis 6, ausgebildet als Saugroboter (1).</claim-text></claim><claim id="c-de-0008" num="0008"><claim-text>Verfahren zur Bedienung eines selbstfahrenden Reinigungsgeräts, insbesondere eines Saugroboters (1), wobei mindestens eine Kamera als optischer Sensor (6) zur Aufnahme<!-- EPO <DP n="10"> --> von Bildern einer Umgebung des Reinigungsgeräts vorgesehen ist, und wobei der Schritt des Auswertens eine Extraktion von Merkmalen wie Kanten oder Mustern aus den Bildern umfasst, mit den folgenden Schritten:
<claim-text>- Auswerten von Signalen mindestens eines am Reinigungsgerät angeordneten optischen Sensors (6) zur Erkennung von Gesten eines Benutzers (9);</claim-text>
<claim-text>- Umsetzen erkannter Gesten in Anweisungen zur Steuerung des Reinigungsgeräts<br/>
und</claim-text>
<claim-text>- Steuern des Reinigungsgeräts gemäß den Anweisungen.</claim-text></claim-text></claim><claim id="c-de-0009" num="0009"><claim-text>Verfahren nach Anspruch 8, wobei zu extrahierende Merkmalen mittels einer Projektionseinrichtung in die Umgebung des Reinigungsgeräts projiziert werden.</claim-text></claim><claim id="c-de-0010" num="0010"><claim-text>Verfahren nach Anspruch 8, wobei der Schritt des Auswertens eine Licht-Laufzeitmessung umfasst.</claim-text></claim><claim id="c-de-0011" num="0011"><claim-text>Verfahren nach einem der Ansprüche 8 bis 11, wobei mindestens zwei Kameras als optische Sensoren zur Aufnahme von Bildern einer Umgebung des Reinigungsgeräts vorgesehen sind, und wobei der Schritt des Auswertens ein stereoskopisches Verfahren umfasst.</claim-text></claim><claim id="c-de-0012" num="0012"><claim-text>Verfahren nach einem der Ansprüche 8 bis 12, wobei ein akustischer Sensor (8) vorgesehen ist und wobei Signale von dem akustischen Sensor (8) ausgewertet und akustische Befehle eines Benutzers erkannt werden.</claim-text></claim><claim id="c-de-0013" num="0013"><claim-text>Verfahren nach Anspruch 12, bei dem das Reinigungsgerät zusätzlich und/oder ergänzend zu den erkannten Gesten anhand erkannter akustischer Befehle gesteuert wird.</claim-text></claim><claim id="c-de-0014" num="0014"><claim-text>Verfahren nach Anspruch 12 oder 13, bei dem die Steuerung des Reinigungsgerätes über Gesten durch erkannte akustische Befehle kontrolliert wird und insbesondere aktiviert und/oder deaktiviert wird.</claim-text></claim></claims><drawings mxw-id="PDW16667719" load-source="patent-office"><!-- EPO <DP n="11"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="207" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="12"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="207" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="161" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="156" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
