<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2678999-A2" country="EP" doc-number="2678999" kind="A2" date="20140101" family-id="47217946" file-reference-id="247163" date-produced="20180823" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146550498" ucid="EP-2678999-A2"><document-id><country>EP</country><doc-number>2678999</doc-number><kind>A2</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-12788758-A" is-representative="NO"><document-id mxw-id="PAPP154824421" load-source="docdb" format="epo"><country>EP</country><doc-number>12788758</doc-number><kind>A</kind><date>20120227</date><lang>EN</lang></document-id><document-id mxw-id="PAPP189754548" load-source="docdb" format="original"><country>EP</country><doc-number>12788758.6</doc-number><date>20120227</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140448935" ucid="US-201161446566-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201161446566</doc-number><kind>P</kind><date>20110225</date></document-id></priority-claim><priority-claim mxw-id="PPC140450225" ucid="US-2012026817-W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2012026817</doc-number><kind>W</kind><date>20120227</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1999882129" load-source="ipcr">H04N   1/409       20060101ALI20151112BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1999884923" load-source="ipcr">H04N   5/232       20060101ALI20151112BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1999885277" load-source="ipcr">H04N   5/217       20110101AFI20151112BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-2123397402" load-source="docdb" scheme="CPC">G02B   7/36        20130101 LI20141224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-2123407339" load-source="docdb" scheme="CPC">H04N   5/23212     20130101 LI20141224BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988107888" load-source="docdb" scheme="CPC">H04N   5/235       20130101 FI20131205BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132183978" lang="DE" load-source="patent-office">FOKUSFEHLERSCHÄTZUNG IN BILDERN</invention-title><invention-title mxw-id="PT132183979" lang="EN" load-source="patent-office">FOCUS ERROR ESTIMATION IN IMAGES</invention-title><invention-title mxw-id="PT132183980" lang="FR" load-source="patent-office">ESTIMATION D'ERREURS DE FOCALISATION DANS DES IMAGES</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR918140893" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>UNIV TEXAS</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR918170630" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>BOARD OF REGENTS, THE UNIVERSITY OF TEXAS SYSTEM</last-name></addressbook></applicant><applicant mxw-id="PPAR918992043" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Board Of Regents, The University Of Texas System</last-name><iid>101246894</iid><address><street>201 West 7th Street</street><city>Austin, TX 78701</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1006116745" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>GEISLER WILSON</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR1006116746" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>GEISLER, WILSON</last-name></addressbook></inventor><inventor mxw-id="PPAR918982149" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>GEISLER, William</last-name><address><street>9801 Ravenwood Cove</street><city>Austin, Texas 78712</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR918171657" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>BURGE JOHANNES</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR918139541" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>BURGE, Johannes</last-name></addressbook></inventor><inventor mxw-id="PPAR918992892" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>BURGE, Johannes</last-name><address><street>5205 Evans Ave. Apt. C</street><city>Austin, Texas 78751</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR918985004" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Harrison Goddard Foote</last-name><iid>100060979</iid><address><street>Belgrave Hall Belgrave Street</street><city>Leeds LS2 8DD</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2012026817-W"><document-id><country>US</country><doc-number>2012026817</doc-number><kind>W</kind><date>20120227</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2012161829-A2"><document-id><country>WO</country><doc-number>2012161829</doc-number><kind>A2</kind><date>20121129</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS548911368" load-source="docdb">AL</country><country mxw-id="DS548909723" load-source="docdb">AT</country><country mxw-id="DS548911369" load-source="docdb">BE</country><country mxw-id="DS548811726" load-source="docdb">BG</country><country mxw-id="DS548917204" load-source="docdb">CH</country><country mxw-id="DS548858545" load-source="docdb">CY</country><country mxw-id="DS548909724" load-source="docdb">CZ</country><country mxw-id="DS548911574" load-source="docdb">DE</country><country mxw-id="DS548858546" load-source="docdb">DK</country><country mxw-id="DS548858547" load-source="docdb">EE</country><country mxw-id="DS548907522" load-source="docdb">ES</country><country mxw-id="DS548811727" load-source="docdb">FI</country><country mxw-id="DS548811728" load-source="docdb">FR</country><country mxw-id="DS548911575" load-source="docdb">GB</country><country mxw-id="DS548858548" load-source="docdb">GR</country><country mxw-id="DS548911576" load-source="docdb">HR</country><country mxw-id="DS548909725" load-source="docdb">HU</country><country mxw-id="DS548917205" load-source="docdb">IE</country><country mxw-id="DS548911577" load-source="docdb">IS</country><country mxw-id="DS548811729" load-source="docdb">IT</country><country mxw-id="DS548858549" load-source="docdb">LI</country><country mxw-id="DS548894397" load-source="docdb">LT</country><country mxw-id="DS548909786" load-source="docdb">LU</country><country mxw-id="DS548894398" load-source="docdb">LV</country><country mxw-id="DS548894399" load-source="docdb">MC</country><country mxw-id="DS548830837" load-source="docdb">MK</country><country mxw-id="DS548830850" load-source="docdb">MT</country><country mxw-id="DS548911578" load-source="docdb">NL</country><country mxw-id="DS548811730" load-source="docdb">NO</country><country mxw-id="DS548894400" load-source="docdb">PL</country><country mxw-id="DS548917554" load-source="docdb">PT</country><country mxw-id="DS548911579" load-source="docdb">RO</country><country mxw-id="DS548917555" load-source="docdb">RS</country><country mxw-id="DS548894401" load-source="docdb">SE</country><country mxw-id="DS548917556" load-source="docdb">SI</country><country mxw-id="DS548811731" load-source="docdb">SK</country><country mxw-id="DS548894402" load-source="docdb">SM</country><country mxw-id="DS548858550" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA102687344" ref-ucid="WO-2012161829-A2" lang="EN" load-source="patent-office"><p num="0000">Estimating focus error in an image involves a training phase and an application phase. In the training phase, an optical system is represented by a point-spread function. An image sensor array is represented by one or more wavelength sensitivity functions, one or more noise functions, and one or more spatial sampling functions. The point-spread function is applied to image patches for each of multiple defocus levels within a specified range to produce training data. Each of the images for each defocus level (i.e. focus error) is sampled using the wavelength sensitivity and spatial sampling functions. Noise is added using the noise functions. The responses from the sensor array to the training data are used to generate defocus filters for estimating focus error within the specified range. The defocus filters are then applied to the image patches of the training data and joint probability distributions of filter responses to each defocus level are characterized. In the application phase, the filter responses to arbitrary image patches are obtained and combined to derive continuous, signed estimates of the focus error of each arbitrary image patch.</p></abstract><abstract mxw-id="PA102882495" ref-ucid="WO-2012161829-A2" lang="EN" source="national office" load-source="docdb"><p>Estimating focus error in an image involves a training phase and an application phase. In the training phase, an optical system is represented by a point-spread function. An image sensor array is represented by one or more wavelength sensitivity functions, one or more noise functions, and one or more spatial sampling functions. The point-spread function is applied to image patches for each of multiple defocus levels within a specified range to produce training data. Each of the images for each defocus level (i.e. focus error) is sampled using the wavelength sensitivity and spatial sampling functions. Noise is added using the noise functions. The responses from the sensor array to the training data are used to generate defocus filters for estimating focus error within the specified range. The defocus filters are then applied to the image patches of the training data and joint probability distributions of filter responses to each defocus level are characterized. In the application phase, the filter responses to arbitrary image patches are obtained and combined to derive continuous, signed estimates of the focus error of each arbitrary image patch.</p></abstract><abstract mxw-id="PA102687345" ref-ucid="WO-2012161829-A2" lang="FR" load-source="patent-office"><p num="0000">L'invention concerne l'estimation d'erreurs de focalisation dans une image, qui comprend une phase de formation et une phase d'application. Dans la phase de formation, un système optique est représenté par une fonction à points étalés. Un réseau de capteurs d'image est représenté par une ou plusieurs fonctions de sensibilité de longueur d'onde, une ou plusieurs fonctions de bruit et une ou plusieurs fonctions d'échantillonnage spatial. La fonction de points étalés est appliquée à des morceaux d'images pour chacun de multiples niveaux de défocalisation dans une plage spécifiée afin de produire des données de formation. Chacune des images pour chacun des niveaux de défocalisation (comme des erreurs de focalisation) est échantillonnée en utilisant les fonctions de sensibilité de longueur d'onde et d'échantillonnage spatial. Du bruit est ajouté en utilisant les fonctions de bruit. Les réponses du réseau de capteurs aux données de formation sont utilisées pour générer des filtres de défocalisation afin d'estimer les erreurs de focalisation dans la plage spécifiée. Les filtres de défocalisation sont ensuite appliqués aux morceaux d'image des données de formation, et des répartitions de probabilité jointes de réponses de filtres à chaque niveau de défocalisation sont caractérisées. Dans la phase d'application, les réponses de filtres à des morceaux d'images arbitraires sont obtenues et combinées afin de dériver des estimées continues signées de l'erreur de focalisation de chaque morceau d'image arbitraire.</p></abstract><abstract mxw-id="PA102882496" ref-ucid="WO-2012161829-A2" lang="FR" source="national office" load-source="docdb"><p>L'invention concerne l'estimation d'erreurs de focalisation dans une image, qui comprend une phase de formation et une phase d'application. Dans la phase de formation, un système optique est représenté par une fonction à points étalés. Un réseau de capteurs d'image est représenté par une ou plusieurs fonctions de sensibilité de longueur d'onde, une ou plusieurs fonctions de bruit et une ou plusieurs fonctions d'échantillonnage spatial. La fonction de points étalés est appliquée à des morceaux d'images pour chacun de multiples niveaux de défocalisation dans une plage spécifiée afin de produire des données de formation. Chacune des images pour chacun des niveaux de défocalisation (comme des erreurs de focalisation) est échantillonnée en utilisant les fonctions de sensibilité de longueur d'onde et d'échantillonnage spatial. Du bruit est ajouté en utilisant les fonctions de bruit. Les réponses du réseau de capteurs aux données de formation sont utilisées pour générer des filtres de défocalisation afin d'estimer les erreurs de focalisation dans la plage spécifiée. Les filtres de défocalisation sont ensuite appliqués aux morceaux d'image des données de formation, et des répartitions de probabilité jointes de réponses de filtres à chaque niveau de défocalisation sont caractérisées. Dans la phase d'application, les réponses de filtres à des morceaux d'images arbitraires sont obtenues et combinées afin de dériver des estimées continues signées de l'erreur de focalisation de chaque morceau d'image arbitraire.</p></abstract><description mxw-id="PDES52885106" ref-ucid="WO-2012161829-A2" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001"> FOCUS ERROR ESTIMATION IN IMAGES </p><p id="p0002" num="0002">CROSS-REFERENCES TO RELATED APPLICATIONS </p><p id="p0003" num="0003"> [0001] This application is a non-provisional patent application of co-pending U.S. Provisional Application Serial No. 61/446566 filed on February 25, 2011, titled "Defocus Estimation in Individual Natural Images", which is hereby expressly incorporated by reference in its entirety for all purposes. </p><p id="p0004" num="0004">STATEMENT AS TO RIGHTS TO INVENTIONS MADE UNDER FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT </p><p id="p0005" num="0005"> [0002] The U.S. Government may own certain rights in this invention pursuant to the terms of the National Institutes of Health Grant No. 2 R01EY11747. </p><p id="p0006" num="0006">BACKGROUND </p><p id="p0007" num="0007"> [0003] The present invention relates to autofocus optical systems, and more particularly to estimation of error in images received through an optical system. </p><p id="p0008" num="0008">[0004] An autofocus optical system (e.g., digital camera, digital video camera, microscope, micro-fabrication device) uses a sensor, a control system and a motor to focus fully </p><p id="p0009" num="0009">automatically or on a manually selected point or area. There are two primary classes of focusing methods that are used by autofocus optical systems to determine the correct focus. One class is referred to as "active" autofocusing. This class of autofocusing determines the focus by emitting a signal (e.g., infrared light, ultrasonic sound wave). In the case of emitting sound waves, the distance to the subject is calculated by measuring the delay in their reflection. In the case of emitting infrared light, the target distance is computed by triangulating the distance to the subject. While active autofocusing methods function in very low light conditions they are unable to focus on very near objects (e.g., macro-photography) and cannot focus through windows because glass reflects the emitted signals. </p><p id="p0010" num="0010">[0005] The other class of autofocusing is referred to as "passive" or "image-based" </p><p id="p0011" num="0011">autofocusing. The focus is determined by analyzing the image entering the optical system. Hence, passive autofocusing methods generally do not direct any energy, such as ultrasonic 
<!-- EPO <DP n="3"/>-->
 sound or infrared light waves, toward the subject. The two primary methods of passive autofocusing are contrast measurement and phase detection. </p><p id="p0012" num="0012">[0006] Contrast measurement is achieved by measuring the intensity differences of pixels on the image sensor within a small area. The intensity difference between adjacent pixels of the sensor generally increases with decreasing focus error. The optical system can thereby be adjusted until the maximum contrast is detected. However, this method of focusing is slow because it searches rather than obtains an estimate of the necessary changes in focus. </p><p id="p0013" num="0013">Additionally, the method depends on the assumption that the contrast tracks perfectly with focus error. This assumption is not strictly correct. Furthermore, contrast measurement does not provide an estimate of defocus sign. That is, it does not calculate whether the optical system is focused in front or behind the subject. </p><p id="p0014" num="0014">[0007] Phase detection autofocusing is achieved by dividing the incoming light into pairs of images and comparing them. The system couples a beam splitter, a small secondary mirror, and two optical prisms to direct the light to a dedicated autofocusing sensor in the optical system. Two optical prisms capture the light rays coming from the opposite sides of the lens and divert them to the autofocusing sensor, creating a simple rangefmder with a base similar to the aperture diameter. The two images are then analyzed for similar light intensity patterns (peaks and valleys) and the phase difference is calculated in order to determine the magnitude of the focus error and to determine whether the optical system is focused in front of or behind the subject. While the phase detection method is fast and accurate and estimates defocus sign, it is very costly to implement because it requires special beam splitters, mirrors, prisms, and sensors. Furthermore, the extra hardware increases the size and weight of the optical system. </p><p id="p0015" num="0015">Additionally, such a method cannot operate in "live-view" mode (a feature that allows an optical system's display screen, such as a digital camera's display screen, to be used as a viewfmder). [0008] If, however, a technique could be developed to perform focus error estimation without iterative search and without additional optical components, then one could obtain the benefits of contrast and phase detection methods without their disadvantages. </p><p id="p0016" num="0016">SUMMARY </p><p id="p0017" num="0017"> [0009] In one embodiment of the present invention, focus error of an optical system is estimated after receiving image data of an object onto a sensor array of an optical system by 
<!-- EPO <DP n="4"/>-->
 calculating a filter response value for each of one or more defocus filters for the optical system, such that the calculated response values comprise a filter response vector of the received image data, then identifying the probability of each possible defocus level, and from these probabilities estimating the defocus level. In this way, the focus error is estimated without splitting the optical path between the objective lens and the sensor array, and the final step of the determination becomes a relatively simple algebraic calculation on the filter response vector of the received image data. Thus, focus error is determined in one computational step, for a much more efficient process as compared to the contrast technique and without the complicated hardware components of the phase detection technique. [0010] In other aspects of the invention, a method for estimating focus error in an image comprises collecting a set of image patches to obtain a statistical description of images. The method further comprises representing an optical system by a point-spread function for each of multiple defocus levels within a range of defocus levels. Additionally, the method comprises representing a sensor array by at least a wavelength sensitivity function and a spatial sampling function. In addition, the method comprises computing a set of defocused image patches falling on the sensor array with the point- spread function for each of the multiple defocus levels. </p><p id="p0018" num="0018">Furthermore, the method comprises sampling each of the set of defocused image patches on the sensor array using the sensor array's wavelength, spatial-sampling and noise functions. The method further comprises obtaining responses from the sensor array for a given class of sensor. In addition, the method comprises generating a set of defocus filters from the sensor responses from the sensor array for each of the multiple defocus levels. In addition, the method comprises determining a parametric description of the filter response distributions for each of the multiple defocus levels for a set of images patches. Additionally, the method comprises computing, by a processor, the focus error from an arbitrary filter response vector and the parametric descriptions of the filter response distributions. </p><p id="p0019" num="0019">[0011] Other forms of the embodiment of the method described above may be provided in a system and in a computer program product. </p><p id="p0020" num="0020">[0012] The foregoing has outlined rather generally the features and technical advantages of one or more embodiments of the present invention in order that the detailed description of the present invention that follows may be better understood. Additional features and advantages of 
<!-- EPO <DP n="5"/>-->
 the present invention will be described hereinafter which may form the subject of the claims of the present invention. </p><p id="p0021" num="0021">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0022" num="0022">[0013] A better understanding of the present invention can be obtained when the following detailed description is considered in conjunction with the following drawings, in which: </p><p id="p0023" num="0023">[0014] Figure 1 is a schematic diagram of a system configured in accordance with an embodiment of the present invention. </p><p id="p0024" num="0024">[0015] Figure 2 is a flow diagram that illustrates the operation of the Figure 1 system for determining estimated focus error in the system of Figure 1. </p><p id="p0025" num="0025">[0016] Figure 3 is a flow diagram that illustrates focus adjustment in a device that utilizes the focus error determination of the Figure 1 system. </p><p id="p0026" num="0026">[0017] Figure 4 is a block diagram showing details of a computer system configured in accordance with an embodiment of the present invention. [0018] Figure 5 is a flow diagram that illustrates estimating the focus error in images in accordance with an embodiment of the present invention. </p><p id="p0027" num="0027">[0019] Figure 6 is a flow diagram that illustrates generating defocus filters in accordance with an embodiment of the present invention. </p><p id="p0028" num="0028">[0020] Figure 7 shows defocus filters for an illustrative optical system such as illustrated in Figure 1. </p><p id="p0029" num="0029">[0021] Figure 8 shows responses to spectra in a training set of images for two of the Figure 7 filters. </p><p id="p0030" num="0030">[0022] Figure 9 shows Gaussian distributions fitted to the Figure 8 filter responses. [0023] Figure 10 shows defocus estimates for the training set of images. DETAILED DESCRIPTION </p><p id="p0031" num="0031"> [0024] The present invention comprises a method, system, and computer program product for estimating the defocus (i.e., focus error) in images. In one embodiment of the present invention, 
<!-- EPO <DP n="6"/>-->
 an optical system is represented by a wave optics model of the point-spread function and a sensor array is represented by wavelength sensitivity, spatial sampling, and noise functions for each sensor class. A training set of sharp image patches is collected. The point-spread function is computed for each of multiple defocus levels within a specified range, for each sensor class. Furthermore, the point-spread function for each of the defocus levels is applied to each image patch, which is then sampled using the wavelength sensitivity and spatial sampling functions for each type of sensor in the sensor array. Noise is added to the sampled response from each sensor element of each type of sensor in the sensor array. The sensor responses from the sensor array are used to generate a set of optimal defocus filters, via a statistical learning procedure. The optimal defocus filters are then applied to the sensor responses to each image patch in the training set. The joint filter response distributions for each of the multiple defocus levels in the training set of image patches are determined (i.e. the conditional likelihood distributions) and are characterized with a parametric function (e.g. a multi-dimensional Gaussian function). </p><p id="p0032" num="0032">[0025] The preceding steps indicate how the optimal defocus filters are learned. The steps described below indicate how the optimal defocus filters are used to obtain optimal defocus estimates for arbitrary image patches. </p><p id="p0033" num="0033">[0026] To estimate the focus error of an arbitrary patch with an arbitrary focus error, the filter response vector (i.e. the response of each filter) to the arbitrary image patch is calculated. The filter response vector and the parametric descriptions of the training filter response distributions are used to determine the probability of each possible defocus level within a specified range. From this probability distribution, a continuous Bayes' optimal estimate of the focus error is obtained. In this manner, a software technique has been developed to perform focus error estimation without the expensive hardware requirements of the phase detection autofocusing method. It has the additional advantage that it can be applied to "live-view" mode and that it can be implemented in cell-phone cameras, mirrorless-lens-interchangeable cameras, point-and-shoot cameras, and other imaging systems that currently use contrast measurement autofocusing and that are too small or have too low a price-point to implement phase detection autofocusing. </p><p id="p0034" num="0034">[0027] In the following description, numerous specific details are set forth to provide a thorough understanding of the present invention. However, it will be apparent to those skilled in the art that the present invention may be practiced without such specific details. In other instances, well-known circuits have been shown in block diagram form in order not to obscure 
<!-- EPO <DP n="7"/>-->
 the present invention in unnecessary detail. For the most part, details considering timing considerations and the like have been omitted inasmuch as such details are not necessary to obtain a complete understanding of the present invention and are within the skills of persons of ordinary skill in the relevant art. [0028] Figure 1 is a schematic diagram of a system configured in accordance with an embodiment of the present invention. The system 100 receives light from an external object 102 into an optical system 104 that images the object light onto a sensor array 106. The optical system is schematically represented in Figure 1 by an objective lens 108 and a pupil lens 110 that are movable relative to each other to adjust focus of the optical system 104, but it should be understood that any number and arrangement of optical elements that can be adjusted to focus object light onto the sensor array 106 may be used. A variety of components may be utilized for the sensor array 106, which may comprise, for example, photoelectric sensors that generate electrical energy in response to receiving light of a predetermined wavelength, such as long (e.g. red), medium (e.g. green), or short (e.g. blue) wavelength light. In a typical system 100, the sensor array 106 includes multiple sensors for red light and multiple sensors for blue light. Each sensor in the array generates a small electrical current in accordance with the intensity of light it receives. Each sensor is referred to as a pixel of the received sensor image. </p><p id="p0035" num="0035">[0029] The electrical current from each of the sensors of the array 106 is provided to a processor 112 for computation. The processor utilizes focus error data, described further below, that is read from a data store 114, such as a memory. The processor determines the sign and magnitude of the focus error in the received image, in accordance with its processing based on the data in the memory 114, and determines the focus error of the received image. The processor 112 can compensate for the determined focus error by adjusting the optical system 104 to implement the compensating directional sign (e.g., either back focus or front focus) and compensating magnitude such that the received image is substantially in focus at the sensor array 106. That is, based on one set of received image data from the sensor array, the processor 112 can calculate the adjustment to the optical system 104 that will be required to bring the received image into focus in one actuation of the optical system. The optical path through the optical system is a direct optical path, in the sense that the optical path from the object 102 to the sensor array 106 is a path that is not interrupted with intervening structures that disperse the collected light. That is, the optical path contains no beam splitters or the like. Thus, the iterative focus 
<!-- EPO <DP n="8"/>-->
 technique of contrast focus systems is avoided, and the complicated optical path and prisms of phase detection focus systems is not needed. </p><p id="p0036" num="0036">[0030] The system 100 that utilizes the focus error estimation described herein may be embodied in a variety of devices that include a sensor array that receives light through the optical system. For example, the optical system may be placed in a digital camera having a </p><p id="p0037" num="0037">photosensitive sensor array and a lens focus system that can adjust the focus of the optical system in accordance with the estimated focus error. Other devices that may utilize the focus error estimation described herein include a digital microscope having a photosensitive sensor array and a lens focus system that can adjust the focus of the microscope's optical system in accordance with the estimated focus error. Similarly, the exemplary device may comprise a digital telescope having a photosensitive sensor array and a lens focus system that can adjust the focus of the telescope's optical system in accordance with the estimated focus error. Another exemplary device that may utilize the focus error estimation described herein can comprise a digital video camera having a photosensitive sensor array and a lens focus system that receives images on the sensor array comprising frames of video, and can adjust the focus of the video camera's optical system in accordance with the estimated focus error. That is, each frame of video may achieve focus using the techniques described herein. A wide variety of mechanical devices controlled by digital camera systems and analysis systems receiving image input may benefit from using the techniques described herein for focus estimation and device control. For example, micro-fabrication, robotics, and expert systems may benefit from the techniques described herein. Other suitable devices and systems for use with the focus error determining technique described herein will occur to those skilled in the art. </p><p id="p0038" num="0038">[0031] Figure 2 is a flow diagram that illustrates the operations described for determining focus error. In the first operation 202, image data is received at the sensor array through the optical system from the external object. After the image data is generated from each pixel of the sensor array, at the next operation 204, filter response values of the received image data are calculated for each of one or more defocus filters for the optical system, such that the calculated response values comprise a filter response vector of the received image data. In the illustrated embodiment, the set of defocus filters are identified using a statistical learning technique, described further below. The filter response values are computed for one or more sensor types in the array. For example, the sensor array may include photosensitive pixels that are sensitive to, 
<!-- EPO <DP n="9"/>-->
 respectively, light waves that are red, blue, or green. If the sign of the focus error is to be estimated, the filters will process image data from at least two of those three sensor channels, selected arbitrarily. If only the magnitude is to be estimated, the filters will process data from one or more of the sensor channels. [0032] In estimating focus error, the system next calculates the probability of each possible defocus level within a specified range given the observed filter response vectors. This is represented by the flow diagram box numbered 206. The next step in the defocus processing, at box 208, is to obtain an estimate of the focus error of the received image data based on the computed probabilities of each defocus level. Standard techniques are used to obtain Bayes' optimal estimates of focus error from the probability distribution (e.g. minimum mean-squared error (MMSE) or maximum a posteriori (MAP) estimates). The focus error determined in this way can provide both the sign (direction) of the focus error and the magnitude of the focus error. The mathematical details of generating the filter response vectors to the training stimuli will be described further below. [0033] Figure 3 provides a flow diagram that illustrates the operation of the focus error estimation technique as applied to a device. </p><p id="p0039" num="0039">[0034] In the first operation of Figure 3, corresponding to the first box 302, the image receiving process is initiated by a user of the device. For example, if the device is a digital camera, the user may depress a shutter button sufficiently to initiate image processing but not sufficient to initiate image capture. Such "image preview" operations are well-known in conventional digital cameras. Similar image preview operations may be available in digital cameras, video cameras, digital microscopes, or digital telescopes that are constructed in accordance with the present invention. The next operation, at box 304, is to determine the estimated focus error in the received image using the technique described in this document, for example, as described in conjunction with the Figure 2 operation. After the focus error is determined, at the box 306, the focus of the optical system in the device is adjusted in a single operation to accommodate the estimated error and move the components of the optical system so as to bring the external object substantially into proper focus. The user can then proceed to process another image, a "yes" outcome at the decision box 308, or the user may proceed with image capture of the received image, a "no" outcome at the decision box. In the case of a "yes" outcome, the device operation cycles back to the image receiving process at box 302. 
<!-- EPO <DP n="10"/>-->
 [0035] Figure 4 is a block diagram showing details of a computer system configured in accordance with an embodiment of the present invention. Figure 4 illustrates an embodiment of a hardware configuration of a computer system 400 that is representative of a hardware environment for practicing the present invention. In Figure 4, the computer system 400 may include a processor 401 coupled to various other components by a system bus 402. An operating system 403 may run on the processor 401 so as to provide control and coordinate the functions of the various components of Figure 4. Program instructions may be executed by the processor 401 through the operating system 403 to provide an application 404 in accordance with the principles of the present invention that implements the various functions or services to be performed in accordance with the description herein. The application 404 may include, for example, functions and operations for estimating defocus in images as discussed further below. </p><p id="p0040" num="0040">[0036] Referring again to Figure 4, a read-only memory ("ROM") 405 may be coupled to the system bus 402 and may include a basic input/output system ("BIOS") that controls certain basic functions of the computer system 400. A random access memory ("RAM") 406 and a disk adapter 407 may also be coupled to the system bus 402. It should be noted that software components including the operating system 403 and application 404 may be loaded into the RAM 406, which may be the main memory of the computer system 400 for execution. The disk adapter 407 may be an integrated drive electronics ("IDE") adapter or the like that communicates with a storage unit 408, e.g., a memory unit, hard disk drive, or solid state drive. It is noted that the program for estimating defocus in images as discussed further below may reside in the disk unit 408 or in the application 404. </p><p id="p0041" num="0041">[0037] The computer system 400 may further include a communications adapter 409 coupled to the bus 402. A communications adapter 409 may interconnect the bus 402 with an outside network (not shown) through a network interface, thereby allowing the computer system 400 to communicate with other similar devices. Alternatively, the computer system 400 may be embedded within a device such as a camera or digital microscope, each having an optical system that directs light from an object onto a sensor array such that the optical system can be adjusted to proper focus in accordance with the description herein. </p><p id="p0042" num="0042">[0038] Input/output (I/O) devices may also be connected to the computer system 400 via a user interface adapter 410 and a display adapter 411. A keyboard 412, mouse 413, and a speaker 414 may all be interconnected to the bus 402 through the user interface adapter 410. Data may be 
<!-- EPO <DP n="11"/>-->
 input to the computer system 400 through any of these devices. A display monitor 415 may be connected to the system bus 402 by the display adapter 411. In this manner, a user can provide inputs to the computer system 400 through the keyboard 412 or mouse 413, and can receive output from the computer system 400 via the display 415 or speaker 414. [0039] As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method, or computer program product. Accordingly, aspects of the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a "circuit", "module", or "system". Furthermore, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon. </p><p id="p0043" num="0043">[0040] Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer-readable signal medium or a non-transitory computer-readable storage medium. A computer-readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the non-transitory computer-readable storage medium would include the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or flash memory), a data storage media such as a compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer-readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device. </p><p id="p0044" num="0044">[0041] A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage 
<!-- EPO <DP n="12"/>-->
 medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus or device. </p><p id="p0045" num="0045">[0042] Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing. </p><p id="p0046" num="0046">[0043] Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++, or the like, and conventional procedural programming languages, such as the "C" programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). </p><p id="p0047" num="0047">[0044] Aspects of the present invention may be described with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the present invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to product a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the function/acts specified in the flowchart and/or block diagram block or blocks. </p><p id="p0048" num="0048">[0045] These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer 
<!-- EPO <DP n="13"/>-->
 readable medium produce an article of manufacture including instructions which implement the function/act specified in the flowchart and/or block diagram block or blocks. </p><p id="p0049" num="0049">[0046] The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the function/acts specified in the flowchart and/or block diagram block or blocks. </p><p id="p0050" num="0050">[0047] As stated in the Background section, phase detection autofocusing is achieved by dividing the incoming light into pairs of images and comparing them. The system couples a beam splitter, a small secondary mirror, and two optical prisms to direct the light to a dedicated autofocusing sensor in the optical system. Two optical prisms capture the light rays coming from the opposite sides of the lens and divert them to the autofocusing sensor, creating a simple rangefmder with a base identical to the lens diameter. The two images are then analyzed for similar light intensity patterns (peaks and valleys) and the phase difference is calculated in order to find if the object is in front focus or back focus position. While the phase detection method is fast and accurate and estimates defocus sign, it is very costly to implement since it requires special beam splitters, mirrors, prisms, and sensors. Furthermore, the extra hardware increases the size and weight of the optical system. Additionally, such a method cannot operate in video mode or "live-view" mode (a feature that allows an optical system's display screen, such as a digital camera's display screen, to be used as a viewfmder). If, however, a software technique could be developed to perform optimal focus error estimation, then the expensive hardware requirements of the phase detection autofocusing method would no longer be required. </p><p id="p0051" num="0051">Furthermore, the weight of the optical system would be decreased. [0048] Thus, using the focus error estimation described herein, the improved performance that is sometimes associated with relatively larger phase detection systems may be obtained in smaller systems, such as camera systems for mobile telephones, interchangeable lens cameras without a mirror (i.e., not single-lens-reflex cameras), and the like that currently use contrast detection focus systems. 
<!-- EPO <DP n="14"/>-->
 [0049] The principles of the present invention provide a technique for performing focus error estimation without the slow iterative procedure required by contrast measurement and without the expensive hardware requirements of the phase detection autofocusing method that can be applied to "live-view" mode as discussed further below. Figure 5 is a flowchart of a method for estimating the focus error in images obtained through an optical system. Figure 6 is a flowchart of a method for generating optimal defocus filters. </p><p id="p0052" num="0052">[0050] Referring to Figure 5, the first operation 501 indicates that the optical system (e.g., a digital camera or digital microscope) is modeled or represented by a point-spread function. </p><p id="p0053" num="0053">[0051] Prior to discussing the modeling of the optical system, a brief discussion of the concept of defocus is deemed appropriate. The defocus of a target image region is defined as the difference between the lens system's current power and the power required to bring the target region into focus as shown below in Equation (EQ 1): </p><p id="p0054" num="0054">Δϋ = D<sub>focus</sub> - D<sub>target</sub> (EQ 1) </p><p id="p0055" num="0055">That is, the technique described herein estimates the focus error (Δϋ), also referred to as the defocus, in each local region of an image, such as a natural image or computer-generated image (or any other class of image). </p><p id="p0056" num="0056">[0052] Estimating defocus, like many visual estimation tasks, suffers from the "inverse optics" problem: It is impossible to determine with certainty, from the image alone, whether image blur is due to defocus or some property of the scene (e.g., fog). Focus error estimation can suffer from a sign ambiguity. Under certain conditions, a point target that is the same dioptric distance nearer or farther than the focus distance will be imaged identically. However, there are a number of constraints which the technique described herein exploits to make a solution possible. </p><p id="p0057" num="0057">[0053] In the described approach to focus error estimation, the input from a scene can be represented by an idealized image I (x, λ) that gives the radiance at each location x = (x,y) in the plane of the sensor array of the optical system, for each wavelength λ. In general, an optical system degrades the idealized image and can be represented by a point-spread function given by psflx, λ, Δϋ), which describes the spatial distribution of light across the sensor array produced by a point target of wavelength λ and defocus Δϋ. The point- spread function can be expanded to make the factors determining its form more explicit, as follows, in Equation (EQ 2): 
<!-- EPO <DP n="15"/>-->
 psf { , ; a(z), W(z, , AD)) (EQ 2) where a(z) specifies the shape, size and transmittance of the aperture, and W(z, λ, AD) is a wave aberration function, which depends on the position z in the plane of the aperture, the wavelength of light λ, the defocus level AD, and other aberrations introduced by the lens system. A defocus level refers to how many diopters, if any, the image is out of focus. The aperture function determines the effect of diffraction on the optical quality of the lens system. The wave aberration function determines degradations in image quality not attributable to diffraction. A perfect lens system (i.e., a lens system limited only by diffraction and defocus) converts light originating from a point on a target object to a converging spherical wavefront. The wave aberration function describes how the actual converging wavefront differs from a perfect spherical wavefront at each point in the pupil aperture. </p><p id="p0058" num="0058">[0054] In the next operation 502, a sensor array is represented by a wavelength sensitivity function s<sub>c</sub>(X), a spatial sampling function samp<sub>c</sub>(x) for each class of sensor c, where a class of sensor refers to those sensors with the same wavelength sensitivity profile (spectral sensitivity function). For example, one class of sensors may be used to sense long wavelength (e.g. red) light while another class may be used to sense short wavelength (e.g. blue) light. These two sensor classes must have different wavelength sensitivity functions. In one embodiment, two or more sensor classes are used to take advantage of chromatic aberrations, which may be used to determine the sign of the defocus (i.e., the direction of focus error) as discussed further herein. That is, two or more sensor classes are used to take advantage of chromatic aberrations, which may be used to determine if the object is in front focus or back focus, in addition to continuously estimating the magnitude of defocus. In another embodiment, one or more sensor classes are used to take advantage of monochromatic aberrations (e.g., astigmatic and spherical aberrations). Monochromatic aberrations and chromatic aberrations may be used simultaneously to estimate the defocus in images (such as natural images or any other class of images, including images such as computer-generated images). The spatial sampling function samp<sub>c</sub>(x) is used for sampling each portion of the image (referred to herein as the "patch") for the particular class of sensor. </p><p id="p0059" num="0059">[0055] In the next operation 503, optimal defocus filters are determined. In one embodiment, the spatial frequency filters that are most diagnostic of defocus are discovered and subsequently the filter responses are used to obtain continuous focus error estimates as discussed further 
<!-- EPO <DP n="16"/>-->
 herein. A detailed description of the sub-steps of the operation 503 for generating optimal defocus filters are discussed below in conjunction with Figure 6. </p><p id="p0060" num="0060">[0056] Referring to Figure 6, in the first operation 601 , a set of randomly selected well-focused (i.e., sharp) image patches (e.g., portions of images) are collected. The image patches are then defocused with the point-spread function for each defocus level within a range of defocus levels. For example, in a typical digital camera system, the defocus levels may lie within a range of defocus levels such as ranging from approximately -2.25 diopters to approximately 2.25 diopters. The range of defocus levels can be matched to the focusing range of the optical system. The range of defocus levels is arranged in discrete, incremental diopter steps (e.g., in steps of 0.25 diopter steps). The incremental diopter step size should be adjusted as necessary to ensure good continuous estimation performance for the device. More particularly, the range of defocus levels and the spacing of the discrete diopter steps are predetermined according to the available aperture settings and focus range of the system that will be performing the focus error estimate computations. Other ranges of defocus levels and incremental diopter steps will be selected in accordance with system resources and the application (e.g., digital camera, digital microscope, digital telescope, or digital video camera), as will be known to those skilled in the art. </p><p id="p0061" num="0061">[0057] It should be noted that an image patch, comprising the sensor array data for any individual image, is typically limited to an arbitrary pixel patch area in the image. For example, the patch area for which the computations of power spectra, transforms, and the like, are carried out may comprise an image area corresponding to a focus reticle of the optical system as viewed by a user. The size of such patch areas is typically on the order of a 64x64 pixel area or a 128x128 pixel area located in the center of the image. </p><p id="p0062" num="0062">[0058] In the next operation 602, the point-spread function is applied for each defocus level to each image patch. [0059] In the next operation 603, each defocused image patch is sampled by the sensor array represented by the wavelength sensitivity function S<sub>C</sub>(X) and spatial sampling function samp<sub>c</sub>(x) for each class of sensor c. </p><p id="p0063" num="0063">[0060] In addition, a noise function may be included in the computation. The noise function may comprise, for example, a random value for noise to be added as a simulation (proxy) for 
<!-- EPO <DP n="17"/>-->
 noise level in the optical system, such as in noise in the response of the sensors in the optical system. </p><p id="p0064" num="0064">[0061] In the next operation 604, the responses from the sensor array are obtained for a given class of sensor. In the operation 605, optical defocus filters are generated from the responses from the sensor array for each of the defocus levels. A typical number of defocus filters for the optical system is from six to eight. The number of defocus filters will be determined by the system resources that are available and the desired accuracy of operation. As noted above, the defocus filters can be determined by a statistical learning technique. The defocus filters represent the features of the images in the training set that are diagnostic of the focus error. [0062] In one embodiment, the optimal defocus filters are obtained via a technique referred to as Accuracy Maximization Analysis (AMA), a task-focused statistical technique for </p><p id="p0065" num="0065">dimensionality reduction. The AMA technique substantially outperforms traditional methods of dimensionality reduction like Principle Component Analysis (PCA). Indeed, the AMA technique returns the optimal filters (i.e., bases) for performing the task of defocus estimation. The AMA technique is described, for example, in Geisler, W.S.; Najemnik, J.; and Ing, A.D. (2009) in Optimal Stimulus Encoders for Natural Tasks, Journal of Vision 9(13): 17, 1-16 ("AMA document"). The AMA document is incorporated herein by reference. </p><p id="p0066" num="0066">[0063] In one embodiment, the spatial pattern of responses for a given class of sensor is as follows in Equation (EQ 3): </p><p id="p0067" num="0067">.(<sup>x</sup>) = ∑[/( ) w( ,A )]i<sub>c</sub> (A) rop<sub>c</sub> (x) + 77<sub>c</sub> (x) (EQ 3) </p><p id="p0068" num="0068"> λ where * represents two-dimensional convolution in x, and J <sub>c</sub> represents a noise function for 
<img id="imgf000017_0001" he="8" wi="6" file="imgf000017_0001.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 </p><p id="p0069" num="0069">the sensor class c. The defocus is estimated from the spatial patterns of responses, given by EQ 3, for the available sensor classes. </p><p id="p0070" num="0070">[0064] An alternative, suitable approximation to the r
<img id="imgf000017_0002" he="8" wi="22" file="imgf000017_0002.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 above may be obtained follows (EQ 3A): 
<img id="imgf000017_0003" he="7" wi="115" file="imgf000017_0003.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 
<!-- EPO <DP n="18"/>-->
 where the r (x approximation of Equation EQ 3 A gives the sensor response for a sensor class c, and where each image channel was obtained by taking the dot product of the wavelength distribution at each pixel with the sensor wavelength sensitivity, and where r\<sub>c</sub> represents a noise function for the sensor class c. The sensor response resulting from this calculation is referred to as the "color channel" sensor response. </p><p id="p0071" num="0071">[0065] The data that is received from the sensor array may be used to learn how to estimate the defocus, such as by looking at the power spectrum of the sensor responses for each patch of image and for each defocus level. In one embodiment, the power spectrum of the sensor responses may be obtained by performing a transform for each sampled image patch. The transform may comprise, for example, a Fast Fourier Transform (FFT); other transforms are also possible: e.g. a cosine transform, or a Hartley transform. Other suitable transforms will be known to those skilled in the art. In this manner, one may be able to compare the spatial frequency content (i.e. the relative amounts of fine and course detail) in a given patch of image to the relative amounts that are characteristic of different defocus levels. [0066] Defocus filters may be determined for sensor arrays with more than one sensor class so that chromatic aberrations can be used to estimate both the magnitude and the sign of defocus. In addition, defocus filters determined for different orientations within one sensor class may be used so that monochromatic aberrations can be used to estimate both the magnitude and the sign of defocus. [0067] Returning to Figure 5, in the operation 504, a training set of image patches, such as the image patches received in the first operation 601 of Figure 6, is applied to the filters generated in the operation 605 of Figure 6. </p><p id="p0072" num="0072">[0068] In the next Figure 5 operation of 505, the joint probability distribution of filter responses to the image patches for the defocus levels is estimated. Specifically, for each defocus level, the filter responses are fit with a Gaussian distribution by calculating the sample mean and covariance matrix. In one embodiment, a vector of responses is output from the sensor array for each defocus level, represented by the vector R. Given an observed filter response vector R, a continuous defocus estimate is obtained by computing a weighted sum of the posterior probabilities across a set of discrete defocus levels in Equation (EQ 4) (discussed further below in connection with operation 506): 
<!-- EPO <DP n="19"/>-->
 A =∑A ,/?(AD, | R) (EQ 4) where AD<sub>j</sub> is one of the N trained defocus levels and /?(ΔΖ | R) is the posterior probability of that defocus level given the observed vector of filter response R. Other estimation methods (e.g. MAP estimation) may be more appropriate when the posterior probability distributions are often bimodal, as occurs with low levels of chromatic aberration; defocus sign is more difficult to estimate with low levels of chromatic aberration. Bayes' rule gives the posterior probability of each specific defocus level AD. in Equation (EQ 5): P(R \ AD. )<sub>P</sub>(AD . ) </p><p id="p0073" num="0073"> P (AD, | R) = <sup>J , [ J &gt;</sup> (EQ 5) </p><p id="p0074" num="0074"> ∑p{R \ AD<sub>k</sub>)p{AD<sub>K</sub>) </p><p id="p0075" num="0075"> k=l where ρ( | ΔΖ is the likelihood of the observed filter response vector given that defocus level, and /?(ΔΖ is the prior probability of that defocus level. In one embodiment, it is assumed that the likelihood for each defocus level is a multivariate Gaussian (one dimension per filter) with mean vector and covariance matrix∑<sub>7</sub> (Equation EQ 6): p(R I AD,) = ga^ (R; /, ,∑, ) (EQ 6) where μ<sub>7</sub>- and∑ <sub>/</sub> were set to the sample mean and covariance matrix of the raw filter responses. Other parametric descriptions of the filter response distributions may provide better performance. If the prior probabilities of the defocus levels are equal, then the prior probabilities will be factored out of EQ 5. In many instances, however, the prior probability of different defocus levels will not be equal, especially if focus distance is taken into account. For example, if the lens is focused on optical infinity, there is a zero probability that defocus is positive (i.e., that the target is beyond optical infinity). This information can be used to increase accuracy of estimates. </p><p id="p0076" num="0076">[0069] Increasing the number of discrete defocus levels in the training set increases the accuracy of the continuous estimates. That is, identification of discrete defocus levels becomes equivalent to continuous estimation as the number of levels increases. However, increasing the number of discrete defocus levels increases the training set size and the computational complexity of learning filters. In practice, with a 2mm aperture, for example, excellent 
<!-- EPO <DP n="20"/>-->
 continuous estimates are obtained using 0.25 diopter steps for training, followed by interpolation to estimate Gaussian distributions between steps. Interpolated distributions are added until the maximum Mahalanobis distance (i.e., d-prime distance) was less than or equal to 0.5. These details may change with a different aperture, or a new optical system. [0070] In a system for which the techniques described herein are used for estimating focus error of a received image, the filter responses as described above may be combined to estimate the defocus of an image patch in a received image. In view of the description herein, those skilled in the art will understand that estimates of focus errors are obtained with the simple weighted summation formula as shown in EQ 4. The vector of filter responses is obtained by taking the dot product of each spatial-frequency filter with the normalized, logged amplitude spectrum of the sensor responses to the patch. As discussed above, the posterior probabilities are obtained by applying Bayes' rule with the fitted multi-dimensional Gaussian probability distributions. EQ 4 gives the Bayes optimal estimate that minimizes the mean-squared error of the estimates when N is sufficiently large. Other estimation techniques may be used, and will be known to those skilled in the art. </p><p id="p0077" num="0077">[0071] Equations EQ 3 (or EQ 3A) and EQ 4 may be applied to arbitrary image patches to obtain an estimate of defocus. These estimates may be used by the optical system to refocus the lens. The focus error, in conjunction with an estimate of focus distance, can also be used to estimate depth, which may be useful for computational vision applications. It should be noted that estimating depth is not necessary for the operation of digital cameras; nevertheless, depth estimates can be obtained with an estimate of focus error and an estimate of focus distance. That is, by having an accurate estimate of the defocus at every point in an image, an estimate of the distance to the corresponding point in the scene can be easily computed, if the focus distance is known. The precision of defocus-based distance estimates depends on the distances to be estimated: for a given aperture size (i.e. f-number), distance estimates from defocus are more precise for near than far distances. The precision of defocus-based distance estimates also depends on the size of the optical system's aperture: for a given focus distance, distance estimates from defocus are more precise for large apertures than small apertures (large apertures produce shallower depth-of-field). Therefore, the method described in Figure 5 is especially useful in applications, such as computational vision, robotics, or microfabrication applications concerned with estimating the distance to nearby objects. 
<!-- EPO <DP n="21"/>-->
 [0072] Figures 7, 8, 9, and 10 provide illustrations of the process for using the focus filters and determining the defocus estimates as described above in connection with Figure 5 and Figure 6. </p><p id="p0078" num="0078">[0073] Figure 7 shows six defocus filters that were selected for an optical system having diffraction-limited and defocus-limited optics, and sensors that are sensitive only to 570 nm light. The filters were selected using the techniques of the AMA Document noted above. Figure 7 shows that the filters have several interesting features. First, they capture most of the relevant information; additional filters add little to overall accuracy. Second, they provide better performance than filters based on principal components analysis or matched templates. Third, they are relatively smooth and hence could be implemented by combining a few relatively simple, center-surround receptive fields like those found in retina or primary visual cortex. Fourth, the filter energy is concentrated in the 5-15 cycles per degree (cpd) frequency range, which is similar to the range known to drive human accommodation (4-8 cpd). </p><p id="p0079" num="0079">[0074] As noted above, the Figure 7 filters selected according to the AMA Document encode information in local amplitude spectra relevant for estimating defocus. However, the Bayesian decoder built into the technique of the AMA Document can be used only with the training stimuli, because that decoder needs access to the mean and variance of each filter's response to each stimulus. In other words, the technique of the AMA Document finds only the optimal filters. The next step is to combine (pool) the filter responses to estimate defocus in arbitrary image patches, having arbitrary defocus level. A standard (i.e., typical) approach was selected. First, the joint probability distribution of filter responses to image patches is estimated for the defocus levels in the training set. For each defocus level, the filter responses are fit with a Gaussian distribution by calculating the sample mean and covariance matrix. Figure 8 shows the joint distribution of the responses for the first two selected filters for several levels of defocus. More particularly, Figure 8 shows filter responses to amplitude spectra in a range of defocus levels for the training set (response for defocus levels of 1.25, 1.75, and 2.25 diopters are not plotted in Figure 8). The symbols represent joint responses from the two most informative filters. Marginal distributions are shown on each axis. Approximate centroids of symbol groups that correspond to each of the plotted defocus levels are indicated in Figure 8 </p><p id="p0080" num="0080">[0075] Figure 9 shows contour plots of the fitted Gaussians distributions. After the joint responses are determined, in the second step, using the joint distributions (which are six- dimensional, one dimension for each filter), defocus estimates are obtained with the weighted 
<!-- EPO <DP n="22"/>-->
 summation formula given by Equation EQ 4 above, where AD<sub>j</sub> is one of the N trained defocus levels, and the conditional probability in EQ. 4 is the posterior probability of that defocus level given the observed vector of filter responses R. Figure 9 shows Gaussian distributions fit to the filter responses. The thicker lines are iso-likelihood contours on the maximum-likelihood surface determined from fits to the response distributions at trained defocus levels. The thinner lines are iso-likelihood contours on interpolated response distributions. Circles in Figure 9 indicate interpolated means separated by a d' (i.e., Mahalanobis distance) of 1. Line segments in Figure 9 show the direction of principle variance and ±1 SD. The response vector is given by the dot product of each filter with the normalized, logged amplitude spectrum. The posterior probabilities are obtained by applying Bayes' rule to the fitted Gaussian probability distributions. Equation EQ. 4 gives the Bayes optimal estimate when the goal is to minimize the mean-squared error of the estimates and when N is sufficiently large, which it is in the illustrated case. </p><p id="p0081" num="0081">[0076] Figure 10 illustrates defocus estimates for the test patches, plotted as a function of defocus for the initial case of a vision system having perfect optics and a single class of sensor. None of the test patches were in the training set. More particularly, Figure 10 shows defocus estimates for test stimuli not belonging to the training set of images. Symbols in Figure 10 represent the mean defocus estimate for each defocus level. Vertical error bars represent 68% (thick bars) and 90% (thin bars) confidence intervals about the mean defocus estimate. Boxes indicate defocus levels not in the training set of images. The equal-sized error bars at both trained and untrained levels indicates that the described technique outputs continuous estimates. </p><p id="p0082" num="0082">[0077] In the results obtained, as illustrated in Figures 7, 8, 9, and 10, precision is relatively high and bias is relatively low, once defocus exceeds approximately 0.25 diopters, which is roughly the defocus detection threshold in humans. Precision decreases at low levels of defocus because a modest change in defocus (e.g., on the order of 0.25 diopters) does not change the amplitude spectra significantly when the base defocus is zero; more substantial changes occur when the base defocus is nonzero. The bias near zero occurs because, in vision systems having perfect optics and sensors sensitive only to a single wavelength, positive and negative defocus levels of identical magnitude yield identical amplitude spectra. Thus, the bias is due to a boundary effect: estimation errors can be made above but not below zero. Applying the method of Figure 5 with more than one class of sensor can remove the bias near zero and can allow 
<!-- EPO <DP n="23"/>-->
 estimation of the sign of defocus (i.e., estimation of whether the focus is in front or behind the subject). </p><p id="p0083" num="0083">[0078] In some implementations, the Figure 5 method may include other and/or additional steps that, for clarity, are not depicted. Further, in some implementations, the method may be executed in a different order presented and that the order presented in the discussion of Figure 5 is illustrative. Additionally, in some implementations, certain steps in the Figure 5 method may be executed in a substantially simultaneous manner or may be omitted. </p><p id="p0084" num="0084">[0079] For example, in one embodiment, it may not be strictly necessary to represent the optical system and sensor array to determine the optimal filters thereby avoiding steps 501 and 502, as long as a collection of image patches with known defocus levels are collected. Once this collection is received in the Figure 6 operation 601, the responses may be obtained from the sensor array for a given class of sensor in step 604. That is, the only critical factor in generating the filters is obtaining a collection of sensor array responses for each of the defocus levels. Such an embodiment may be of particular usefulness in microscopy or microfabrication in which the objects being imaged all lie in one depth plane. </p><p id="p0085" num="0085">[0080] It should be understood that the principles of the present invention may be adapted to specific classes of image-capturing devices and imaged environments. For example, the techniques described herein may be tailored to be used for various settings (e.g., portrait, landscape, macro photography) of a digital camera. As noted above, other suitable devices for application of the techniques described herein include digital microscopes, digital telescopes, and digital video cameras. Other devices and adaptations for using the techniques described herein will occur to those skilled in the art. </p><p id="p0086" num="0086">[0081] Although the method, system and computer program product are described in connection with several embodiments, it is not intended to be limited to the specific forms set forth herein, but on the contrary, it is intended to cover such alternatives, modifications and equivalents, as can be reasonably included within the spirit and scope of the invention as defined by the appended claims. 
</p></description><claims mxw-id="PCLM46552613" ref-ucid="WO-2012161829-A2" lang="EN" load-source="patent-office"><claim id="clm-0001" num="1"><!-- EPO <DP n="24"/>--><claim-text> WHAT IS CLAIMED IS: 1. A method for focus error processing, the method comprising: </claim-text><claim-text> receiving image data of an object onto a sensor array of an optical system; </claim-text><claim-text> calculating a filter response value of a defocus filter to a patch of the received image data for each defocus filter in a set of one or more defocus filters for the optical system, such that the calculated filter response values for the set of defocus filters produces a filter response vector of the received image data; </claim-text><claim-text> calculating a probability for each of multiple defocus levels within a predetermined set of defocus levels from the filter response vector of the received image data; and </claim-text><claim-text> determining an estimate of focus error from the received image data based on the calculated probabilities of the multiple defocus levels. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The method as in claim 1, wherein calculating the probability of each of the multiple defocus levels comprises using a parametric function of a training set of images comprising image patches having defocus levels corresponding to the predetermined set of defocus levels to determine a probability for each defocus level of the predetermined set of defocus levels. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The method as in claim 1, wherein determining an estimate of focus error comprises selecting a defocus level from the predetermined set of defocus levels that is most similar to the training filter response vectors associated with a defocus level. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The method as in claim 3, wherein selecting a defocus level comprises selecting from the predetermined set of defocus levels according to a nearest neighbor technique to determine the most similar training filter response vector. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method as in claim 1, wherein calculating a filter response value for each defocus filter in the set of one or more defocus filters comprises: </claim-text><claim-text> calculating a Fourier transform of the patch of the received image data; </claim-text><claim-text> computing a radially averaged power spectrum from the Fourier transform of the received image data; and <!-- EPO <DP n="25"/>--> calculating a dot product of the radially averaged power spectrum with each defocus filter to produce a response value for each filter; </claim-text><claim-text> wherein the set of response values for the filters comprises the filter response vector. </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The method as in claim 1, further comprising: </claim-text><claim-text> adjusting the optical system to compensate for the determined estimate of focus error and to obtain an image at the sensor array that is substantially in focus. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method as in claim 6, wherein the optical system comprises a digital camera. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The method as in claim 6, wherein the optical system comprises a digital microscope. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. The method as in claim 6, wherein the optical system comprises a digital telescope. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The method as in claim 6, wherein the optical system comprises a digital video camera. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. The method as in claim 6, wherein the optical system comprises a device that is controlled in response to the received image data. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. A system for focus error processing, the system comprising: an optical system; </claim-text><claim-text> a sensor array that receives image data of an object viewed through the optical system; and </claim-text><claim-text> a processor configured to calculate a filter response value of a defocus filter to a patch of the received image data for each of one or more defocus filters for the optical system, such that the calculated filter response values comprise a filter response vector of the received image data, <!-- EPO <DP n="26"/>--> calculate a probability for each of multiple defocus levels within a predetermined set of defocus levels from the filter response vector of the received image data, and </claim-text><claim-text> determine an estimate of focus error in the received image data based on the calculated probabilities of the multiple defocus levels. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The system as in claim 12, wherein the processor calculates the probability of each of the multiple defocus levels using a parametric function of a training set of images comprising image patches having defocus levels corresponding to the predetermined set of defocus levels to determine a probability for each defocus level of the predetermined set of defocus levels. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The system as in claim 12, wherein the processor determines an estimate of focus error by selecting a defocus level from the predetermined set of defocus levels that is most similar to the training filter response vectors associated with a defocus level. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The system as in claim 14, wherein the processor selects a defocus level by selecting from the predetermined set of defocus levels according to a nearest neighbor technique to determine the most similar training filter response vector. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. The system as in claim 12, wherein the processor calculates a filter response vector comprising a filter response value for each of the one or more defocus filters by calculating a Fourier transform of the patch of the received image data, computing a radially averaged power spectrum from the Fourier transform of the received image data, and calculating a dot product of the radially averaged power spectrum with each defocus filter to produce a response value for each filter, wherein the set of response values for the filters comprises the filter response vector. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. The system as in claim 12, wherein the processor is further configured to adjust the optical system to compensate for the determined estimate of focus error and to obtain an image at the sensor array that is substantially in focus. </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. The system as in claim 17, wherein the optical system comprises a digital camera. <!-- EPO <DP n="27"/>--> </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The system as in claim 17, wherein the optical system comprises a digital microscope. </claim-text></claim><claim id="clm-0020" num="20"><claim-text> 20. The system as in claim 17, wherein the optical system comprises a digital telescope. </claim-text></claim><claim id="clm-0021" num="21"><claim-text>21. The system as in claim 17, wherein the optical system comprises a digital video camera. </claim-text></claim><claim id="clm-0022" num="22"><claim-text>22. The system as in claim 17, wherein the optical system comprises a device that is controlled in response to the received image data.. </claim-text></claim><claim id="clm-0023" num="23"><claim-text>23. A method for estimating focus error in an image received through an optical system, the method comprising: </claim-text><claim-text> computing filter responses for a set of image patches from a training set of images, for each of multiple defocus levels in a predetermined set of defocus levels for each image patch in the training set of images, for each of one or more defocus filters in a set of defocus filters for the optical system; </claim-text><claim-text> obtaining a distribution of filter response vectors from said computed filter responses for the set of image patches from the training set of images, for each defocus level of the multiple defocus levels; and </claim-text><claim-text> estimating a focus error for each defocus level based on the distribution of filter response vectors. </claim-text></claim><claim id="clm-0024" num="24"><claim-text>24. The method as in claim 23, wherein the set of defocus filters is determined from the training set of images for each defocus level using a point-spread function that represents the optical system, for each of the multiple defocus levels in the predetermined set of defocus levels for each image patch in the training set of images; </claim-text><claim-text> wherein each image patch in the training set of images is sampled using at least one wavelength sensitivity function, at least one a spatial sampling function, and at least one noise function for at least one class of sensor in a sensor array of the optical system, producing a representation of the image received through the optical system at the sensor array; </claim-text><claim-text> wherein the set of defocus filters is obtained from said sensor responses from said sensor array for each of said multiple defocus levels. <!-- EPO <DP n="28"/>--> </claim-text></claim><claim id="clm-0025" num="25"><claim-text>25. The method as in claim 23, wherein the set of defocus filters is obtained from said filter responses for each of said multiple defocus levels using a statistical technique comprising accuracy maximization analysis (AMA). </claim-text></claim><claim id="clm-0026" num="26"><claim-text>26. The method as in claim 23, wherein the set of defocus filters is obtained from said filter responses for each of said multiple defocus levels using a statistical technique comprising principle components analysis (PCA). </claim-text></claim><claim id="clm-0027" num="27"><claim-text>27. The method as in claim 23, the method further comprising: characterizing each said filter response vector distribution by fitting a parametric function to the distribution, wherein the parametric function comprises a conditional likelihood distribution function. </claim-text></claim><claim id="clm-0028" num="28"><claim-text>28. The method as in claim 27, the method further comprising: interpolating conditional likelihood distributions for defocus levels that have values between defocus levels in the said set of training defocus levels. </claim-text></claim><claim id="clm-0029" num="29"><claim-text>29. The method as in claim 23, further comprising: </claim-text><claim-text> estimating a focus error for an arbitrary image patch by </claim-text><claim-text> calculating a defocus filter response value for each of the one or more defocus filters in response to the arbitrary image patch, </claim-text><claim-text> computing a probability of each defocus level using said set of conditional likelihood distributions and using the training filter response vector; </claim-text><claim-text> wherein the training filter response vector is generated in response to the arbitrary image patch, from the said set of defocus filters, and said combination of filter responses from said set of defocus filters. </claim-text></claim><claim id="clm-0030" num="30"><claim-text>30. The method as in claim 23, wherein said range of defocus error is within a range from approximately 0 diopters to approximately 2.25 diopters at an interval of </claim-text><claim-text>approximately 0.25 diopters. <!-- EPO <DP n="29"/>--> </claim-text></claim><claim id="clm-0031" num="31"><claim-text>31. The method as in claim 23, wherein said range of defocus error is within a range from approximately -2.25 diopters to approximately 2.25 diopters at an interval of approximately 0.25 diopters. </claim-text></claim><claim id="clm-0032" num="32"><claim-text>32. The method as in claim 23, further comprising: </claim-text><claim-text> performing a Fourier transform of said sensor responses from said sensor array for said class of sensor. </claim-text></claim><claim id="clm-0033" num="33"><claim-text>33. The method as in claim 23, further comprising: </claim-text><claim-text> obtaining responses from said sensor array for more than one class of sensor in the sensor array. </claim-text></claim><claim id="clm-0034" num="34"><claim-text>34. The method as in claim 23, wherein the set of defocus filters is determined for more than one class of sensor in a sensor array of the optical system, such that the set of defocus filters provide information that defines magnitude and sign of defocus level. </claim-text></claim><claim id="clm-0035" num="35"><claim-text>35. A computer program product embodied in a computer readable storage medium for estimating focus error, the computer program product comprising programming instructions recorded on the medium such that, when executed by a computer processor, the programming instructions provide a method for: </claim-text><claim-text> computing filter responses for a set of image patches from a training set of images, for each of multiple defocus levels in a predetermined set of defocus levels for each image patch in the training set of images, for each of one or more defocus filters in a set of defocus filters for the optical system; </claim-text><claim-text> obtaining a distribution of filter response vectors from said computed filter responses for the set of image patches from the training set of images, for each defocus level of the multiple defocus levels; and </claim-text><claim-text> estimating a focus error for each defocus level based on the distribution of filter response vectors. </claim-text></claim><claim id="clm-0036" num="36"><claim-text>36. The computer program product as in claim 35, wherein the set of defocus filters is determined from the training set of images for each defocus level using a point-spread <!-- EPO <DP n="30"/>--> function that represents the optical system, for each of the multiple defocus levels in the predetermined set of defocus levels for each image patch in the training set of images; </claim-text></claim><claim id="clm-0037" num="37"><claim-text> wherein each image patch in the training set of images is sampled using at least one wavelength sensitivity function, a spatial sampling function, and at least one noise function for a class of sensor in a sensor array of the optical system, producing a representation of the image received through the optical system at the sensor array; </claim-text></claim><claim id="clm-0038" num="38"><claim-text> wherein the set of defocus filters is obtained from said sensor responses from said sensor array for each of said multiple defocus levels. 37. The computer program product as in claim 35, wherein the set of defocus filters is obtained from said filter responses for each of said multiple defocus levels using a statistical technique comprising accuracy maximization analysis (AMA). 38. The computer program product as in claim 35, wherein the set of defocus filters is obtained from said filter responses for each of said multiple defocus levels using a statistical technique comprising principle components analysis (PCA). 39. The computer program product as in claim 35, wherein the processor further performs: </claim-text></claim><claim id="clm-0039" num="39"><claim-text> characterizing each said filter response vector distribution by fitting a parametric function to the distribution, wherein the parametric function comprises a conditional likelihood distribution function. 40. The computer program product as in claim 27, the method further comprising: </claim-text></claim><claim id="clm-0040" num="40"><claim-text> interpolating conditional likelihood distributions for defocus levels that have values between defocus levels in the said set of training defocus levels. 41. The computer program product as in claim 35, wherein the processor further performs: </claim-text></claim><claim id="clm-0041" num="41"><claim-text> estimating a focus error for an arbitrary image patch by </claim-text></claim><claim id="clm-0042" num="42"><claim-text> calculating a defocus filter response value for each of the one or more defocus filters in response to the arbitrary image patch, <!-- EPO <DP n="31"/>--> computing a probability of each defocus level using said set of conditional likelihood distributions and using the training filter response vector; </claim-text></claim><claim id="clm-0043" num="43"><claim-text> wherein the training filter response vector is generated in response to the arbitrary image patch, from the said set of defocus filters, and said combination of filter responses from said set of defocus filters. 42. The computer program product as in claim 35, wherein said range of defocus error is within a range from approximately 0 diopters to approximately 2.25 diopters at an interval of approximately 0.25 diopters. 43. The computer program product as in claim 35, wherein said range of defocus error is within a range from approximately -2.25 diopters to approximately 2.25 diopters at an interval of approximately 0.25 diopters. 44. The computer program product as in claim 35, further comprising: </claim-text></claim><claim id="clm-0044" num="44"><claim-text> performing a Fourier transform of said sensor responses from said sensor array for said class of sensor 45. The computer program product as in claim 35, further comprising:. </claim-text></claim><claim id="clm-0045" num="45"><claim-text> obtaining responses from said sensor array for more than one class of sensor in the sensor array. </claim-text></claim><claim id="clm-0046" num="46"><claim-text>46. The computer program product as in claim 35, wherein the set of defocus filters is determined for more than one class of sensor in a sensor array of the optical system, such that the set of defocus filters provide information that defines magnitude and sign of defocus level. </claim-text></claim><claim id="clm-0047" num="47"><claim-text>47. A system comprising: </claim-text><claim-text> a memory unit for storing a computer program for estimating focus error in an image; and </claim-text><claim-text> a processor coupled to said memory unit, wherein said processor comprises circuitry for, responsive to executing said computer program: <!-- EPO <DP n="32"/>--> computing filter responses for a set of image patches from a training set of images, for each of multiple defocus levels in a predetermined set of defocus levels for each image patch in the training set of images, for each of one or more defocus filters in a set of defocus filters for the optical system; </claim-text><claim-text> obtaining a distribution of filter response vectors from said computed filter responses for the set of image patches from the training set of images, for each defocus level of the multiple defocus levels; and </claim-text><claim-text> estimating a focus error for each defocus level based on the distribution of filter response vectors . </claim-text></claim><claim id="clm-0048" num="48"><claim-text>48. The system as in claim 47, wherein the set of defocus filters is determined from the training set of images for each defocus level using a point-spread function that represents the optical system, for each of the multiple defocus levels in the predetermined set of defocus levels for each image patch in the training set of images; </claim-text><claim-text> wherein each image patch in the training set of images is sampled using at least one wavelength sensitivity function, a spatial sampling function, and at least one noise function for a class of sensor in a sensor array of the optical system, producing a representation of the image received through the optical system at the sensor array; </claim-text><claim-text> wherein the set of defocus filters is obtained from said sensor responses from said sensor array for each of said multiple defocus levels. </claim-text></claim><claim id="clm-0049" num="49"><claim-text>49. The system as in claim 47, wherein the set of defocus filters is obtained from said filter responses for each of said multiple defocus levels using a statistical technique comprising accuracy maximization analysis (AMA). </claim-text></claim><claim id="clm-0050" num="50"><claim-text>50. The system as in claim 47, wherein the set of defocus filters is obtained from said filter responses for each of said multiple defocus levels using a statistical technique comprising principle components analysis (PCA). </claim-text></claim><claim id="clm-0051" num="51"><claim-text>51. The system as in claim 47, wherein the processor further performs: </claim-text><claim-text> characterizing each said filter response vector distribution by fitting a parametric function, wherein the parametric function comprises a conditional likelihood distribution function. <!-- EPO <DP n="33"/>--> </claim-text></claim><claim id="clm-0052" num="52"><claim-text>52. The system as in claim 51 , the method further comprising: interpolating conditional likelihood distributions for defocus levels that have values between defocus levels in the said set of training defocus levels. </claim-text></claim><claim id="clm-0053" num="53"><claim-text>53. The system as in claim 47, wherein the processor further performs: </claim-text><claim-text> estimating a focus error for an arbitrary image patch by </claim-text><claim-text> calculating a defocus filter response value for each of the one or more defocus filters in response to the arbitrary image patch, </claim-text><claim-text> computing a probability of each defocus level using said set of conditional likelihood distributions and using the training filter response vector; </claim-text><claim-text> wherein the training filter response vector is generated in response to the arbitrary image patch, from the said set of defocus filters, and said combination of filter responses from said set of defocus filters. </claim-text></claim><claim id="clm-0054" num="54"><claim-text>54. The system as in claim 47, wherein said range of defocus error is within a range from approximately 0 diopters to approximately 2.25 diopters at an interval of </claim-text><claim-text>approximately 0.25 diopters. </claim-text></claim><claim id="clm-0055" num="55"><claim-text>55. The system as in claim 47, wherein said range of defocus error is within a range from approximately -2.25 diopters to approximately 2.25 diopters at an interval of approximately 0.25 diopters. </claim-text></claim><claim id="clm-0056" num="56"><claim-text>56. The system as in claim 47, further comprising: </claim-text><claim-text> performing a Fourier transform of said sensor responses from said sensor array for said class of sensor</claim-text></claim><claim id="clm-0057" num="57"><claim-text>57. The system as in claim 47, further comprising:. </claim-text><claim-text> obtaining responses from said sensor array for more than one class of sensor in the sensor array. </claim-text></claim><claim id="clm-0058" num="58"><claim-text>58. The system as in claim 47„ wherein the set of defocus filters is determined for more than one class of sensor in a sensor array of the optical system, such that the set of defocus filters provide information that defines magnitude and sign of defocus level <!-- EPO <DP n="34"/>--> </claim-text></claim><claim id="clm-0059" num="59"><claim-text>59. A method of determining focus error in an image received at a sensor array of an optical system, the method comprising: </claim-text><claim-text> computing filter responses for a set of training image patches, received on a sensor array, that were defocused in simulation using a point-spread function that represents the optical system, for each of multiple defocus levels within a range of defocus for each image patch in the set of training image patches, for a plurality of filters; </claim-text><claim-text> sampling each of said set of training image patches on said sensor array using a wavelength sensitivity function and a spatial sampling function that represent the sensor array; </claim-text><claim-text> obtaining responses from said sensor array for a class of sensor in the sensor array; </claim-text><claim-text> generating a set of defocus filters from said sensor responses from said sensor array for each of said multiple defocus levels; </claim-text><claim-text> determining a probability likelihood of each filter response vector by obtaining the distribution of filter response vectors for all image patches having the same defocus level in the set of training image patches; </claim-text><claim-text> characterizing the filter response distributions corresponding to each defocus level in the training set with a parametric multi-dimensional function; </claim-text><claim-text> interpolating conditional likelihood distributions for defocus levels in between the defocus levels in the training set; and </claim-text><claim-text> deriving an estimate of focus error for an arbitrary image patch with an unknown focus error by: </claim-text><claim-text> computing the filter response vector in response to the arbitrary patch, computing the probability of each defocus level within the specified range, and </claim-text><claim-text> deriving an estimate of focus error from the probability distribution. </claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
