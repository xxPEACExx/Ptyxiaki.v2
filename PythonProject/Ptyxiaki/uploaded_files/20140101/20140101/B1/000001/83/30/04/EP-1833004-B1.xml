<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-1833004-B1" country="EP" doc-number="1833004" kind="B1" date="20140101" family-id="38110195" file-reference-id="299948" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146553711" ucid="EP-1833004-B1"><document-id><country>EP</country><doc-number>1833004</doc-number><kind>B1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-07250778-A" is-representative="YES"><document-id mxw-id="PAPP154827634" load-source="docdb" format="epo"><country>EP</country><doc-number>07250778</doc-number><kind>A</kind><date>20070223</date><lang>EN</lang></document-id><document-id mxw-id="PAPP220071228" load-source="docdb" format="original"><country>EP</country><doc-number>07250778.3</doc-number><date>20070223</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140448549" ucid="JP-2006061917-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2006061917</doc-number><kind>A</kind><date>20060307</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130723</date></intention-to-grant-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988128286" load-source="docdb">G06K   9/46        20060101AFI20111213BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1988130050" load-source="docdb">G06K   9/00        20060101ALI20111213BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1988113228" load-source="docdb" scheme="CPC">G06K   9/00248     20130101 FI20130101BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132193617" lang="DE" load-source="patent-office">Vorrichtung und Verfahren zum Ermitteln von Merkmalspunkten</invention-title><invention-title mxw-id="PT132193618" lang="EN" load-source="patent-office">Apparatus for detecting feature point and method of detecting feature point</invention-title><invention-title mxw-id="PT132193619" lang="FR" load-source="patent-office">Appareil et procédé de détection de points caractéristiques</invention-title><citations><non-patent-citations><nplcit><text>VALENTE S ET AL: "FACE TRACKING AND REALISTIC ANIMATIONS FOR TELECOMMUNICANT CLONES", IEEE MULTIMEDIA, IEEE SERVICE CENTER, NEW YORK, NY, US, vol. 7, no. 1, 1 January 2000 (2000-01-01), pages 34 - 42, XP000908488, ISSN: 1070-986X, DOI: 10.1109/93.839309</text><sources><source mxw-id="PNPL66839895" load-source="docdb" name="EXA"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR918169026" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>TOSHIBA KK</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR918144767" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KABUSHIKI KAISHA TOSHIBA</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918158981" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>TAKEGUCHI TOMOYUKI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918135314" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>TAKEGUCHI, TOMOYUKI</last-name></addressbook></inventor><inventor mxw-id="PPAR918997650" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Takeguchi, Tomoyuki, Toshiba Corporation</last-name><address><street>I.P. Division 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918141674" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>KOZAKAYA TATSUO</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918153625" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>KOZAKAYA, TATSUO</last-name></addressbook></inventor><inventor mxw-id="PPAR918997648" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>Kozakaya, Tatsuo, Toshiba Corporation</last-name><address><street>I.P. Division 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918160026" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>NISHIURA MASAHIDE</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918170740" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>NISHIURA, MASAHIDE</last-name></addressbook></inventor><inventor mxw-id="PPAR918997649" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>Nishiura, Masahide, Toshiba Corporation</last-name><address><street>I.P. Division 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918133731" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>YAMAGUCHI OSAMU</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918169074" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>YAMAGUCHI, OSAMU</last-name></addressbook></inventor><inventor mxw-id="PPAR918997651" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>Yamaguchi, Osamu, Toshiba Corporation</last-name><address><street>I.P. Division 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918141882" load-source="docdb" sequence="5" format="epo"><addressbook><last-name>YUASA MAYUMI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918132465" load-source="docdb" sequence="5" format="intermediate"><addressbook><last-name>YUASA, MAYUMI</last-name></addressbook></inventor><inventor mxw-id="PPAR918997652" load-source="patent-office" sequence="5" format="original"><addressbook><last-name>Yuasa, Mayumi, Toshiba Corporation</last-name><address><street>I.P. Division 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR918997654" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>KABUSHIKI KAISHA TOSHIBA</last-name><iid>100154269</iid><address><street>1-1 Shibaura 1-chome, Minato-ku</street><city>Tokyo 105-8001</city><country>JP</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR918997653" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Maury, Richard Philip</last-name><iid>100025539</iid><address><street>Marks &amp; Clerk LLP 90 Long Acre</street><city>London WC2E 9RA</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS548978733" load-source="docdb">DE</country><country mxw-id="DS548866099" load-source="docdb">FR</country><country mxw-id="DS548978734" load-source="docdb">GB</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><description mxw-id="PDES63957365" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">The present invention relates to an apparatus for detecting a feature point that detects a feature point of a target object from an input image, and a method of detecting the feature point.</p><heading id="h0001">BACKGROUND OF THE INVENTION</heading><p id="p0002" num="0002">In an apparatus for detecting feature point in the related art, there is a method of detecting a small number of feature points such as a nose and eyes on a face image and then setting a new different feature point using the positions of the detected feature points as disclosed in Japanese Application Kokai <patcit id="pcit0001" dnum="JP2004265267A"><text>2004-265267</text></patcit> .</p><p id="p0003" num="0003">There is also a method of limiting the range of<!-- EPO <DP n="2"> --> searching the different feature point using the positions of the facial feature points detected on the image as disclosed in Japanese Application Kokai <patcit id="pcit0002" dnum="JP10086696A"><text>10-86696</text></patcit>.</p><p id="p0004" num="0004">In these methods, the detection of the second feature point is achieved by using a relative positional relationship with the detected first feature points on a stored two-dimensional image. However, there is a problem such that the second feature points cannot be detected correctly because the relative positional relationship of the feature points calculated when the face is oriented to the front does not indicate correct positions when the orientation of the face has changed.</p><p id="p0005" num="0005">As an example of the problem shown above, a method of detecting a point corresponding to a mouth center, which is a midpoint between the left end and the right end of the mouth, will be considered. As disclosed in Japanese Application Kokai <patcit id="pcit0003" dnum="JP2003187247A"><text>2003-187247</text></patcit>, there is a method of detecting the lip area by sorting color components and defining the both ends of the lip area as lip ends, and the midpoint of the lip ends as the mouth center. When the face is oriented to the front, the mouth center is likely to be almost the same as a midpoint between the mouth ends. However, when the orientation of the face has changed, or when the shape of the portion around the mouth has changed because the facial expression has changed, the<!-- EPO <DP n="3"> --> position of the mouth center dose not match the midpoint, and hence the position of the mouth center cannot be obtained correctly.</p><p id="p0006" num="0006">As described above, there is the method of detecting the first feature point and detecting the second feature point using the result of the first detection in the background of the detection of a feature point. However, since the relative positional relationship between the feature point obtained in the detection of the first feature point and the feature point obtained in the detection of the second feature point is defined on a two-dimensional image, when the orientation of the target object at the moment when the relative positional relationship is defined is different from the actual orientation of the target object, correct detection of the feature point cannot be achieved.</p><p id="p0007" num="0007"><nplcit id="ncit0001" npl-type="s"><text>KIN-MAN LAM ET AL: "AN ANALYTIC-TO-HOLISTIC APPROACH FOR FACE RECOGNITION BASED ON A SINGLE FRONTAL VIEW", TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, vol. 20, no. 7, 1 January 1998 (1998-01-01). Pages 673-686</text></nplcit>, XP055013352, D01:1109/34.689299 and <nplcit id="ncit0002" npl-type="s" url="http://ww.eurecom.fr/util/publicdownload.fr.htm?id=945"><text>A.C. Andrés Del Valle ET AL: "Facial Expression Analysis Robust to 3D Head Pose Motion", International Conference on Multimedia and Expo, 1 January 2002 (2002-01-01), pages 1-4, XP055013297, Retrieved from the Internet: URL:http://ww.eurecom.fr/util/publicdownload.fr.htm?id=945 [retrieved on 2011-1-29</text></nplcit>] and <nplcit id="ncit0003" npl-type="b"><text>FENG G C ET AL: "Recognition of Head-&amp;-Shoulder Face Image Using Virtual Frontal-View Image", IEEE TRANACTIONS ON SYSTEMS, MAN AND CYBERNETICS. PART A: SYSTEMS AND HUMANS, vol. 30, no. 6, 1 November 2000 (2000-11-01), XP011056349, IEEE SERVICE CENTER, PISCATAWAY, NJ, US ISSN: 1083-4427</text></nplcit> disclose an apparatus for detecting a specific feature point from an image including a target object comprising: an image input unit that inputs an image of the target object; a three dimensional shape information storing unit that stores three-dimensional shape information of a model and three-dimensional position information of reference points in the model, the reference points each differing from the specific feature point; a correspondent reference point input unit that inputs position information of correspondent reference points of the reference feature points on the input image; a correspondent feature point acquiring unit that acquires position information of correspondent feature point of the specific feature point in the model: a correspondence relation acquiring unit that acquires a correspondence relation between the input image and the three-dimensional shape information using the position information of the correspondent reference points and the three-dimensional position information of the reference points; a seek area setting unit that sets a seek area for the specific feature pint on the input image by projecting a correspondent area containing the correspondent feature point on surface of the model to the input image<!-- EPO <DP n="4"> --> based on the correspondence relation; an image information acquiring unit that acquires image information of the seek area from the input image; and a feature point detecting unit that detects the position of the specific feature point in the input image from the image information.</p><p id="p0008" num="0008">The invention is the apparatus, method and program of claims 1, 5 and 9.</p><p id="p0009" num="0009">In order to solve the problems in the related art as described above, the invention can provide an apparatus for detecting a feature point and a method of detecting a feature point in which the feature point can be detected correctly even when the orientation of a target object is changed.</p><p id="p0010" num="0010">According to an embodiment of the present invention,<!-- EPO <DP n="5"> --> an apparatus for detecting a specific feature point from an image including a target object comprises: an image input unit that inputs an image of the target object; a three-dimensional shape information storing unit that stores three-dimensional shape information of a model and three-dimensional position information of reference points in the model, the reference points each differing from the specific feature point; a correspondent reference point input unit that inputs position information of correspondent reference points of the reference feature points on the input image; a correspondent feature point acquiring unit that acquires position information of correspondent feature point of the specific feature point in the model; a correspondence relation acquiring unit that acquires a correspondence relation between the input image and the three-dimensional shape information using the position information of the correspondent reference points and the three-dimensional position information of the reference points; a seek area setting unit that sets a seek area for the specific feature point on the input image by projecting a correspondent area containing the correspondent feature point on surface of the model to the<!-- EPO <DP n="6"> --> input image based on the correspondence relation ; an image information acquiring unit that acquires image information of the seek area from the input image; and a feature point detecting unit that detects the position of the specific feature point in the input image from the image information.</p><p id="p0011" num="0011">According to the embodiment of the present invention, the feature point can be detected corresponding to the three-dimensional change of the orientation of the target object or the change in shape of the target object.</p><heading id="h0002">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0012" num="0012"><ul><li><figref idrefs="f0001">Fig. 1</figref> is a block diagram showing a configuration of an apparatus for detecting a feature point according to an embodiment of the present invention;</li><li><figref idrefs="f0002">Fig. 2</figref> is a flowchart showing an operation of the apparatus for detecting a feature point according to an embodiment;</li><li><figref idrefs="f0003">Fig. 3</figref> is a drawing showing a problem in detection of the feature point in the related art;</li><li><figref idrefs="f0004">Fig. 4</figref> is a drawing showing a process performed in a correspondence relation acquiring unit according to an embodiment;</li><li><figref idrefs="f0005">Fig. 5</figref> is a drawing showing a process performed in a seek area acquiring unit according to an embodiment;<!-- EPO <DP n="7"> --> and .</li></ul></p><p id="p0013" num="0013"><figref idrefs="f0006">Fig. 6</figref> is a drawing showing a process performed in a feature point detecting unit according to an embodiment.</p><heading id="h0003">DETAILED DESCRIPTION OF THE INVENTION</heading><p id="p0014" num="0014">An apparatus for detecting a feature point according to an embodiment of the present invention will be described.</p><heading id="h0004">Embodiment</heading><p id="p0015" num="0015">Referring to <figref idrefs="f0001 f0002 f0003 f0004 f0005 f0006">Fig. 1 to Fig. 6</figref>, an apparatus for detecting a feature point according to an embodiment will be described. In this embodiment, a case of employing a face as a target object and detecting a midpoint of a mouth from a face image will be described.</p><heading id="h0005">(1) Configuration of an apparatus for detecting a feature point</heading><p id="p0016" num="0016"><figref idrefs="f0001">Fig. 1</figref> is a block diagram showing an apparatus for detecting a feature point according to the embodiment.</p><p id="p0017" num="0017">The apparatus for detecting a feature point includes an image input unit 110 that acquires a face image to be processed, a three-dimensional shape information holding unit 200 that holds an average three-dimensional shape of the face, a correspondence relation acquiring unit 120<!-- EPO <DP n="8"> --> that estimates the correspondence relation between the face image and a three-dimensional shape, a seek area acquiring unit 130 that sets a seek area for the mouth center on the input image using the acquired correspondence relation, and a feature point detecting unit 140 that determines the position of the mouth center from the seek area.</p><p id="p0018" num="0018">Functions of these units 110 to 140 and 200 are implemented by a program stored in a computer.</p><heading id="h0006">(2) Operation of the apparatus for detecting a feature point</heading><p id="p0019" num="0019">Referring now to <figref idrefs="f0001">Fig. 1</figref> and <figref idrefs="f0002">Fig. 2</figref>, the operation of the apparatus for detecting a feature point will be described. <figref idrefs="f0002">Fig. 2</figref> is a flowchart showing an operation of the apparatus for detecting a feature point.</p><heading id="h0007">(2-1) Step A1</heading><p id="p0020" num="0020">In the image input unit 110, a digital image including a face area of a person as a target of feature detection is acquired by a digital camera, a scanner, or the like.</p><heading id="h0008">(2-2) Step A2</heading><p id="p0021" num="0021">In the correspondence relation acquiring unit 120, a feature points other than the mouth center, which are referred to as reference feature points, are acquired from the facial image acquired by the image input unit 110.<!-- EPO <DP n="9"> --> The reference feature points may include pupils, nasal spouts, mouth ends, for example. However, the position and the number of the reference feature points are not limited as long as there are at least four points in different planes. As a method of acquiring the reference feature points, they may be acquired by a manual input in which the positions are specified manually by a mouse, or by an automatic detecting method as disclosed in Japanese Patent No. <patcit id="pcit0004" dnum="JP3279913B"><text>3279913</text></patcit> being incorporated herein by reference.</p><heading id="h0009">(2-3) Step A3</heading><p id="p0022" num="0022">When the right and left mouth end points are acquired as the reference feature points here, the mouth center can be detected as a midpoint of the both left and right mouth endpoints by a method disclosed in the above-described Japanese Application Kokai No.<patcit id="pcit0005" dnum="JP2003187247A"><text>2003-187247</text></patcit>, the entire contents of which are incorporated herein by reference. As shown in <figref idrefs="f0003">Fig. 3</figref>, when the face is oriented to the front, the coordinate of the mouth center almost match the coordinate of the midpoint between the mouth ends. However, when the orientation of the face is changed, the coordinate of the mouth center does not match the coordinate of the midpoint between the mouth ends. When the facial expression is changed such as smile, the shape of the mouth is changed into an arcuate shape, so that it can easily be imagined that the coordinate of the<!-- EPO <DP n="10"> --> mouth center is different from the coordinate of the midpoint between the mouth ends.</p><p id="p0023" num="0023">Therefore, in this embodiment, a relation between ' the face image and the three-dimensional shape is calculated using the coordinates of the reference feature points on the acquired face image and the positions of the reference feature points on the face shape stored in the three-dimensional shape information holding unit 200.</p><p id="p0024" num="0024">Firstly, as shown in the upper left in <figref idrefs="f0004">Fig. 4</figref>, three-dimensional shape information on a face and positions of the reference feature points on the three-dimensional shape are prepared in the three-dimensional shape information holding unit 200. The three-dimensional shape information of the face may be obtained by measuring the three-dimensional shape of a person in the input image, or may be a representative three-dimensional shape of the face obtained, for example, by averaging several three-dimensional shapes or by preparing with modeling software.</p><p id="p0025" num="0025">Subsequently, with a method disclosed in Japanese Application Kokai <patcit id="pcit0006" dnum="JP2003141552A"><text>2003-141552</text></patcit> using factorization, the entire contents of which are incorporated herein by reference, a movement matrix M which represents the correspondence relation is calculated from a shape matrix S in which the positions of the reference feature points on the three-dimensional shape information are arranged<!-- EPO <DP n="11"> --> and a measurement matrix W in which the positions of the reference feature points on the input image are arranged.</p><p id="p0026" num="0026">For example, when six feature points are obtained from the input image of the face as shown in <figref idrefs="f0004">Fig. 4</figref>, when the coordinates of the six points are represented by vectors a1, a2, ... a6, the measurement matrix W is W=[a1, a2, ...a6], that is, a matrix of 2 × 6. When the coordinates of the positions of the feature points on the corresponding three-dimensional model are represented by vectors b1, b2, ...b6, the shape matrix S is S=[b1, b2,...b6], that is, a matrix of 3 × 6. Therefore, the movement matrix M representing the correspondence relation between the three-dimensional model and the input face image is a matrix of 2 × 3 which can be calculated from an expression (1). <maths id="math0001" num="(1)"><math display="block"><mtable><mtr><mtd><mi mathvariant="normal">M</mi><mo>=</mo><msup><mi>WS</mi><mi mathvariant="normal">T</mi></msup></mtd><mtd><msup><mfenced><msup><mi>SS</mi><mi mathvariant="normal">T</mi></msup></mfenced><mrow><mo>-</mo><mn mathvariant="normal">1</mn></mrow></msup></mtd></mtr></mtable></math><img id="ib0001" file="imgb0001.tif" wi="51" he="10" img-content="math" img-format="tif"/></maths></p><heading id="h0010">(2-4) Step A4</heading><p id="p0027" num="0027">When the obtained movement matrix M is used, the position "a" of the point on the two-dimensional image (two-dimensional vector) corresponding to an arbitrary point "b" on the three-dimensional shape (three-dimensional vector) can be calculated from an expression (2). <maths id="math0002" num="(2)"><math display="block"><mi mathvariant="normal">a</mi><mo>=</mo><mi>Mb</mi></math><img id="ib0002" file="imgb0002.tif" wi="32" he="10" img-content="math" img-format="tif"/></maths></p><p id="p0028" num="0028">Here, it is also possible to fix the position of the<!-- EPO <DP n="12"> --> mouth center on the input image uniquely by projecting the coordinate of the mouth center on the three-dimensional shape onto the input image. However, when a representative three dimensional shape of the face is used as the three-dimensional shape, there is no guarantee that it is projected accurately on the mouth center due to the difference between the face of the person in the input image and the three-dimensional shape of the face. In addition, since the face is a non-rigid object, if the facial expression is changed, even when the three-dimensional shape of the person in question is used, there is no more guarantee that the position of the mouth center on the three-dimensional shape is projected accurately on the mouth center in the input image.</p><p id="p0029" num="0029">Therefore, in the seek area acquiring unit 130, as shown in <figref idrefs="f0005">Fig. 5</figref>, a position obtained by projecting the center line of the face on the three-dimensional shape onto the input image using the movement matrix M acquired by the correspondence relation acquiring unit 120 is determined as the seek area. Even when the three-dimensional shape is different from the actual shape of the person in the input image, the position of the mouth center is expected to be anywhere on the center line of the face. Even when the facial expression is changed, it seems that the mouth center exists on the centerline of the face.<!-- EPO <DP n="13"> --></p><heading id="h0011">(2-5) Step A5</heading><p id="p0030" num="0030">Lastly, in the feature point detecting unit 140, as shown in <figref idrefs="f0005">Fig. 5</figref>, brightness of pixels existing on the center line of the face (that is, an axis of lateral symmetry of the face) in the input face image are expressed into a one-dimensional waveform, and from this waveform, the position of the mouth center is detected.</p><p id="p0031" num="0031">As the method of detecting the mouth center form the acquired one-dimensional waveform, for example, there is a method shown in <figref idrefs="f0006">Fig. 6</figref>. Firstly, brightness array is filtered by a Laplacian of Gaussian filter (LoG filter). The mouth center falls within the mouth area, and it can be imagined that the brightness of the mouth area is lower than the skin color around the mouth. Therefore, a position having the value which is equal to or smaller than zero and represents the minimum value is acquired from the waveform after having filtered by the Laplacian filter and the Gaussian filter as a candidate of the position of the mouth center.</p><heading id="h0012">(2-6) Step A6</heading><p id="p0032" num="0032">When no candidate of the mouth center is obtained in the mouth center candidate acquisition, the procedure may be terminated by defining as undetectable, or may be terminated after having outputted the midpoint between the mouth ends as the position of the mouth center if the<!-- EPO <DP n="14"> --> mouth ends are included in the reference feature points.</p><heading id="h0013">(2-7) Step A7</heading><p id="p0033" num="0033">When there is a plurality of candidates of mouth center in the mouth center candidate acquisition, an evaluation value which represents a possibility of the mouth center is calculated at each mouth center candidate. For example, in this case, an inverse number of the value at the candidate position on the waveform after having filtered by the Laplacian of Gaussian filter is employed as the evaluation value.</p><heading id="h0014">(2-8) Step A8</heading><p id="p0034" num="0034">The point whose evaluation value is the highest out of the evaluation values at the respective mouth center candidates is determined as an initial position of the mouth center.</p><heading id="h0015">(2-9) Step A9</heading><p id="p0035" num="0035">Since the initial position of the detected mouth center is obtained from the waveform after having filtered by the Laplacian of Gaussian filter, there is an error between the original waveform and the minimum value. A scale parameter of the Gaussian filter is reduced stepwise, the initial position of the mouth center is shifted to a position of the closest local minimum value in each step, and the final position of the mouth center is determined.</p><heading id="h0016">(3) Effect</heading><!-- EPO <DP n="15"> --><p id="p0036" num="0036">In this manner, with the apparatus for detecting a feature point according to the embodiment, the mouth center can be detected corresponding to the change of the orientation of the face or the change of the facial expression by estimating the correspondence relation between the input image and the three-dimensional shape information and detecting the feature points after defining the position on the centerline of the face as the search range.</p><heading id="h0017">(4) Modification</heading><p id="p0037" num="0037">The feature point to be detected in this embodiment is not limited to the mouth center.</p><p id="p0038" num="0038">For example, points of the upper and lower lip contours on the centerline of the face can be obtained. Detection of the points on the upper and lower lip contour on the centerline of the face can be realized by finding intersections with zero on the waveform of the brightness array on the center line of the face and filtering the same by the Laplacian of Gaussian filter, and detecting closest two intersections with zero with the intermediary of the mouth center therebetween.</p><p id="p0039" num="0039">A nose root, which is a nose root point, can be detected. Detection of the nose root point is realized by detecting the positions of both pupils in advance, obtaining the brightness array on the centerline of the<!-- EPO <DP n="16"> --> face and filtering the same by the Laplacian of Gaussian filter to find the minimum value in the brightness array on the centerline of the face as the nose root candidate points, and extracting the candidate point which is the closest to the center of gravity of the positions of the both pupils.</p><p id="p0040" num="0040">Example not pertaining to the invention</p><p id="p0041" num="0041">The apparatus for detecting a feature point according to an example not pertaining to the invention will be described. In this example, a heart is employed as a target object, and will be described on the basis of an X-ray image as the two-dimensional image measured the heart by an X-ray cardiography.</p><p id="p0042" num="0042">The x-ray image of the heart is a projected image obtained, for example, by imaging blood vessels using iodine preparation which is a positive contrast agent in the target blood vessels. In the case of the coronary angiography, arterial surrounding the heart is observed using the contrast agent. In this case, the branch points of the thick main blood vessels can be detected easily as the reference feature points. On the other hand, positions of branch points of the thin blood vessels, or branch points which cannot be imaged due to thrombus cannot be estimated easily from the observed X-ray image. Therefore, this example is intended to detect the<!-- EPO <DP n="17"> --> feature points of the branch points of the blood vessels.</p><heading id="h0018">(1) Configuration of the apparatus for detecting a feature point</heading><p id="p0043" num="0043">The apparatus for detecting a feature point in this example is the same as that in the embodiment, and includes the image input unit 110 that acquires an X-ray image to be processed, the three-dimensional shape information holding unit 200 that holds the average three-dimensional shape model of a heart and coronary artery, a correspondence relation acquiring unit 120 that estimates the correspondence relation between the X-ray image and the three-dimensional shape of the heart, a seek area acquiring unit 130 that sets a seek area for the branch points of the blood vessels on the input image using the acquired correspondence relation, and a feature point detecting unit 140 that determines the positions of the branch points of the blood vessels from the seek area.</p><p id="p0044" num="0044">The functions of these units 110 to 140 and 200 are implemented by a program stored in the computer.</p><heading id="h0019">(2) Operation of the apparatus for detecting a feature point</heading><p id="p0045" num="0045">Subsequently, the operation of the apparatus for detecting a feature point will be described.</p><p id="p0046" num="0046">Firstly, the image input unit 110 input an X-ray image.<!-- EPO <DP n="18"> --></p><p id="p0047" num="0047">Then, the correspondence relation acquiring unit 120 detects at least four branch points of thick blood vessels which can be detected easily as reference feature points from the X-ray image obtained by the image input unit 110, and estimates the correspondence relation between the heart and the coronary artery with respect to the three-dimensional shape model. The method described in the embodiment may be employed as the method of estimation.</p><p id="p0048" num="0048">Then, as regards the feature points which are difficult to detect from the X-ray image, the seek area acquiring unit 130 projects the positions of the feature points on the model which are difficult to detect and the three-dimensional plane area on the periphery thereof on the two-dimensional plane area of the X-ray image.</p><p id="p0049" num="0049">Subsequently, the projected plane area of the X-ray image corresponds to the seek area for the feature points, and the correction of brightness to the optimal value or the like is performed in the seek area. This is for facilitating the detection from the next step on.</p><p id="p0050" num="0050">Finally, in the feature point detecting unit 140, the positions which seem to be the branches of the blood vessels in the seek area by the corner detection method are specified by the edge detection.</p><p id="p0051" num="0051">In this manner, with the apparatus for detecting a<!-- EPO <DP n="19"> --> feature point according to the example, by estimating the correspondence relation between the X-ray image and the three-dimensional shape information and determining the planner small area on the X-ray image obtained by projecting the planer small area including the feature points on the three-dimensional shape as the seek area, the feature points which have a small range of change in brightness and hence is difficult to specify the position can be detected.</p><heading id="h0020">Modifications</heading><p id="p0052" num="0052">The present invention is not limited to the above-described embodiment, and components may be modified and embodied without departing the scope of the invention as claimed.</p></description><claims mxw-id="PCLM56979584" lang="DE" load-source="patent-office"><!-- EPO <DP n="24"> --><claim id="c-de-01-0001" num="0001"><claim-text>Vorrichtung zum Ermitteln eines spezifischen Merkmalpunktes aus einem Bild, das ein menschliches Gesicht einschließt, umfassend:
<claim-text>eine Bildeingabeeinheit (110), die dafür konfiguriert ist, ein Bild des menschlichen Gesichts einzugeben;</claim-text>
<claim-text>ein Dreidimensionalform-Informationsspeichereinheit (200), die dafür konfiguriert ist, dreidimensionale Forminformation eines Modells des menschlichen Gesichts und dreidimensionale Positionsinformation von Referenzmerkmalpunkten in dem Modell zu speichern, wobei die Referenzmerkmalpunkte sich jeweils vom spezifischen Merkmalpunkt unterscheiden;</claim-text>
<claim-text>eine Eingabeeinheit für entsprechende Referenzpunkte, die dafür konfiguriert ist, Positionsinformation der entsprechenden Referenzmerkmalpunkte auf dem eingegebenen Bild automatisch oder manuell einzugeben;</claim-text>
<claim-text>eine Erfassungseinheit für entsprechende Merkmalpunkte, die dafür konfiguriert ist, dreidimensionale Positionsinformation desjenigen Merkmalpunkts, der dem spezifischen Merkmalpunkt im Modell entspricht, zu erfassen;</claim-text>
<claim-text>eine Korrespondenzbeziehungserfassungseinheit (120), die dafür konfiguriert ist, eine Korrespondenzbeziehung zwischen dem eingegebenen Bild und der dreidimensionalen Forminformation unter Verwendung der Positionsinformation der Referenzmerkmalpunkte auf dem eingegebenen Bild und der dreidimensionalen Positionsinformation der Referenzmerkmalpunkte des Modells zu erfassen;</claim-text>
<claim-text>eine Durchsuchungsbereichfestlegungseinheit (130), die dafür konfiguriert ist, einen Durchsuchungsbereich für den spezifischen Merkmalpunkt auf dem eingegebenen Bild durch Projizieren eines entsprechenden Bereichs, der den entsprechenden Merkmalpunkt auf der Oberfläche des Modells enthält, auf das eingegebene Bild auf der Grundlage der Korrespondenzbeziehung festzulegen;</claim-text>
<claim-text>eine Bildinformationserfassungseinheit, die dafür konfiguriert ist, Bildinformation des Durchsuchungsbereichs aus dem eingegebenen Bild zu erfassen; und</claim-text>
<claim-text>eine Merkmalpunktermittlungseinheit (140), die dafür konfiguriert ist, die Position des spezifischen Merkmalpunkts im eingegebenen Bild aus der Bildinformation zu ermitteln;</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b>:
<claim-text>der entsprechende Bereich eine Mittellinie des menschlichen Gesichts ist,</claim-text><claim-text>die Bildinformation eine eindimensionale Wellenform ist, in der die Helligkeit von Bildpunkten, die auf der Mittellinie des menschlichen Gesichts im eingegebenen Bild existieren, ausgedrückt wird,<!-- EPO <DP n="25"> --></claim-text><claim-text>der entsprechende Merkmalpunkt eine Mundmitte ist, die dem Mittelpunkt zwischen dem linken Ende und dem rechten Ende der Lippe des Modells des menschlichen Gesichts entspricht, und</claim-text><claim-text>die Merkmalpunktermittlungseinheit dafür konfiguriert ist, in der eindimensionalen Wellenform eine Position mit der geringsten Helligkeit als eine Position der Mundmitte zu ermitteln.</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Vorrichtung nach Anspruch 1,<br/>
worin die Merkmalpunktermittlungseinheit (140) dafür konfiguriert ist, eine Helligkeitsmatrix in der eindimensionalen Wellenform durch einen Laplace-Operator eines Gauß-Filters zu filtern und aus der Wellenform, nachdem sie gefiltert worden ist, eine Position mit einem Wert, der gleich oder kleiner als null ist und einen Minimalwert darstellt, als die Position der Mundmitte zu ermitteln.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Vorrichtung nach Anspruch 1,<br/>
worin die Merkmalpunktermittlungseinheit dafür konfiguriert ist, eine Helligkeitsmatrix in der eindimensionalen Wellenform durch einen Laplace-Operator eines Gauß-Filters zu filtern, um aus der Wellenform, nachdem sie gefiltert worden ist, Nulldurchgänge zu finden, und ferner dafür, die beiden am engsten beieinanderliegenden Nulldurchgänge mit dazwischenliegender Mundmitte als Punkte auf einer oberen und unteren Lippenkontur auf der Mittellinie des menschlichen Gesichts zu ermitteln.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Vorrichtung nach Anspruch 1,<br/>
worin die Merkmalpunktermittlungseinheit ferner dafür konfiguriert ist, die Positionen beider Pupillen in dem menschlichen Gesicht zu ermitteln, eine Helligkeitsmatrix in der eindimensionalen Wellenform durch einen Laplace-Operator eines Gauß-Filters zu filtern, um aus der Wellenform, nachdem sie gefiltert worden ist, die Position mit einem Minimalwert als Nasenwurzel-Kandidatenpunkte zu finden, und um ferner denjenigen Nasenwurzel-Kandidatenpunkt, der dem Schwerpunkt der Positionen der beiden Pupillen am nächsten liegt, als eine Nasenwurzel zu ermitteln.</claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Verfahren zum Ermitteln eines spezifischen Merkmalpunktes aus einem Bild, das ein menschliches Gesicht einschließt, umfassend:
<claim-text>Eingeben eines Bildes des menschlichen Gesichts;</claim-text>
<claim-text>Speichern dreidimensionaler Forminformation eines Modells des menschlichen Gesichts und dreidimensionaler Positionsinformation von Referenzmerkmalpunkten in dem Modell, wobei die Referenzmerkmalpunkte sich jeweils vom spezifischen Merkmalpunkt unterscheiden;</claim-text>
<claim-text>automatisches oder manuelles Eingeben von Positionsinformation der entsprechenden Referenzmerkmalpunkte auf dem eingegebenen Bild;</claim-text>
<claim-text>Erfassen dreidimensionaler Positionsinformation desjenigen Merkmalpunkts, der dem spezifischen Merkmalpunkt im Modell entspricht;<!-- EPO <DP n="26"> --></claim-text>
<claim-text>Erfassen einer Korrespondenzbeziehung zwischen dem eingegebenen Bild und der dreidimensionalen Forminformation unter Verwendung der Positionsinformation der Referenzmerkmalpunkte auf dem eingegebenen Bild und der dreidimensionalen Positionsinformation der Referenzmerkmalpunkte des Modells;</claim-text>
<claim-text>Festlegen eines Durchsuchungsbereichs für den spezifischen Merkmalpunkt auf dem eingegebenen Bild durch Projizieren eines entsprechenden Bereichs, der den entsprechenden Merkmalpunkt auf der Oberfläche des Modells enthält, auf das eingegebene Bild auf der Grundlage der Korrespondenzbeziehung;</claim-text>
<claim-text>Erfassen von Bildinformation des Durchsuchungsbereichs aus dem eingegebenen Bild; und</claim-text>
<claim-text>Ermitteln der Position des spezifischen Merkmalpunkts im eingegebenen Bild aus der Bildinformation;</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b>:
<claim-text>der entsprechende Bereich eine Mittellinie des menschlichen Gesichts ist,</claim-text><claim-text>die Bildinformation eine eindimensionale Wellenform ist, in der die Helligkeit von Bildpunkten, die auf der Mittellinie des menschlichen Gesichts im eingegebenen Bild existieren, ausgedrückt wird,</claim-text><claim-text>der entsprechende Merkmalpunkt eine Mundmitte ist, die dem Mittelpunkt zwischen dem linken Ende und dem rechten Ende der Lippe des Modells des menschlichen Gesichts entspricht, und</claim-text><claim-text>in der eindimensionalen Wellenform eine Position mit der geringsten Helligkeit als eine Position der Mundmitte ermittelt wird.</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Verfahren nach Anspruch 5,<br/>
worin eine Helligkeitsmatrix in der eindimensionalen Wellenform durch einen Laplace-Operator eines Gauß-Filters gefiltert wird und aus der Wellenform, nachdem sie gefiltert worden ist, eine Position mit einem Wert, der gleich oder kleiner als null ist und einen Minimalwert darstellt, als die Position der Mundmitte ermittelt wird.</claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Verfahren nach Anspruch 5,<br/>
worin eine Helligkeitsmatrix in der eindimensionalen Wellenform durch einen Laplace-Operator eines Gauß-Filters gefiltert wird, Nulldurchgänge aus der Wellenform nach deren Filtern gefunden werden, und ferner die beiden am engsten beieinanderliegenden Nulldurchgänge mit dazwischenliegender Mundmitte als Punkte auf einer oberen und unteren Lippenkontur auf der Mittellinie des menschlichen Gesichts ermittelt werden.</claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Verfahren nach Anspruch 5,<br/>
worin ferner die Positionen beider Pupillen in dem menschlichen Gesicht ermittelt werden, eine Helligkeitsmatrix in der eindimensionalen Wellenform durch einen Laplace-Operator<!-- EPO <DP n="27"> --> eines Gauß-Filters gefiltert wird, eine Position mit einem Minimalwert aus der Wellenform nach deren Filtern als Nasenwurzel-Kandidatenpunkte gefunden wird, und ferner derjenige Nasenwurzel-Kandidatenpunkt, der dem Schwerpunkt der Positionen der beiden Pupillen am nächsten liegt, als eine Nasenwurzel ermittelt wird.</claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Programm zum Ermitteln eines spezifischen Merkmalpunktes aus einem Bild, das ein menschliches Gesicht einschließt, durch einen Computer, wobei das Programm Anweisungen zum Bewirken, dass der Computer das Verfahren nach einem der Ansprüche 5 bis 8 durchführt, umfasst.</claim-text></claim></claims><claims mxw-id="PCLM56979585" lang="EN" load-source="patent-office"><!-- EPO <DP n="20"> --><claim id="c-en-01-0001" num="0001"><claim-text>An apparatus for detecting a specific feature point from an image including a human face comprising:
<claim-text>an image input unit (110) configured to input an image of the human face;</claim-text>
<claim-text>a three-dimensional shape information storing unit (200) configured to store three-dimensional shape information of a model of the human face and three-dimensional position information of reference feature points in the model, the reference feature points each differing from the specific feature point;</claim-text>
<claim-text>a correspondent reference point input unit configured to input position information of the correspondent reference feature points on the input image automatically or manually;</claim-text>
<claim-text>a correspondent feature point acquiring unit configured to acquire three dimensional position information of the correspondent feature point of the specific feature point in the model;</claim-text>
<claim-text>a correspondence relation acquiring unit (120) configured to acquire a correspondence relation between the input image and the three-dimensional shape information using the position information of the reference feature points on the input image and the three-dimensional position information of the reference feature points of the model;</claim-text>
<claim-text>a seek area setting unit (130) configured to set a seek area for the specific feature point on the input image by projecting a correspondent area containing the correspondent feature point on surface of the model to the input image based on the correspondence relation;</claim-text>
<claim-text>an image information acquiring unit configured to acquire image information of the seek area from the input image; and</claim-text>
<claim-text>a feature point detecting unit (140) configured to detect the position of the specific feature point in the input image from the image information; <b>characterized by</b></claim-text>
<claim-text>wherein the correspondent area is a center line of the human face,</claim-text>
<claim-text>the image information is a one-dimensional waveform into which brightness of pixels existing on the center line of the human face in the input image is expressed,</claim-text>
<claim-text>the correspondent feature point is a mouth center which corresponds to the midpoint between the left end and the right end of the lip of the model of the human face, and</claim-text>
<claim-text>the feature point detecting unit is configured to detect a position having lowest brightness in the one-dimensional waveform as a position of the mouth center.</claim-text><!-- EPO <DP n="21"> --></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>The apparatus of Claim 1,<br/>
wherein the feature point detecting unit (140) is configured to filter a brightness array in the one-dimensional waveform by a Laplacian of a Gaussian filter and to detect a position having a value which is equal to or smaller than zero and represents a minimum value from the waveform after having been filtered as the position of the mouth center.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>The apparatus of Claim 1,<br/>
wherein the feature point detecting unit is configured to filter a brightness array in the one-dimensional waveform by a Laplacian of a Gaussian filter, to find intersections with zero from the waveform after having been filtered, and further to detect the closest two intersections with zero with an intermediary of the mouth center therebetween as points on an upper and lower lip contour on the center line of the human face.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>The apparatus of Claim 1,<br/>
wherein the feature point unit is configured further to detect the positions of both pupils in the human face, to filter a brightness array in the one-dimensional waveform by a Laplacian of a Gaussian filter, to find the position having a minimum value from the waveform after having been filtered as nose root candidate points, and further to detect the nose root candidate point which is closest to the center of gravity of the positions of the both pupils as a nose root.</claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>A method of detecting a specific feature point from an image including a human face, comprising:
<claim-text>inputting an image of the human face;</claim-text>
<claim-text>storing three-dimensional shape information of a model of the human face and three-dimensional position information of reference feature points in the model, the reference feature points each differing from the specific feature point;</claim-text>
<claim-text>inputting position information of the correspondent reference feature points on the input image automatically or manually;</claim-text>
<claim-text>acquiring three-dimensional position information of the correspondent feature point of the specific feature point<!-- EPO <DP n="22"> --> in the model;</claim-text>
<claim-text>acquiring a correspondence relation between the input image and the three-dimensional shape information using the position information of the reference feature points on the input image and the three-dimensional position information of the reference feature points of the model;</claim-text>
<claim-text>setting a seek area for the specific feature point on the input image by projecting a correspondent area containing the correspondent feature point on surface of the model to the input image based on the correspondence relation;</claim-text>
<claim-text>acquiring image information of the seek area from the input image; and</claim-text>
<claim-text>detecting the position of the specific feature point in the input image from the image information; <b>characterized by</b></claim-text>
<claim-text>wherein the correspondent area is a center line of the human face,</claim-text>
<claim-text>the image information is a one-dimensional waveform into which brightness of pixels existing on the center line of the human face in the input image Is expressed,</claim-text>
<claim-text>the correspondent feature point is a mouth center which corresponds to the midpoint between the left end and the right end of the lip of the model of the human face, and</claim-text>
<claim-text>a position having lowest brightness in the one-dimensional waveform is detected as a position of the mouth center.</claim-text></claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>The method of Claim 5,<br/>
wherein a brightness array in one-dimensional waveform is filtered by a Laplacian of a Gaussian filter, and a position having a value which is equal to or smaller than zero and represents a minimum value is detected from the waveform after having been filtered as the position of the mouth center.</claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>The method of Claim 5,<br/>
wherein a brightness array in the one-dimensional waveform is filtered by a Laplacian of a Gaussian filter, intersections with zero are found from the waveform after having been filtered, and the closest two intersections with zero with an intermediary of the mouth center therebetween are further detected as points on an upper and lower lip contour on the center line of the human face.</claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>The method of Claim 5,<br/>
<!-- EPO <DP n="23"> -->wherein the positions of both pupils in the human face are further detected, a brightness array in the one-dimensional waveform is filtered by a Laplacian of a Gaussian filter, a position having a minimum value is found from the waveform after having been filtered as nose root candidate points, and the nose root candidate point which is closest to the center of gravity of the positions of the both pupils is further detected as a nose root.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>A program for detecting a specific feature point by a computer from an image including a human face, the program comprising instructions for causing the computer to execute the method of any of claims 5 to 8.</claim-text></claim></claims><claims mxw-id="PCLM56979586" lang="FR" load-source="patent-office"><!-- EPO <DP n="28"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Appareil pour la détection d'un point caractéristique spécifique à partir d'une image contenant un visage humain, comprenant :
<claim-text>une unité d'entrée d'image (110) conçue pour entrer une image du visage humain ;</claim-text>
<claim-text>une unité de stockage d'informations de forme tridimensionnelle (200) conçue pour stocker des informations de forme tridimensionnelle d'un modèle du visage humain et des informations de position tridimensionnelle de points caractéristiques de référence dans le modèle, les points caractéristiques de référence différant chacun du point caractéristique spécifique ;</claim-text>
<claim-text>une unité d'entrée de point de référence correspondant conçue pour entrer des informations de position des points caractéristiques de référence correspondants sur l'image d'entrée de façon automatique ou manuelle ;</claim-text>
<claim-text>une unité d'acquisition de point caractéristique correspondant conçue pour acquérir des informations de position tridimensionnelle du point caractéristique correspondant du point caractéristique spécifique dans le modèle ;</claim-text>
<claim-text>une unité d'acquisition de relation de correspondance (120) conçue pour acquérir une relation de correspondance entre l'image d'entrée et les informations de forme tridimensionnelle en utilisant les informations de position des points caractéristiques de référence sur l'image d'entrée et les informations de position tridimensionnelle des points caractéristiques de référence du modèle ;</claim-text>
<claim-text>une unité de définition de région de recherche (130) conçue pour définir une région de recherche pour le point caractéristique spécifique sur l'image d'entrée en projetant une région correspondante contenant le point caractéristique correspondant sur la surface du modèle sur l'image d'entrée en fonction de la relation de correspondance ;</claim-text>
<claim-text>une unité d'acquisition d'informations d'image conçue pour acquérir des informations d'image de la région de recherche à partir de l'image d'entrée ; et</claim-text>
<claim-text>une unité de détection de point caractéristique (140) conçue pour détecter la position du point caractéristique spécifique dans l'image d'entrée à partir des informations d'image ;</claim-text>
<claim-text><b>caractérisé en ce que</b></claim-text>
<claim-text>la région correspondante est l'axe médian du visage humain,</claim-text>
<claim-text>les informations d'image sont une forme d'onde monodimensionnelle dans laquelle la luminosité de pixels existant sur l'axe médian du visage humain dans l'image d'entrée est exprimée,</claim-text>
<claim-text>le point caractéristique correspondant est le centre de la bouche qui correspond au point médian entre l'extrémité gauche et l'extrémité droite de la lèvre du modèle du visage humain, et<!-- EPO <DP n="29"> --></claim-text>
<claim-text>l'unité de détection de point caractéristique est conçue pour détecter une position ayant la luminosité la plus faible dans la forme d'onde monodimensionnelle comme étant la position du centre de la bouche.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Appareil selon la revendication 1,<br/>
dans lequel l'unité de détection de point caractéristique (140) est conçue pour filtrer un réseau de luminosité dans la forme d'onde monodimensionnelle par un Laplacien d'un filtre gaussien et pour détecter une position ayant une valeur qui est inférieure ou égale à zéro et représente une valeur minimale à partir de la forme d'onde une fois celle-ci filtrée comme étant la position du centre de la bouche.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Appareil selon la revendication 1,<br/>
dans lequel l'unité de détection de point caractéristique est conçue pour filtrer un réseau de luminosité dans la forme d'onde monodimensionnelle par un Laplacien d'un filtre gaussien, pour trouver des croisements avec zéro dans la forme d'onde une fois celle-ci filtrée, et pour détecter en outre les deux croisements avec zéro les plus proches entre lesquels se trouve un intermédiaire du centre de la bouche comme étant des points d'un contour de la lèvre supérieure et de la lèvre inférieure sur l'axe médian du visage humain.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Appareil selon la revendication 1,<br/>
dans lequel l'unité de détection de point caractéristique est conçue en outre pour détecter la position des deux pupilles du visage humain, pour filtrer un réseau de luminosité dans la forme d'onde monodimensionnelle par un Laplacien d'un filtre gaussien, pour trouver la position ayant une valeur minimale à partir de la forme d'onde une fois celle-ci filtrée comme étant de possibles points de la racine du nez, et pour détecter en outre le point possible de la racine du nez qui est le plus proche du centre de gravité des positions des deux pupilles comme étant la racine du nez.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Procédé pour la détection d'un point caractéristique spécifique à partir d'une image contenant un visage humain, comprenant :
<claim-text>l'entrée d'une image du visage humain ;</claim-text>
<claim-text>le stockage d'informations de forme tridimensionnelle d'un modèle du visage humain et d'informations de position tridimensionnelle de points caractéristiques de référence dans le modèle, les points caractéristiques de référence différant chacun du point caractéristique spécifique ;</claim-text>
<claim-text>l'entrée d'informations de position des points caractéristiques de référence correspondants sur l'image d'entrée de façon automatique ou manuelle ;</claim-text>
<claim-text>l'acquisition d'informations de position tridimensionnelle du point caractéristique correspondant du point caractéristique spécifique dans le modèle ;</claim-text>
<claim-text>l'acquisition d'une relation de correspondance entre l'image d'entrée et les informations de forme tridimensionnelle au moyen des informations de position des<!-- EPO <DP n="30"> --> points caractéristiques de référence sur l'image d'entrée et des informations de position tridimensionnelle des points caractéristiques de référence du modèle ;</claim-text>
<claim-text>la définition d'une région de recherche pour le point caractéristique spécifique sur l'image d'entrée par projection d'une région correspondante contenant le point caractéristique correspondant sur la surface du modèle sur l'image d'entrée en fonction de la relation de correspondance ;</claim-text>
<claim-text>l'acquisition d'informations d'image de la région de recherche à partir de l'image d' entrée ; et</claim-text>
<claim-text>la détection de la position du point caractéristique spécifique dans l'image d'entrée à partir des informations d'image ;</claim-text>
<claim-text><b>caractérisé en ce que</b></claim-text>
<claim-text>la région correspondante est l'axe médian du visage humain,</claim-text>
<claim-text>les informations d'image sont une forme d'onde monodimensionnelle dans laquelle la luminosité de pixels existant sur l'axe médian du visage humain dans l'image d'entrée est exprimée,</claim-text>
<claim-text>le point caractéristique correspondant est le centre de la bouche qui correspond au point médian entre l'extrémité gauche et l'extrémité droite de la lèvre du modèle du visage humain, et</claim-text>
<claim-text>une position ayant la luminosité la plus faible dans la forme d'onde monodimensionnelle est détectée comme étant la position du centre de la bouche.</claim-text></claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Procédé selon la revendication 5,<br/>
dans lequel un réseau de luminosité dans une forme d'onde monodimensionnelle est filtré par un Laplacien d'un filtre gaussien, et une position ayant une valeur qui est inférieure ou égale à zéro et représente une valeur minimale est détectée à partir de la forme d'onde une fois celle-ci filtrée comme étant la position du centre de la bouche.</claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Procédé selon la revendication 5,<br/>
dans lequel un réseau de luminosité dans la forme d'onde monodimensionnelle est filtré par un Laplacien d'un filtre gaussien, des croisements avec zéro sont trouvés à partir de la forme d'onde une fois celle-ci filtrée, et les deux croisements avec zéro les plus proches entre lesquels se trouve un intermédiaire du centre de la bouche sont en outre détectés comme étant des points d'un contour de la lèvre supérieure et de la lèvre inférieure sur l'axe médian du visage humain.</claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Procédé selon la revendication 5,<br/>
dans lequel les positions des deux pupilles du visage humain sont en outre détectées, un réseau de luminosité dans la forme d'onde monodimensionnelle est filtré par un Laplacien d'un filtre gaussien, une position ayant une valeur minimale est trouvée à partir de la forme d'onde une fois celle-ci filtrée comme étant de possibles points de la racine du nez, et le point possible de la racine du nez qui est le plus proche du centre de gravité des positions des deux pupilles est détecté en outre comme étant la racine du nez.<!-- EPO <DP n="31"> --></claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Programme pour la détection d'un point caractéristique spécifique par un ordinateur à partir d'une image contenant un visage humain, le programme comprenant des instructions destinées à faire effectuer par l'ordinateur le procédé selon l'une quelconque des revendications 5 à 8.</claim-text></claim></claims><drawings mxw-id="PDW16668909" load-source="patent-office"><!-- EPO <DP n="32"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="138" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="222" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="158" he="218" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="165" he="228" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="165" he="225" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="165" he="225" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
