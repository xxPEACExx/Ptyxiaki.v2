<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-1923866-B1" country="EP" doc-number="1923866" kind="B1" date="20140101" family-id="37727469" file-reference-id="287183" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146553575" ucid="EP-1923866-B1"><document-id><country>EP</country><doc-number>1923866</doc-number><kind>B1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-06782692-A" is-representative="YES"><document-id mxw-id="PAPP154827498" load-source="docdb" format="epo"><country>EP</country><doc-number>06782692</doc-number><kind>A</kind><date>20060811</date><lang>JA</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140453644" ucid="JP-2005233195-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2005233195</doc-number><kind>A</kind><date>20050811</date></document-id></priority-claim><priority-claim mxw-id="PPC140454752" ucid="JP-2006315953-W" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2006315953</doc-number><kind>W</kind><date>20060811</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130731</date></intention-to-grant-date><search-report-dispatch-date><date>20091207</date></search-report-dispatch-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988113519" load-source="ipcr">G10L  15/20        20060101ALI20130715BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1988129558" load-source="ipcr">G10L  21/0272      20130101AFI20130715BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1988131118" load-source="ipcr">H04R   1/40        20060101ALI20130715BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1988138617" load-source="ipcr">G10L  21/0216      20130101ALN20130715BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1988098305" load-source="docdb" scheme="CPC">G10L  15/20        20130101 FI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988102202" load-source="docdb" scheme="CPC">G10L  21/0272      20130101 LI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988110457" load-source="docdb" scheme="CPC">H04R   1/406       20130101 LI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988118179" load-source="docdb" scheme="CPC">H04R2499/11        20130101 LA20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988127686" load-source="docdb" scheme="CPC">G10L2021/02166     20130101 LA20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988128997" load-source="docdb" scheme="CPC">H04R2499/13        20130101 LA20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988136945" load-source="docdb" scheme="CPC">H04R2430/20        20130101 LA20130101BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132193209" lang="DE" load-source="patent-office">Schallquellen-Trenneinrichtung, Spracherkennungseinrichtung, tragbares Telefon, Schallquellen-Trennverfahren und Programm</invention-title><invention-title mxw-id="PT132193210" lang="EN" load-source="patent-office">Sound source separating device, speech recognizing device, portable telephone, sound source separating method, and program</invention-title><invention-title mxw-id="PT132193211" lang="FR" load-source="patent-office">Dispositif de séparation de source sonore, dispositif de reconnaissance de la parole, téléphone portable, procédé de séparation de son, et programme</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR918158151" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ASAHI CHEMICAL IND</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR918170903" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>ASAHI KASEI KABUSHIKI KAISHA</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918157380" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>NAGAHAMA KATSUMASA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918170648" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>NAGAHAMA, KATSUMASA</last-name></addressbook></inventor><inventor mxw-id="PPAR918997227" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>NAGAHAMA, KATSUMASA</last-name><address><street>1-2, Yuraku-cho 1-chome, Chiyoda-ku,</street><city>Tokyo, 1008440</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918155680" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>MATSUI SHINYA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR918141462" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>MATSUI, SHINYA</last-name></addressbook></inventor><inventor mxw-id="PPAR918997226" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>MATSUI, SHINYA</last-name><address><street>1-2, Yuraku-cho 1-chome, Chiyoda-ku,</street><city>Tokyo, 1008440</city><country>JP</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR918997229" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Asahi Kasei Kabushiki Kaisha</last-name><iid>100079532</iid><address><street>2-6, Dojimahama 1-chome Kita-ku</street><city>Osaka-shi Osaka 530-8205</city><country>JP</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR918997228" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>HOFFMANN EITLE</last-name><iid>100061036</iid><address><street>Patent- und Rechtsanwälte Arabellastrasse 4</street><city>81925 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="JP-2006315953-W"><document-id><country>JP</country><doc-number>2006315953</doc-number><kind>W</kind><date>20060811</date><lang>JA</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2007018293-A1"><document-id><country>WO</country><doc-number>2007018293</doc-number><kind>A1</kind><date>20070215</date><lang>JA</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS548976832" load-source="docdb">DE</country><country mxw-id="DS548978827" load-source="docdb">FR</country><country mxw-id="DS548976833" load-source="docdb">GB</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><description mxw-id="PDES63957279" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">Technical Field</heading><p id="p0001" num="0001">The present invention relates to a sound source separation device, a speech recognition device, a mobile telephone, a sound source separation method, and a program which uses a plurality of microphones and separates a sound source signal of a target sound source from a signal which consists of a plurality of acoustic signals emitted from a plurality of sound sources such as a plurality of speech signals and various background noises.</p><heading id="h0002">Background Art</heading><p id="p0002" num="0002">When a specific speech signal or the like is to be recorded under various environments, it is difficult to record only a signal of a target sound using a microphone since there are various noise sources in a peripheral environment, and therefore some sort of noise reduction processing or sound source separation processing is necessary.<br/>
<!-- EPO <DP n="2"> -->As an example where such processing is especially needed, there is an automotive environment. Under an automotive environment, due to the spread of mobile phones, a microphone installed far from speaker's mouth in a car is typical situation of a telephone call using a mobile phone during driving, and thereby speech quality is significantly degraded. In addition, also when speech recognition is performed during driving under an automotive environment, because utterance is performed in similar condition, speech recognition performance is degraded. The current progress in speech recognition technology makes speech recognition much more robust against stationary noise than before. However, the current speech recognition technology has a problem of degradation of recognition performance when a plurality of speakers speaks simultaneously. Since the current speech recognition technology does not provide high speech recognition accuracy for a mixed simultaneous speech of two speakers, a passenger other than a speaker is not allowed to speak during the speech recognition device is used. Under this situation, behavior of the passenger is restricted. Although a principal independent component analysis method or the like have applied to separate such sound sources, it is not practical enough because of computational complexity, variations of the number of sound sources, and the like.</p><p id="p0003" num="0003"><!-- EPO <DP n="3"> --> To solve the above described problems, various kinds of methods, which use a plurality of microphones installed in a car cabin and record only a speech from a certain direction, have been proposed. However, it is difficult to secure space for installing many microphones in a car cabin. Furthermore, it is also difficult to use microphones having uniform characteristics under the cost restriction. Therefore, a method is desired which allows the number of microphones to be as small as possible and uneven characteristic of the microphones.<br/>
Generally, when a plurality of microphones are used, the lower the cost of microphones, the larger the uneven sensitivity characteristics thereof, and it is said that uneven frequency characteristics is about ±3 dB. In processing of an addition type array such as a delay-and-sum array in microphone array technologies, these uneven characteristic only prevents microphone array performance to be achieved as designed. However, in a so-called subtraction type array such as an adaptive array, these uneven characteristic may degrade performance especially in low frequencies less than or equal to about 1 kHz compared to a case where one microphone is used.</p><p id="p0004" num="0004">Uneven characteristic of microphones as sensors are a critical problem for the microphone array technology. To solve this problem, methods for uniforming sensitivities of a plurality of microphone elements have been proposed in Patent Documents 1 to 5, etc.<br/>
<!-- EPO <DP n="4"> -->Conventionally, as for a microphone array that utilizes an adaptive beamforming processing technology which has a great effect with a few number of microphones, various methods such as a generalized sidelobe canceller (GSC), a Frost type beamformer, and a reference signal method have been known as described in, for example, Non-Patent Documents 1 and 2.</p><p id="p0005" num="0005">The adaptive beamforming processing is basically a processing to suppress noise by a filter which forms a directional beam having a directional null (blind spot) in a direction of a noise source, and a generalized sidelobe canceller is known to have relatively high performance especially among them. However, the GSC has a problem that a target signal is suppressed and degraded when the target signal is arrived from a direction different from a direction of a specified target signal source. To deal with this problem, Patent Documents 6 and 7 discloses a method which performs processing in a frequency domain so that a computational complexity is reduced, and a direction of a speaker and a direction of a specific noise are successively detected by using filter coefficients in the frequency domain, a target sound is separated from a noise other than the target sound to some extent, and spectral subtraction is applied as well, so that a noise from unknown arrival direction and a diffuse noise are reduced. The article "<nplcit id="ncit0001" npl-type="s"><text>Adaptive parameter compensation for robust hands-free speech recognition using a dual beamforming microphone array", McCowan J A et al, Proceedings of 2001 International Symposium on Intelligent Multimedia</text></nplcit>, Video and Speech Processing, discloses a speech recognition system that uses the outputs of a dual beamformer comprising signal beamformer and noise beamformer in order to perform parameter compensation.
<ul><li>[Patent Document 1] <patcit id="pcit0001" dnum="JP5131866A"><text>JP5-131866A</text></patcit><!-- EPO <DP n="5"> --></li><li>[Patent Document 2] <patcit id="pcit0002" dnum="JP2002099297A"><text>JP2002-99297A</text></patcit></li><li>[Patent Document 3] <patcit id="pcit0003" dnum="JP2003153372A"><text>JP2003-153372A</text></patcit></li><li>[Patent Document 4] <patcit id="pcit0004" dnum="JP2004343700A"><text>JP2004-343700A</text></patcit></li><li>[Patent Document 5] <patcit id="pcit0005" dnum="JP2004289762A"><text>JP2004-289762A</text></patcit></li><li>[Patent Document 6] <patcit id="pcit0006" dnum="JP2001100800A"><text>JP2001-100800A</text></patcit></li><li>[Patent Document 7] <patcit id="pcit0007" dnum="JP2000047699A"><text>JP2000-47699A</text></patcit></li><li>[Non-Patent Document 1] The Institute of Electronics, Information and Communication Engineers, "Acoustic Systems and Digital Processing"</li><li>[Non-Patent Document 2] HAykin, "ADAptive Filter Theory (Prentice HAll)"</li></ul></p><heading id="h0003">Disclosure of the Invention</heading><p id="p0006" num="0006">In the technology which combines the adaptive beamformer with the spectral subtraction as described in the Patent Documents 6 and 7, a reference signal, which is a suppressed target sound, is needs to be generated as an input signal of an adaptive filter section. If there is an uneven sensitivity of microphone elements, the target sound cannot be sufficiently suppressed and thus the target sound is contained in both the reference signal and a target signal in the adaptive filter section. Therefore the target sound is distorted and quality of the sound is degraded as a result of application of adaptive filter processing. Also in a closed space where an initial reflection is strong, a target sound is<!-- EPO <DP n="6"> --> leaked into a reference signal so that the similar phenomenon occurs.<br/>
To solve this problem, there is a method of compensating sensitivity of microphone elements, which measures a uneven characteristic of microphone elements in assembly process to generate data for compensation, and compensates the characteristic based on the data when the microphone elements are used, or which calibrates individual differences of sensitivities of microphone elements and successive changes caused by aging change of components or peripheral temperature using a reference signal from a specific direction while in use. However, there are problems about increase of production cost, when a reference signal should be played, how to determine whether or not a recorded signal contains only a reference signal, and so on.</p><p id="p0007" num="0007">Paying attention to operation of an adaptive filter, in general, if there is a high correlation between a reference signal and a target signal, estimation process of the adaptive filter cannot perform well and thereby estimation accuracy is significantly degraded. As an example, there is a case where a target sound and a noise are both speech signals.<br/>
A similar problem occurs in an echo canceller, and in an adaptive filter which estimates an echo mixed from a far-end signal into a near-end signal, when two persons speak simultaneously at far and near ends, a so-called<!-- EPO <DP n="7"> --> double-talk state occurs and thereby performance of the echo canceller is degraded. Therefore, it is an important condition to maintain the performance how precise a double-talk state is detected in order to stop parameter estimation of an adaptive filter or slow down the speed of the estimation.</p><p id="p0008" num="0008">Therefore, also in the case of the adaptive filter, if a target sound and a specific noise are both speech, a state in which the both are speaking needs to be detected. The more frequent this state is, the more frequent adaptive filter estimation is stopped. As a result, there is a problem that noise reduction performance of the adaptive filter is degraded.<br/>
Further, in a situation where a large diffuse noise such as a noise of a running car is generated, estimation accuracy of the adaptive filter is degraded due to the diffuse noise. And as a result, filter parameter estimated by adaptive filter degrades an estimation accuracy of arrival direction of a speaker and a specific noise, and thus overall system performance is degraded.</p><p id="p0009" num="0009">In addition, in the case that the above method described in the Patent Documents 6 and 7 is applied to a noise component suppression device using a low-cost microphone for an automobile or the like, a desired operation cannot be achieved as an entire device if an initial reflections of a sound source in a car cabin is<!-- EPO <DP n="8"> --> strong, a diffuse noise component is large, a uneven characteristic of used microphone elements is large (about ±3 db), and a target signal and a specific noise are generated simultaneously and their correlation is high because a driver and a passenger speak simultaneously, or so on.<br/>
The present invention is made in view of the above described problems, and is intended to provide a sound source separation device, a speech recognition device, a mobile telephone, a sound source separation method, and a program which allow to separate a sound source signal of a target sound source from a mixed sound which includes sound source signals emitted from a plurality of sound sources without being affected by uneven sensitivity of microphone elements.<br/>
To solve the above described problems, an invention according to claim 1 provides a sound source separation device for separating a sound source signal of a target sound source from a mixed sound which includes sound source signals emitted from a plurality of sound sources using at least two microphones arranged separately from each other comprising: beamforming means for performing a first beamforming processing to attenuate a sound source signal arriving from a predetermined direction by performing computations using first coefficients on an output signal of said microphones, and performing a second beamforming processing to attenuate a sound source signal arriving from a direction symmetrical to said<!-- EPO <DP n="9"> --> predetermined direction with respect to a line that is perpendicular to a straight line connecting the two microphones by performing computations using second coefficients which are complex conjugate of said first coefficients in a frequency domain on the output signal of said microphones; power computation means for computing power spectrum information with respect to each of sound source signals obtained by said beamforming means; and target sound spectrum extraction means for extracting spectrum information of a target sound source based on a difference between the power spectrum information calculated by said power computation means.</p><p id="p0010" num="0010">According to this invention, the sound source separation device provides a directional characteristics which is not affected by sensitivities of microphone elements, and allows to separate a sound source signal of a target sound source from a mixed sound which includes sound source signals emitted from a plurality of sound sources without being affected by uneven sensitivity of the microphone elements.<br/>
An invention according to claim 2 is the sound source separation device according to claim 1, wherein said beamforming means applies said first beamforming processing and said second beamforming processing to each of (A) a combination of any two microphones among three microphones which arranged separately from each other<!-- EPO <DP n="10"> --> and (B) another combination of two microphones among said three microphones.</p><p id="p0011" num="0011">According to this invention, a sound source signal from a target sound source which locates in each area whose boundary is a perpendicular line to a straight line connecting two microphones among three microphones.<br/>
An invention according to claim 3 is the sound source separation device according to claim 1 or 2, further comprising directional characteristics control means for applying a delay to an output signal of a microphone.<br/>
According to this invention, directional characteristics can be compensated to its optimum condition by applying a delay, and separation performance of sound sources can be improved.</p><p id="p0012" num="0012">An invention according to claim 4 is the sound source separation device according to claim 3, wherein said directional characteristics control means virtually generates output signals of three microphones by applying a delay to an output signal of at least one microphone among two microphones.<br/>
According to this invention, since output signals from three microphones can be virtually generated, a sound source signal arriving from a direction of a perpendicular line to a straight line connecting two<!-- EPO <DP n="11"> --> microphones can be separated and extracted using only the two microphones.</p><p id="p0013" num="0013">An invention according to claim 5 is the sound source separation device according to claim 3 or 4, further comprising arrival direction estimating means for estimating a arrival direction of said sound source signal, wherein said directional characteristics control means applies a delay to an output signal of the microphone such that two sound sources virtually locate symmetrically with respect to a line that is perpendicular to a straight line connecting the two microphones based on an arrival direction estimated by said arrival direction estimating means.<br/>
According to this invention, delay operation can be performed such that high separation performance of sound sources is obtained.</p><p id="p0014" num="0014">An invention according to claim 6 is the sound source separation device according to any one of claims 1 to 5, further comprising spectral subtraction means for performing spectral subtraction processing on the power spectrum information extracted by said target sound spectrum extraction means.<br/>
According to this invention, a stationary noise whose arrival direction is unknown, a diffuse noise, and the like can be suppressed by performing the spectral subtraction processing.<!-- EPO <DP n="12"> --></p><p id="p0015" num="0015">An invention according to claim 7 is the sound source separation device according to any one of claims 1 to 6, further comprising stationary noise reduction means for performing processing to reduce noise before the processing by said beamforming means is performed.<br/>
According to this invention, generation of a perceptually unpleasant noise such as a musical noise can be reduced.<br/>
An invention according to claim 8 provides a speech recognition device comprising the sound source separation device according to any one of claims 1 to 7 and speech recognition means for performing speech recognition of a sound source signal separated by the sound source separation device.</p><p id="p0016" num="0016">According to this invention, highly accurate speech recognition can be performed based on a separated sound source signal with high accuracy.<br/>
An invention according to claim 9 is the speech recognition device according to claim 8, further comprising recognition vocabulary list storage means for storing a driver's seat side recognition vocabulary list which is a list of candidates of vocabulary spoken from a driver's seat side of a vehicle and a passenger's seat side recognition vocabulary list which is a list of candidates of vocabulary spoken from a passenger's seat side, wherein said speech recognition means performs speech recognition processing of a sound source signal<!-- EPO <DP n="13"> --> separated by said sound source separation device based on the driver's seat side recognition vocabulary list and the passenger's seat side recognition vocabulary list stored in said recognition vocabulary list storage means.</p><p id="p0017" num="0017">According to this invention, since the speech recognition device performs speech recognition processing based on the passenger's seat side recognition vocabulary list and the driver's seat side recognition vocabulary list stored in the recognition vocabulary list storage means, an optimal vocabulary item can be selected from a vocabulary list separately between a driver's seat side and a passenger's seat side so that accurate speech recognition can be performed.<br/>
An invention according to claim 10 is the speech recognition device according to claim 8 or 9, further comprising: state transition means for managing a current state of a vehicle; valid vocabulary list storage means for storing a valid vocabulary list of the passenger's seat side and the driver's seat side depending on a state of the vehicle; and control means for determining whether a vocabulary item recognized by said speech recognition means is valid or not based on the current state of the vehicle managed by said state transition means and the vocabulary list stored in said valid vocabulary list storage means, and controlling depending on the determination result.</p><p id="p0018" num="0018"><!-- EPO <DP n="14"> --> According to this invention, control contributes to the comforts of persons in vehicle because validness of recognized vocabulary item can be determined based on a current state of a vehicle and a valid vocabulary list, and control can be performed depending on the determination result. In addition, a valid vocabulary list and contents of control can be designed freely so that flexibility can be given to application design using speech recognition.<br/>
An invention according to claim 11 provides a mobile phone comprising the sound source separation device according to any one of claims 1 to 7.</p><p id="p0019" num="0019">According to this invention, a mobile phone can be used as a sound collecting microphone in a medium sized conference room and the like.<br/>
An invention according to claim 12 provides a sound source separation method comprising: a sound source signal receiving step of inputting sound source signals emitted from a plurality of sound sources to at least two microphones arranged separately from each other; a beamforming processing step of performing a first beamforming processing and a second beamforming processing to attenuate sound source signals arriving from predetermined directions symmetrical with respect to a line that is perpendicular to a straight line connecting two microphones respectively by performing computations using two weighted coefficients which are complex<!-- EPO <DP n="15"> --> conjugate to each other in a frequency domain on an output signal of said microphone respectively; a power computation step of computing power spectrum information with respect to each of sound source signals obtained in said beamforming processing step; and a target sound spectrum extracting step of extracting spectrum information of a target sound source based on a difference between the power spectrum information calculated in said power computation step.</p><p id="p0020" num="0020">An invention according to claim 13 provides a program for causing a computer to perform: an output signal acquisition step of acquiring an output signal which includes sound source signals emitted from a plurality of sound sources are mixed from at least two microphones arranged separately from each other; a beamforming processing step of performing a first beamforming processing and a second beamforming processing to attenuate sound source signals arriving from predetermined directions symmetrical with respect to a line that is perpendicular to a straight line connecting two microphones respectively by performing computations using two weighted coefficients which are complex conjugate to each other in a frequency domain on the output signal acquired in said output signal acquisition step respectively; a power computation step of computing power spectrum information with respect to each of sound source signals obtained in said beamforming processing<!-- EPO <DP n="16"> --> step; and a target sound spectrum extracting step of extracting spectrum information of a target sound source based on a difference between the power spectrum information calculated in said power computation step.<br/>
According to the present invention, a first beamforming processing and a second beamforming processing are performed for attenuating sound source signals arriving from predetermined directions symmetrical with respect to a perpendicular line to a straight line connecting two microphones respectively by performing operations using two weighted coefficients which are complex conjugate to each other in a frequency domain, thereby the sound source signals arriving from the predetermined directions symmetrical with respect to the perpendicular line to the straight line connecting two the microphones respectively are attenuated, and spectrum information of a target sound source is extracted based on a difference between respective power spectrum information of sound source signals obtained by the first beamforming processing and the second beamforming processing. Therefore, a directional characteristics which is not affected by sensitivity of microphone elements can be achieved, and a sound source signal from a target sound source can be separated from a mixed sound which contains sound source signals emitted from a plurality of sound sources without being affected by uneven sensitivity of the microphone elements.<!-- EPO <DP n="17"> --></p><heading id="h0004">Brief Description of the Drawings</heading><p id="p0021" num="0021"><ul><li><figref idrefs="f0001">Figure 1</figref> is a diagram which shows a basic configuration of a sound source separation system according to a first embodiment of the present invention;</li><li><figref idrefs="f0002">Figure 2</figref> is a diagram which shows one example of a type of microphone according to the embodiment;</li><li><figref idrefs="f0002">Figure 3</figref> is a diagram which shows a configuration of a beamformer section according to the embodiment;</li><li><figref idrefs="f0003">Figure 4</figref> is a diagram which shows a configuration of a power computation section according to the embodiment;</li><li><figref idrefs="f0003">Figure 5</figref> is a diagram which shows a configuration of a target sound spectrum extraction section according to the embodiment;</li><li><figref idrefs="f0004">Figure 6</figref> is a diagram for describing a null beamformer according to the embodiment;</li><li><figref idrefs="f0004">Figure 7</figref> is a diagram for describing the null beamformer according to the embodiment;</li><li><figref idrefs="f0005">Figure 8</figref> is a diagram which shows one example of a directional characteristic of a difference result according to the embodiment;</li><li><figref idrefs="f0006">Figure 9</figref> is a diagram which shows directional characteristic of conjugate beamformers when a sensitivity of microphone elements is varied according to the embodiment;</li><li><figref idrefs="f0007">Figure 10</figref> is a diagram which shows directional characteristics of a single beamformer when a difference<!-- EPO <DP n="18"> --> of sensitivity of microphone elements is varied according to the embodiment;</li><li><figref idrefs="f0008">Figure 11</figref> is a diagram which shows a 360-degree directional characteristic of the sound source separation device according to the embodiment;</li><li><figref idrefs="f0009">Figure 12</figref> is a diagram which shows an example of individual recording of two speakers using a directional characteristic of the sound source separation device according to the embodiment;</li><li><figref idrefs="f0010">Figure 13</figref> is a diagram which shows an example of a simple hands-free device and a microphone for a speech recognition device using a directional characteristic of the sound source separation device according to the embodiment;</li><li><figref idrefs="f0011">Figure 14</figref> is a diagram which shows mounting positions of microphones in an evaluation experiment according to the embodiment;</li><li><figref idrefs="f0012">Figure 15</figref> is a diagram which shows conditions of the evaluation experiment according to the embodiment;</li><li><figref idrefs="f0012">Figures 16A and 16B</figref> are diagrams which show a result of the evaluation experiment according to the embodiment;</li><li><figref idrefs="f0013">Figure 17</figref> is a diagram which shows a configuration of a sound source separation system according to a second embodiment;</li><li><figref idrefs="f0014">Figure 18</figref> is a diagram which shows a configuration of a beamformer according to the embodiment;<!-- EPO <DP n="19"> --></li><li><figref idrefs="f0015">Figure 19</figref> is a diagram which shows a configuration of a sound source separation system according to a third embodiment;</li><li><figref idrefs="f0016">Figure 20</figref> is a diagram which shows a configuration of another sound source separation system according to the embodiment;</li><li><figref idrefs="f0017">Figure 21</figref> is a diagram which shows a configuration of a target sound spectrum extraction section according to the embodiment;</li><li><figref idrefs="f0018">Figure 22</figref> is a diagram which shows a configuration of a sound source separation system according to a fourth embodiment;</li><li><figref idrefs="f0019">Figure 23</figref> is a diagram which shows a configuration of a sound source separation system according to a fifth embodiment;</li><li><figref idrefs="f0020">Figure 24</figref> is a diagram which shows an example of directional characteristic control according to the embodiment;</li><li><figref idrefs="f0020">Figures 25A and 25B</figref> are diagrams which show a configuration of a directional characteristic control section according to the embodiment;</li><li><figref idrefs="f0021">Figure 26</figref> is a diagram which shows a configuration of a sound source separation system according to a sixth embodiment;</li><li><figref idrefs="f0022">Figure 27</figref> is a diagram which shows a configuration of a sound source separation system according to a seventh embodiment;<!-- EPO <DP n="20"> --></li><li><figref idrefs="f0023">Figure 28</figref> is a diagram for describing positions of sound sources to be separated according to the embodiment;</li><li><figref idrefs="f0023">Figure 29</figref> is a diagram which shows an example of installation of a unidirectional microphone according to the embodiment;</li><li><figref idrefs="f0024">Figure 30</figref> is a diagram which shows one example of a configuration of a target sound extraction section according to the embodiment;</li><li><figref idrefs="f0025">Figure 31</figref> is a diagram which shows one example of a configuration of the target sound extraction section according to the embodiment;</li><li><figref idrefs="f0026">Figure 32</figref> is a diagram which shows one example of a configuration of the target sound extraction section according to the embodiment;</li><li><figref idrefs="f0027">Figure 33</figref> is a diagram which shows an example of speech input to a personal computer using a sound source separation device according to the embodiment;</li><li><figref idrefs="f0028">Figure 34</figref> is a diagram for describing a target sound area and a noise area according to an eighth embodiment;</li><li><figref idrefs="f0029">Figures 35A and 35B</figref> are diagrams for describing a delay operation according to the embodiment;</li><li><figref idrefs="f0030">Figure 36</figref> is a diagram which shows a configuration example of directional characteristics control means according to the embodiment;</li><li><figref idrefs="f0031">Figure 37</figref> is a diagram which shows a configuration example of a sound source separation device system according to the embodiment;<!-- EPO <DP n="21"> --></li><li><figref idrefs="f0032">Figure 38</figref> is a diagram which shows one example of a processing scheme in a target sound extraction section according to the embodiment;</li><li><figref idrefs="f0033">Figure 39</figref> is a diagram which shows one example of a processing scheme in the target sound extraction section according to the embodiment;</li><li><figref idrefs="f0034">Figure 40</figref> is a diagram which shows a configuration of a speech recognition system for controlling on-vehicle equipment according to a ninth embodiment;</li><li><figref idrefs="f0035">Figures 41A and 41B</figref> are diagrams which shows a mobile phone according to a tenth embodiment;</li><li><figref idrefs="f0036">Figure 42</figref> is a diagram which shows a microphone arrangement in a sound source separation system according to an eleventh embodiment;</li><li><figref idrefs="f0037">Figure 43</figref> is a diagram which shows an environment to which a sound source separation system is applied according to a twelfth embodiment;</li><li><figref idrefs="f0037">Figure 44</figref> is a diagram which shows a state of input speech to which the sound source separation system is applied according to the embodiment;</li><li><figref idrefs="f0038">Figure 45</figref> is a diagram which shows a configuration of a guidance speech elimination section according to the embodiment;</li><li><figref idrefs="f0039">Figure 46</figref> is a diagram which shows a configuration of a target speech extraction section according to the embodiment;<!-- EPO <DP n="22"> --></li><li><figref idrefs="f0040">Figure 47</figref> is a diagram which shows another configuration of the guidance speech elimination section according to the embodiment;</li><li><figref idrefs="f0041">Figure 48</figref> is a diagram which shows a configuration of a target speech extraction section in the another configuration of the guidance speech elimination section according to the embodiment;</li><li><figref idrefs="f0041">Figure 49</figref> is a diagram which shows another environment to which the sound source separation system is applied according to the embodiment;</li><li><figref idrefs="f0042">Figure 50</figref> is a diagram which shows another state of input speech to which the sound source separation system is applied according to the embodiment; and</li><li><figref idrefs="f0042">Figure 51</figref> is a diagram which shows another configuration of the sound source separation system according to the embodiment.</li></ul></p><heading id="h0005">Best Mode for Carrying Out the Invention</heading><p id="p0022" num="0022">Hereinafter, embodiments according to the present invention will be described with reference to the drawings.</p><heading id="h0006">&lt;First Embodiment&gt;</heading><p id="p0023" num="0023"><figref idrefs="f0001">Figure 1</figref> is a diagram which shows a basic configuration of a sound source separation system according to the first embodiment of the present invention. This system is comprised of two microphones (hereinafter referred to as MIC) 10, 11 and a sound<!-- EPO <DP n="23"> --> source separation device 1. This sound source separation device 1 includes a CPU that performs overall control and arithmetic processing, hardware including storage devices a ROM, a RAM, and a hard disk device, and software including a program, data, and the like stored in the storage devices, which are not shown. Function blocks shown in <figref idrefs="f0001">Figure 1</figref> are implemented by these hardware and software.</p><p id="p0024" num="0024">The two MICs 10 and 11 are non-directional MICs and arranged separately by a few centimeters from each other on a plane. Although the MICs 10 and 11 are basically omnidirectional, unidirectional microphones may be used as shown in <figref idrefs="f0002">Figure 2</figref>. The MICs 10 and 11 receive signals emitted from two sound sources R1 and R2. At this time, these two sound sources R1 and R2 locate in two areas divided by a perpendicular line to a straight line connecting the two MICs 10 and 11 as a boundary (hereinafter referred to as "left and right of a perpendicular line") respectively, but their location do not have to be symmetrical with respect to the perpendicular line.</p><p id="p0025" num="0025">Frequency analysis is performed for each output of the MICs 10 and 11 which pick up two sound source signals by spectrum analysis sections 20 and 21. The frequency-analyzed sound source signals is filtered by beamformers 30 and 31 which form symmetrical directional nulls in the<!-- EPO <DP n="24"> --> left and right of the perpendicular line to the straight line connecting the two MICs 10 and 11 in the beamformer section 3. Power of the filer output is computed by power computation sections 40 and 41. And processing is performed by target sound spectrum extraction sections 50 and 51 such that respective differences are computed, and a computed result value is outputted if the value is grater than or equal to a certain value, and if not, the value is set to zero. These processings differs from common processing for forming a directional null for a specific noise, and configures the beamformer section 3 under some conditions and performs the above described processings, so that the conventional problem of degradation of characteristics of a microphone array due to uneven sensitivity of microphone elements can be solved, and a directional characteristic which separates sounds from left and right with respect to the perpendicular line as described above can be achieved for a broad frequency band. Each functional block will be described in detail below.</p><heading id="h0007">&lt;Beamformer section&gt;</heading><p id="p0026" num="0026">First, a configuration of the beamformer section 3 will be described with reference to <figref idrefs="f0002">Figure 3</figref>. In <figref idrefs="f0002">Figure 3</figref>, signals x<sub>1</sub>(ω) and x<sub>2</sub>(ω) which are decomposed for each frequency component by spectrum analysis sections 20 and 21 are inputted, and are multiplied by weighted coefficients of filter w<sub>1</sub>(ω), w<sub>2</sub>(ω), w<sub>1</sub>*(ω), w<sub>2</sub>*(ω) (*<!-- EPO <DP n="25"> --> indicates complex conjugate) respectively by multipliers 100a, 100b, 100c and 100d. And two multiplication results are added by adders 100e and 100f, and filtering processing results ds<sub>1</sub>(ω), ds<sub>2</sub>(ω) are outputted as outputs of them. In this way, the beamformer section 3 forms directional nulls in symmetrical positions with respect to a perpendicular line to a straight line connecting the MICs 10 and 11 by using complex conjugate filter coefficients.</p><heading id="h0008">&lt;Power computation section&gt;</heading><p id="p0027" num="0027">Next, power computation sections 40 and 41 will be described with reference to <figref idrefs="f0003">Figure 4</figref>. The power computation sections 40 and 41 convert outputs ds<sub>1</sub>(ω) and ds<sub>2</sub>(ω) from the beamformer 30 and the beamformer 31 to power spectrum information ps<sub>1</sub>(ω) and ps<sub>2</sub>(ω) by the following calculating equations. <maths id="math0001" num=""><math display="block"><msub><mi>ps</mi><mn mathvariant="normal">1</mn></msub><mfenced><mi mathvariant="normal">ω</mi></mfenced><mo>=</mo><msup><mfenced open="[" close="]" separators=""><mi>Re</mi><mfenced separators=""><msub><mi>ds</mi><mn mathvariant="normal">1</mn></msub><mfenced><mi mathvariant="normal">ω</mi></mfenced></mfenced></mfenced><mn mathvariant="normal">2</mn></msup><mo>+</mo><msup><mfenced open="[" close="]" separators=""><mi>Im</mi><mfenced separators=""><msub><mi>ds</mi><mn mathvariant="normal">1</mn></msub><mfenced><mi mathvariant="normal">ω</mi></mfenced></mfenced></mfenced><mn mathvariant="normal">2</mn></msup></math><img id="ib0001" file="imgb0001.tif" wi="84" he="5" img-content="math" img-format="tif"/></maths> <maths id="math0002" num=""><math display="block"><msub><mi mathvariant="normal">s</mi><mn mathvariant="normal">2</mn></msub><mfenced><mi mathvariant="normal">ω</mi></mfenced><mo>=</mo><msup><mfenced open="[" close="]" separators=""><mi>Re</mi><mfenced separators=""><msub><mi>ds</mi><mn>2</mn></msub><mfenced><mi mathvariant="normal">ω</mi></mfenced></mfenced></mfenced><mn mathvariant="normal">2</mn></msup><mo>+</mo><msup><mfenced open="[" close="]" separators=""><mi>Im</mi><mfenced separators=""><msub><mi>ds</mi><mn>2</mn></msub><mfenced><mi mathvariant="normal">ω</mi></mfenced></mfenced></mfenced><mn mathvariant="normal">2</mn></msup></math><img id="ib0002" file="imgb0002.tif" wi="81" he="5" img-content="math" img-format="tif"/></maths></p><heading id="h0009">&lt;Target sound spectrum extraction section&gt;</heading><p id="p0028" num="0028">Next, the target sound spectrum extraction sections 50 and 51 will be described with reference to <figref idrefs="f0003">Figure 5</figref>. ps<sub>1</sub>(ω) and ps<sub>2</sub>(ω), which are outputs of the power computation sections 40 and 41, are used as two inputs to the target sound spectrum extraction sections 50 and 51. The target sound spectrum extraction sections 50 and 51 receive power spectrum information outputted from the two beamformers 30 and 31 as inputs, and output left and<!-- EPO <DP n="26"> --> right target sounds as outputs respectively. The target sound spectrum extraction sections 50 and 51 are internally composed of difference calculation sections 500 and 510 and coefficient conversion sections 501 and 511.<br/>
The difference calculation section 500 subtracts power spectrum information of the beamformer 31 from that of the beamformer 30 in the subtractor 500a, and similarly the difference calculation section 510 subtracts power spectrum information of the beamformer 30 from that of the beamformer 31 in the subtractor 510a, and their results are then inputted to the coefficient conversion sections 501 and 511 respectively. The coefficient conversion sections 501 and 511 are blocks for separating left and right sounds respectively. The coefficient conversion sections 501 and 511 output spectrum information that are values greater than or equal to certain threshold values as signals from target directions respectively. Although a threshold value is typically "0" as used herein, an optimum threshold value may be obtained by actual measurement and individually set depending on a use environment.</p><heading id="h0010">&lt;Operation&gt;</heading><p id="p0029" num="0029">Next, operation of the entire sound source separation device system will be described with reference to <figref idrefs="f0001">Figure 1</figref>.<br/>
<!-- EPO <DP n="27"> -->Firstly, two omnidirectional or directional MICs 10 and 11 are arranged separately by a few centimeters from each other, and signals emitted from two sound sources are received by the MICs 10 and 11. Then, each of mixed signals of two sound source signals received by the MICs 10 and 11 are frequency-analyzed by the spectrum analysis sections 20 and 21. Although a generally used technique such as fast Fourier transform is used here, it may be a spectrum analysis technique such as filter bank. Spectrum analysis processing is performed every fixed period of about 10 msec.<br/>
The spectrum-analyzed two signals are filtered by the beamformers 30 and 31 in which directional nulls are formed symmetrically with respect to the perpendicular line to the straight line connecting the MICs 10 and 11, so that a signal from a specific direction is attenuated. However, in this case, it is not intended to precisely estimate an arrival direction of specific sound source and direct a directional null to the precisely estimated sound source direction. This filtering using inputs of two channels are performed for each frequency component, and outputs of the beamformers 30 and 31 are converted to spectrum power information by the power computation sections 40 and 41, at the same time phase information Φ<sub>1</sub> and Φ<sub>2</sub> are extracted from the outputs of the beamformers 30 and 31 by a phase extraction sections 60 and 61. Then, the outputs of the beamformers 30 and 31 which have been converted to the spectrum power information by the power<!-- EPO <DP n="28"> --> calculation sections 40 and 41 are sent to the target sound spectrum extraction sections 50 and 51. Then power spectrum information of a sound source signal coming from right direction (0 degree to 90 degrees) is extracted by the target sound spectrum extraction section 50, power spectrum information of a sound source signal coming from left direction (-90 degrees to 0 degree) is extracted by the target sound spectrum extraction section 51.</p><p id="p0030" num="0030">In a case the power spectrum information extracted by the target sound spectrum extraction section 51 is used as preprocessing of speech recognition processing, the relevant power spectrum information is sent to an acoustic parameter analysis section (not shown) in which acoustic analysis processing is performed on the power spectrum information. On the other hand, when the extracted power spectrum information of the sound source signals needs to be converted back to time signal, the phase information extracted by the phase extraction sections 60 and 61 and the spectrum information extracted by the target sound spectrum extraction sections 50 and 51 are inputted to a time waveform conversion sections 70 and 71 to be converted back to time signal information.</p><heading id="h0011">&lt;Design example of null beamformer&gt;</heading><p id="p0031" num="0031">In the following, it will be proved that, if directional nulls in symmetrical positions with respect to the perpendicular line to the straight line connecting the two MICs 10 and 11 are formed in the beamformers 30<!-- EPO <DP n="29"> --> and 31 in the beamformer section 3, a directional characteristic (directivity) is not affected by sensitivity of microphone elements.</p><p id="p0032" num="0032">In a case where two microphone elements are used, as shown in <figref idrefs="f0004">Figure 6</figref>, a design example of a null beamformer in which a gain for a target direction θ<sub>1</sub> is 1 and one directional null (gain 0) is formed in another direction θ<sub>2</sub> will be described below.<br/>
When an output signal of the null beamformer is S(f)=[s<sub>1</sub>(f), s<sub>2</sub>(f)]', and an observed signal is X(f)=[x<sub>1</sub>(f), x<sub>2</sub>(f)]', W(f, θ<sub>1</sub>, θ<sub>2</sub>)=[w<sub>1</sub>(f), w<sub>2</sub>(f)]' which is a weighted coefficients vector of the null beamformer at a given frequency f (where ' denotes transposition operation) can be obtained by the following calculation.</p><p id="p0033" num="0033"><maths id="math0003" num="&lt;Equation 1&gt;"><math display="block"><mtable columnalign="left"><mtr><mtd><mi>S</mi><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced><mo>=</mo><mi>W</mi><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced><mo>⁢</mo><mi>X</mi><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><mi>W</mi><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>w</mi><mn>1</mn></msub><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>w</mi><mn>2</mn></msub><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced></mtd></mtr><mtr><mtd><mi>then</mi><mo>,</mo></mtd></mtr><mtr><mtd><mfenced open="[" close="]"><mtable><mtr><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd></mtr><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd></mtr></mtable></mfenced><mo>⁢</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>w</mi><mn>1</mn></msub><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>w</mi><mn>2</mn></msub><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced></mtd></mtr><mtr><mtd><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>w</mi><mn>1</mn></msub><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi>w</mi><mn>2</mn></msub><mfenced><mi>f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced><mo>=</mo><msup><mfenced open="[" close="]"><mtable><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd></mtr><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mi>j</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><msub><mi mathvariant="italic">πfd</mi><mn>2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi>c</mi></mfenced></mtd></mtr></mtable></mfenced><mrow><mo>-</mo><mn>1</mn></mrow></msup><mfenced open="[" close="]"><mtable><mtr><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></math><img id="ib0003" file="imgb0003.tif" wi="120" he="64" img-content="math" img-format="tif"/></maths></p><p id="p0034" num="0034">On the other hand, as shown in <figref idrefs="f0004">Figure 7</figref>, when a target direction and a null direction are set in directions line-symmetrical to the positions shown in<!-- EPO <DP n="30"> --> <figref idrefs="f0004">Figure 6</figref> with respect to the perpendicular line to the straight line connecting the two MICs 10 and 11, a weighted coefficients vector W(f, -θ<sub>1</sub>, -θ<sub>2</sub>) = [w<sub>1</sub>(f), w<sub>2</sub>(f)]' can be obtained by the following calculation.</p><p id="p0035" num="0035"><maths id="math0004" num="&lt;Equation 2&gt;"><math display="block"><mtable columnalign="left"><mtr><mtd><mi mathvariant="normal">S</mi><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced><mo>=</mo><mi mathvariant="normal">W</mi><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced><mspace width="1em"/><mi>X</mi><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><mi mathvariant="normal">W</mi><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>1</mn></msub><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>2</mn></msub><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></math><img id="ib0004" file="imgb0004.tif" wi="108" he="32" img-content="math" img-format="tif"/></maths></p><p id="p0036" num="0036">Then,</p><p id="p0037" num="0037"><maths id="math0005" num="&lt;Equation 3&gt;"><math display="block"><mtable><mtr><mtd><mfenced open="[" close="]"><mtable><mtr><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd></mtr><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd></mtr></mtable></mfenced><mo>⁢</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>1</mn></msub><mfenced><mi mathvariant="normal">f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>2</mn></msub><mfenced><mi mathvariant="normal">f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced></mtd></mtr><mtr><mtd><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>1</mn></msub><mfenced><mi mathvariant="normal">f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>2</mn></msub><mfenced><mi mathvariant="normal">f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced><mo>=</mo><msup><mfenced open="[" close="]"><mtable><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>1</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd></mtr><mtr><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">1</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd><mtd><mi>exp</mi><mfenced open="[" close="]" separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>⁢</mo><msub><mi mathvariant="normal">πfd</mi><mn mathvariant="normal">2</mn></msub><mo>⁢</mo><mi>sin</mi><msub><mi>θ</mi><mn>2</mn></msub><mo>/</mo><mi mathvariant="normal">c</mi></mfenced></mtd></mtr></mtable></mfenced><mrow><mo>-</mo><mn>1</mn></mrow></msup><mfenced open="[" close="]"><mtable><mtr><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mtd></mtr></mtable></math><img id="ib0005" file="imgb0005.tif" wi="155" he="41" img-content="math" img-format="tif"/></maths></p><p id="p0038" num="0038">is calculated. A relationship between the two weighted coefficients vectors is:</p><p id="p0039" num="0039"><maths id="math0006" num="&lt;Equation 4&gt;"><math display="block"><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>1</mn></msub><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>2</mn></msub><mo>⁢</mo><mfenced separators=""><mi mathvariant="normal">f</mi><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>1</mn></msub><mo>,</mo><mo>-</mo><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced><mo>=</mo><msup><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>1</mn></msub><mfenced><mi mathvariant="normal">f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn>2</mn></msub><mfenced><mi mathvariant="normal">f</mi><msub><mi>θ</mi><mn>1</mn></msub><msub><mi>θ</mi><mn>2</mn></msub></mfenced></mtd></mtr></mtable></mfenced><mo>*</mo></msup></math><img id="ib0006" file="imgb0006.tif" wi="79" he="25" img-content="math" img-format="tif"/></maths></p><p id="p0040" num="0040">and therefore respective weighted coefficients have complex conjugate relationship.<br/>
<!-- EPO <DP n="31"> -->Next, a directional characteristic is derived in the power computation sections 40 and 41 and the target sound spectrum extraction sections 50 and 51. For calculation of the directional characteristic, weight vector W and directional vector V are defined by the following expressions.</p><p id="p0041" num="0041"><maths id="math0007" num="&lt;Equation 5&gt;"><math display="block"><mi mathvariant="normal">W</mi><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn></msub></mtd></mtr><mtr><mtd><msub><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn></msub></mtd></mtr></mtable></mfenced><mo>,</mo><mi mathvariant="normal">V</mi><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>ν</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>ν</mi><mn>2</mn></msub></mtd></mtr></mtable></mfenced></math><img id="ib0007" file="imgb0007.tif" wi="43" he="26" img-content="math" img-format="tif"/></maths></p><p id="p0042" num="0042">Then, directional characteristic of an output of the power computation section 40 ps<sub>1</sub>(ω) and an output of the power computation section 41 ps<sub>2</sub>(ω) can be expressed as follows.</p><p id="p0043" num="0043"><maths id="math0008" num="&lt;Equation 6&gt;"><math display="block"><mtable columnalign="left"><mtr><mtd><msub><mi>ps</mi><mn mathvariant="normal">1</mn></msub><mfenced><mi>ω</mi></mfenced><mo>=</mo><mfenced open="[" close="]" separators=""><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup></mfenced><mo>⁢</mo><msup><mfenced open="[" close="]" separators=""><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup></mfenced><mi mathvariant="normal">H</mi></msup><mo>=</mo><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">V</mi><mo>*</mo></msup></mtd></mtr><mtr><mtd><msub><mi>ps</mi><mn>2</mn></msub><mfenced><mi>ω</mi></mfenced><mo>=</mo><mfenced open="[" close="]" separators=""><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mfenced><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup></mfenced><mo>*</mo></msup></mfenced><mo>⁢</mo><msup><mfenced open="[" close="]" separators=""><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mfenced><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup></mfenced><mo>*</mo></msup></mfenced><mi mathvariant="normal">H</mi></msup><mo>=</mo><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><mi mathvariant="normal">W</mi><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mi mathvariant="normal">H</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">V</mi><mo>*</mo></msup></mtd></mtr></mtable></math><img id="ib0008" file="imgb0008.tif" wi="84" he="23" img-content="math" img-format="tif"/></maths></p><p id="p0044" num="0044">where * denotes conjugate operation and H denotes conjugate transposition operation. Therefore, dr<sub>1</sub>(ω) which is an output of the difference calculation section 500 in the target sound spectrum extraction section 50 can be obtained as follows.</p><p id="p0045" num="0045"><!-- EPO <DP n="32"> --><maths id="math0009" num=""><math display="block"><mtable><mtr><mtd><msub><mi>dr</mi><mn>1</mn></msub><mfenced><mi>ω</mi></mfenced><mo>=</mo><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">V</mi><mo>*</mo></msup><mo>-</mo><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><mi mathvariant="normal">W</mi><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mi mathvariant="normal">H</mi></msup><mo>⁢</mo><msup><mi mathvariant="normal">V</mi><mo>*</mo></msup><mo>=</mo><msup><mi mathvariant="normal">V</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><mfenced separators=""><msup><mi mathvariant="normal">W</mi><mo>*</mo></msup><mo>⁢</mo><msup><mi mathvariant="normal">W</mi><mi mathvariant="normal">T</mi></msup><mo>-</mo><msup><mi>WW</mi><mi mathvariant="normal">H</mi></msup></mfenced><mo>⁢</mo><msup><mi mathvariant="normal">V</mi><mo>*</mo></msup></mtd></mtr><mtr><mtd><mo>=</mo><mn>2</mn><mo>×</mo><mi>Re</mi><mo>⌊</mo><msubsup><mi mathvariant="normal">w</mi><mn>1</mn><mo>*</mo></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>ν</mi><mn>2</mn><mo>*</mo></msubsup><mo>⌋</mo><mo>-</mo><mn>2</mn><mo>×</mo><mi>Re</mi><mo>⌊</mo><msub><mi mathvariant="normal">w</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi mathvariant="normal">w</mi><mn>2</mn><mo>*</mo></msubsup><mo>⁢</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>ν</mi><mn>2</mn><mo>*</mo></msubsup><mo>⌋</mo></mtd></mtr></mtable></math><img id="ib0009" file="imgb0009.tif" wi="105" he="27" img-content="math" img-format="tif"/></maths></p><p id="p0046" num="0046">Now, α is introduced as a parameter representing an uneven sensitivity of MIC elements, and a sensitivity of one MIC element is assumed to be α times of that of the other MIC element. In this case, the fact that output of one MIC is α times of output of the other MIC is equivalent to the fact that weight multiplied to one channel is α times of that to the other channel. Accordingly, assuming w<sub>2</sub>=α w<sub>org2</sub> in consideration of an uneven sensitivity of MIC elements,</p><p id="p0047" num="0047"><maths id="math0010" num="&lt;Equation 8&gt;"><math display="block"><msub><mi>dr</mi><mn>1</mn></msub><mfenced><mi>ω</mi></mfenced><mo>=</mo><mi>α</mi><mfenced open="{" close="}" separators=""><mn>2</mn><mo>×</mo><mi>Re</mi><mfenced open="[" close="]" separators=""><msubsup><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn><mo>*</mo></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">w</mi><mrow><mi>org</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mo>⁢</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>ν</mi><mn>2</mn><mo>*</mo></msubsup></mfenced><mo>-</mo><mn>2</mn><mo>×</mo><mi>Re</mi><mfenced open="[" close="]" separators=""><msubsup><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn><mo>*</mo></msubsup><mo>⁢</mo><msubsup><mi mathvariant="normal">w</mi><mrow><mi>org</mi><mo>⁢</mo><mn>2</mn></mrow><mo>*</mo></msubsup><mo>⁢</mo><msub><mi>ν</mi><mn>1</mn></msub><mo>⁢</mo><msubsup><mi>ν</mi><mn>2</mn><mo>*</mo></msubsup></mfenced></mfenced></math><img id="ib0010" file="imgb0010.tif" wi="108" he="17" img-content="math" img-format="tif"/></maths></p><p id="p0048" num="0048">Thus, a directional characteristic is not changed even if a sensitivity of microphone element is changed.<br/>
Then, if a sound source is sufficiently away from a MIC, i.e., in case of a plane wave, the direction vector V is expressed as:</p><p id="p0049" num="0049"><maths id="math0011" num="&lt;Equation 9&gt;"><math display="block"><mi mathvariant="normal">V</mi><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><msub><mi>ν</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>ν</mi><mn>2</mn></msub></mtd></mtr></mtable></mfenced><mo>=</mo><mfenced open="[" close="]"><mtable><mtr><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mi>exp</mi><mfenced separators=""><mo>-</mo><mi mathvariant="normal">j</mi><mo>⁢</mo><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>π</mi></mrow><mi>λ</mi></mfrac><mo>⁢</mo><mi>d sin</mi><mo>⁢</mo><mi>θ</mi></mfenced></mtd></mtr></mtable></mfenced></math><img id="ib0011" file="imgb0011.tif" wi="74" he="33" img-content="math" img-format="tif"/></maths></p><p id="p0050" num="0050">As a result,<!-- EPO <DP n="33"> --></p><p id="p0051" num="0051"><maths id="math0012" num="&lt;Equation 10&gt;"><math display="block"><mi>dr</mi><mo>⁢</mo><msub><mfenced><mi>ω</mi></mfenced><mn>1</mn></msub><mo>=</mo><mi>α</mi><mfenced open="{" close="}" separators=""><mn>2</mn><mo>×</mo><mi>Re</mi><mfenced open="[" close="]" separators=""><msubsup><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn><mo>*</mo></msubsup><mo>⁢</mo><msub><mi mathvariant="normal">w</mi><mrow><mi>org</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></msub><mspace width="1em"/><mi>exp</mi><mfenced separators=""><mi mathvariant="normal">j</mi><mo>⁢</mo><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>π</mi></mrow><mi>λ</mi></mfrac><mo>⁢</mo><mi>d sin</mi><mo>⁢</mo><mi>θ</mi></mfenced></mfenced><mo>-</mo><mn>2</mn><mo>×</mo><mi>Re</mi><mfenced open="[" close="]" separators=""><msubsup><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn><mo>*</mo></msubsup><mo>⁢</mo><msubsup><mi mathvariant="normal">w</mi><mrow><mi>org</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow><mn>2</mn></msubsup><mo>⁢</mo><mi>exp</mi><mfenced separators=""><mi mathvariant="normal">j</mi><mo>⁢</mo><mfrac><mrow><mn>2</mn><mo>⁢</mo><mi>π</mi></mrow><mi>λ</mi></mfrac><mo>⁢</mo><mi>d sin</mi><mo>⁢</mo><mi>θ</mi></mfenced></mfenced></mfenced></math><img id="ib0012" file="imgb0012.tif" wi="163" he="23" img-content="math" img-format="tif"/></maths></p><p id="p0052" num="0052">However, the above described technique retains a similar characteristic also in the case of a spherical wave.<br/>
<figref idrefs="f0005">Figure 8</figref> is an example of a directional characteristic of a difference result in a design in which, when a directional null is formed in direction of ±45 degrees from the perpendicular line as described above, constrains for maintaining gains of the beamformers 30 and 31 are specified in symmetrical positions (±45 degrees) with respect to the perpendicular line. As can be seen from <figref idrefs="f0005">Figure 8</figref>, with the boundary in 0 degree direction, a sound source signal coming from right direction (right direction when viewed from the MICs 10 and 11 to the sound sources R1 and R2, hereinafter it is referred in the same manner) (0 degree to 90 degrees) has a positive value, and sound source signal coming from a left direction (-90 degrees to 0 degree) has a negative value. Thereby, whether an arrival direction is left or right can be determined for each of frequency components.</p><heading id="h0012">&lt;Experiment result of directional characteristics &gt;</heading><!-- EPO <DP n="34"> --><p id="p0053" num="0053">To illustrate that a directional characteristics of a microphone array is not affected by a sensitivity of a MIC elements by applying above described processing in which weighted coefficients used in multipliers of the beamformers 30 and 31 are complex conjugate to each other, an example where calculation of a directional characteristics is performed when a difference of MIC element sensitivities α is varied to 0 db, +6 db, and -6 db is shown in <figref idrefs="f0006">Figure 9</figref>. Although a directional characteristics symmetrical with respect to the perpendicular line to the straight line connecting the MICs 10 and 11 is not shown in the figure, the corresponding directional characteristics are symmetrical to that shown in <figref idrefs="f0006">Figure 9</figref>. As can be seen from <figref idrefs="f0006">Figure 9</figref>, when a gain is varied, an output level of the array microphone is varied, but a directional characteristics is not varied. Thus, a stable directional characteristics can be achieved in spite of using low-cost MICs and uneven sensitivity of MIC elements. In addition, while a directional null is formed in direction of ±45 degrees in the directional characteristics shown in the figure, since the directional characteristics has some width as can be seen from the figure, the directional null does not need to be precisely formed with respect to an actual target sound source. In addition, a directional characteristics of a single beamformer is shown in <figref idrefs="f0007">Figure 10</figref> in which a difference of sensitivity of MIC elements α is varied to<!-- EPO <DP n="35"> --> 0 db, +6 db, and -6 db, and it can be seen from this figure that, if the sensitivity of MIC element is varied by 6 db, operation desired for the beamformer to form a direction null in a specific direction is almost unachievable. On the other hand, it is the most remarkable feature of the present invention that a resulting directional characteristics is the same as in the case that sensitivity of MIC elements are uniform in spite of using a beamformer with degraded directional characteristics as shown in <figref idrefs="f0007">Figure 10</figref>.<br/>
The technique for forming a sharp directional null in a specific direction by a beamformer can be theoretically realized even with a few MICs, and therefore a measured signal is used for increasing an S/N ratio or used in a blocking matrix portion which is a reference signal generation section of a generalized sidelobe canceller that is often used as an adaptive filter. However, the technique can not form a directional null with performance as designed because of difference of sensitivity of MIC elements as described above. And this is one of major reasons why a predetermined performance cannot be achieved in an actual environment when mass production is intended.</p><p id="p0054" num="0054"><figref idrefs="f0008">Figure 11</figref> shows an example where a directional characteristics of 360 degree is obtained using the sound source separation device 1 as described above. As can been seen from the figure, the sound source separation<!-- EPO <DP n="36"> --> device 1 has a directional characteristics for each 180 degree, and two directional characteristics are separated by their boundary without overlapping. As a further feature, such directional characteristics are not affected by sensitivity of MIC elements. In a liner array, a directional characteristics in a range other than range from -90 degrees to 90 degrees is symmetrical to that in a range from 0 degree to 90 degrees, and from -90 degrees to 0 degree. In this way, a directional characteristics is allowed to be divided into two zones with a boundary of a perpendicular line to a line connecting two microphones.</p><p id="p0055" num="0055">Examples using this directional characteristics are shown in <figref idrefs="f0009">Figure 12</figref> and <figref idrefs="f0010">Figure 13</figref>. <figref idrefs="f0009">Figure 12</figref> is an application to a voice memo device. Conventionally, a voice memo device has been used for making a memo of a conference in a conference or meeting. In this example, a recorded content may be difficult to listen because a surrounding noise and voices of two persons are recorded simultaneously. In this case, two MICs 10 and 11 are directed to two speakers respectively, and contents of conversation at each person's side is enhanced and recorded by the sound source separation device 1, so that it is easy to listen at a later day. By using the sound source separation device 1 as described above, voices of facing two persons can be separated and collected, so that the device can be applied to a voice memo for<!-- EPO <DP n="37"> --> minutes of conference and simultaneous speech recognition for each of two person's speech in minutes of conference.</p><p id="p0056" num="0056"><figref idrefs="f0010">Figure 13</figref> shows an example of application to a simple hands-free device and a microphone for a speech recognition device. Recently, an internet conference and the like have been held using a personal computer (hereinafter referred to a "PC"). When an internet conference is held from home or the like, a PC without echo canceller function requires a countermeasure such as a headset microphone or the like to reduce the echo from loudspeaker to a MIC. However, people do not easily accept an apparatus like a headset microphone closely worn on a part of human body. To address this, an echo canceller function may be implemented as software inside a PC, or a hands-free device with built-in echo canceller function may be connected to outside of the PC. If the echo canceller function is implemented inside the PC, played speech to a loudspeaker and input speech from microphone needs to be synchronized, and thus there are problems for implementation that a delay between played speech and input speech is large, a delay depends on PC model, and so on. It is costly that the hands-free device with built-in echo canceller function is connected to outside of the PC.</p><p id="p0057" num="0057">On the other hand, in the present example, although the MICs 10 and 11 have to be arranged between a<!-- EPO <DP n="38"> --> loudspeaker and a speaker, they are not required to be synchronized with a played signal of the loudspeaker. Because sound source separation can be performed based on software installed on the PC by preparing two MICs 10 and 11 and feeding signals from them into PC, its implementation is easy.<br/>
In addition, it considered as an environment under which speech recognition is performed most frequently at home that users speak voice command toward a controlled device such as a television. In this case, a loudspeaker of the television or the like controlled device is placed facing toward a speaker side from a controlled device side, and it may happen that the speaker wants to speak a control command to a microphone device in a situation where speech or various guidance is coming from the loudspeaker. In this situation, it is necessary that the sound coming from loudspeaker to the microphone device is attenuated by some sort of means. A speech from the controlled device can be easily separated from a speech coming from the loudspeaker by using the sound source separation device 1, thereby speech recognition performance is improved.</p><heading id="h0013">&lt;Result of evaluation experiment&gt;</heading><p id="p0058" num="0058">Next, result of evaluation experiment will be described. Conventionally, speech recognition has been applied to device control in automobiles and other purposes. Although technologies for reducing noises such<!-- EPO <DP n="39"> --> as a blower sound of an air conditioner, a road noise, and a engine sound have conventionally been established, a speech recognition technology without being affected by a voice and the like has not been established. In view of the above, it is important to provide a speech recognition technology which realizes the followings features: (1) separation between speech of a driver in a driver's seat and speech of a passenger in a passenger's seat (hereinafter referred to as " speech of the driver's seat and the passenger's seat"), (2) movement of a speaker's head position is allowable, (3) both large beam width and separation performance are achieved at the same time, (4) function is provided using a few MICs, and (5) a driver or a passenger is not forced to be quiet during use of speech recognition.</p><p id="p0059" num="0059">To show the effectiveness of the sound source separation device 1, speech recognition experiment was conducted in which two speakers spoke simultaneously in a car, recording was performed by two MICs, and sound sources were separated by the sound source separation device. This sound source separation device has been developed for purpose of separation of two sound sources, and allows, for example, separation of speech of the driver's seat and the passenger's seat as usage in an automobile. As installation positions of the MICs for this purpose, a central part in the automobile such as a mounting position L1 or a mounting position L2 is<!-- EPO <DP n="40"> --> suitable as shown in <figref idrefs="f0011">Figure 14</figref>. If they are installed in the mounting position L2, since a rear-view mirror 400 faces toward the driver, the MICs may be installed such that a mounting direction roughly faces toward front when the MICs are mounted or a directional characteristics control function described later may be provided. Since beam width is originally large in the sound source separation device 1, precise positioning is not required. In addition, when they are installed in the mounting position L2, it may be useful for suppressing a reflection from backside to devise a microphone module or use a directional microphone.<br/>
Conditions of the evaluation experiment are shown in <figref idrefs="f0012">Figure 15</figref>, and results of the evaluation experiment after sound separation are shown in <figref idrefs="f0012">Figures 16A and 16B</figref>. As shown in <figref idrefs="f0012">Figure 16A</figref>, in a case of simultaneous speech of two speakers in a car (in-car simultaneous speech), when processing was not applied, i.e., in a conventional method 1 which uses only 1-channel microphone, 29% (in stopping), 27% (in driving at 60 km/h) were shown, and when the present speech separation technique was applied, improvements were shown as 78%(in stopping), 78%(in driving at 60 km/h). In addition, <figref idrefs="f0012">Figure 16B</figref> shows the evaluation result of the percentage of the case that a speech of the passenger's seat side is speech-recognized in mistake for a speech of the driver's seat side or a speech of the driver's seat side is speech-recognized in mistake for a speech of the passenger's<!-- EPO <DP n="41"> --> seat side. When the conventional 1-channel microphone was only used (in-car one person speech), such speech recognition results were outputted in 93% of all the speeches (rejection rate 7%), and when the present technique was applied, the percentage of output of such speech recognition results was 0% (rejection rate 100%). In addition, performance comparison in stop states was performed in which "METHOD AND DEVICE FOR NOISE COMPONENT SUPPRESSION PROCESSING METHOD" (Patent No. 3484112) as a conventional art using two MICs was defined as a conventional method 2. Although the conventional method 2 is a scheme which performs adaptive beamforming processing while estimating arrival directions of a target sound and a noise, emphasizes the target sound and the noise, and performs spectral subtraction in a frequency domain to subtract a signal of the emphasized noise from a signal of the emphasized target sound, for preventing the effect of estimation error of arrival direction, processing was performed in which a target sound and a noise are both assumed to be known (arrive from a fixed direction), and an optimum value of an adaptive beamformer section is obtained, and then a target sound (a speaker 1) and a noise (a speaker 2) are simultaneously played, and the target sound is extracted (see <figref idrefs="f0012">Figure 16A</figref>).</p><heading id="h0014">&lt;Second embodiment&gt;</heading><!-- EPO <DP n="42"> --><p id="p0060" num="0060">Next, the second embodiment will be described. <figref idrefs="f0013">Figure 17</figref> shows a configuration of a sound source separation system according to the second embodiment. Although inputs from the MICs 10 and 11 are converted into frequency components at first by the spectrum analysis sections 20 and 21 in the above described first embodiment, in the present embodiment, a directional null is formed in a time domain by beamformers 80 and 81 at first, and generate a signal in which a signal from a specific arrival direction is attenuated by beamformers 80 and 81, and then conversion into frequency components is performed by the spectrum analysis sections 20 and 21. In <figref idrefs="f0013">Figure 17</figref>, those having the same functions as in <figref idrefs="f0001">Figure 1</figref> are given the same reference numbers. Configurations of the beamformers 80 and 81 are implemented by performing filter processing configured as a form of an FIR filter or the like as shown in <figref idrefs="f0014">Figure 18</figref>. At this time, coefficients of the FIR filter can be obtained by converting the weighted coefficients that are in complex conjugate pairs in the frequency domain shown in <figref idrefs="f0002">Figure 3</figref> into filter coefficients in a time domain.</p><heading id="h0015">&lt;Third embodiment&gt;</heading><p id="p0061" num="0061">Next, the third embodiment will be described. <figref idrefs="f0015">Figure 19</figref> and <figref idrefs="f0016">Figure 20</figref> are diagrams which show configurations of sound source separation systems according to the third embodiment. As described above, the target sound spectrum extraction sections 50 and 51<!-- EPO <DP n="43"> --> shown in <figref idrefs="f0001">Figure 1</figref> and <figref idrefs="f0013">Figure 17</figref> are implemented as the configuration shown in <figref idrefs="f0003">Figure 5</figref>, and are configured to perform sound source separation processing using an optimum threshold value obtained by experiment. On the other hand, as shown in <figref idrefs="f0005">Figure 8</figref>, it is understood that dr<sub>i</sub>(ω)(i=1, 2) which are outputs of the difference calculation sections 500 and 501 in the target sound spectrum extraction sections 50 and 51 are point-symmetric with respect to the front 0 degree. Therefore, when thresholds are set to "0" in the coefficient conversion sections 501 and 511 in the target sound spectrum extraction sections 50 and 51 by checking only signs in the calculation sections 500 and 501, if it is positive, power spectrum information of a sound source signal coming from right direction (0 degree to 90 degrees) is extracted, and if it is negative, power spectrum information of a sound source signal coming from left direction (-90 degrees to 0 degree) is extracted. Accordingly, the entire configurations shown in <figref idrefs="f0001">Figure 1</figref> and <figref idrefs="f0013">Figure 17</figref> are simplified to configurations as shown in <figref idrefs="f0015">Figure 19</figref> and <figref idrefs="f0016">Figure 20</figref>. A target sound spectrum extraction section 90 in <figref idrefs="f0015">Figure 19</figref> and <figref idrefs="f0016">Figure 20</figref> is implemented as a configuration shown in <figref idrefs="f0017">Figure 21</figref>.</p><p id="p0062" num="0062">In <figref idrefs="f0017">Figure 21</figref>, power spectrum information of the beamformers 30 and 31 computed by the power computation sections 40 and 41 is inputted to a difference calculation section 900 inside the target sound spectrum<!-- EPO <DP n="44"> --> extraction section 90. Then, subtraction processing is performed by a subtractor 900a, and sound source signals only from targets direction are extracted by coefficient conversion sections 910 and 920 respectively. Specifically, the coefficient conversion section 910 is a block for extracting a sound source from right direction (0 degree to 90 degrees), and if an input is positive, its spectrum information is outputted as information coming from right direction (0 degree to 90 degrees), and if an input is negative, spectrum information is considered as information of a sound source arriving from a direction other than the target, and thus not outputted. On the other hand, the coefficient conversion section 920 is a block for extracting a sound source from left direction (-90 degrees to 0 degree), and if an input is negative, its spectrum information is outputted as information coming from left direction (-90 degrees to 0 degree), and if an input is positive, spectrum information is considered as information of a sound source arriving from a direction other than the target, and thus not outputted. The above described operation allows separation of sound signals arriving from left and right with respect to the perpendicular line to the straight line connecting the two MICs 10 and 11.<br/>
Configurations of the sound source separation system shown in <figref idrefs="f0015">Figure 19</figref> and the sound source separation system shown in <figref idrefs="f0016">Figure 20</figref> are different in that beamforming processing is performed in a frequency domain or a time<!-- EPO <DP n="45"> --> domain. In <figref idrefs="f0015">Figure 19</figref>, beamforming processing is performed in a frequency domain, and in <figref idrefs="f0016">Figure 20</figref>, it is performed in a time domain.</p><heading id="h0016">&lt;Fourth embodiment&gt;</heading><p id="p0063" num="0063">Next, the fourth embodiment will be described. <figref idrefs="f0018">Figure 22</figref> is a diagram which shows a configuration of a sound source separation system according to the fourth embodiment. Spectral subtraction sections 100 and 101 shown in the figure performs spectral subtraction processing for eliminating a stationary noise, a diffuse noise, and the like whose arrival direction is unknown, which have superimposed on each target sound extracted by the target sound spectrum extraction sections 50 and 51. This configuration is effective when it is used in an environment in which there is a fan noise of an air-conditioner or a projector, or the like in a meeting room, but is particularly effective when it is used in an automobile. Such as in a case where a driver and a passenger in a passenger's seat is on board in an automobile, for example, speech of the driver's seat and the passenger's seat can be extracted separately by using the foregoing scheme. However, one whose arriving direction is unknown and diffuse noise such as a blower sound of an air conditioner, a road noise, and a wind noise cannot be eliminated by the foregoing method. Affection of these noises can be eliminated by adding spectrum subtraction processing in a subsequent stage of<!-- EPO <DP n="46"> --> the process. As the spectrum subtraction processing, there are a type in which a voice activity detection is performed for an input signal of one MIC, a noise spectrum in non-speech section is estimated, and a previously estimated noise component is scaled and subtracted from the speech section; and a type in which, using a spectrum of a signal in which a noise is dominantly recorded and a spectrum of a signal in which a speech is dominantly recorded, a spectrum of the signal in which the noise is dominantly recorded is scaled and subtracted from a spectrum of the signal in which the speech is dominantly recorded. Processing based on a 1-microphone system is enough for speech recognition in many cases, and this is employed also in the present embodiment. To support this, in the sound source separation system according to the present embodiment, a voice activity detection section 110 and the spectral subtraction sections 100 and 101 are newly added to the sound source separation system according to the first embodiment.</p><p id="p0064" num="0064">In <figref idrefs="f0018">Figure 22</figref>, sound sources R1 and R2 are target sounds, and a sound source R3 represents noises such as a stationary noise whose arrival direction is unknown and a diffuse noise. Many of these noises do not have clear directivity. In the case of these noises, at output of a target sound spectrum extraction sections, a noise having weak directivity is often contained in a target sound<!-- EPO <DP n="47"> --> spectrum extraction section which extracts a sound source from its direction, and a noise having no directivity, an impact noise occurred when a joint of a road is passed, or the like is often detected in spectrums extracted from left and right alternately. However, these noises can be eliminated by the spectral subtraction sections 100 and 101. As a spectral subtraction, a continuous spectral subtraction which does not require voice activity detection section may be used.</p><heading id="h0017">&lt;Fifth embodiment&gt;</heading><p id="p0065" num="0065">Next, the fifth embodiment will be described. <figref idrefs="f0019">Figure 23</figref> shows a configuration of a sound source separation system according to the fifth embodiment. In the present embodiment, a measure is shown in a case where the two target sound sources R1 and R2 to be separated are placed far away from positions symmetrical with respect to the perpendicular line to the straight line connecting the two MICs 10 and 11. In the present embodiment, for this measure, the sound source separation system includes arrival direction estimation section 120 for detecting approximate positions of two target sound sources R1 and R2, and delay operation is applied to input of one of the MICs by a directional characteristics control section 140 so as to make the two target sound sources R1 and R2 to be separated virtually symmetrical with respect to the perpendicular line to the straight line connecting the two MICs 10 and 11 as much as possible with approximate sound source arrival<!-- EPO <DP n="48"> --> direction information of the two target sound sources R1 and R2 based on its positions estimated by the arrival direction estimation section 120.</p><p id="p0066" num="0066"><figref idrefs="f0020">Figure 24</figref> shows a situation where two sound sources R1 and R2 are symmetrical with respect to a straight line which has rotated by θτ with respect to the perpendicular line to the straight line connecting the MICs 10 and 11. In this case, a situation equivalent to θτ rotation can be achieved by applying a certain delay τd to a signal captured by one of the MICs. In addition, it may be configured that filter parameters of the beamformers 30 and 31 which are optimum for separating two sound sources are prepared based on positional relationships with respect to the perpendicular line to the straight line connecting the two MICs for some combinations of two sound sources, beamformer's filter parameters which are assumed to be optimum for separating the two sound sources in a current situation are selected by beamformer control based on approximate arrival direction information of the two sound sources from the arrival direction estimation section 120, and the selected beamformer's filter parameters are set in the beamformers 30 and 31.</p><p id="p0067" num="0067"><figref idrefs="f0020">Figures 25A and 25B</figref> show configuration examples of the directional characteristics control section 140. A configuration example shown in <figref idrefs="f0020">Figure 25A</figref> shows an<!-- EPO <DP n="49"> --> example in which a certain delay τd is applied to each frequency component in a frequency domain of a signal captured by one of the MICs. In the case of the configuration example shown in <figref idrefs="f0020">Figure 25A</figref>, x<sub>1</sub>(ω) is multiplied by e<sup>-j</sup>ωτ by the multiplier 100a, thereby achieving a delay operation. On the other hand, when beamformer is performed in a time domain, filtering processing needs to be performed by a delay device 100b in an FIR filter form as shown in <figref idrefs="f0020">Figure 25B</figref>.<br/>
In addition, instead of applying a delay to input of one of the MICs, a half of the delay may be applied to each of inputs of both the MICs so that the same quantity of the delay operation is achieved as an entire processing. In other words, instead of applying a delay τd to a signal captured by one of the MICs, a delay τd/2 is applied to the signal captured by the one of the MICs and a delay -τd/2 is applied to a signal captured by the other of the MICs, so that a total delay difference can be τd.</p><heading id="h0018">&lt;Sixth embodiment&gt;</heading><p id="p0068" num="0068">Next, the sixth embodiment will be described. <figref idrefs="f0021">Figure 26</figref> is a diagram which shows a configuration of a sound source separation system according to the sixth embodiment. The sound source separation system according to the present embodiment has a configuration which lays weight on audibility in consideration of application to hands-free call in an automobile and the like. Such as<!-- EPO <DP n="50"> --> in a case where a driver and a passenger in a passenger's seat is on board in an automobile, for example, speech of the driver's seat and the passenger's seat can be separately extracted using the foregoing scheme. However, one whose arriving direction is unknown and diffuse noise such as a blower sound of an air conditioner, a road noise, and a wind noise cannot be eliminated by the foregoing method. In these cases, as described in the fourth embodiment, influence of these noises can be eliminated by adding spectrum subtraction processing in a subsequent stage of the process, and this is most suitable for applications such as speech recognition in which sound quality in terms of audibility is not taken in account. However, when this is used in a MIC for a hands-free call device, a residual unremoved noise called a musical noise may become a problem. In the present invention, since left and right sounds are separated with respect to the perpendicular line to the straight line connecting the two MICs 10 and 11, a noise such as a diffuse noise whose directivity continually varies may be allocated into left and right separation results irregularly, thereby degrading sound quality.</p><p id="p0069" num="0069">Therefore, in the present embodiment, utilizing that the sound source separation scheme according to the present invention is not affected by time variation of a microphone gain, post-filter processing which is typically used in a subsequent stage of microphone array<!-- EPO <DP n="51"> --> processing is added in a previous stage of beamformer processing so that a diffuse noise, a stationary noise, and the like are reduced, thereby a musical noise after sound source separation is prevented.</p><heading id="h0019">&lt;Seventh embodiment&gt;</heading><p id="p0070" num="0070">Next, the seventh embodiment will be described. <figref idrefs="f0022">Figure 27</figref> is a diagram which shows a configuration of a sound source separation system according to the seventh embodiment. This sound source separation system illustrates a configuration in which three MICs 10, 11 and 12 are used to separate three sound sources. In the figure, using MICs 10 and 12 and MICs 11 and 12, left and right sound source signals are separated with respect to a perpendicular line to a straight line connecting respective two MICs, and using a total of four sound source signals which have been separated using two pairs of MICs, a sound source R3 arriving from the vicinity of front of the MICs 10 and 11 is finally separated by a target sound extracting section 160.</p><p id="p0071" num="0071">When sound sources to be separated by this configuration is described using <figref idrefs="f0023">Figure 28</figref>, sound sources arriving from right and left to a perpendicular line can be separated with respect to a perpendicular line a and a perpendicular line b which are connecting two MICs respectively as shown in <figref idrefs="f0023">Figure 28</figref>. In <figref idrefs="f0023">Figure 28</figref>, when assuming that a sound source locates in each area of zones A, B and C, sound source signals arriving from zone<!-- EPO <DP n="52"> --> A and zones B and C can be separated using the perpendicular line a, and sound source signals arriving from zones A and B and zone C can be separated using the perpendicular line b. In <figref idrefs="f0022">Figure 27</figref>, blocks for performing these separations are separation sections b0 and b1. From a signal on which the three sound sources are superimposed, the separation section b0 can separate a sound source signal S<sub>A</sub>(ω) from zone A and a signal in which sound source signals S<sub>Bmix</sub>(ω) and S<sub>Cmix</sub>(ω) from zones B and C are mixed, and similarly the separation section b1 can separate a signal in which sound source signals S<sub>Amix</sub>(ω) and S<sub>Bmix</sub>(ω) from zones A and B are mixed and a sound source signal S<sub>C</sub>(ω) from zone C, and thus S<sub>A</sub>(ω) and S<sub>C</sub>(ω) have been separated in this stage. From the four signals obtained in this way, S<sub>B</sub>(ω) can be obtained by a predetermined operation in a power spectrum domain by the target sound extraction section 160. As used herein, S<sub>Amix</sub>(ω), S<sub>Bmix</sub>(ω), S<sub>Cmix</sub>(ω) represent respective signals which mixed with other signals.</p><p id="p0072" num="0072">Although it is assumed that sound source information does not exists in zone D or, even if sound information exists, its level is low in the above description, if a sound source exists in zone D, interfusion of a sound signal from zone D can be greatly reduced by using directional microphones as the three MICs 10, 11 and 12.<br/>
<figref idrefs="f0023">Figure 29</figref> shows an example of installation of a unidirectional microphone. While, in general, when<!-- EPO <DP n="53"> --> directional microphones are used in this way, performance as designed may not be achieved at a beamformer section of an array microphone due to uneven directional characteristics of each MIC or the like, certain performance can be achieved without being affected by uneven directional characteristics as in the case where the present scheme is not affected by uneven sensitivity of MIC elements by its nature.</p><p id="p0073" num="0073">A processing scheme in the target sound extraction section 160 will be described in detail with reference to <figref idrefs="f0024 f0025 f0026">Figures 30 to 32</figref>. A processing scheme shown in <figref idrefs="f0024">Figure 30</figref> is a processing scheme similar to 2-channel spectral subtraction processing. Thus a sum of power spectrums of a target sound and an interfering sound has been determined for a channel, and a power spectrum of an interfering sound has been determined for the other channel, therefore a target sound S<sub>B</sub>(ω) can be obtained by performing subtraction on them by a subtractor 100a.</p><p id="p0074" num="0074">In a processing scheme shown in <figref idrefs="f0025">Figure 31</figref>, since two signals in which an interfering sound is superimposed on a target sound can be obtained, they are added by an adder 101b so that a value of power spectrum information of the target sound is doubled, and on the other hand, interfering sounds are summed by the adder 101a so that a power spectrum of the interfering sound is obtained, and then it is multiplied by a certain factor (O.E.F 1 to 2)<!-- EPO <DP n="54"> --> by a multiplier 101c and a difference between the output of the multiplier 101c and the output of the adder 101b is calculated by a subtractor 101d, thereby the target sound is extracted. In addition, since the sound volume is larger than that of the original sound signal at the stage of output from the subtractor 101d, level adjustment is performed by a level adjustment section 101e.</p><p id="p0075" num="0075">In a processing scheme shown in <figref idrefs="f0026">Figure 32</figref>, minimum value calculations 102a and 102b are used instead of the adders 101a and 101b shown in <figref idrefs="f0025">Figure 31</figref>. Although in <figref idrefs="f0025">Figure 31</figref> sound quality is good when O.E.F is larger than 1.0 in many cases, O.E.F is preferably around 1.0 in <figref idrefs="f0026">Figure 32</figref>. Although minimum value calculation is performed in <figref idrefs="f0026">Figure 32</figref>, maximum value calculation may be performed instead of minimum value calculation.<br/>
Although there may be a case where a position of a target sound source to be separated is placed far away from a position at which optimum separation performance is obtained by the present technique, as described in the fifth embodiment, operation for obtaining optimum separation performance as much as possible can be performed by applying a delay to an input signal outputted from a MIC so as to virtually change an arrival direction of a sound source.</p><p id="p0076" num="0076"><!-- EPO <DP n="55"> --><figref idrefs="f0027">Figure 33</figref> shows a usage example of the sound source separation system according to the present embodiment. For purpose of speech input to a PC, shown example has a directional characteristics to capture a speech from the front side of the PC by using three MICs 10, 11 and 12 with a small area for implementation.</p><heading id="h0020">&lt;Eighth embodiment&gt;</heading><p id="p0077" num="0077">Next, the eighth embodiment will be described. While, in the foregoing embodiments, (1) an embodiment which separates sound from left and right with respect to a straight line connecting between MICs using the two MICs, and (2) an embodiment which separates a sound from front side and sounds from the left and right of it using three MICs have been described, there is a case that it is desirable that a sound from front of a line connecting the two MICs 10 and 11 is separated and extracted using the two MICs 10 and 11 as shown in <figref idrefs="f0028">Figure 34</figref>.</p><p id="p0078" num="0078">In this case, directional characteristics control means applies a delay to an output signal from one of the two MICs 10 and 11 to create a virtual microphone position of the third channel as shown in <figref idrefs="f0029">Figure 35B</figref>, so that 3-microphone input shown in <figref idrefs="f0029">Figure 35A</figref> can be virtually realized. <figref idrefs="f0030">Figure 36</figref> shows a configuration example of the directional characteristics control means shown in <figref idrefs="f0029">Figures 35A and 35B</figref> which performs delay operation. While Di (i=1,2,3,4) denotes a delay element in the figure, in actual delay operation, delay operation<!-- EPO <DP n="56"> --> may be performed in a time domain, or may be performed in a frequency domain after spectrum analysis.</p><p id="p0079" num="0079"><figref idrefs="f0031">Figure 37</figref> shows a configuration example of a sound source separation device system according to the present embodiment. Directional characteristics control means 141 and 142 are composed of spectrum analysis sections 20 and 21 and delay elements which performs delay processing. In a processing sequence, delay processing may be performed after spectrum analysis processing (Type. 1 in the figure), or spectrum analysis processing may be performed after delay processing (Type. 2 in the figure). Output signals of the directional characteristics control means 141 and 142 are processed by the beamformers 30 and 31 and the power computation sections 40 and 41 or the like, which are blocks in later half part of the present technique from NBF, and the processed signals are inputted to a target sound extraction section 52.</p><p id="p0080" num="0080">In <figref idrefs="f0032">Figure 38</figref> and <figref idrefs="f0033">Figure 39</figref>, examples of processing schemes in the target sound extraction section 52 are shown. <figref idrefs="f0032">Figure 38</figref> shows one example of a processing scheme in which θ<sub>1</sub> and θ<sub>2</sub> are angles symmetrical with respect to the perpendicular line to the straight line connecting the MICs 11 and 12, and <figref idrefs="f0033">Figure 39</figref> shows one example of a processing scheme in which θ<sub>1</sub> and θ<sub>2</sub> are<!-- EPO <DP n="57"> --> angles asymmetrical with respect to the perpendicular line.</p><heading id="h0021">&lt;Ninth embodiment&gt;</heading><p id="p0081" num="0081">Next, the ninth embodiment will be described. <figref idrefs="f0034">Figure 40</figref> is a diagram which shows a configuration of a speech recognition system for controlling on-vehicle equipment according to the ninth embodiment. In the present embodiment, an example in which the sound source separation device 1 according to the present invention is applied to the speech recognition system for controlling on-vehicle equipment provided in a vehicle such as an automobile. In this example of application, speech of the driver's seat and the passenger's seat are captured by the two MICs 10 and 11, the speech of the driver's seat and the passenger's seat are separated by the sound source separation device 1. And control of equipment, system response, and the like are performed by using only valid recognition result of each of the speech from the driver's seat and the passenger's seat depending on voice activity detection, speech recognition processing, speech recognition result and a running state of an automobile and other driving states. Thereby, improvement of reliability, extension of flexibility of response, and the like of the speech recognition system for controlling on-vehicle equipment are provided.</p><p id="p0082" num="0082">The speech recognition system for controlling on-vehicle equipment stores a passenger's seat side<!-- EPO <DP n="58"> --> recognition vocabulary list 190, a driver's seat side recognition vocabulary list 191, a passenger's seat side valid vocabulary list 210, and a driver's seat side valid vocabulary list 211 in a storage device, as characteristic data of this system. The driver's seat side recognition vocabulary list 191 is a list of candidates of vocabulary spoken from the driver's seat side, and the passenger's seat side recognition vocabulary list 190 is a list of candidates of vocabulary spoken from the passenger's seat side. The driver's seat side valid vocabulary list 211 is a list of valid vocabulary in the driver's seat side depending on a state of the vehicle (a running state of the automobile and other driving states). The passenger's seat side valid vocabulary list 210 is a list of valid vocabulary in the passenger's seat side depending on a state of the vehicle. "Valid" used herein refers to a state in which output of a control command corresponding to a vocabulary item (a voice command) is allowed.</p><p id="p0083" num="0083">Operation of the present system will be described with reference to <figref idrefs="f0034">Figure 40</figref>. Speech spoken by a driver in a driver's seat and a passenger in a passenger's seat are captured by the two MICs 10 and 11, separated into a speech of the driver's seat and a speech of the passenger's seat by the sound source separation device 1, and then inputted to voice activity detection sections 170 and 171 and speech recognition sections 180 and 181,<!-- EPO <DP n="59"> --> which have been prepared for the driver and the passenger in the passenger's seat respectively. In this case, because speech of the two speakers are accurately separated at output of the sound source separation device 1 according to the present embodiment, speech sections of the both speaker can be accurately separated by the passenger's seat side voice activity detection section 170 and the driver's seat side voice activity detection section 171, as well as information in which a speech of the other speaker is suppressed can be provided to the passenger's seat side speech recognition section 180 and the driver's seat side speech recognition section 181, so that speech recognition processing without being affected by a speech of the other speaker can be accurately performed.</p><p id="p0084" num="0084">In this example of application, speech recognition sections 180 and 181 are provided with the passenger's seat side recognition vocabulary list 190 and the driver's seat side recognition vocabulary list 191 for indicating what kind of vocabulary should be recognized independent of a state of the system, which are dedicated to the sections respectively. Each of the speech recognition sections 180 and 181 performs speech recognition according to these vocabulary lists and outputs a speech recognition result to a control section/state transition section 200.<br/>
<!-- EPO <DP n="60"> -->A state transition section 201 provided in the control section/state transition section 200 can transit to a next state based on the speech recognition result and a current state. A control section 202 provided in the control section/state transition section 200 checks which voice commands can be responded (a control command can be outputted), based on the current state obtained from the state transition section 201 and the speech recognition result from the speech recognition sections 180 and 181, and the passenger's seat side valid vocabulary list 210 and the driver's seat side valid vocabulary list 211 which have been prepared for the passenger's seat side and the driver's seat side respectively. In the valid vocabulary lists 210 and 211 shown in the figure, when a set of a state and a voice command corresponds to "O", response to the voice command is allowed. For example, in the driver's seat side, voice commands to which a response is allowed when a state is "driving" are "more", "turn on light", and " cool down", and voice commands to which a response is forbidden are "air-conditioner control", "CD", and "MD".</p><p id="p0085" num="0085">Then, response is made only to an allowed voice command, and control is performed for operating an air-conditioner, turning on a light, or so on, so that an occupant of the vehicle can get comfortable in the car. In addition, reliability of the speech recognition system for controlling on-vehicle equipment can be improved as<!-- EPO <DP n="61"> --> well as higher degree of flexibility of specification design can be provided for creating an application using speech recognition. According to the above described usage example, simultaneous speech of a driver in a drive's seat and a passenger in a passenger's seat can be recognized at the same time, and whether speech is from the driver's seat or the passenger's seat can be surely detected and recognized even if either one speaks, so that behavior of a passenger is not limited, and a speaker and a response to a voice command of the speaker can be designed individually.</p><heading id="h0022">&lt;Tenth embodiment&gt;</heading><p id="p0086" num="0086">Next, the tenth embodiment will be described.</p><p id="p0087" num="0087"><figref idrefs="f0035">Figures 41A and 41B</figref> show a mobile phone 300 according to the present embodiment. The mobile phone 300 is mounted with MICs 10 and 11 and a not shown sound source separation device. This mobile phone 300 is normally used for visual telephone, and can be used as a sound collecting microphone by switching a mode. <figref idrefs="f0035">Figure 41A</figref> is a diagram which shows that the MICs 10 and 11 operate as a sound collecting microphone for visual telephone, and <figref idrefs="f0035">Figure 41B</figref> is a diagram which shows that the MICs 10 and 11 operate as a microphone. It can be used in a situation where, in a medium-sized conference room or the like a presenter does not need to use a microphone since the room is not so large, but the room is so spacious that a speech of the presenter is difficult to hear when the voice is small.<!-- EPO <DP n="62"> --></p><p id="p0088" num="0088">As described above, at least two MICs are arranged separately from each other, directional nulls are formed in a time domain or a frequency domain at angles symmetrical with respect to a perpendicular line to a straight line connecting the two MICs by beamformers, signals are converted into a frequency domain if a directional null is formed in a time domain, a difference between power spectrums of the both beamformers is calculated, and coefficient conversion is performed on the obtained result, and thereby directional characteristics having some width with respect to left and right directional nulls are formed, so that sound source separation can be performed. In this way, a feature that a directional characteristics is not affected by sensitivity of MIC elements can be achieved, and without being affected by uneven sensitivity of MIC elements, a deviation from a supposed arrival direction of a sound source and a strong initial reflection can be handled by a moderately wide directional characteristics, and therefore a stable separation characteristics of two sound sources can be achieved.</p><heading id="h0023">&lt;Eleventh embodiment&gt;</heading><p id="p0089" num="0089">Next, the eleventh embodiment will be described. <figref idrefs="f0036">Figure 42</figref> shows an example in which a target sound in zone A is extracted in a situation where the target sound to be extracted locates in zone A (for example, a zone of a driver's seat) and interfering sounds locates in<!-- EPO <DP n="63"> --> places other than that (zone B, zone C and zone D). When an array microphone 2001 ((For example, it is located in the front of a car cabin) for example, it is mounted at a rear-view mirror) is used, although sounds locating in zone A/C (for example, zones of a driver's seat and a seat behind it) and zone B/D (for example, zones of a passenger's seat and a seat behind it) can be separated, sounds locating in zone A (for example the zone of the driver's seat) and zone C (for example, the zone of the seat behind it) cannot be separated. However, by providing an array microphone 2002 using the present technique in a position which is a boundary of zone A/B and zone C/D as shown in the figure, sounds locating in zone A/B and zone C/D can be separated, and therefore only a sound locating in zone A can be extracted.</p><p id="p0090" num="0090">Specifically, if speakers A, B, C and D locating in zone A, B, C and D speak simultaneously, by using the array microphone 2002 placed on the boundary of zone A/B and zone C/D at first, a sound from zone A/B and a sound from zone C/D can be separated. Then, a sound from zone A/C and a sound form zone B/D can be separated by the microphone array 2001. Lastly, the sound from zone A/C obtained by using the microphone array 2001 is compared with the sound from zone A/B obtained by using the microphone array 2002 for each frequency domain, and thereby a frequency component existing in both of the sounds in common can be separated as a sound from zone A.<!-- EPO <DP n="64"> --> Sounds also from zones B, C and D can be obtained by similar processing respectively.</p><heading id="h0024">&lt;Twelfth embodiment&gt;</heading><p id="p0091" num="0091">Next, the twelfth embodiment will be described. In <figref idrefs="f0037">Figure 43</figref>, a situation is assumed where equipment is operated by speech recognition in an environment such as an automobile. <figref idrefs="f0037">Figure 44</figref> shows a relation between a guidance speech for the equipment operation and speech of speakers.</p><p id="p0092" num="0092">In this case, a guidance speech such as "Please say your destination." is played from a loudspeaker 15 to prompt timing of speech of a speaker A, and then a machine sound such as "pip" is played, and after that, the speaker A speaks a voice command. However, as the user gets familiar with equipment operation using voice commands, a situation where the speaker A starts speak during guidance speech is played occurs as shown in <figref idrefs="f0037">Figure 44</figref>, thereby speech recognition performance degrades.<br/>
As a measure against such a situation, an echo canceller is generally used to adaptively estimate and eliminate a guidance sound mixed in a sound captured from the MIC 10. As other measures, as shown in <figref idrefs="f0038 f0039 f0040 f0041">Figures 45 to 48</figref>, spectral subtraction is performed with respect to on input signal to the MIC 10 after frequency analysis (<figref idrefs="f0038">Figure 45</figref>, <figref idrefs="f0039">Figure 46</figref>), and a major contained speech which is guidance speech or speech of speaker A for each<!-- EPO <DP n="65"> --> frequency component is estimated, and only a frequency component in which major contained speech is the speech of the speaker A is extracted as speech of the speaker A (<figref idrefs="f0040">Figure 47</figref>, <figref idrefs="f0041">Figure 48</figref>).</p><p id="p0093" num="0093">In <figref idrefs="f0038">Figure 45</figref> and <figref idrefs="f0040">Figure 47</figref>, a filter section 1001 is a filter which simulates an acoustic reflection path from the loudspeaker 15 to the MIC 10, and it may be obtained previously using an impulse response from the loudspeaker 15 to the MIC 10 or may be dynamically obtained by adaptive filter processing.<br/>
A gain operation section 1002 is a section for determining an over subtraction factor that is used in performing spectral subtraction, and selects one gain from about 1 to 10 according to volume of the loudspeaker 15 and uses the selected gain.</p><p id="p0094" num="0094">In addition, a target sound extraction section 1003 in <figref idrefs="f0038">Figure 45</figref> performs processing as shown in <figref idrefs="f0039">Figure 46</figref> based on outputs from the gain operation section 1002 and the spectrum analysis section 21 and outputs a signal of processing result to a time waveform conversion section 1004.<br/>
A threshold calculation section 1011 in <figref idrefs="f0040">Figure 47</figref> determines a threshold th based on average energy of guidance speech.<br/>
In addition, a target sound extraction section 1012 in <figref idrefs="f0040">Figure 47</figref> performs processing as shown in <figref idrefs="f0041">Figure 48</figref><!-- EPO <DP n="66"> --> based on outputs from the threshold calculation section 1011 and the spectrum analysis section 21 and outputs a signal of processing result to the time waveform conversion section 1004. Incidentally, th<sub>mim</sub> shown in <figref idrefs="f0041">Figure 48</figref> becomes a threshold value for determining that X<sub>Far</sub>(ω) shown in the figure is a valid input.</p><p id="p0095" num="0095">In addition, the time waveform conversion section 1004 performs processing similar to the time waveform conversion sections 70 and 71 of the first embodiment.<br/>
The conventional method can handle a situation where only the speaker A speaks by the above described configuration as shown in <figref idrefs="f0037">Figure 43</figref>. However, there may be a case where not only the speaker A but also the speaker B speaks as shown in <figref idrefs="f0042">Figure 50</figref> when there are not only the speaker A (for example, an occupant in the driver's seat) but also the speaker B (for example, an occupant in the passenger's seat) as shown in <figref idrefs="f0041">Figure 49</figref>, and this situation cannot be handled by the above described configuration.</p><p id="p0096" num="0096">As shown in <figref idrefs="f0042">Figure 51</figref>, this situation can be handled by combining the present technique with guidance speech elimination sections 1021 and 1022 shown in <figref idrefs="f0038">Figure 45</figref> or <figref idrefs="f0040">47</figref>.<br/>
Specifically, in <figref idrefs="f0042">Figure 51</figref>, an input on which a guidance speech, a speech of the speaker A, and a speech of the speaker B are superimposed is given to the<!-- EPO <DP n="67"> --> MIC 10 and the MIC 11 in a situation where a guidance speech is played from the loudspeaker 15, and both the speaker A and the speaker B speak simultaneously. In this case, the guidance sound deleting section 1021 and the guidance speech elimination section 1022 eliminate the guidance speech by the method shown in <figref idrefs="f0038">Figure 45</figref> or <figref idrefs="f0040">47</figref>, and as a result, outputs a signal on which both speech of the speaker A and the speaker B are superimposed. In addition, to avoid waste of computation for input to the present technique to be used as post-processing, a frequency component is inputted to the present technique (<figref idrefs="f0001">Figure 1</figref>) as is without being converted back to a time waveform. Processing of a spectrum analysis section is omitted since frequency component information is inputted in the latter stage of the present technique, and it is directly inputted to a beamformer section and processed by applying the present technique, so that a speech of the speaker A and a speech of the speaker B can be obtained as an output result independently, and therefore reliability and performance of the speech recognition device and flexibility of application can be greatly enhanced.</p><p id="p0097" num="0097">In addition, by combination of the above described various functions and delay operation of a signal from a MIC, a directional characteristics that is narrow in front direction can be provided and a sound source signal only from a specific direction can be detected.<br/>
<!-- EPO <DP n="68"> -->In addition, since high separation performance can be ensured even in low frequencies without widening an interval between MICs, space for implementation can be reduced and application to portable devices and the like is allowed.<br/>
In the above described embodiments, each functional block of a sound source separation system is implemented by a program, it may be implemented by hardware using circuitry or the like.</p><heading id="h0025">Industrial Applicability</heading><p id="p0098" num="0098">The present invention is applicable to all of industries in which sound sources needs to be accurately separated such as for a speech recognition device, a car navigation system, a sound collector, a recorder, and control of equipment by a voice command.</p></description><claims mxw-id="PCLM56979326" lang="DE" load-source="patent-office"><!-- EPO <DP n="75"> --><claim id="c-de-01-0001" num="0001"><claim-text>Schallquellen-Trenneinrichtung zum Trennen eines Schallquellensignals von einer Zielschallquelle von einem gemischten Schall, der von mehreren Schallquellen abgestrahlte Schallquellensignale umfasst, unter Anwendung von mindestens zwei voneinander getrennt angeordneten Mikrofonen, aufweisend:
<claim-text>Beamforming-Mittel für</claim-text>
<claim-text>Ausführen einer ersten Beamforming-Bearbeitung zum Dämpfen eines aus einer im Voraus festgelegten Richtung ankommenden Schallquellensignals durch Ausführen von Berechnungen unter Anwendung von ersten Koeffizienten auf ein Ausgangssignal von den Mikrofonen, und</claim-text>
<claim-text>Ausführen einer zweiten Beamforming-Bearbeitung zum Dämpfen eines, aus einer zu der im Voraus festgelegten Richtung in Bezug auf eine Linie, die rechtwinklig zu einer die beiden Mikrofone miteinander verbindenden Geraden, verläuft, symmetrischen Richtung ankommenden Schallquellensignals durch Ausführen von Berechnungen unter Anwendung zweiter Koeffizienten, die komplexe Konjugationen der ersten Koeffizienten in einer Frequenzdomäne auf dem Ausgangssignal der Mikrofone ist;</claim-text>
<claim-text>Leistungsberechnungsmittel zum Berechnen der Leistungsspektrums-Information im Hinblick auf jedes der Schallquellensignale, die von den Beamforming-Mitteln erhalten werden; und</claim-text>
<claim-text>Zielschallspektrums-Extraktionsmittel zum Extrahieren von Spektrumsinformation aus einer Zielschallquelle auf Basis einer Differenz zwischen der durch die<!-- EPO <DP n="76"> --> Leistungsberechnungsmittel berechneten Leistungsspektrums-Information.</claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Schallquellen-Trenneinrichtung nach Anspruch 1, wobei die Beamforming-Mittel die erste Beamforming-Bearbeitung und die zweite Beamforming-Bearbeitung auf jede von (A) einer Kombination von beliebigen zwei von drei getrennt voneinander angeordneten Mikrofonen und (B) eine andere Kombination von zwei Mikrofonen unter den drei Mikrofonen angewendet wird.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Schallquellen-Trenneinrichtung nach Anspruch 1 oder 2, ferner aufweisend Richtungscharakteristik-Steuermittel für Anwenden einer Verzögerung auf ein Ausgangssignal von einem Mikrofon.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Schallquellen-Trenneinrichtung nach Anspruch 3, wobei das Richtungscharakteristik-Steuermittel durch Anwenden einer Verzögerung auf ein Ausgangssignal von mindestens einem Mikrofon unter zwei Mikrofonen wirklich Ausgangssignale von drei Mikrofonen erzeugt.</claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Schallquellen-Trenneinrichtung nach Anspruch 3 oder 4, ferner aufweisend Ankunftsrichtung-Bestimmungsmittel für Bestimmen einer Ankunftsrichtung des Schallquellensignals,<br/>
wobei das Ankunftsrichtung-Bestimmungsmittel eine Verzögerung auf ein Ausgangssignal des Mikrofons derart anwendet, dass zwei Schallquellen in Bezug auf eine Linie, die rechtwinklig zu einer die beiden Mikrofone miteinander verbindenden Geraden verläuft, auf Basis einer vom Ankunftsrichtungs-Bestimmungsmittel bestimmten Ankunftsrichtung praktisch symmetrisch platziert sind.</claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Schallquellen-Trenneinrichtung nach einem der<!-- EPO <DP n="77"> --> Ansprüche 1 bis 5, ferner aufweisend Mittel zur spektralen Subtraktion zum Ausführen von spektraler Subtraktionsbearbeitung an der vom Zielschallspektrums-Extraktionsmittel extrahierten Leistungsspektrums-Information.</claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Schallquellen-Trenneinrichtung nach einem der Ansprüche 1 bis 6, ferner aufweisend Mittel zur stationären Geräuschreduktion für Ausführen einer Bearbeitung zur Geräuschreduzierung bevor die Ausführung durch das Beamforming-Mittel stattfindet.</claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Spracherkennungseinrichtung, aufweisend die Schallquellen-Trenneinrichtung nach einem der Ansprüche 1 bis 7 und Spracherkennungsmittel zum Ausführen einer Spracherkennung von einem durch die Schallquellen-Trenneinrichtung getrennten Schallquellensignal.</claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Spracherkennungseinrichtung nach Anspruch 8, ferner aufweisend Erkennungsvokabularlisten-Speichermittel zum Speichern einer fahrersitzseitigen Erkennungsvokabularliste, die eine Liste von Vokabularkandidaten ist, die von der Fahrersitzseite aus in einem Fahrzeug gesprochen werden, und einer beifahrersitzseitigen Erkennungsvokabularliste, die eine Liste von Vokabularkandidaten ist, die von der Beifahrersitzseite aus gesprochen werden,<br/>
wobei das Spracherkennungsmittel Spracherkennungsbearbeitung eines Schallquellensignals auf Basis der im Erkennungsvokabularlisten-Speichermittel gespeicherten fahrersitzseitigen Erkennungsvokabularliste und beifahrersitzseitigen Erkennungsvokabularliste ausführt.</claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Spracherkennungseinrichtung nach Anspruch 8 oder 9,<!-- EPO <DP n="78"> --> ferner aufweisend:
<claim-text>Statusübergangsmittel für Management eines gegenwärtigen Status eines Fahrzeugs;</claim-text>
<claim-text>gültiges Vokabularlisten-Speichermittel für Speichern einer gültigen Vokabularliste der Beifahrersitzseite und der Fahrersitzseite in Abhängigkeit von einem Status des Fahrzeugs; und</claim-text>
<claim-text>Steuermittel zum Entscheiden, ob ein vom Spracherkennungsmittel erkannter Vokabularposten auf Basis des gegenwärtigen, durch das Statusübergangsmittel gemanagten Status des Fahrzeugs und der in dem gültigen Vokabularlisten-Speichermittel gespeicherten Vokabularliste gültig ist oder nicht, und zum Steuern in Abhängigkeit vom Ergebnis der Entscheidung.</claim-text></claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>Mobiltelefon, aufweisend die Schallquellen-Trenneinrichtung nach einem der Ansprüche 1 bis 7.</claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>Schallquellen-Trennverfahren, aufweisend:
<claim-text>einen Schritt des Schallquellensignal-Empfangs mit Eingeben von Schallquellensignalen, die von mehreren Schallquellen an mindestens zwei getrennt voneinander angeordnete Mikrofone abgestrahlt werden;</claim-text>
<claim-text>einen Schritt des Ausführens einer ersten Beamforming-Bearbeitung und einer zweiten Beamforming-Bearbeitung zum Dämpfen von, aus einer zu im Voraus festgelegten Richtungen in Bezug auf eine Linie, die rechtwinklig zu einer zwei Mikrofone miteinander verbindenden Geraden, verläuft, symmetrischen Richtung ankommenden Schallquellensignalen durch Ausführen von Berechnungen unter Anwendung von zwei gewichteten Koeffizienten,<!-- EPO <DP n="79"> --> die komplexe Konjugationen zueinander in einer Frequenzdomäne auf einem Ausgangssignal der jeweiligen Mikrofone sind;</claim-text>
<claim-text>einen Schritt der Leistungsberechnung mit Berechnen der Leistungsspektrums-Information im Hinblick auf jedes der Schallquellensignale, die im Schritt der Beamforming-Bearbeitung erhalten worden sind; und</claim-text>
<claim-text>einen Schritt der Zielschallquellenspektrums-Extraktion mit Extrahieren von Spektrumsinformation aus einer Zielschallquelle auf Basis einer Differenz zwischen der im Schritt der Leistungsberechnung berechneten Leistungsspektrums-Information.</claim-text></claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>Programm, das bewirkt, dass ein Computer Folgendes ausführt:
<claim-text>einen Schritt der Ausgangssignal-Erfassung mit Erfassen eines Ausgangssignals, das von mehreren Schallquellen von mindestens zwei getrennt voneinander angeordneten Mikrofonen abgestrahlte Schallquellensignale aufweist;</claim-text>
<claim-text>einen Schritt der Beamforming-Bearbeitung mit Ausführen einer ersten Beamforming-Bearbeitung und einer zweiten Beamforming-Bearbeitung zum Dämpfen von, aus einer zu im Voraus festgelegten Richtungen in Bezug auf eine Linie, die rechtwinklig zu einer zwei Mikrofone miteinander verbindenden Geraden, verläuft, symmetrischen Richtung ankommenden Schallquellensignalen durch Ausführen von Berechnungen unter Anwendung von zwei gewichteten Koeffizienten, die komplexe Konjugationen zueinander in einer Frequenzdomäne auf einem Ausgangssignal der jeweiligen Mikrofone sind;</claim-text>
<claim-text>einen Schritt der Leistungsberechnung mit Berechnen der Leistungsspektrums-Information im Hinblick auf jedes<!-- EPO <DP n="80"> --> der Schallquellensignale, die im Schritt der Beamforming-Bearbeitung erhalten worden sind; und</claim-text>
<claim-text>einen Schritt der Zielschallspektrums-Extraktion mit Extrahieren von Spektrumsinformation aus einer Zielschallquelle auf Basis einer Differenz zwischen der im Schritt der Leistungsberechnung berechneten Leistungsspektrums-Information.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56979327" lang="EN" load-source="patent-office"><!-- EPO <DP n="69"> --><claim id="c-en-01-0001" num="0001"><claim-text>A sound source separation device for separating a sound source signal of a target sound source from a mixed sound which includes sound source signals emitted from a plurality of sound sources using at least two microphones arranged separately from each other comprising:
<claim-text>beamforming means for</claim-text>
<claim-text>performing a first beamforming processing to attenuate a sound source signal arriving from a predetermined direction by performing computations using first coefficients on an output signal of said microphones, and</claim-text>
<claim-text>performing a second beamforming processing to attenuate a sound source signal arriving from a direction symmetrical to said predetermined direction with respect to a line that is perpendicular to a straight line connecting the two microphones by performing computations using second coefficients which are complex conjugate of said first coefficients in a frequency domain on the output signal of said microphone;</claim-text>
<claim-text>power computation means for computing power spectrum information with respect to each of sound source signals obtained by said beamforming means; and</claim-text>
<claim-text>target sound spectrum extraction means for extracting spectrum information of a target sound source<!-- EPO <DP n="70"> --> based on a difference between the power spectrum information calculated by said power computation means.</claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>The sound source separation device according to claim 1, wherein said beamforming means applies said first beamforming processing and said second beamforming processing to each of (A) a combination of any two microphones among three microphones which are arranged separately from each other and (B) another combination of two microphones among said three microphones.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>The sound source separation device according to claim 1 or 2, further comprising directional characteristics control means for applying a delay to an output signal of a microphone.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>The sound source separation device according to claim 3, wherein said directional characteristics control means virtually generates output signals of three microphones by applying a delay to an output signal of at least one microphone among two microphones.</claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>The sound source separation device according to claim 3 or 4, further comprising arrival direction estimating means for estimating a arrival direction of said sound source signal,<br/>
wherein said directional characteristics control means applies a delay to an output signal of the microphone such that two sound sources virtually locate symmetrically with respect to a line that is perpendicular to a straight line connecting the two microphones based on an<!-- EPO <DP n="71"> --> arrival direction estimated by said arrival direction estimating means.</claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>The sound source separation device according to any one of claims 1 to 5, further comprising spectral subtraction means for performing spectral subtraction processing on the power spectrum information extracted by said target sound spectrum extraction means.</claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>The sound source separation device according to any one of claims 1 to 6, further comprising stationary noise reduction means for performing processing to reduce noise before the processing by said beamforming means is performed.</claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>A speech recognition device comprising the sound source separation device according to any one of claims 1 to 7 and speech recognition means for performing speech recognition of a sound source signal separated by the sound source separation device.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>The speech recognition device according to claim 8, further comprising recognition vocabulary list storage means for storing a driver seat side recognition vocabulary list which is a list of candidates of vocabulary spoken from a driver's seat side of a vehicle and a passenger's seat side recognition vocabulary list which is a list of candidates of vocabulary spoken from a passenger's seat side,<br/>
wherein said speech recognition means performs speech recognition processing of a sound source signal<!-- EPO <DP n="72"> --> separated by said sound source separation device based on the driver's seat side recognition vocabulary list and the passenger's seat side recognition vocabulary list stored in said recognition vocabulary list storage means.</claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>The speech recognition device according to claim 8 or 9, further comprising:
<claim-text>state transition means for managing a current state of a vehicle;</claim-text>
<claim-text>valid vocabulary list storage means for storing a valid vocabulary list of the passenger's seat side and the driver's seat side depending on a state of the vehicle; and</claim-text>
<claim-text>control means for determining whether a vocabulary item recognized by said speech recognition means is valid or not based on the current state of the vehicle managed by said state transition means and the vocabulary list stored in said valid vocabulary list storage means, and controlling depending on the determination result.</claim-text></claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>A mobile phone comprising the sound source separation device according to any one of claims 1 to 7.</claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>A sound source separation method comprising:
<claim-text>a sound source signal receiving step of inputting sound source signals emitted from a plurality of sound sources to at least two microphones arranged separately from each other;<!-- EPO <DP n="73"> --></claim-text>
<claim-text>a beamforming processing step of performing a first beamforming processing and a second beamforming processing to attenuate sound source signals arriving from predetermined directions symmetrical with respect to a line that is perpendicular to a straight line connecting two microphones respectively by performing computations using two weighted coefficients which are complex conjugate to each other in a frequency domain on an output signal of said microphone respectively;</claim-text>
<claim-text>a power computation step of computing power spectrum information with respect to each of sound source signals obtained in said beamforming processing step; and</claim-text>
<claim-text>a target sound spectrum extracting step of extracting spectrum information of a target sound source based on a difference between the power spectrum information calculated in said power computation step.</claim-text></claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>A program for causing a computer to perform:
<claim-text>an output signal acquisition step of acquiring an output signal which includes sound source signals emitted from a plurality of sound sources from at least two microphones arranged separately from each other;</claim-text>
<claim-text>a beamforming processing step of performing a first beamforming processing and a second beamforming processing to attenuate sound source signals arriving from predetermined directions symmetrical with respect to a line that is perpendicular to a straight line connecting two microphones respectively by performing computations using<!-- EPO <DP n="74"> --> two weighted coefficients which are complex conjugate to each other in a frequency domain on the output signal acquired in said output signal acquisition step respectively;</claim-text>
<claim-text>a power computation step of computing power spectrum information with respect to each of sound source signals obtained in said beamforming processing step; and</claim-text>
<claim-text>a target sound spectrum extracting step of extracting spectrum information of a target sound source based on a difference between the power spectrum information calculated in said power computation step.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56979328" lang="FR" load-source="patent-office"><!-- EPO <DP n="81"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Dispositif de séparation de source sonore permettant de séparer un signal de source sonore d'une source sonore cible d'un son mixte qui comprend des signaux de sources sonores émis depuis une pluralité de sources sonores à l'aide d'au moins deux microphones agencés séparément l'un de l'autre comprenant :
<claim-text>un moyen de formage de faisceau permettant</claim-text>
<claim-text>de réaliser un premier traitement de formage de faisceau pour atténuer un signal de source sonore arrivant d'une direction prédéterminée en réalisant des calculs à l'aide de premiers coefficients sur un signal de sortie desdits microphones, et</claim-text>
<claim-text>de réaliser un deuxième traitement de formage de faisceau pour atténuer un signal de source sonore arrivant d'une direction symétrique à ladite direction prédéterminée par rapport à une ligne qui est perpendiculaire à une ligne droite connectant les deux microphones en réalisant des calculs à l'aide de deuxièmes coefficients qui sont un conjugué complexe desdits premiers coefficients dans un domaine de fréquence sur le signal de sortie dudit microphone ;</claim-text>
<claim-text>des moyens de calcul de puissance permettant de calculer des informations de spectre de puissance par rapport à chacun des signaux de sources sonores obtenus par ledit moyen de formage de faisceau ; et</claim-text>
<claim-text>des moyens d'extraction de spectre sonore cible permettant d'extraire des informations de spectre d'une source sonore cible sur la base d'une différence entre<!-- EPO <DP n="82"> --> les informations de spectre de puissance calculées par lesdits moyens de calcul de puissance.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Dispositif de séparation de source sonore selon la revendication 1, dans lequel ledit moyen de formage de faisceau applique ledit premier traitement de formage de faisceau et ledit deuxième traitement de formage de faisceau à chacune (A) d'une combinaison de l'un quelconque de deux microphones parmi trois microphones qui sont agencés séparément les uns des autres et (B) d'une autre combinaison de deux microphones parmi lesdits trois microphones.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Dispositif de séparation de source sonore selon la revendication 1 ou 2, comprenant en outre des moyens de commande de caractéristiques directionnelles permettant d'appliquer un délai à un signal de sortie d'un microphone.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Dispositif de séparation de source sonore selon la revendication 3, dans lequel lesdits moyens de commande de caractéristiques directionnelles génèrent virtuellement des signaux de sortie de trois microphones en appliquant un délai à un signal de sortie d'au moins un microphone parmi deux microphones.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Dispositif de séparation de source sonore selon la revendication 3 ou 4, comprenant en outre un moyen d'estimation de direction d'arrivée permettant d'estimer une direction d'arrivée dudit signal de source sonore,<br/>
dans lequel lesdits moyens de commande de caractéristiques directionnelles appliquent un délai à un signal de sortie du microphone de telle sorte que deux sources sonores se positionnent virtuellement<!-- EPO <DP n="83"> --> symétriquement par rapport à une ligne qui est perpendiculaire à une ligne droite connectant les deux microphones sur la base d'une direction d'arrivée estimée par ledit moyen d'estimation de direction d'arrivée.</claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Dispositif de séparation de source sonore selon l'une quelconque des revendications 1 à 5, comprenant en outre des moyens de soustraction spectrale permettant de réaliser un traitement de soustraction spectrale sur les informations de spectre de puissance extraites par lesdits moyens d'extraction de spectre sonore cible.</claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Dispositif de séparation de source sonore selon l'une quelconque des revendications 1 à 6, comprenant en outre des moyens de réduction de bruit stationnaire permettant de réaliser un traitement pour réduire le bruit avant que le traitement par ledit moyen de formage de faisceau soit réalisé.</claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Dispositif de reconnaissance de la parole comprenant le dispositif de séparation de source sonore selon l'une quelconque des revendications 1 à 7 et un moyen de reconnaissance de la parole permettant de réaliser une reconnaissance de la parole d'un signal de source sonore séparé par le dispositif de séparation de source sonore.</claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Dispositif de reconnaissance de la parole selon la revendication 8, comprenant en outre des moyens de stockage de listes de vocabulaire de reconnaissance permettant de stocker une liste de vocabulaire de reconnaissance côté siège conducteur permettant de stocker une liste de vocabulaire de reconnaissance côté siège conducteur qui est une liste de candidats d'un vocabulaire utilisé depuis un côté siège conducteur d'un<!-- EPO <DP n="84"> --> véhicule et une liste de vocabulaire de reconnaissance côté siège passager qui est une liste de candidats d'un vocabulaire utilisé depuis un côté siège passager,<br/>
dans lequel ledit moyen de reconnaissance de la parole réalise un traitement de reconnaissance de la parole d'un signal de source sonore séparé par ledit dispositif de séparation de source sonore sur la base de la liste de vocabulaire de reconnaissance côté siège conducteur et de la liste de vocabulaire de reconnaissance côté siège passager stockées dans lesdits moyens de stockage de listes de vocabulaire de reconnaissance.</claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Dispositif de reconnaissance de la parole selon la revendication 8 ou 9, comprenant en outre :
<claim-text>des moyens de transition d'état permettant de gérer un état actuel d'un véhicule ;</claim-text>
<claim-text>des moyens de stockage de listes de vocabulaire valides permettant de stocker une liste de vocabulaire valide du côté siège passager et du côté siège conducteur en fonction d'un état du véhicule ; et</claim-text>
<claim-text>des moyens de commande permettant de déterminer si un élément de vocabulaire reconnu par ledit moyen de reconnaissance de la parole est valide ou non sur la base de l'état actuel du véhicule géré par lesdits moyens de transition d'état et la liste de vocabulaire stockée dans lesdits moyens de stockage de listes de vocabulaire valides, et d'effectuer un contrôle en fonction du résultat de la détermination.</claim-text><!-- EPO <DP n="85"> --></claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Téléphone mobile comprenant le dispositif de séparation de source sonore selon l'une quelconque des revendications 1 à 7.</claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Procédé de séparation de source sonore comprenant :
<claim-text>une étape de réception de signaux de sources sonores consistant à entrer des signaux de sources sonores émis depuis une pluralité de sources sonores vers au moins deux microphones agencés séparément l'un de l'autre ;</claim-text>
<claim-text>une étape de traitement de formage de faisceau consistant à réaliser un premier traitement de formage de faisceau et un deuxième traitement de formage de faisceau pour atténuer des signaux de sources sonores arrivant de directions prédéterminées symétriques par rapport à une ligne qui est perpendiculaire à une ligne droite connectant deux microphones respectivement en réalisant des calculs à l'aide de deux coefficients pondérés qui sont un conjugué complexe l'un à l'autre dans un domaine de fréquence sur un signal de sortie dudit microphone respectivement ;</claim-text>
<claim-text>une étape de calcul de puissance consistant à calculer des informations de spectre de puissance par rapport à chacun des signaux de sources sonores obtenus lors de ladite étape de traitement de formage de faisceau ; et</claim-text>
<claim-text>une étape d'extraction de spectre sonore cible consistant à extraire des informations de spectre d'une source sonore cible sur la base d'une différence entre les informations de spectre de puissance calculées lors de ladite étape de calcul de puissance.</claim-text><!-- EPO <DP n="86"> --></claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Programme permettant de faire en sorte qu'un ordinateur réalise :
<claim-text>une étape d'acquisition de signal de sortie consistant à acquérir un signal de sortie qui comprend des signaux de sources sonores émis depuis une pluralité de sources sonores depuis au moins deux microphones agencés séparément l'un de l'autre ,</claim-text>
<claim-text>une étape de traitement de formage de faisceau consistant à réaliser un premier traitement de formage de faisceau et un deuxième traitement de formage de faisceau pour atténuer des signaux de sources sonores arrivant de directions prédéterminées symétriques par rapport à une ligne qui est perpendiculaire à une ligne droite connectant deux microphones respectivement en réalisant des calculs à l'aide de deux coefficients pondérés qui sont un conjugué complexe l'un à l'autre dans un domaine de fréquence sur le signal de sortie acquis lors de ladite étape d'acquisition de signal de sortie respectivement ;</claim-text>
<claim-text>une étape de calcul consistant à calculer des informations de spectre de puissance par rapport à chacun des signaux de sources sonores obtenus lors de ladite étape de traitement de formage de faisceau ; et</claim-text>
<claim-text>une étape d'extraction de spectre sonore cible consistant à extraire des informations de spectre d'une source sonore cible sur la base d'une différence entre les informations de spectre de puissance calculées lors de ladite étape de calcul de puissance.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW16668836" load-source="patent-office"><!-- EPO <DP n="87"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="158" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="88"> --><figure id="f0002" num="2,3"><img id="if0002" file="imgf0002.tif" wi="135" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="89"> --><figure id="f0003" num="4,5"><img id="if0003" file="imgf0003.tif" wi="165" he="218" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="90"> --><figure id="f0004" num="6,7"><img id="if0004" file="imgf0004.tif" wi="75" he="193" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="91"> --><figure id="f0005" num="8"><img id="if0005" file="imgf0005.tif" wi="165" he="178" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="92"> --><figure id="f0006" num="9"><img id="if0006" file="imgf0006.tif" wi="165" he="204" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="93"> --><figure id="f0007" num="10"><img id="if0007" file="imgf0007.tif" wi="165" he="186" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="94"> --><figure id="f0008" num="11"><img id="if0008" file="imgf0008.tif" wi="155" he="101" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="95"> --><figure id="f0009" num="12"><img id="if0009" file="imgf0009.tif" wi="155" he="147" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="96"> --><figure id="f0010" num="13"><img id="if0010" file="imgf0010.tif" wi="123" he="177" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="97"> --><figure id="f0011" num="14"><img id="if0011" file="imgf0011.tif" wi="161" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="98"> --><figure id="f0012" num="15,16A,16B"><img id="if0012" file="imgf0012.tif" wi="163" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="99"> --><figure id="f0013" num="17"><img id="if0013" file="imgf0013.tif" wi="154" he="219" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="100"> --><figure id="f0014" num="18"><img id="if0014" file="imgf0014.tif" wi="116" he="127" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="101"> --><figure id="f0015" num="19"><img id="if0015" file="imgf0015.tif" wi="151" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="102"> --><figure id="f0016" num="20"><img id="if0016" file="imgf0016.tif" wi="154" he="219" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="103"> --><figure id="f0017" num="21"><img id="if0017" file="imgf0017.tif" wi="144" he="187" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="104"> --><figure id="f0018" num="22"><img id="if0018" file="imgf0018.tif" wi="146" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="105"> --><figure id="f0019" num="23"><img id="if0019" file="imgf0019.tif" wi="159" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="106"> --><figure id="f0020" num="24,25A,25B"><img id="if0020" file="imgf0020.tif" wi="132" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="107"> --><figure id="f0021" num="26"><img id="if0021" file="imgf0021.tif" wi="143" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="108"> --><figure id="f0022" num="27"><img id="if0022" file="imgf0022.tif" wi="140" he="199" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="109"> --><figure id="f0023" num="28,29"><img id="if0023" file="imgf0023.tif" wi="114" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="110"> --><figure id="f0024" num="30"><img id="if0024" file="imgf0024.tif" wi="104" he="89" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="111"> --><figure id="f0025" num="31"><img id="if0025" file="imgf0025.tif" wi="125" he="182" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="112"> --><figure id="f0026" num="32"><img id="if0026" file="imgf0026.tif" wi="117" he="178" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="113"> --><figure id="f0027" num="33"><img id="if0027" file="imgf0027.tif" wi="159" he="203" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="114"> --><figure id="f0028" num="34"><img id="if0028" file="imgf0028.tif" wi="87" he="88" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="115"> --><figure id="f0029" num="35A,35B"><img id="if0029" file="imgf0029.tif" wi="151" he="219" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="116"> --><figure id="f0030" num="36"><img id="if0030" file="imgf0030.tif" wi="137" he="108" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="117"> --><figure id="f0031" num="37"><img id="if0031" file="imgf0031.tif" wi="165" he="191" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="118"> --><figure id="f0032" num="38"><img id="if0032" file="imgf0032.tif" wi="165" he="201" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="119"> --><figure id="f0033" num="39"><img id="if0033" file="imgf0033.tif" wi="165" he="197" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="120"> --><figure id="f0034" num="40"><img id="if0034" file="imgf0034.tif" wi="153" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="121"> --><figure id="f0035" num="41A,41B"><img id="if0035" file="imgf0035.tif" wi="135" he="161" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="122"> --><figure id="f0036" num="42"><img id="if0036" file="imgf0036.tif" wi="165" he="122" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="123"> --><figure id="f0037" num="43,44"><img id="if0037" file="imgf0037.tif" wi="146" he="212" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="124"> --><figure id="f0038" num="45"><img id="if0038" file="imgf0038.tif" wi="116" he="226" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="125"> --><figure id="f0039" num="46"><img id="if0039" file="imgf0039.tif" wi="156" he="79" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="126"> --><figure id="f0040" num="47"><img id="if0040" file="imgf0040.tif" wi="117" he="228" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="127"> --><figure id="f0041" num="48,49"><img id="if0041" file="imgf0041.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="128"> --><figure id="f0042" num="50,51"><img id="if0042" file="imgf0042.tif" wi="165" he="178" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
