<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2418621-B1" country="EP" doc-number="2418621" kind="B1" date="20140101" family-id="44508871" file-reference-id="315072" date-produced="20180823" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146552489" ucid="EP-2418621-B1"><document-id><country>EP</country><doc-number>2418621</doc-number><kind>B1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-11176236-A" is-representative="YES"><document-id mxw-id="PAPP154826412" load-source="docdb" format="epo"><country>EP</country><doc-number>11176236</doc-number><kind>A</kind><date>20110802</date><lang>EN</lang></document-id><document-id mxw-id="PAPP222722018" load-source="docdb" format="original"><country>EP</country><doc-number>11176236.5</doc-number><date>20110802</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140447125" ucid="KR-20100078461-A" load-source="docdb"><document-id format="epo"><country>KR</country><doc-number>20100078461</doc-number><kind>A</kind><date>20100813</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130911</date></intention-to-grant-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988097709" load-source="docdb">G06T   7/00        20060101AFI20111122BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL1988129643" load-source="docdb">G06T  19/00        20110101ALI20111122BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861408832" load-source="docdb" scheme="CPC">G06T   7/70        20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1861413050" load-source="docdb" scheme="CPC">G06T  19/006       20130101 FI20170105BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132189951" lang="DE" load-source="patent-office">Vorrichtung und Verfahren zur Bereitstellung von erweiterter Realitätsinformation</invention-title><invention-title mxw-id="PT132189952" lang="EN" load-source="patent-office">Apparatus and method for providing augmented reality information</invention-title><invention-title mxw-id="PT132189953" lang="FR" load-source="patent-office">Appareil et procédé pour fournir des informations de réalité améliorées</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR918159716" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>PANTECH CO LTD</last-name><address><country>KR</country></address></addressbook></applicant><applicant mxw-id="PPAR918155322" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>PANTECH CO., LTD.</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918150941" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CHOI JUNG-HAK</last-name><address><country>KR</country></address></addressbook></inventor><inventor mxw-id="PPAR918135845" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CHOI, JUNG-HAK</last-name></addressbook></inventor><inventor mxw-id="PPAR918995127" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>CHOI, JUNG-HAK</last-name><address><street>Pantech R&amp;D Center, I-2, DMC Sangam-dong, Mapo-gu</street><city>121-270 Seoul</city><country>KR</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR918995129" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Pantech Co., Ltd.</last-name><iid>100991909</iid><address><street>Pantech R&amp;D Center I-2, DMC Sangam-dong Mapo-gu</street><city>Seoul 121-270</city><country>KR</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR918995128" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Jordan, Volker Otto Wilhelm</last-name><suffix>et al</suffix><iid>100043178</iid><address><street>Weickmann &amp; Weickmann Patentanwälte Postfach 860 820</street><city>81635 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS548955648" load-source="docdb">AL</country><country mxw-id="DS548848075" load-source="docdb">AT</country><country mxw-id="DS548950768" load-source="docdb">BE</country><country mxw-id="DS548822928" load-source="docdb">BG</country><country mxw-id="DS548948584" load-source="docdb">CH</country><country mxw-id="DS548950769" load-source="docdb">CY</country><country mxw-id="DS548848076" load-source="docdb">CZ</country><country mxw-id="DS548955649" load-source="docdb">DE</country><country mxw-id="DS548950774" load-source="docdb">DK</country><country mxw-id="DS548950775" load-source="docdb">EE</country><country mxw-id="DS548961371" load-source="docdb">ES</country><country mxw-id="DS548822929" load-source="docdb">FI</country><country mxw-id="DS548948585" load-source="docdb">FR</country><country mxw-id="DS548955654" load-source="docdb">GB</country><country mxw-id="DS548950776" load-source="docdb">GR</country><country mxw-id="DS548955655" load-source="docdb">HR</country><country mxw-id="DS548848077" load-source="docdb">HU</country><country mxw-id="DS548948586" load-source="docdb">IE</country><country mxw-id="DS548950777" load-source="docdb">IS</country><country mxw-id="DS548822930" load-source="docdb">IT</country><country mxw-id="DS548950782" load-source="docdb">LI</country><country mxw-id="DS548822931" load-source="docdb">LT</country><country mxw-id="DS548959896" load-source="docdb">LU</country><country mxw-id="DS548822932" load-source="docdb">LV</country><country mxw-id="DS548822933" load-source="docdb">MC</country><country mxw-id="DS548959897" load-source="docdb">MK</country><country mxw-id="DS548959898" load-source="docdb">MT</country><country mxw-id="DS548961372" load-source="docdb">NL</country><country mxw-id="DS548875817" load-source="docdb">NO</country><country mxw-id="DS548961373" load-source="docdb">PL</country><country mxw-id="DS548822934" load-source="docdb">PT</country><country mxw-id="DS548848078" load-source="docdb">RO</country><country mxw-id="DS548822935" load-source="docdb">RS</country><country mxw-id="DS548961374" load-source="docdb">SE</country><country mxw-id="DS548948587" load-source="docdb">SI</country><country mxw-id="DS548875818" load-source="docdb">SK</country><country mxw-id="DS548875819" load-source="docdb">SM</country><country mxw-id="DS548959899" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><description mxw-id="PDES63956847" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">BACKGROUND</heading><p id="p0001" num="0001">Field</p><p id="p0002" num="0002">The following description relates to an apparatus and method for providing augmented reality information.</p><p id="p0003" num="0003">Description of the Background</p><p id="p0004" num="0004">Augmented reality (AR) refers to a computer graphic technique that combines virtual objects or information with an image of a real-world environment to display the virtual elements as if they were present in the real environment.</p><p id="p0005" num="0005">Unlike a general virtual reality technology, which provides only virtual objects in a virtual space, AR technology provides a view of reality that is blended with virtual objects, thereby providing supplementary information which is difficult to obtain in reality. In addition, the general virtual reality technology is applicable to a limited range of fields such as, game technology, whereas due to the above characteristics, AR can be applied to various fields and hence has increasingly gained attention as a future display technology suitable to a ubiquitous environment.<!-- EPO <DP n="2"> --></p><p id="p0006" num="0006">For example, if a tourist traveling in London views a street in a certain direction through a camera built in a mobile phone having various features, such as global positioning system (GPS), the mobile phone may display augmented reality (AR) information about the surrounding environment and objects, such as restaurants and stores having sales, along the street as an information overlay on an image of a real street captured by the camera.</p><p id="p0007" num="0007">In this case, since the mobile phone only provides AR information about the objects included in an image obtained by the camera, if objects that the tourist is interested in viewing are positioned behind other objects in the image, it is difficult for the tourist to obtain AR information of the concealed objects.</p><p id="p0008" num="0008">A device and a method for providing augmented reality information about objects that are invisible to the humans is known from the <patcit id="pcit0001" dnum="US2009293012A1"><text>US 2009/293012 A1</text></patcit>. The handheld device allows to display on a display unit information about objects concealed from view by a reference object in the line of sight of a user of the device. The reference object is set as the first optically or acoustically opaque object along the line of sight.</p><p id="p0009" num="0009">An AR-engine is disclosed by <patcit id="pcit0002" dnum="US2009128449A1"><text>US 2009/128449 A1</text></patcit>, which creates a model of a region of the user's environment in the direction of the user's field of vision. Based on this model it is then determined that an object within the user's field of vision is invisible to the user. A representation of this concealed object is displayed on a transparent display, which can be a pair of AR glasses or a windshield of an automotive, in front of the user.</p><p id="p0010" num="0010">A representation of spatial query regions and a method for determining a direction from two positions of objects is known for example from <patcit id="pcit0003" dnum="WO2010075455A1"><text>WO 2010/075455 A1</text></patcit> and <patcit id="pcit0004" dnum="US2006195858A1"><text>US 2006/195858 A1</text></patcit>.</p><p id="p0011" num="0011">It is an object of the present invention to provide a mobile device and a method for providing AR information about objects that are behind other objects in an image acquired by a camera, that allows for an accurate selection of reference objects from the displayed objects.</p><heading id="h0002">SUMMARY</heading><p id="p0012" num="0012">Exemplary embodiments of the present invention provide an apparatus and method for providing augmented reality (AR) information of a concealed object which is behind another object.</p><p id="p0013" num="0013">Additional features of the invention will be set forth in the description which follows, and in part will be apparent from the description, or may be learned by practice of the invention.</p><p id="p0014" num="0014">An exemplary embodiment provides a method for providing augmented information (AR) of concealed objects according to claim 1.<!-- EPO <DP n="3"> --><!-- EPO <DP n="4"> --></p><p id="p0015" num="0015">An exemplary embodiment provides a terminal to provide augmented reality (AR) information of concealed objects, according to claim 9.<!-- EPO <DP n="5"> --></p><p id="p0016" num="0016">An exemplary embodiment provides a communication system according to claim 13.</p><p id="p0017" num="0017">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are intended to provide further explanation of the invention as claimed. Other features and aspects will be apparent from the following detailed description, the drawings, and the claims.</p><heading id="h0003">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0018" num="0018">The accompanying drawings, which are included to provide a further understanding of the invention and are incorporated in and constitute a part of this specification, illustrate embodiments of the invention, and together with the description serve to explain the principles of the invention.</p><p id="p0019" num="0019"><figref idrefs="f0001">FIG. 1</figref> is a diagram illustrating a communication system to provide augmented reality (AR) information according to an exemplary embodiment.</p><p id="p0020" num="0020"><figref idrefs="f0002">FIG. 2</figref> is a diagram illustrating an apparatus to provide AR information according<!-- EPO <DP n="6"> --> to an exemplary embodiment.</p><p id="p0021" num="0021"><figref idrefs="f0003">FIG. 3</figref> is a diagram illustrating a server to provide AR information according to an exemplary embodiment.</p><p id="p0022" num="0022"><figref idrefs="f0004">FIG. 4</figref> is a flowchart for a method of providing AR information according to an exemplary embodiment.</p><p id="p0023" num="0023"><figref idrefs="f0005">FIG. 5</figref> is a diagram illustrating a focal point setting screen according to an exemplary embodiment.</p><p id="p0024" num="0024"><figref idrefs="f0006">FIG. 6</figref> is a diagram illustrating a focal point setting screen according to an exemplary embodiment.</p><p id="p0025" num="0025"><figref idrefs="f0007">FIG. 7</figref> is a diagram illustrating a user interface to provide concealed object information according to an exemplary embodiment.</p><p id="p0026" num="0026"><figref idrefs="f0008">FIG. 8</figref> is a diagram illustrating a user interface to provide concealed object information according to an exemplary embodiment.</p><p id="p0027" num="0027">Throughout the drawings and the detailed description, unless otherwise described, the same drawing reference numerals will be understood to refer to the same elements, features, and structures. The relative size and depiction of these elements may be exaggerated for clarity, illustration, and convenience.</p><heading id="h0004">DETAILED DESCRIPTION</heading><p id="p0028" num="0028">The invention is described more fully hereinafter with reference to the accompanying drawings, in which exemplary embodiments of the invention are shown. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein. Rather, these exemplary embodiments are provided<!-- EPO <DP n="7"> --> so that this disclosure is thorough, and will fully convey the scope of the invention to those skilled in the art.</p><p id="p0029" num="0029">Hereinafter, "object" refers to a real object, a real person, and/or a marker or sign, which may be present in a real-world environment. In addition, "visible object" refers to an object that can be recognized from an image captured from a real environment, and generally refers to an object present in reality, such as a building, a person, a mountain, and the like. "Concealed object" refers to a real object which cannot be recognized from the image captured from a real environment since the object is placed behind another visible object. The above terms may be modified according to the purpose and practice of a user or an operator, and thus the definition of the above terms should be made based on contents throughout the specification.</p><p id="p0030" num="0030"><figref idrefs="f0001">FIG. 1</figref> is a diagram illustrating a communication system to provide augmented reality (AR) information according to an exemplary embodiment.</p><p id="p0031" num="0031">Referring to <figref idrefs="f0001">FIG. 1</figref>, the communication system may include at least one or more apparatuses 110 (hereinafter, referred to as "terminals") to provide AR information and a server 130 (hereinafter, referred to as "server") 130 to provide AR information for an AR service to the terminals 110, and the terminals 110 and the server 130 are connected to each other via a wired and/or wireless communication network. In addition, each terminal 110 may be provided with its position information from a position determination system 120 through the communication network.</p><p id="p0032" num="0032">The terminals 110 may be applicable to any types of apparatuses which can recognize an object included in an image and display AR information corresponding to the recognized object, and the apparatuses may include mobile communication terminals, such as personal digital assistants (PDAs), smart phones, and navigation terminal devices, and personal<!-- EPO <DP n="8"> --> computers, such as desktop computers and laptop computers.</p><p id="p0033" num="0033"><figref idrefs="f0002">FIG. 2</figref> is a diagram illustrating an apparatus or terminal to provide AR information according to an exemplary embodiment.</p><p id="p0034" num="0034">The terminal may include an image acquiring unit 210, a display unit 220, a position information obtaining unit 230, a control unit 260, a manipulating unit 240, and a communication unit 250.</p><p id="p0035" num="0035">The image acquiring unit 210 acquires an image by capturing a real environment image, and outputs the image to the control unit 260. The image acquiring unit 210 may be a camera or an image sensor. In addition, the image acquiring unit 210 may be a camera that is capable of zooming in/out an image or being rotated automatically or manually by the control of the control unit 260. However, aspects are not limited thereto such that the image acquiring unit 210 may acquire an image of a real environment from an outside source or from a memory. The display unit 220 outputs an image input from an external source, and for example, in <figref idrefs="f0002">FIG. 2</figref>, the display unit 220 may output at least one of the image obtained by the image acquiring unit 210, a focal point setting screen, concealed object information, user interface information, or AR information of an object of interest, and combinations thereof. The position information obtaining unit 230 may receive position information of the terminal 110 from the position determination system 120, and output the position information, and may include, for example, a global positioning system (GPS) receiver to receive a position information signal from a GPS satellite and/or a system to receive a position information signal(s) from a communication network.</p><p id="p0036" num="0036">The manipulating unit 240, which is a user interface, may receive information from a user, and may include a key input unit that creates key information upon pressing a key<!-- EPO <DP n="9"> --> button, a touch sensor, a mouse, a touchscreen, and/or the like. In the example, the manipulating unit 240 may receive at least one of a concealed object information request signal, reference object selection information, and object of interest selection information. The communication unit 250 may process a signal received from the outside through the wired and/or wireless communication network and output the processed signal to the control unit 260, and may process an internal output signal received from the control unit 260 such that the signal can be transmitted to the outside through the wired and/or wireless communication network.</p><p id="p0037" num="0037">The control unit 260 may control the above elements to provide AR information of a concealed object, and may be a hardware processor or a software module to be executed on such a hardware processor.</p><p id="p0038" num="0038"><figref idrefs="f0003">FIG. 3</figref> is a diagram illustrating a server to provide AR information according to an exemplary embodiment.</p><p id="p0039" num="0039">The server may include a communication unit 310, an object recognition information storage unit 320, an AR information storage unit 330, a map information storage unit 340, and a control unit 350.</p><p id="p0040" num="0040">The communication unit 310 may process a signal received from the outside through a wired and/or wireless communication network and output the processed signal to the control unit 350, and may process an inner output signal received from the control unit 350 such that the signal can be transmitted to the outside through the wired and/or wireless communication network. The object recognition information storage unit 330 may store recognition information for recognizing objects, and, for example, may store properties, such as outlines and colors of the objects. Accordingly, the control unit 350 may identify an object by comparing properties contained in object recognition information received from the terminal and properties contained<!-- EPO <DP n="10"> --> in the object recognition information stored in the object recognition storage unit 320.</p><p id="p0041" num="0041">The AR information storage unit 330 may store pieces of AR information that are related to an object. For example, if an object is a given tree, the AR information storage unit 330 may store a name of the tree, a habitat, ecological characteristics, etc. as tag images. The map information storage unit 340 may store map information. In response to receiving position information from a terminal, the control unit 350 may detect or determine map information corresponding to the received position information from the map information storage unit 340.</p><p id="p0042" num="0042">The control unit 350 may control the above described elements to provide AR information of a concealed object, and may be a hardware processor or a software module to be executed on such a hardware processor.</p><p id="p0043" num="0043">An example of a method for providing AR information of a concealed object by the system as described above will now be described with reference to <figref idrefs="f0004 f0005 f0006 f0007 f0008">FIGS. 4 to 8</figref>.</p><p id="p0044" num="0044"><figref idrefs="f0004">FIG. 4</figref> is a flowchart for a method of providing AR information according to an exemplary embodiment, <figref idrefs="f0005">FIG. 5</figref> is a diagram illustrating a focal point setting screen according to an exemplary embodiment, <figref idrefs="f0006">FIG. 6</figref> is a diagram illustrating a focal point setting screen according to an exemplary embodiment, <figref idrefs="f0007">FIG. 7</figref> is a diagram illustrating a user interface to provide concealed object information according to an exemplary embodiment, and <figref idrefs="f0008">FIG. 8</figref> is a diagram illustrating a user interface to provide concealed object information according to an exemplary embodiment.</p><p id="p0045" num="0045">Referring to <figref idrefs="f0004">FIG. 4</figref>, at 405, the terminal drives the image acquiring unit 210 to acquire an image of the real environment, and outputs the acquired image to the display unit 220. The terminal may control the image acquiring unit 210 to acquire the image in response to an input from the manipulating unit 240; however, aspects are not limited thereto such that the<!-- EPO <DP n="11"> --> image acquiring unit 210 may automatically acquire such image. At 410, the terminal receives an AR information request with respect to a concealed object that is not shown because, for example, another object in the image is in front of or conceals the object, and then, at 415, the terminal defines a reference object from among one or more visible objects included in the acquired image.</p><p id="p0046" num="0046">There may be various examples of how to define a reference object at 415.</p><p id="p0047" num="0047">For example, the terminal may select and define a reference object according to previously set criteria, and as an example, the most visible object among the visible objects may be set as the reference object.</p><p id="p0048" num="0048">For another example, the reference object may be set by selection information input from the user. A cross-shaped focal point 510 as shown in <figref idrefs="f0005">FIG. 5</figref> or a rectangular focal area 610 as shown in the example illustrated in <figref idrefs="f0006">FIG. 6</figref> may be display on a screen, and the user may apply focal point adjustment information in connection with a reference object selected by the user to the created focal point or area, and then select a visible object located or disposed at the focal point or area as the reference object according to the focal point adjustment information. For another example, the user may directly touch the screen to input reference object selection information.</p><p id="p0049" num="0049">At 420, the terminal obtains recognition information of the reference object. For example, property information (i.e., outline and/or color) of a real object, which may be an object of interest, is extracted from an image acquired from a camera as recognition information.</p><p id="p0050" num="0050">At 425, the terminal obtains position information about a position at which the acquired image is or was captured. If the image is obtained by the terminal in real-time, the terminal may obtain its position information from the position determination system. However,<!-- EPO <DP n="12"> --> if the image is previously obtained or provided from an external source, the terminal may receive image capturing position information from the user or from data regarding or within the image. Here, operations 420 and 425 may be performed simultaneously, or the order of operations 420 and 425 may be rearranged.</p><p id="p0051" num="0051">At 430, the terminal issues a concealed object information request to the server. In this case, the terminal may transmit reference object recognition information and image capturing position information. In addition to the reference object recognition information and the image capturing position information, information about a range of the concealed object may be transmitted, and, for example, referring to <figref idrefs="f0007">FIG. 7</figref>, the range information may include a depth D from an image capturing position to the reference object along and/or about an image capturing direction of the terminal and an angle of view θ at the image capturing position. Although not illustrated, the range information of the concealed object may be previously defined or be input in real-time by the user. If the server has previously defined the range information, the range information may not be transmitted from the terminal.</p><p id="p0052" num="0052">In response to receiving the concealed object information request from the terminal, the server analyzes the received reference object recognition information and the image capturing position information at 435. More specifically, the server searches the map information storage unit 340 (see <figref idrefs="f0003">FIG. 3</figref>) to detect or determine a map corresponding to the received image capturing position information, and analyzes the reference object recognition information to determine the position of the reference object on the detected map. The server uses the determined reference object position and the image capturing position information to obtain information about an image capturing direction. For example, the image capturing direction may be a direction from the image capturing position to the reference object may be<!-- EPO <DP n="13"> --> determined as the image capturing direction, and the image capturing direction may be determined from the reference object recognition information, the image capturing position information, and the like, and/or may be determined gyroscopically.</p><p id="p0053" num="0053">At 440, the server detects or determines information about concealed objects positioned behind the reference object along and/or about the direction from the image capturing position to the reference object, that is, the image capturing direction, according to the analysis result obtained at 435. The server may detect or determine information about concealed objects present within a range determined by the above described depth D and angle of view θ and the image capturing direction obtained at 435. For example, referring to <figref idrefs="f0006">FIG. 6</figref>, seven concealed objects may be determine.</p><p id="p0054" num="0054">The server transmits the determined information about concealed objects to the terminal at 445. The information about concealed objects may be displayed in or as a user interface 710 as shown in <figref idrefs="f0007">FIG. 7</figref>, which displays positions and, additionally, sizes of the concealed objects with respect to the reference object.</p><p id="p0055" num="0055">However, aspects are not limited thereto such that various modifications of the above user interface may be made, and for example, in order to identify the concealed objects, the user interface may assign identification numbers to the respective concealed objects as shown in <figref idrefs="f0007">FIG. 7</figref>, display the respective concealed objects with different colors, or allocate identifiers to the respective concealed objects.</p><p id="p0056" num="0056">There may be various methods of assigning the identification numbers. For example, as shown in <figref idrefs="f0007">FIG. 7</figref>, the identification numbers may be sequentially assigned to the concealed objects, starting from the concealed object closest to the reference object to the concealed object farthest from the reference position along and/or about the image capturing<!-- EPO <DP n="14"> --> direction. For example, as shown in <figref idrefs="f0008">FIG. 8</figref>, the identification numbers may be assigned to the concealed objects in a clockwise direction.</p><p id="p0057" num="0057">The terminal outputs the concealed object information to the display unit 220 (see <figref idrefs="f0002">FIG. 2</figref>) at 445. For example, the concealed object information output to the display unit 220 may be displayed as the user interface 710 on the whole screen of the display unit 220 as shown in <figref idrefs="f0007">FIG. 7</figref>, may be displayed on a region of the screen which is displaying a preview image, or may be displayed in the form of a popup window. Additionally, the user interface 710 may be varied according to information input from the user.</p><p id="p0058" num="0058">At 455, object of interest selection information about an object of interest for which AR information is to be obtained from among the output concealed objects is received through the manipulating unit 240. The method of receiving the object of interest selection information may be varied in different embodiments. For example, the user may touch the screen to select an object of interest, or may directly input identification information of the object of interest, such as an identification number, an identifier, and identification color. In response to receiving the object of interest selection information from the user, the terminal transmits the received object of interest selection information to the server at 460. That is, the terminal determines identification information of the object of interest and transmits it to the server.</p><p id="p0059" num="0059">At 465, the server detects or determines AR information related to an object corresponding to the object of interest selection information transmitted from the terminal.</p><p id="p0060" num="0060">At 480, in response to receiving the AR information from the server, the terminal displays the received AR information combined with the corresponding object of interest. However, since the object of interest is a concealed object, the object of interest may be displayed overlaying on a preview image, or information about the object of interest may be<!-- EPO <DP n="15"> --> displayed on a part of the preview image.</p><p id="p0061" num="0061">As described above, AR information can be provided with respect to a concealed object that is behind or concealed by other objects and thus cannot be recognized by a camera.</p><p id="p0062" num="0062">Although the examples above are described as being implemented in a system that is configured to include an AR information provision terminal apparatus and an AR information provision server which are connected to each other through a communication network, the terminal and server may be implemented as being integrated into a single apparatus.</p><p id="p0063" num="0063">It will be apparent to those skilled in the art that various modifications and variation can be made in the present invention without departing from the scope of the invention. Thus, it is intended that the present invention cover the modifications and variations of this invention provided they come within the scope of the appended claims.</p><p id="p0064" num="0064">An apparatus, system, and method for providing augmented reality (AR) information of a concealed object are disclosed. The method for providing AR information of a concealed object by a terminal connectable to a server via a wired and/or wireless communication network may include acquiring an image of a real environment; defining a reference object included in the acquired image; obtaining image capturing position information about a position of the image and reference object recognition information of the defined reference object; transmitting the obtained image capturing position information and the reference object recognition information to the server; receiving information about concealed objects from the server, the concealed objects being disposed behind the reference object along or about a direction from the image capturing position to the reference object; and outputting the received information about concealed objects.</p></description><claims mxw-id="PCLM56978030" lang="DE" load-source="patent-office"><!-- EPO <DP n="22"> --><claim id="c-de-01-0001" num="0001"><claim-text>Verfahren zum Bereitstellen von erweiterten Informationen (AR) verborgener Objekte unter Verwendung eines mit einem Server (130) über ein kabelgebundenes oder/und kabelloses Kommunikationsnetzwerk verbindbaren Endgeräts (110), wobei das Verfahren auf Seiten des Endgeräts (110) umfasst:
<claim-text>Erfassen (405) eines Bildes einer realen Umgebung mittels einer Bild-Erfassungseinheit (210) des Endgeräts (110);</claim-text>
<claim-text>Empfangen von Referenzobjekt-Auswahlinformationen von einem Benutzer;</claim-text>
<claim-text>Definieren (415) eines Referenzobjekts, welches in dem empfangenen Bild enthalten ist;</claim-text>
<claim-text>Erhalten (425) von Bildaufnahmepositions-Informationen über eine Position des Bildes und Erhalten (420) von Referenzobjekt-Erkennungsinformationen des definierten Referenzobjekts;</claim-text>
<claim-text>Empfangen von Informationen über die verborgenen Objekte, wobei die verborgenen Objekte hinter dem Referenzobjekt angeordnet sind, in einer Richtung von der Bildaufnahmeposition aus in Richtung des Referenzobjekts betrachtet; und</claim-text>
<claim-text>Ausgeben der empfangenen Informationen über die verborgenen Objekte auf einer Anzeigeeinheit (220) des Endgeräts (110),</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b> das Verfahren ferner umfasst: auf Seiten des Endgeräts (110):
<claim-text>a1) Definieren des Referenzobjekts aus einer Mehrzahl von sichtbaren Objekten, welche in dem erfassten Bild enthalten sind, auf Grundlage der Referenzobjekt-Auswahlinformationen;</claim-text>
<claim-text>a2) Erhalten der Referenzobjekt-Erkennungsinformationen durch Entnehmen von Eigenschafts-Informationen in Form von Umriss- oder/und<!-- EPO <DP n="23"> --> Farb-Informationen des definierten Referenzobjekts aus dem erfassten Bild; und</claim-text>
<claim-text>a3) Übertragen der erhaltenen Bildaufnahmepositions-Informationen und der Referenzobjekt-Erkennungsinformationen an den Server (130);</claim-text>
<claim-text>a4) Empfangen der Informationen über die verborgenen Objekte an dem Endgerät (110) von dem Server (130).</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Verfahren nach Anspruch 1, wobei das Definieren des Referenzobjekts umfasst:
<claim-text>Erzeugen eines Fokuspunkts auf einem Bildschirm, auf welchem das Bild ausgegeben wird; und</claim-text>
<claim-text>Definieren eines Objekts, welches an dem Fokuspunkt angeordnet ist, als das Referenzobjekt.</claim-text></claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Verfahren nach Anspruch 1 oder 2, wobei die Informationen über verborgene Objekte Informationen von verborgenen Objekten sind, welche innerhalb eines Bereichs vorliegen, welcher auf Grundlage des Referenzobjekts definiert ist,<br/>
wobei der Bereich vorzugsweise durch eine Blicktiefe und einen Blickwinkel in der Richtung von der Bildaufnahmeposition aus in Richtung des Referenzobjekts betrachtet, bestimmt wird.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Verfahren nach einem der Ansprüche 1 bis 3, wobei die Informationen über verborgene Objekte Benutzeroberflächen-Informationen sind, welche Positionen von verborgenen Objekten bezüglich des Referenzobjekts umfassen,<br/>
wobei die verborgenen Objekte vorzugsweise in den Benutzeroberflächen-Informationen durch verschiedene ldentifikationsnummern oder -farben für einzelne verborgene Objekte repräsentiert werden.</claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Verfahren nach einem der Ansprüche 1 bis 4, ferner umfassend:
<claim-text>als Reaktion auf ein Erhalten von Interessenauswahl-Informationen über ein interessierendes Objekt, welches aus verborgenen Objekten<!-- EPO <DP n="24"> --> ausgewählt wird, Erhalten von AR-Informationen entsprechend dem interessierenden Objekt; und</claim-text>
<claim-text>Ausgeben der erhaltenen AR-Informationen.</claim-text></claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Verfahren nach einem der Ansprüche 1 bis 5,<br/>
wobei das Verfahren auf Seiten des Servers (130) umfasst:
<claim-text>als Reaktion auf ein Empfangen (430) einer Anfrage von einem der Endgeräte nach Informationen über ein verborgenes Objekt, Analysieren (435) von Eigenschafts-Informationen eines Referenzobjekts als Referenzobjekt-Erkennungsinformationen und von dem Endgerät (110) erhaltenen Bildaufnahmepositions-Informationen;</claim-text>
<claim-text>Bestimmen (440) von Informationen über wenigstens eines oder mehrere verborgene Objekte, welche hinter dem Referenzobjekt aus einer Richtung von einer Bildaufnahmeposition zu dem Referenzobjekt aus gesehen angeordnet sind, entsprechend dem Analyseergebnis; und</claim-text>
<claim-text>Übertragen der bestimmten Informationen über die verborgenen Objekte an das Endgerät (110).</claim-text></claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Verfahren nach Anspruch 6, wobei das Analysieren (435) der Referenzobjekt-Erkennungsinformationen und der Bildaufnahmepositions-Informationen umfasst:
<claim-text>Bestimmen einer Karte entsprechend der Bildaufnahmepositions-Informationen,</claim-text>
<claim-text>Analysieren der Referenzobjekt-Erkennungsinformationen zum Bestimmen einer Position des Referenzobjekts auf der bestimmten Karte, und</claim-text>
<claim-text>Bestimmen einer Bildaufnahmerichtung entsprechend der bestimmten Referenzobjektposition und der Bildaufnahmeposition.</claim-text></claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Verfahren nach Anspruch 6 oder 7, wobei das Bestimmen (440) der Informationen ein Bestimmen verborgener Objekte umfasst, welche innerhalb eines Bereichs angeordnet sind, welcher entsprechend einer Blicktiefe und eines Blickwinkels in der erhaltenen Bildaufnahmerichtung bestimmt worden ist.<!-- EPO <DP n="25"> --></claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Endgerät (110) zum Bereitstellen erweiterter Informationen (AR) verborgener Objekte, wobei das Endgerät (110) umfasst:
<claim-text>eine Bild-Erfassungseinheit (210), welche dazu eingerichtet ist, ein Bild einer realen Umgebung zu erfassen;</claim-text>
<claim-text>eine Anzeigeeinheit (220), welche dazu eingerichtet ist, das Bild und Informationen über die verborgenen Objekte auszugeben;</claim-text>
<claim-text>eine Kommunikationseinheit, welche dazu eingerichtet ist, auf einen Server (130) zuzugreifen und ein mit dem Server (130) kommuniziertes Signal zu prozessieren; und</claim-text>
<claim-text>eine Steuer-/Regeleinheit (260), welche dazu eingerichtet ist:
<claim-text>Referenzobjekt-Auswahlinformationen von einem Benutzer zu erhalten, ein in dem erhaltenen Bild enthaltenes Referenzobjekt zu definieren, Bildaufnahmepositions-Informationen über eine Position des Bildes zu erhalten,</claim-text>
<claim-text>Referenzobjekt-Erkennungsinformationen des definierten Referenzobjekts zu erhalten,</claim-text>
<claim-text>Informationen über die verborgenen Objekte zu empfangen, welche in einer Richtung von der Bildaufnahmeposition in Richtung des Referenzobjekts aus gesehen vorliegen, und</claim-text>
<claim-text>die empfangenen Informationen über verborgene Objekte auf der Anzeigeeinheit (220) auszugeben,</claim-text>
<claim-text><b>dadurch gekennzeichnet, dass</b></claim-text>
<claim-text>die Steuer-/Regeleinheit (260) ferner dazu eingerichtet ist:</claim-text></claim-text>
<claim-text>das Referenzobjekt aus einer Mehrzahl von sichtbaren Objekten zu definieren, welche in dem erhaltenen Bild enthalten sind, auf Grundlage der Referenzobjekt-Auswahlinformationen;</claim-text>
<claim-text>die Referenzobjekt-Erkennungsinformationen durch Entnehmen von Eigenschafts-Informationen in Form von Umriss- oder/und Farb-Informationen des definierten Referenzobjekts aus dem empfangenen Bild zu extrahieren, und<!-- EPO <DP n="26"> --></claim-text>
<claim-text>die erhaltenen Bildaufnahmepositions-Informationen und Referenzobjekt-Erkennungsinformationen an den Server (130) mittels der Kommunikationseinheit (250) zu übertragen, und</claim-text>
<claim-text>von dem Server (130) die Informationen über die verborgenen Objekte zu empfangen.</claim-text></claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Endgerät (110) nach Anspruch 9, ferner umfassend:
<claim-text>eine Bedieneinheit (240) zum Empfangen von Benutzerinformationen,</claim-text>
<claim-text>wobei die Steuer-/Regeleinheit (260) Referenzobjekt-Auswahlinformationen von mittels der Bedieneinheit (240) empfängt, um das Referenzobjekt zu definieren,</claim-text>
<claim-text>wobei vorzugsweise die Steuer-/Regeleinheit (260) einen Fokuspunkt (510) auf der Bildausgabe der Anzeigeeinheit (220) anzeigt und ein Objekt an dem Fokuspunkt (510) als das Referenzobjekt entsprechend von der Bedieneinheit (240) eingegebenen Fokuspunkt-Anpassungsinformationen definiert.</claim-text></claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>Endgerät (110) nach Anspruch 9 oder 10, wobei die Informationen über verborgene Objekte Benutzeroberflächen-Informationen sind, welche Positionen verborgener Objekte bezüglich des Referenzobjekts umfassen,<br/>
wobei vorzugsweise die verborgenen Objekte in den Benutzeroberflächen-Informationen durch verschiedene Identifikationsnummern oder -farben für einzelne verborgene Objekte repräsentiert werden.</claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>Endgerät (110) nach Anspruch 10 oder 11, wobei die Steuer-/Regeleinheit (260) als Reaktion auf ein Empfangen von Auswahlinformationen über ein aus den verborgenen Objekten ausgewähltes interessierendes Objekt AR-Informationen entsprechend dem ausgewählten interessierenden Objekt von dem Server (130) erhält und die erhaltenen AR-Informationen an die Anzeigeeinheit ausgibt.<!-- EPO <DP n="27"> --></claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>Kommunikationssystem umfassend wenigstens ein Endgerät (110) nach einem der Ansprüche 9 bis 12 und umfassend einen Server (130) zum Bereitstellen von erweiterten Informationen (AR) über verborgene Objekte, wobei der Server (130) umfasst:
<claim-text>eine Kommunikationseinheit (310), welche dazu eingerichtet ist, ein mit wenigstens einem oder mehreren Endgeräten (110) über ein kabelgebundenes oder/und kabelloses Kommunikationsnetzwerk kommuniziertes Signal zu prozessieren;</claim-text>
<claim-text>eine Steuer-/Regeleinheit (350), welche dazu eingerichtet ist:
<claim-text>als Reaktion auf ein Empfangen eines Anfragesignals über ein verborgenes Objekt von dem Endgerät (110) durch die Kommunikationseinheit (310) Referenzobjekt-Erkennungsinformationen, welche Eigenschafts-Informationen in Form von Umriss- oder/und Farb-Informationen des Referenzobjekts sind, und von dem Endgerät (110) empfangene Bildaufnahmepositions-Informationen zu analysieren,</claim-text>
<claim-text>Informationen über wenigstens eines oder mehrere verborgene Objekte zu bestimmen, welche hinter dem Referenzobjekt in einer Richtung von einer Bildaufnahmeposition aus in Richtung des Referenzobjekts gesehen entsprechend des Analyseergebnisses zu bestimmen, und</claim-text>
<claim-text>die bestimmten Informationen über verborgene Objekte an das Endgerät (110) zu übertragen.</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0014" num="0014"><claim-text>Kommunikationssystem nach Anspruch 13, wobei der Server (130) ferner umfasst:
<claim-text>eine Karteninformation-Speichereinheit (340) zum Speichern von Karteninformationen,</claim-text>
<claim-text>wobei die Steuer-/Regeleinheit (350) dazu eingerichtet ist, eine Karte aus der Karteninformation-Speichereinheit (340) entsprechend den empfangenen Bildaufnahmepositions-Informationen zu bestimmen, die Referenzobjekt-Erkennungsinformationen zu analysieren, um eine Position des Referenzobjekts auf der bestimmten Karte zu analysieren, und eine Bildaufnahmerichtung entsprechend der bestimmten Referenzobjektposition<!-- EPO <DP n="28"> --> und der Bildaufnahmepositions-Informationen zu erhalten.</claim-text></claim-text></claim><claim id="c-de-01-0015" num="0015"><claim-text>Kommunikationssystem nach Anspruch 14, wobei die Steuer-/Regeleinheit (350) dazu eingerichtet ist, verborgene Objekte zu bestimmen, welche innerhalb eines Bereichs angeordnet sind, welcher entsprechend einer Blicktiefe (D) und eines Blickwinkels (θ) in der erhaltenen Bildaufnahmerichtung bestimmt worden ist.</claim-text></claim></claims><claims mxw-id="PCLM56978031" lang="EN" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-en-01-0001" num="0001"><claim-text>A method for providing augmented information (AR) of concealed objects using a terminal (110) connectable to a server (130) via a wired and/or wireless communication network, the method comprising on the side of the terminal (110):
<claim-text>acquiring (405) an image of a real environment through an image acquiring unit (210) of the terminal (110);</claim-text>
<claim-text>receiving reference object selection information from a user;</claim-text>
<claim-text>defining (415) a reference object included in the acquired image;</claim-text>
<claim-text>obtaining (425) image capturing position information about a position of the image and obtaining (420) reference object recognition information of the defined reference object;</claim-text>
<claim-text>receiving information about the concealed objects, the concealed objects being disposed behind the reference object about a direction from the image capturing position to the reference object; and</claim-text>
<claim-text>outputting the received information about the concealed objects on a display unit (220) of the terminal (110),</claim-text>
<claim-text><b>characterized in that</b> the method further comprises: on the side of the terminal (110):
<claim-text>a1) defining the reference object from among a plurality of visible objects included in the acquired image based on said reference object selection information;</claim-text>
<claim-text>a2) obtaining the reference object recognition information by extracting property information in the form of outline and/or color information of the defined reference object from the acquired image, and;<!-- EPO <DP n="17"> --></claim-text>
<claim-text>a3) transmitting the obtained image capturing position information and the reference object recognition information to the server (130);</claim-text>
<claim-text>a4) receiving the information about the concealed objects at the terminal (110) from the server (130).</claim-text></claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>The method of claim 1, wherein the defining of the reference object comprises:
<claim-text>creating a focal point on a screen on which the image is output; and</claim-text>
<claim-text>defining an object located at the focal point as the reference object.</claim-text></claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>The method of claim 1 or 2, wherein the information about concealed objects is information of concealed objects which are present within a range defined based on the reference object,<br/>
wherein the range preferably is determined by a depth and an angle of view in the direction from the image capturing position to the reference object.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>The method of one of claims 1 to 3, wherein the information about concealed objects is user interface information that includes positions of concealed objects with respect to the reference object,<br/>
wherein the concealed objects preferably are represented in the user interface information by different identification numbers or colors for individual concealed objects.</claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>The method of one of claims 1 to 4, further comprising:
<claim-text>in response to receiving object of interest selection information about an object of interest selected from concealed objects, obtaining AR information corresponding to the object of interest; and</claim-text>
<claim-text>outputting the obtained AR information.</claim-text></claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>The method of one of the claims 1 to 5,<br/>
<!-- EPO <DP n="18"> -->wherein the method comprises on the side of the server (130):
<claim-text>in response to receiving (430) a concealed object information request from one of the terminals, analyzing (435) property information of a reference object, as reference object recognition information, and image capturing position information received from the terminal (110);</claim-text>
<claim-text>determining (440) information about at least one or more concealed objects disposed behind a the reference object about a direction from an image capturing position to the reference object according to the analysis result; and</claim-text>
<claim-text>transmitting the determined information about the concealed objects to the terminal (110).</claim-text></claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>The method of claim 6, wherein the analyzing (435) of the reference object recognition information and the image capturing position information comprises:
<claim-text>determining a map corresponding to the image capturing position information,</claim-text>
<claim-text>analyzing the reference object recognition information to determine a position of the reference object on the determined map, and</claim-text>
<claim-text>determining an image capturing direction according to the determined reference object position and the image capturing position.</claim-text></claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>The method of claim 6 or 7, wherein the determining (440) of the information comprises determining concealed objects disposed within a range determined according to a depth and an angle of view in the obtained image capturing direction.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>A terminal (110) to provide augmented reality (AR) information of concealed objects, the terminal (110) comprising:
<claim-text>an image acquiring unit (210) adapted to acquire an image of a real environment;</claim-text>
<claim-text>a display unit (220) adapted to output the image and<!-- EPO <DP n="19"> --> information about the concealed objects;</claim-text>
<claim-text>a communication unit adapted to access a server (130) and process a signal communicated with the server (130); and</claim-text>
<claim-text>a control unit (260) adapted:
<claim-text>to receive reference object selection information from a user,</claim-text>
<claim-text>to define a reference object included in the acquired image,</claim-text>
<claim-text>to obtain image capturing position information about a position of the image,</claim-text>
<claim-text>to obtain reference object recognition information of the defined reference object,</claim-text>
<claim-text>to receive information about the concealed objects present about a direction from the image capturing position to the reference object, and</claim-text>
<claim-text>to output the received information about concealed objects to the display unit (220),</claim-text>
<claim-text><b>characterized in that</b></claim-text>
<claim-text>the control unit (260) is further adapted to:
<claim-text>define the reference object from among a plurality of visible objects included in the acquired image based on said reference object selection information;</claim-text>
<claim-text>obtain the reference object recognition information by extracting property information in the form of outline and/or color information of the defined reference object from the acquired image, and</claim-text>
<claim-text>transmit the obtained image capturing position information and reference object recognition information to the server (130) through the communication unit (250), and</claim-text>
<claim-text>to receive from the server (130) the information about the concealed objects.</claim-text></claim-text></claim-text></claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>The terminal (110) of claim 9, further comprising:
<claim-text>a manipulating unit (240) to receive user information,<!-- EPO <DP n="20"> --></claim-text>
<claim-text>wherein the control unit (260) receives reference object selection information from via the manipulating unit (240) to define the reference object,</claim-text>
<claim-text>wherein preferably the control unit (260) displays a focal point (510) on the image output to the display unit (220) and defines an object at the focal point (510) as the reference object according to focal point adjustment information input from the manipulating unit (240).</claim-text></claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>The terminal (110) of claim 9 or 10, wherein the information about concealed objects is user interface information that includes positions of concealed objects with respect to the reference object,<br/>
wherein preferably the concealed objects are represented in the user interface information by different identification numbers or colors for individual concealed objects.</claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>The terminal (110) of claim 10 or 11, wherein the control unit (260), in response to receiving selection information about an object of interest selected from concealed objects, obtains AR information corresponding to the selected object of interest from the server (130), and outputs the obtained AR information to the display unit.</claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>A communication system comprising at least one terminal (110) according to one of the claims 9 to 12, and comprising a server (130) to provide augmented reality (AR) information of concealed objects, the server (130) comprising:
<claim-text>a communication unit (310) adapted to process a signal communicated with at least one or more terminals (110) via a wired and/or wireless communication network;</claim-text>
<claim-text>a control unit (350) adapted to:
<claim-text>analyze in response to receiving concealed object information request signal from the terminal (110) through the communication unit (310), reference object recognition information, which are property information in the form of outline and/or color<!-- EPO <DP n="21"> --> information, of the reference object, and image capturing position information received from the terminal (110),</claim-text>
<claim-text>determine information about at least one or more concealed objects that are disposed behind the reference object about a direction from an image capturing position to the reference object according to the analysis result, and</claim-text>
<claim-text>transmit the determined information about concealed objects to the terminal (110).</claim-text></claim-text></claim-text></claim><claim id="c-en-01-0014" num="0014"><claim-text>The communication system of claim 13, wherein the server (130) further comprising:
<claim-text>a map information storage unit (340) to store map information,</claim-text>
<claim-text>wherein the control unit (350) is adapted to determines a map from the map information storage unit (340) corresponding to the received image capturing position information, to analyzes the reference object recognition information to determine a position of the reference object on the determined map, and to obtains an image capturing direction according to the determined reference object position and the image capturing position information.</claim-text></claim-text></claim><claim id="c-en-01-0015" num="0015"><claim-text>The communication system of claim 14, wherein the control unit (350) is adapted to determine concealed objects disposed within a range determined according to a depth (D) and an angle of view (θ) in the obtained image capturing direction.</claim-text></claim></claims><claims mxw-id="PCLM56978032" lang="FR" load-source="patent-office"><!-- EPO <DP n="29"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Un procédé de fourniture d'informations à réalité augmentée (AR) d'objets masqués en utilisant un terminal (110) qui peut être raccordé à un serveur (130) par l'intermédiaire d'un réseau de communication câblé et/ou sans fil, le procédé comprenant du côté du terminal (110) :
<claim-text>l'acquisition (405) d'une image d'un environnement réel par l'intermédiaire d'une unité d'acquisition d'images (210) du terminal (110) ;</claim-text>
<claim-text>la réception d'informations de sélection d'un objet de référence provenant d'un utilisateur ;</claim-text>
<claim-text>la définition (415) d'un objet de référence inclus dans l'image acquise ;</claim-text>
<claim-text>l'obtention (425) d'informations de position de capture d'image à propos d'une position de l'image et l'obtention (420) d'informations de reconnaissance d'un objet de référence de l'objet de référence défini ;</claim-text>
<claim-text>la réception d'informations à propos des objets masqués, les objets masqués étant disposés derrière l'objet de référence, autour d'une direction de la position de capture d'image par rapport à l'objet de référence ; et</claim-text>
<claim-text>la sortie des informations reçues à propos des objets masqués sur une unité d'affichage (220) du terminal (110),</claim-text>
<claim-text><b>caractérisé en ce que</b> le procédé comprend par ailleurs du côté du terminal (110) :
<claim-text>a1) la définition de l'objet de référence à partir d'une pluralité d'objets visibles inclus dans l'image acquise sur la base desdites informations de sélection d'un objet de référence ;</claim-text>
<claim-text>a2) l'obtention des informations de reconnaissance d'un objet de référence par extraction des informations de propriété sous la forme des informations du contour et/ou de couleur de l'objet de référence défini à partir de l'image acquise, et ;</claim-text>
<claim-text>a3) la transmission des informations de position de capture d'image obtenues et des informations de reconnaissance d'un objet de référence au serveur (130) ;</claim-text>
<claim-text>a4) la réception des informations à propos des objets masqués à hauteur du terminal<!-- EPO <DP n="30"> --> (110) du serveur (130).</claim-text></claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Le procédé selon la revendication 1, dans lequel la définition de l'objet de référence comprend :
<claim-text>la création d'un point focal sur un écran sur lequel l'image est sortie ; et la définition d'un objet situé à hauteur du point focal comme l'objet de référence.</claim-text></claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Le procédé selon l'une des revendications 1 ou 2, dans lequel les informations à propos des objets masqués sont des informations d'objets masqués qui sont présents dans une gamme définie sur la base de l'objet de référence,<br/>
dans lequel la gamme est de préférence déterminée par une profondeur et un angle de vue dans la direction de la position de capture d'image par rapport à l'objet de référence.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Le procédé selon l'une des revendications 1 à 3 dans lequel les informations à propos des objets masqués sont des informations d'interface utilisateur qui comprennent des positions des objets masqués par rapport à l'objet de référence,<br/>
dans lequel les objets masqués sont de préférence représentés dans les informations d'interface utilisateur par différents nombres ou couleurs d'identification pour des objets masqués individuels.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Le procédé selon l'une des revendications 1 à 4, comprenant par ailleurs :
<claim-text>en réponse à la réception des informations de sélection d'un objet d'intérêt à propos d'un objet d'intérêt choisi à partir des objets masqués, l'obtention d'informations AR correspondant à l'objet d'intérêt ; et</claim-text>
<claim-text>la sortie des informations AR obtenues.</claim-text></claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Le procédé selon l'une des revendications 1 à 5,<br/>
dans lequel le procédé comprend du côté du serveur (130) :
<claim-text>en réponse à la réception (430) d'une demande d'information sur un objet masqué d'un des terminaux, l'analyse (435) des informations de propriété d'un objet de référence en tant qu'information de reconnaissance d'un objet de référence et des informations de position de capture d'image reçues à partir du terminal (110) ;<!-- EPO <DP n="31"> --></claim-text>
<claim-text>la détermination (440) d'informations à propos d'au moins un ou plusieurs objets masqués disposés derrière l'objet de référence autour d'une direction à partir d'une position de capture d'image par rapport à l'objet de référence selon le résultat de l'analyse ; et</claim-text>
<claim-text>la transmission des informations déterminées à propos des objets masqués au terminal (110).</claim-text></claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Le procédé selon la revendication 6, dans lequel l'analyse (435) des informations de reconnaissance d'un objet de référence et les informations de position de capture d'image comprennent :
<claim-text>la détermination d'une carte correspondant aux informations de position de capture d'image,</claim-text>
<claim-text>l'analyse des informations de reconnaissance d'un objet de référence pour déterminer une position de l'objet de référence sur la carte déterminée, et</claim-text>
<claim-text>la détermination d'une direction de capture d'image selon la position de l'objet de référence déterminée et la position de capture d'image.</claim-text></claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Le procédé selon l'une des revendications 6 ou 7, dans lequel la détermination (440) des informations comprend la détermination des objets masqués disposés dans une gamme déterminée selon une profondeur et un angle de vue dans la direction de capture d'image obtenue.</claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Un terminal (110) pour fournir des informations à réalité augmentée (AR) d'objets masqués, le terminal (110) comprenant :
<claim-text>une unité d'acquisition d'image (210) destinée à acquérir une image d'un environnement réel ;</claim-text>
<claim-text>une unité d'affichage (220) destinée à sortir l'image et les informations à propos des objets masqués ;</claim-text>
<claim-text>une unité de communication destinée à accéder à un serveur (130) et à traiter un signal communiqué avec le serveur (130) ; et</claim-text>
<claim-text>une unité de commande (260) destinée à :
<claim-text>recevoir des informations de sélection d'un objet de référence à partir d'un utilisateur ;<!-- EPO <DP n="32"> --></claim-text>
<claim-text>définir un objet de référence inclus dans l'image acquise ;</claim-text>
<claim-text>obtenir des informations de position de capture d'image à propos d'une position de l'image ;</claim-text>
<claim-text>obtenir des informations de reconnaissance d'un objet de référence de l'objet de référence défini;</claim-text>
<claim-text>recevoir les informations à propos des objets masqués présents autour d'une direction de la position de capture d'image par rapport à l'objet de référence, et</claim-text>
<claim-text>sortir les informations reçues à propos des objets masqués sur l'unité d'affichage (220),</claim-text>
<claim-text><b>caractérisé en ce que</b></claim-text>
<claim-text>l'unité de commande (260) est par ailleurs destinée à :</claim-text></claim-text>
<claim-text>définir l'objet de référence à partir d'une pluralité d'objets visibles inclus dans l'image acquise sur la base desdites informations de sélection d'un objet de référence ;</claim-text>
<claim-text>obtenir les informations de reconnaissance d'un objet de référence par extraction des informations de propriété sous la forme d'informations de contours et/ou de couleur de l'objet de référence défini à partir de l'image acquise, et</claim-text>
<claim-text>transmettre les informations de position de capture d'image et les informations de reconnaissance d'un objet de référence obtenues au serveur (130) par l'intermédiaire de l'unité de communication (250), et</claim-text>
<claim-text>recevoir du serveur (130) les informations à propos des objets masqués.</claim-text></claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Le terminal (110) selon la revendication 9, comprenant par ailleurs :
<claim-text>une unité de manipulation (240) pour recevoir les informations de l'utilisateur,</claim-text>
<claim-text>dans lequel l'unité de commande (260) reçoit des informations de sélection d'un objet de référence par l'intermédiaire de l'unité de manipulation (240) pour définir l'objet de référence,</claim-text>
<claim-text>dans lequel l'unité de commande (260) affiche de préférence un point focal (510) sur l'image sortie vers l'unité d'affichage (220) et définit un objet à hauteur du point focal (510) comme l'objet de référence selon l'entrée d'informations d'ajustement du point focal à partir de l'unité de manipulation (240).</claim-text></claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Le terminal (110) selon l'une des revendications 9 ou 10, dans lequel les informations à propos des objets masqués sont des informations d'interface utilisateur qui comprennent<!-- EPO <DP n="33"> --> des positions d'objets masqués par rapport à l'objet de référence,<br/>
dans lequel les objets masqués sont de préférence représentés dans les informations de l'interface utilisateur par différents nombres ou couleurs d'identification pour des objets masqués individuels.</claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Le terminal (110) selon l'une des revendications 10 ou 11, dans lequel l'unité de commande (260), en réponse à la réception d'informations de sélection à propos d'un objet d'intérêt choisi parmi les objets masqués, obtient des informations AR correspondant à l'objet d'intérêt choisi à partir du serveur (130) et sort les informations AR obtenues vers l'unité d'affichage.</claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Un système de communication comprenant au moins un terminal (110) selon l'une des revendications 9 à 12 et comprenant un serveur (130) pour fournir des informations de réalité augmentée (AR) d'objets masqués, le serveur (130) comprenant :
<claim-text>une unité de communication (310) destinée à traiter un signal communiqué avec au moins un ou plusieurs terminaux (110) par l'intermédiaire d'un réseau de communication câblé et/ou sans fil ;</claim-text>
<claim-text>une unité de commande (350) destinée à :
<claim-text>analyser, en réponse à la réception d'un signal de demande d'informations d'un objet masqué à partir du terminal (110) par l'intermédiaire de l'unité de communication (310),</claim-text>
<claim-text>des informations de reconnaissance d'un objet de référence qui sont des informations de propriété sous la forme d'informations de contour et/ou de couleur de l'objet de référence et les informations de position de capture d'image reçues à partir du terminal (110),</claim-text>
<claim-text>déterminer les informations à propos d'au moins un ou plusieurs objets masqués qui sont disposés derrière l'objet de référence autour d'une direction à partir d'une position de capture d'image par rapport à l'objet de référence selon le résultat de l'analyse, et</claim-text>
<claim-text>transmettre les informations déterminées à propos d'objets masqués au terminal (110).</claim-text></claim-text></claim-text></claim><claim id="c-fr-01-0014" num="0014"><claim-text>Le système de communication selon la revendication 13 dans lequel le serveur (130) comprend par ailleurs :
<claim-text>une unité de stockage d'informations sur carte (340) pour stocker des informations sur<!-- EPO <DP n="34"> --> carte,</claim-text>
<claim-text>dans lequel l'unité de commande (350) est destinée à déterminer une carte à partir de l'unité de stockage des informations sur carte (340) correspondant aux informations de position de capture d'image reçues, à analyser les informations de reconnaissance d'un objet de référence pour déterminer une position de l'objet de référence sur la carte déterminée et obtenir une direction de capture d'image selon la position de l'objet de référence déterminé et les informations de position de capture d'image.</claim-text></claim-text></claim><claim id="c-fr-01-0015" num="0015"><claim-text>Le système de communication selon la revendication 14, dans lequel l'unité de commande (350) est destinée à déterminer les objets masqués disposés dans une gamme déterminée selon une profondeur (D) et un angle de vue (θ) dans la direction de capture d'image obtenue.</claim-text></claim></claims><drawings mxw-id="PDW16668460" load-source="patent-office"><!-- EPO <DP n="35"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="142" he="142" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="143" he="146" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="103" he="157" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="150" he="215" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="133" he="155" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="115" he="153" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="158" he="208" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="42"> --><figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="155" he="214" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
