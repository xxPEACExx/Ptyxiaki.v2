<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2240887-B1" country="EP" doc-number="2240887" kind="B1" date="20140101" family-id="39144682" file-reference-id="318260" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146553439" ucid="EP-2240887-B1"><document-id><country>EP</country><doc-number>2240887</doc-number><kind>B1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-08775812-A" is-representative="NO"><document-id mxw-id="PAPP154827362" load-source="docdb" format="epo"><country>EP</country><doc-number>08775812</doc-number><kind>A</kind><date>20080630</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140445158" ucid="GB-0800364-A" load-source="docdb"><document-id format="epo"><country>GB</country><doc-number>0800364</doc-number><kind>A</kind><date>20080109</date></document-id></priority-claim><priority-claim mxw-id="PPC140445675" ucid="GB-2008002262-W" load-source="docdb"><document-id format="epo"><country>GB</country><doc-number>2008002262</doc-number><kind>W</kind><date>20080630</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130710</date></intention-to-grant-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988108375" load-source="ipcr">G06K   9/46        20060101AFI20090805BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1988104647" load-source="docdb" scheme="CPC">G06F  17/30247     20130101 LI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988121669" load-source="docdb" scheme="CPC">G06K   9/4633      20130101 FI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988127023" load-source="docdb" scheme="CPC">G06K   9/4671      20130101 LI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988131253" load-source="docdb" scheme="CPC">G06K   9/6211      20130101 LI20130101BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132192801" lang="DE" load-source="patent-office">MERKMALBASIERTE SIGNATUREN FÜR BILDIDENTIFIZIERUNG</invention-title><invention-title mxw-id="PT132192802" lang="EN" load-source="patent-office">FEATURE-BASED SIGNATURES FOR IMAGE IDENTIFICATION</invention-title><invention-title mxw-id="PT132192803" lang="FR" load-source="patent-office">SIGNATURES À BASE DE CARACTÉRISTIQUES POUR L'IDENTIFICATION D'IMAGE</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR918137871" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>MITSUBISHI ELEC R&amp;D CT EUROPE</last-name><address><country>NL</country></address></addressbook></applicant><applicant mxw-id="PPAR918134664" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>MITSUBISHI ELECTRIC R &amp; D CENTRE EUROPE B.V.</last-name></addressbook></applicant><applicant mxw-id="PPAR918155840" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>MITSUBISHI ELECTRIC CORP</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR918138101" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>MITSUBISHI ELECTRIC CORPORATION</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918170335" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>BOBER MIROSLAW</last-name><address><country>GB</country></address></addressbook></inventor><inventor mxw-id="PPAR918166093" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>BOBER, MIROSLAW</last-name></addressbook></inventor><inventor mxw-id="PPAR918996881" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>BOBER, MIROSLAW</last-name><address><street>21 Wykeham Road Merrow</street><city>Guildford, Surrey GU1 2SE</city><country>GB</country></address></addressbook></inventor><inventor mxw-id="PPAR918141232" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>BRASNETT PAUL</last-name><address><country>GB</country></address></addressbook></inventor><inventor mxw-id="PPAR918152020" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>BRASNETT, PAUL</last-name></addressbook></inventor><inventor mxw-id="PPAR918996882" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>BRASNETT, PAUL</last-name><address><street>41a The Avenue</street><city>Surbiton, Surrey KT5 8JW</city><country>GB</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR918996885" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Mitsubishi Electric R &amp; D Centre Europe B.V.</last-name><iid>101414147</iid><address><street>Capronilaan 46</street><city>1119 NS Schiphol Rijk</city><country>NL</country></address></addressbook></assignee><assignee mxw-id="PPAR918996883" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>Mitsubishi Electric Corporation</last-name><iid>101105869</iid><address><street>7-3, Marunouchi 2-chome Chiyoda-ku</street><city>Tokyo 100-8310</city><country>JP</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR918996884" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Whitlock, Holly Elizabeth Ann</last-name><suffix>et al</suffix><iid>101003964</iid><address><street>R.G.C. Jenkins &amp; Co 26 Caxton Street</street><city>London SW1H 0RJ</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="GB-2008002262-W"><document-id><country>GB</country><doc-number>2008002262</doc-number><kind>W</kind><date>20080630</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2009087340-A1"><document-id><country>WO</country><doc-number>2009087340</doc-number><kind>A1</kind><date>20090716</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS548828683" load-source="docdb">AT</country><country mxw-id="DS548976569" load-source="docdb">BE</country><country mxw-id="DS548977815" load-source="docdb">BG</country><country mxw-id="DS548861213" load-source="docdb">CH</country><country mxw-id="DS548971610" load-source="docdb">CY</country><country mxw-id="DS548828684" load-source="docdb">CZ</country><country mxw-id="DS548976570" load-source="docdb">DE</country><country mxw-id="DS548971611" load-source="docdb">DK</country><country mxw-id="DS548971612" load-source="docdb">EE</country><country mxw-id="DS548984309" load-source="docdb">ES</country><country mxw-id="DS548977816" load-source="docdb">FI</country><country mxw-id="DS548977817" load-source="docdb">FR</country><country mxw-id="DS548976571" load-source="docdb">GB</country><country mxw-id="DS548971613" load-source="docdb">GR</country><country mxw-id="DS548976572" load-source="docdb">HR</country><country mxw-id="DS548828685" load-source="docdb">HU</country><country mxw-id="DS548861214" load-source="docdb">IE</country><country mxw-id="DS548971614" load-source="docdb">IS</country><country mxw-id="DS548977818" load-source="docdb">IT</country><country mxw-id="DS548971615" load-source="docdb">LI</country><country mxw-id="DS548974759" load-source="docdb">LT</country><country mxw-id="DS548828686" load-source="docdb">LU</country><country mxw-id="DS548974760" load-source="docdb">LV</country><country mxw-id="DS548974761" load-source="docdb">MC</country><country mxw-id="DS548883534" load-source="docdb">MT</country><country mxw-id="DS548984314" load-source="docdb">NL</country><country mxw-id="DS548977819" load-source="docdb">NO</country><country mxw-id="DS548883535" load-source="docdb">PL</country><country mxw-id="DS548861215" load-source="docdb">PT</country><country mxw-id="DS548984315" load-source="docdb">RO</country><country mxw-id="DS548883536" load-source="docdb">SE</country><country mxw-id="DS548976573" load-source="docdb">SI</country><country mxw-id="DS548977820" load-source="docdb">SK</country><country mxw-id="DS548971616" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><description mxw-id="PDES63957210" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>BACKGROUND TO THE INVENTION</b></heading><p id="p0001" num="0001">The present invention relates to a method and apparatus for representing an image, and, in addition, a method and apparatus for comparing or matching images, for example, for the purposes of searching or validation.</p><heading id="h0002"><b>DESCRIPTION OF THE RELATED ART</b></heading><p id="p0002" num="0002">The present invention relates to improvements upon the image identification techniques disclosed in earlier, co-pending European patent application No: <patcit id="pcit0001" dnum="EP06255239A"><text>EP 06255239.3</text></patcit>, and UK patent application Nos: <patcit id="pcit0002" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>, <patcit id="pcit0003" dnum="GB0712388A"><text>GB 0712388.8</text></patcit> and <patcit id="pcit0004" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>, the contents of which are incorporated herein by reference. Details of the inventions and embodiments disclosed in these earlier, co-pending patent applications apply analogously to the present invention and embodiments.</p><p id="p0003" num="0003">The image identification techniques used in the methods and apparatuses described in <patcit id="pcit0005" dnum="EP06255239A"><text>EP 06255239.3</text></patcit>, <patcit id="pcit0006" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>, <patcit id="pcit0007" dnum="GB0712388A"><text>GB 0712388.8</text></patcit> and <patcit id="pcit0008" dnum="GB0719833A"><text>GB 0719833.6</text></patcit> each extract a short binary descriptor from an image (see <figref idrefs="f0002">Figure 2</figref>). These techniques address many drawbacks of the prior art, and, in particular, are characterised by:
<ul><li>reduced computational complexity for both feature extraction and matching,</li><li>reduced image descriptor size,</li><li>increased robustness to various image modifications, and</li><li>reduced false alarm rate to 0.05 parts per million (ppm) level while maintaining detection rate of 99.8% for a wide range of image modifications.</li></ul><!-- EPO <DP n="2"> --></p><p id="p0004" num="0004">These methods offer very high levels of robustness to many common image modifications. However, they may not provide the required level of robustness to a class of image modifications, where a significant part of image content is lost or replaced, such as translation and cropping.</p><p id="p0005" num="0005">Significantly improved robustness to a class of image modifications involving partial loss of image content is therefore desired.</p><p id="p0006" num="0006"><nplcit id="ncit0001" npl-type="b"><text>Huang et al: "Palmprint verification based on principal lines" Pattern Recognition, Elsevier, GB, vol 41, no. 4, 6 December 2007 (2007-12-06), pages 1316-1328</text></nplcit>, uses the Radon transform for "linear feature detection".</p><p id="p0007" num="0007"><nplcit id="ncit0002" npl-type="s"><text>Mikolajczyk K et al: "A performance evaluation of local descriptors" IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Service Center, Los Alamitos, CA, US, vol. 27, no. 10, 1 October 2005 (2005-10-01), pages 1615-1630</text></nplcit> relates to local descriptors.</p><p id="p0008" num="0008"><nplcit id="ncit0003" npl-type="s"><text>Srisuk S et al: "Face authentication using the trace transform" Proceedings 2003 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2003), Madison, WI, June 18-20, 2003; Los Alamitos, CA, vol 1, 18 June 2003 (2003-06-18), pages 304-312</text></nplcit> relates to using the Trace transform for recognition.</p><heading id="h0003"><b>SUMMARY OF THE INVENTION</b></heading><p id="p0009" num="0009">In accordance with a first aspect, the present invention provides a method of deriving a representation of an image as defined in accompanying claim 1.</p><p id="p0010" num="0010">The present invention concerns a new approach to representing an image based on a small number of regions of interest described by region-based Trace-transform descriptors.<!-- EPO <DP n="3"> --></p><p id="p0011" num="0011">In accordance with embodiments of the present invention, a representative small set of interest or feature points are selected from an image. Constraints are used in the selection procedure such as an exclusion zone around previously selected features. A binary "feature signature" is extracted from<!-- EPO <DP n="4"> --> each selected region of interest, using a modified Trace-transform based method. The feature signature is used to characterise the local neighbourhood of the feature. An image is represented by the plurality of local feature signatures and their geometric relations.</p><p id="p0012" num="0012">In accordance with embodiments of the present invention, an image matching procedure is performed as a two stage process that involves rapidly identifying candidate sets of matching pairs of feature points using the cumulative Hamming distance before applying geometric constraints to increase the accuracy of the method. Specifically, the information provided by the geometry of the feature points selected from the image helps to significantly reduce the false alarm rate. The geometric information may also be used to determine the class of image transformation and the transformation parameters.</p><p id="p0013" num="0013">Embodiments of the present invention are designed to be complementary to the image identification apparatuses and methods described by <patcit id="pcit0009" dnum="EP06255239A"><text>EP 06255239.3</text></patcit>, <patcit id="pcit0010" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>, <patcit id="pcit0011" dnum="GB0712388A"><text>GB 0712388.8</text></patcit> and <patcit id="pcit0012" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>. The present invention advantageously provides additional robustness to certain image modifications. However, using state of the art processing resources, independent use of the technique of the present invention may not offer the same performance, in terms of search speed, as the techniques proposed in the above-referenced earlier, co-pending patent applications. In particular, the process of identifying potential pairs of features according to embodiments of the present invention may be at least an order of magnitude slower than the previously proposed methods. Whilst a method according to the present invention and one or more methods of the earlier co-pending patent applications could be used independently, in would be preferable to combine the methods. In this way, the results from using faster methods may then be passed for further analysis into the relatively slower method of the present<!-- EPO <DP n="5"> --> invention, as described below. When applied jointly, the performance benefits in terms of the speed, false detection rates and robustness are maximised.</p><p id="p0014" num="0014">Other features and advantages of the invention will be apparent from the following description and accompanying claims.</p><heading id="h0004"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading><p id="p0015" num="0015">Embodiments of the invention will now be described with reference to the accompanying drawings, of which:
<ul><li><figref idrefs="f0001">Fig. 1a</figref> shows an image;</li><li><figref idrefs="f0001">Fig. 1b</figref> shows a reduced version of the image of <figref idrefs="f0001">Fig. 1a</figref>;</li><li><figref idrefs="f0001">Fig. 1c</figref> shows a rotated version of the image of <figref idrefs="f0001">Fig. 1a</figref>;</li><li><figref idrefs="f0001">Fig. 1d</figref> shows a blurred version of the image of <figref idrefs="f0001">Fig. 1a</figref>;</li><li><figref idrefs="f0001">Fig. 1e</figref> shows a flipped (left-right) version of the image of <figref idrefs="f0001">Fig. 1a</figref>;</li><li><figref idrefs="f0001">Fig. 1f</figref> shows a heavily compressed version of the image of <figref idrefs="f0001">Fig. 1a</figref>;</li><li><figref idrefs="f0001">Fig. 1g</figref> shows a cropped version of the image of <figref idrefs="f0001">Fig. 1a</figref>;</li><li><figref idrefs="f0002">Fig. 2</figref> shows an image and a bit string representation of the image according to the prior art;</li><li><figref idrefs="f0002">Fig. 3</figref> shows an example set of interest points detected by a gradient-based feature detector;</li><li><figref idrefs="f0002">Fig. 4</figref> shows the image boundary region from which no features are chosen and a final set of features based on strength;</li><li><figref idrefs="f0003">Fig. 5</figref> shows interest points found to match using the Hamming distance from (a) an original image and (b) a rotated version of (a), in which the line lengths between the interest points are used to apply geometric constraints; and</li><li><figref idrefs="f0003">Fig. 6</figref> is a block diagram of an apparatus according to an embodiment of the present invention.</li></ul><!-- EPO <DP n="6"> --></p><heading id="h0005"><b>DETAILED DESCRIPTION OF THE EMBODIMENTS</b></heading><p id="p0016" num="0016">Various embodiments for deriving a representation of an image, specifically an image identifier, and for using such a representation/identifier for the purposes of, for example, identification, matching or validation of an image or images, will be described below. The present invention is especially useful for, but is not restricted to, identifying an image. In the described embodiments, an "image identifier" (also referred to simply as "identifier", "signature" or "image signature") is an example of a representation of an image and the term is used merely to denote a representation of an image, or descriptor.</p><p id="p0017" num="0017">The skilled person will appreciate that the specific design details of an image identification apparatus and method, according to an embodiment of the invention, and the derivation of an image identifier for use in image identification, is determined by the requirements related to the type of image modifications it should be robust to, the size of the identifier, extraction and matching complexity, target false-alarm rate, etc.</p><p id="p0018" num="0018">The following example illustrates a generic design that results in an identifier that is robust to the following modifications to an image (this is not an exhaustive list):
<ul><li>Colour reduction,</li><li>Blurring,</li><li>Brightness Change,</li><li>Flip (left-right &amp; top-bottom),</li><li>Greyscale Conversion,</li><li>Histogram Equalisation,</li><li>JPEG Compression,</li><li>Noise,</li><li>Rotation,<!-- EPO <DP n="7"> --></li><li>Cropping,</li><li>Scaling,</li><li>Translation,</li><li>Skewing and</li><li>Perspective change.</li></ul></p><p id="p0019" num="0019">It has been found that the proposed design may typically achieve a low false-alarm rate of less than 10 parts per million (ppm) on a broad class of images and typical detection rates above 95.0%.</p><p id="p0020" num="0020"><figref idrefs="f0001">Fig. 1</figref> shows an example of an image and modified versions of the image. More specifically, <figref idrefs="f0001">Fig. 1a</figref> is an original image, <figref idrefs="f0001">Fig. 1b</figref> is a reduced (scaled) version of the image of <figref idrefs="f0001">Fig. 1a, Fig. 1c</figref> is a rotated version of the image of <figref idrefs="f0001">Fig. 1a, Fig. 1d</figref> is a blurred version of the image of <figref idrefs="f0001">Fig. 1a, Fig. 1e</figref> is a flipped version of the image of <figref idrefs="f0001">Fig. 1a, Fig. 1f</figref> is a compressed version of the image of <figref idrefs="f0001">Fig. 1a and Fig. 1g</figref> is a cropped version of the image of <figref idrefs="f0001">Fig. 1a</figref>.</p><p id="p0021" num="0021">An embodiment of the invention derives a representation of an image, and more specifically, an image identifier, by processing signals and/or image data corresponding to the image.</p><p id="p0022" num="0022">Consistent with <patcit id="pcit0013" dnum="EP06255239A"><text>EP 06255239.3</text></patcit>, <patcit id="pcit0014" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>, <patcit id="pcit0015" dnum="GB0712388A"><text>GB 0712388.8</text></patcit> and <patcit id="pcit0016" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>, in the initial stage of extraction of the image identifier the image is optionally pre-processed by resizing and filtering. The resizing step is used to normalise the images before processing. The filtering step can comprise of filtering to remove effects such as aliasing it can also include region selection and tapering. In one embodiment the image is resized to a resolution of 192xN or Nx192, where N≥192 and preserving the aspect ratio. In another embodiment the image is resized to a square of 192x192. The image is then low pass filtered with a 3x3 Gaussian kernel. A circular region is extracted<!-- EPO <DP n="8"> --> from the centre of the image for further processing. The pre-processing steps are optional and can include any combination of the above.</p><p id="p0023" num="0023">In the next stage interest points are detected in the image. One embodiment uses a feature detector to detect potential features in an image and their strength based on measuring image gradients. One suitable image gradient feature detector is a Harris feature detector. Example results of the Harris feature detector, applied to the image of <figref idrefs="f0002">Fig. 2</figref>, are shown in <figref idrefs="f0002">Fig. 3</figref>. A selection procedure is applied to choose a representative set of features. In an embodiment, up to 16 features are chosen. A boundary is set around the edge of the image from which no features are chosen, in the embodiment the boundary is 16 pixels wide. The strongest feature from the allowable features amongst the detected features is selected first. The subsequent features are selected by order of strength with the restriction that a feature is not selected if it is within an exclusion zone around the previously selected features. In one embodiment, a circular region with a radius of 10 pixels is used as the exclusion zone. The selection process terminates once a predetermined number of representative features (e.g. in the preferred embodiment 16 pixels) has been selected or no more features remain. A set of features selected from those detected in <figref idrefs="f0002">Fig. 3</figref> is shown in <figref idrefs="f0002">Fig. 4</figref>.</p><p id="p0024" num="0024">Now that the location of the feature (or interest) points has been selected, their position is preferably refined to sub-pixel resolution using for example a method such as described in <nplcit id="ncit0004" npl-type="s" url="http://opencvlibrary.sourceforge.net/CvReference"><text>OpenCV (FindCornerSubPix) 03/01/2008., which can be obtained on the Internet at http://opencvlibrary.sourceforge.net/CvReference</text></nplcit>, reference [1]).</p><p id="p0025" num="0025">In <patcit id="pcit0017" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>, incorporated herein by reference, a method for extracting a binary image signature or image representation is disclosed. In an embodiment of the present invention, the method of <patcit id="pcit0018" dnum="GB0719833A"><text>GB 0719833.6</text></patcit> is applied to extract a binary "feature signature" from regions around feature points.<!-- EPO <DP n="9"> --> Thus, each feature signature is a binary representation of an image region associated with a detected feature. In one embodiment a circular image region, having a radius of 32 pixels, centred on a feature point is selected and the binary signature is computed for the image region. The circular region is preferably extracted to sub-pixel level accuracy, using cubic interpolation, for example. The extracted region is then processed to derive a corresponding image identifier, using the method according to <patcit id="pcit0019" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>. A brief summary of this method is provided below.</p><p id="p0026" num="0026">A Trace transform <i>T(d, θ)</i> of the image region is performed by projecting all possible lines, parameterised by d, <i>θ</i> over the image and applying a first functional T over these lines. The result of the Trace transform may be decomposed to reduce its resolution in either or both of its dimensions d, <i>θ</i> in accordance with the method disclosed in <patcit id="pcit0020" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>. A second functional P may be applied to the columns of the Trace transform to give a vector of real numbers (i.e. a one dimensional function), as in the methods of <patcit id="pcit0021" dnum="EP06255239A"><text>EP 06255239.3</text></patcit> and <patcit id="pcit0022" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>. This second functional P is known as the diametrical functional and the resulting vector is known as the circus function.</p><p id="p0027" num="0027">A third functional, the circus functional, may be applied to the circus function to give a single number, although this step is not used in the preferred embodiment.</p><p id="p0028" num="0028">Alternatively, in accordance with the method of <patcit id="pcit0023" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>, a so-called "band-circus" function may be obtained, as an alternative to the circus function discussed above, which restricts the Trace transform to a subset of lines of the image, by selecting and processing values from only a part of the Trace transform.</p><p id="p0029" num="0029">The properties of the result can be controlled by appropriate choices of the first, second and/or third functional (Trace, diametrical and circus).<!-- EPO <DP n="10"> --></p><p id="p0030" num="0030">A binary identifier for the image region is extracted from the circus function (or the band-circus function) via a frequency representation. For example, a function <i>c</i>(<i>ω</i>) may be defined on the magnitude coefficients of the Fourier transform. One illustration of this function is taking the difference of neighbouring coefficients <maths id="math0001" num=""><math display="block"><mi>c</mi><mfenced><mi>ω</mi></mfenced><mo>=</mo><mfenced open="|" close="|" separators=""><mi>F</mi><mfenced><mi>ω</mi></mfenced></mfenced><mo>-</mo><mfenced open="|" close="|" separators=""><mi>F</mi><mo>⁢</mo><mfenced separators=""><mi>ω</mi><mo>+</mo><mn>1</mn></mfenced></mfenced></math><img id="ib0001" file="imgb0001.tif" wi="45" he="10" img-content="math" img-format="tif"/></maths></p><p id="p0031" num="0031">A binary string can be extracted by applying a threshold to the resulting vector, such that <maths id="math0002" num=""><math display="block"><msub><mi>b</mi><mi>ω</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable><mtr><mtd><mn>0</mn><mo>,</mo></mtd><mtd><mi>c</mi><mfenced><mi>ω</mi></mfenced><mo>&lt;</mo><mi>S</mi></mtd></mtr><mtr><mtd><mn>1</mn><mo>,</mo></mtd><mtd><mi>c</mi><mfenced><mi>ω</mi></mfenced><mo>≥</mo><mi>S</mi></mtd></mtr></mtable></mrow><mspace width="2em"/><mi>for all</mi><mspace width="1em"/><mi>ω</mi><mn>.</mn></math><img id="ib0002" file="imgb0002.tif" wi="64" he="18" img-content="math" img-format="tif"/></maths></p><p id="p0032" num="0032">Suitable choices for S include <i>S</i> = 0 and S = mean(<i>c</i>). The binary representation of the image region is then made up of these binary string <i>values B</i>= {<i>b</i><sub>0</sub><i>,...,b<sub>n</sub></i>}<i>.</i></p><p id="p0033" num="0033">In an embodiment of the present invention, since local image regions are used (as opposed to entire images as in <patcit id="pcit0024" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>) a number of modifications need to be made to the signature extraction technique described therein. In particular, two identifiers are extracted from the circus functions of the full Trace transform using the functionals: <maths id="math0003" num="(1)"><math display="block"><mi>max</mi><mfenced separators=""><mi>ξ</mi><mfenced><mi>t</mi></mfenced></mfenced><mo>,</mo></math><img id="ib0003" file="imgb0003.tif" wi="126" he="9" img-content="math" img-format="tif"/></maths> and <maths id="math0004" num="(2)"><math display="block"><mo>∫</mo><mfenced open="|" close="|" separators=""><mi>ξ</mi><mfenced><mi>t</mi></mfenced><mo>⁢</mo><mi>ʹ</mi></mfenced><mo>⁢</mo><mi mathvariant="normal">d</mi><mo>⁢</mo><mi>t</mi><mn>.</mn></math><img id="ib0004" file="imgb0004.tif" wi="124" he="13" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="11"> --> in accordance with the method disclosed in <patcit id="pcit0025" dnum="EP06255239A"><text>EP 06255239.3</text></patcit>.</p><p id="p0034" num="0034">A further six, "multi resolution" identifiers may be extracted from circus functions obtained by decomposing (or sub-sampling) the distance (d) parameter of the Trace transform by a factor of 8, 16 and 32 and applying both (1) and (2), in accordance with the method disclosed in <patcit id="pcit0026" dnum="GB0700468A"><text>GB 0700468.2</text></patcit>.</p><p id="p0035" num="0035">A further three functions are selected from bands in the Trace transform, in accordance with the method disclosed in <patcit id="pcit0027" dnum="GB0712388A"><text>GB 0712388.8</text></patcit>, and two functions are extracted from the trace-annulus functions and one function is extracted from the trace-circular function, in accordance with the method disclosed in <patcit id="pcit0028" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>.</p><p id="p0036" num="0036">Each of these 14 component identifiers is 8 bits, giving a total of 112 bits for each feature identifier. The coordinates (in the pre-processed image) of the feature points are stored along with the identifier, as geometric information about the corresponding feature/image region. Optionally a subset of the 112 bits could be selected or derived.</p><p id="p0037" num="0037">The complete set of feature points may be represented in a number of ways. In the preferred embodiment the image descriptor is a bit stream containing a number of fields of information, the first field is the number of features. Then each feature is represented in the bitstream as the feature point coordinates (x and y), followed by the identifier. Optionally, rather than store the number of features, padding could be used with a flag set to represent an unused field.</p><p id="p0038" num="0038">As mentioned previously, the full matching procedure between the image features involves two stages.<!-- EPO <DP n="12"> --></p><p id="p0039" num="0039">A first step involves comparing all features from one image with all features from the second image. To perform matching between two feature signatures <i>B</i><sub>1</sub> and <i>B</i><sub>2</sub> , both of length <i>N,</i> the Hamming distance is taken: <maths id="math0005" num="(3)"><math display="block"><mi>H</mi><mfenced><msub><mi>B</mi><mn>1</mn></msub><msub><mi>B</mi><mn>2</mn></msub></mfenced><mo>=</mo><mstyle displaystyle="false"><munder><mo>∑</mo><mspace width="1em"/></munder><mo>⁢</mo><msub><mi>B</mi><mn>1</mn></msub><mo>⊗</mo><msub><mi>B</mi><mn>2</mn></msub></mstyle><mo>,</mo></math><img id="ib0005" file="imgb0005.tif" wi="124" he="12" img-content="math" img-format="tif"/></maths> where ⊗ is the exclusive OR (XOR) operator.</p><p id="p0040" num="0040">In an embodiment, the three unique pairs with the minimum Hamming distance H are found and the feature-based cumulative Hamming distance between two images is the sum of the three individual Hamming distances.</p><p id="p0041" num="0041">A second step involves applying a geometric constraint to determine if the images are a genuine or false match. The application of the geometric constraint is relatively computationally expensive, compared to the Hamming distance. To minimise complexity only image pairs with a cumulative Hamming distance below a predefined threshold for potential matches are compared using the geometric method. In one possible embodiment, a set of three feature points (a, b and c) are chosen. Using these three points a ratio of line lengths between the features can be used as a constraint, as shown in <figref idrefs="f0003">Fig. 5</figref>. The ratio of lines lengths is calculated by finding the Euclidean distances between the set of three feature points <i>ab, ac</i> and <i>bc</i>, using the associated coordinate information in the identifier. Two line length ratios are found <maths id="math0006" num=""><math display="block"><msub><mi>L</mi><mn>1</mn></msub><mo>=</mo><mfrac><mi mathvariant="italic">ab</mi><mi mathvariant="italic">ac</mi></mfrac><mo>,</mo></math><img id="ib0006" file="imgb0006.tif" wi="19" he="14" img-content="math" img-format="tif"/></maths> and <maths id="math0007" num=""><math display="block"><msub><mi>L</mi><mn>2</mn></msub><mo>=</mo><mfrac><mi mathvariant="italic">ab</mi><mi mathvariant="italic">bc</mi></mfrac><mn>.</mn></math><img id="ib0007" file="imgb0007.tif" wi="22" he="15" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="13"> --></p><p id="p0042" num="0042">Measures of distance between the line length ratios from images A and B are defined as: <maths id="math0008" num=""><math display="block"><msub><mi>G</mi><mn>1</mn></msub><mo>=</mo><mfrac><mfenced open="|" close="|" separators=""><msubsup><mi>L</mi><mn>1</mn><mi>A</mi></msubsup><mo>-</mo><msubsup><mi>L</mi><mn>1</mn><mi>B</mi></msubsup></mfenced><mrow><msubsup><mi>L</mi><mn>1</mn><mi>A</mi></msubsup><mo>+</mo><msubsup><mi>L</mi><mn>1</mn><mi>B</mi></msubsup></mrow></mfrac><mo>,</mo><mspace width="1em"/><msub><mi>G</mi><mn>2</mn></msub><mo>=</mo><mfrac><mfenced open="|" close="|" separators=""><msubsup><mi>L</mi><mn>2</mn><mi>A</mi></msubsup><mo>-</mo><msubsup><mi>L</mi><mn>2</mn><mi>B</mi></msubsup></mfenced><mrow><msubsup><mi>L</mi><mn>2</mn><mi>A</mi></msubsup><mo>+</mo><msubsup><mi>L</mi><mn>2</mn><mi>B</mi></msubsup></mrow></mfrac><mn>.</mn></math><img id="ib0008" file="imgb0008.tif" wi="59" he="20" img-content="math" img-format="tif"/></maths></p><p id="p0043" num="0043">Advantageously, the ratio of line lengths between defined points in an image are invariant to transformations such as rotation, translation and scaling, such that the above geometric constraint is suitable for matching an image that has undergone such geometric transformations. However, other suitable geometric constraints may be used, consistent with design requirements.</p><p id="p0044" num="0044">In an embodiment two thresholds are applied to the distance measures, one to constrain the magnitude of the individual measures and the other to constrain the sum of the distances.</p><p id="p0045" num="0045">Alternatively multiple candidate sets of three feature point pairs can be preselected using a cumulative (and/or pairwise) Hamming distance constraint. The geometric constraint can then be applied to each of the (possibly ordered by for example cumulative Hamming distance) candidate sets in order until a geometric match is found or no more candidate sets remain.</p><p id="p0046" num="0046">The image matching technique of the present invention, as described above, may be combined with other image matching techniques, such as the method disclosed in <patcit id="pcit0029" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>. A descriptor is generated for each image that contains all of the individual identifiers. In the preferred embodiment the identifier bitstream contains two identifiers based on <patcit id="pcit0030" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>, and the descriptor for the present invention. Of the two identifiers based on <patcit id="pcit0031" dnum="GB0719833A"><text>GB 0719833.6</text></patcit> the first is generated by pre-processing the image to maintain the image aspect ratio and the other where it is processed to a square.<!-- EPO <DP n="14"> --></p><p id="p0047" num="0047">Preferably the matching is performed first with the faster algorithm of <patcit id="pcit0032" dnum="GB0719833A"><text>GB 0719833.6</text></patcit> at a very low false alarm threshold. Any image pairs below the threshold are considered to be a match, and pairs above the threshold (that is not considered a match by <patcit id="pcit0033" dnum="GB0719833A"><text>GB 0719833.6</text></patcit>) are then processed in accordance with the present invention. Firstly, the cumulative Hamming distance is applied and sets of three pairs above the threshold are considered to be different (no match). A geometric constraint is applied for sets of three pairs below the cumulative Hamming distance threshold. If the geometric measure for the set of three pairs is below the threshold level for the geometric measure the image pair are considered to be a match; otherwise they are considered to be different.</p><p id="p0048" num="0048">An apparatus for implementing the present invention, according to an embodiment, for carrying the above methods is shown in <figref idrefs="f0003">Fig. 6</figref>. The implementation involves building a database of identifiers 240 for images stored in an image database 230. The two databases 230 and 240 may be the same or separated databases. The apparatus allows searching for an identifier 260 extracted from a query image 250 to find matches in the identifier database 140. A, possibly ordered, list of images is returned to the user 290 or a query application.</p><p id="p0049" num="0049">As the skilled person will appreciate, many variations and modification may be made to the described embodiments. For example, whilst the described embodiments identify image regions, from which the image signature is derived, based on feature detection, other techniques may be used to identify image regions that are representative of, or significant within, the image. Moreover, whilst the described embodiment uses a feature detector based on an image gradient method, many other suitable methods may be used for the detection of interest points or regions in an image. Similarly, other techniques<!-- EPO <DP n="15"> --> for selecting the best or strongest detected feature or interest points are possible.</p><p id="p0050" num="0050">In addition, it is possible to use one or more of a variety of geometric constraints other than the distance measures relating to the ratio of lines lengths associated with a set of three feature points, used in the described embodiment.. For example, any number of features points can be used in conjunction with distance measures. Alternatively, other geometric constraints may be based on angles between feature points. Moreover, an affine model may be used to define the geometric constraint.</p><p id="p0051" num="0051">It is intended to include all such variations, modifications and equivalents which fall within the spirit and scope of the present invention.</p><heading id="h0006"><b>REFERENCES</b></heading><p id="p0052" num="0052"><ul><li>[1] Open CV Reference Manual, Chapter 1: Image Processing Gradients, Edge and Corners - "FindCornerSubPix": http://opencvlibrary.sourceforge.net/CvReference</li></ul></p></description><claims mxw-id="PCLM56979119" lang="DE" load-source="patent-office"><!-- EPO <DP n="18"> --><claim id="c-de-01-0001" num="0001"><claim-text>Verfahren zum Ableiten einer Darstellung eines Bildes durch die Verarbeitung von Signalen, die dem Bild entsprechen, wobei das Verfahren umfasst:
<claim-text>Identifizieren mehrerer Bereiche des Bildes, wobei jeder Bereich für das Bild kennzeichnend oder bezeichnend ist; und</claim-text>
<claim-text>für jeden erkannten Bildbereich:
<claim-text>Berechnen von mindestens einer Funktion von mindestens einem Teil des Bildbereichs; und</claim-text>
<claim-text>Verwenden der Funktion zum Erhalten einer intermediären Darstellung von mindestens dem Teil des Bildbereichs, und</claim-text></claim-text>
<claim-text>Ableiten der Darstellung des Bildes von den intermediären Darstellungen von mindestens zwei der mehreren Bildbereiche,</claim-text>
<claim-text>wobei das verfahren das Identifizieren mehrerer Bereiche des Bildes durch Folgendes umfasst:
<claim-text>Identifizieren von Eigenschaftspunkten des Bildes, und</claim-text>
<claim-text>Auswählen von Bildbereichen, die erkannten Eigenschaftspunkten des Bildes entsprechen,</claim-text>
<claim-text>basierend auf einem vordefinierten Bildbereich um einen Eigenschaftspunkt eines Bildes, Anwenden einer Trace-Transformation mithilfe der Funktion auf jeden der Bildbereiche, um mehrere Darstellungen abzuleiten, die von Kreisfunktionen (circus functions) der Trace-Transformation mithilfe unterschiedlicher Funktionen extrahiert werden, um dadurch die Darstellung des Bildes zu erhalten.</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Verfahren zum Identifizieren eines Bildes umfassend:<!-- EPO <DP n="19"> -->
<claim-text>Ableiten einer Darstellung des Bildes mithilfe des Verfahrens nach einem vorangegangenen Anspruch, und</claim-text>
<claim-text>zuordnen der Darstellung zum Bild.</claim-text></claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Verwendung für die Übermittlung oder den Empfang einer Darstellung, die mithilfe des Verfahrens nach einem der Ansprüche 1 bis 3 abgeleitet wird.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Vorrichtung, die zum Ausführen des Verfahrens nach einem der Ansprüche 1 bis 3 eingerichtet ist.</claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Vorrichtung nach Anspruch 4, die ein Steuergerät zum Steuern des Betriebs des Geräts zum Durchführen des Verfahrens nach einem der Ansprüche 1 bis 3 umfasst.</claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Vorrichtung nach Anspruch 4, die ferner eines oder mehrere von Speichermitteln zum Speichern von Bildern und/oder Darstellungen von Bildern, beispielsweise einer Bilddatenbank und/oder einer Deskriptordatenbank, Anzeigemitteln und Bildauswahlmitteln umfasst.</claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Computerspeichermedium, das Anweisungen umfasst, die, wenn sie von einem Computer ausgeführt werden, das verfahren nach einem der Ansprüche 1 bis 3 ausführen.</claim-text></claim></claims><claims mxw-id="PCLM56979120" lang="EN" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-en-01-0001" num="0001"><claim-text>A method of deriving a representation of an image by processing signals corresponding to the image, the method comprising:
<claim-text>identifying a plurality of regions of the image, wherein each region is representative of, or significant to, the image; and</claim-text>
<claim-text>for each identified image region:
<claim-text>computing at least one functional of at least part of the image region; and</claim-text>
<claim-text>using the functional to obtain an intermediate representation of the at least part of the image region, and</claim-text></claim-text>
<claim-text>deriving the representation of the image from the intermediate representations of two or more of the plurality of image regions,</claim-text>
<claim-text>the method comprising identifying a plurality of regions of the image by:
<claim-text>identifying feature points of the image, and</claim-text>
<claim-text>selecting image regions corresponding to identified feature points of the image,</claim-text>
<claim-text>based on a predefined image area around a feature point of an image, applying a Trace transform using said functional to each said image region to derive a plurality of representations, extracted from circus functions of the Trace transform using different functionals, thereby obtaining the representation of the image.</claim-text></claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>A method for identifying an image, comprising:
<claim-text>deriving a representation of the image using the method of any preceding claim, and</claim-text>
<claim-text>associating the representation with the image.</claim-text></claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>Use for transmission or reception of a representation derived using the method of any of claims 1 to 3.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>Apparatus arranged to execute the method of any one of claims 1 to 3.</claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>Apparatus as claimed in claim 4, comprising a control device for controlling operation of the device to carry out the method of any one of claims 1 to 3.<!-- EPO <DP n="17"> --></claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>Apparatus as claimed in claim 4, further comprising one or more of storage means for storing images and/or representations of images, for example, an image database and/or a descriptor database, display means, and image selecting means.</claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>A computer storage medium comprising instructions that, when executed by a computed, perform the method as claimed in any one of claims 1 to 3.</claim-text></claim></claims><claims mxw-id="PCLM56979121" lang="FR" load-source="patent-office"><!-- EPO <DP n="20"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Procédé de dérivation d'une représentation d'une image en traitant des signaux correspondant à l'image, le procédé comprenant :
<claim-text>l'identification d'une pluralité de régions de l'image, dans lequel chaque région est représentative de l'image, ou significative de celle-ci ; et</claim-text>
<claim-text>pour chaque région d'image identifiée :
<claim-text>le calcul d'au moins une fonction d'une partie au moins de la région d'image ; et</claim-text>
<claim-text>l'utilisation de la fonction pour obtenir une représentation intermédiaire d'au moins la partie de la région d'image ; et</claim-text></claim-text>
<claim-text>la dérivation de la représentation de l'image à partir des représentations intermédiaires de deux ou plusieurs de la pluralité de régions d'image ;</claim-text>
<claim-text>le procédé comprenant l'identification d'une pluralité de régions de l'image par :
<claim-text>l'identification des points caractéristiques de l'image ; et</claim-text>
<claim-text>la sélection des régions d'image correspondant aux points caractéristiques identifiés sur l'image ;</claim-text>
<claim-text>sur la base d'une zone d'image prédéfinie autour d'un point caractéristique d'une image, l'application d'une transformée Trace en utilisant ladite fonction, à chacune desdites régions d'image pour dériver une pluralité de représentations, extraites à partir de fonctions cercles de la transformée Trace en utilisant différentes fonctions, permettant ainsi d'obtenir la représentation de l'image.</claim-text></claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Procédé d'identification d'une image, comprenant :
<claim-text>la dérivation d'une représentation de l'image en utilisant le procédé selon l'une quelconque des revendications précédentes ; et</claim-text>
<claim-text>l'association de la représentation à l'image.</claim-text><!-- EPO <DP n="21"> --></claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Utilisation destinée à une émission ou à une réception d'une représentation dérivée en utilisant le procédé selon l'une quelconque des revendications 1 à 3.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Appareil agencé pour exécuter le procédé selon l'une quelconque des revendications 1 à 3.</claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Appareil selon la revendication 4, comprenant un dispositif de commande destiné à commander le fonctionnement du dispositif pour réaliser le procédé selon l'une quelconque des revendications 1 à 3.</claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Appareil selon la revendication 4, comprenant en outre un ou plusieurs moyens de stockage destinés à stocker des images et/ou des représentations d'images, par exemple, une base de données d'images et/ou une base de données de descripteurs, des moyens d'affichage, et des moyens de sélection d'images.</claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Support de stockage informatique comprenant des instructions qui, quand lorsqu'elles sont exécutées par un ordinateur, réalisent le procédé selon l'une quelconque des revendications 1 à 3.</claim-text></claim></claims><drawings mxw-id="PDW16668774" load-source="patent-office"><!-- EPO <DP n="22"> --><figure id="f0001" num="1(a),1(b),1(c),1(d),1(e),1(f),1(g)"><img id="if0001" file="imgf0001.tif" wi="130" he="190" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="23"> --><figure id="f0002" num="2,3,4"><img id="if0002" file="imgf0002.tif" wi="111" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="24"> --><figure id="f0003" num="5(a),5(b),6"><img id="if0003" file="imgf0003.tif" wi="165" he="176" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
