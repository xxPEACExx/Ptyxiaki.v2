<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2372642-B1" country="EP" doc-number="2372642" kind="B1" date="20140101" family-id="44370610" file-reference-id="315064" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146552820" ucid="EP-2372642-B1"><document-id><country>EP</country><doc-number>2372642</doc-number><kind>B1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-11001864-A" is-representative="YES"><document-id mxw-id="PAPP154826743" load-source="docdb" format="epo"><country>EP</country><doc-number>11001864</doc-number><kind>A</kind><date>20110307</date><lang>EN</lang></document-id><document-id mxw-id="PAPP220353866" load-source="docdb" format="original"><country>EP</country><doc-number>11001864.5</doc-number><date>20110307</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140450468" ucid="EP-10002758-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>10002758</doc-number><kind>A</kind><date>20100316</date></document-id></priority-claim><priority-claim mxw-id="PPC140446616" ucid="EP-11001864-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>11001864</doc-number><kind>A</kind><date>20110307</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130726</date></intention-to-grant-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988097039" load-source="docdb">G06T   7/00        20060101AFI20110826BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861409304" load-source="docdb" scheme="CPC">G06T   7/75        20170101 FI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1861413303" load-source="docdb" scheme="CPC">G06T2207/30252     20130101 LA20170105BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132190944" lang="DE" load-source="patent-office">Verfahren und System zur Erkennung von bewegenden Objekten</invention-title><invention-title mxw-id="PT132190945" lang="EN" load-source="patent-office">Method and system for detecting moving objects</invention-title><invention-title mxw-id="PT132190946" lang="FR" load-source="patent-office">Procédé et système de détection d'objets en mouvement</invention-title><citations><non-patent-citations><nplcit><text>M. MAGNOR ET AL: "Model-based analysis of multi-video data", 6TH IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION, 2004., 28 March 2004 (2004-03-28), pages 41 - 45, XP055039981, ISBN: 978-0-78-038387-6, DOI: 10.1109/IAI.2004.1300941</text><sources><source mxw-id="PNPL67285129" load-source="docdb" name="EXA"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR918136242" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SONY CORP</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR918149143" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SONY CORPORATION</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918158215" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>HARRES DENNIS</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918165756" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>Harres, Dennis</last-name></addressbook></inventor><inventor mxw-id="PPAR918995580" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Harres, Dennis</last-name><address><street>Obere Waiblinger Strasse 144b</street><city>70374 Stuttgart</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918172463" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>UNRUH CHRISTIAN</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR918133376" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>UNRUH, CHRISTIAN</last-name></addressbook></inventor><inventor mxw-id="PPAR918995577" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>UNRUH, CHRISTIAN</last-name><address><street>Breitlingstraße 16</street><city>70184 Stuttgart</city><country>DE</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR918995579" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Sony Corporation</last-name><iid>101002650</iid><address><street>1-7-1 Konan Minato-ku</street><city>Tokyo 108-0075</city><country>JP</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR918995578" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Müller - Hoffmann &amp; Partner</last-name><iid>100061044</iid><address><street>Patentanwälte Innere Wiener Strasse 17</street><city>81667 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS548967492" load-source="docdb">AL</country><country mxw-id="DS548825292" load-source="docdb">AT</country><country mxw-id="DS548967493" load-source="docdb">BE</country><country mxw-id="DS548851052" load-source="docdb">BG</country><country mxw-id="DS548961825" load-source="docdb">CH</country><country mxw-id="DS548879478" load-source="docdb">CY</country><country mxw-id="DS548825293" load-source="docdb">CZ</country><country mxw-id="DS548967498" load-source="docdb">DE</country><country mxw-id="DS548879479" load-source="docdb">DK</country><country mxw-id="DS548879480" load-source="docdb">EE</country><country mxw-id="DS548959156" load-source="docdb">ES</country><country mxw-id="DS548851053" load-source="docdb">FI</country><country mxw-id="DS548970053" load-source="docdb">FR</country><country mxw-id="DS548967499" load-source="docdb">GB</country><country mxw-id="DS548879481" load-source="docdb">GR</country><country mxw-id="DS548967500" load-source="docdb">HR</country><country mxw-id="DS548825294" load-source="docdb">HU</country><country mxw-id="DS548961938" load-source="docdb">IE</country><country mxw-id="DS548879482" load-source="docdb">IS</country><country mxw-id="DS548970062" load-source="docdb">IT</country><country mxw-id="DS548879483" load-source="docdb">LI</country><country mxw-id="DS548851054" load-source="docdb">LT</country><country mxw-id="DS548958348" load-source="docdb">LU</country><country mxw-id="DS548851055" load-source="docdb">LV</country><country mxw-id="DS548851056" load-source="docdb">MC</country><country mxw-id="DS548958349" load-source="docdb">MK</country><country mxw-id="DS548958350" load-source="docdb">MT</country><country mxw-id="DS548967501" load-source="docdb">NL</country><country mxw-id="DS548970063" load-source="docdb">NO</country><country mxw-id="DS548851057" load-source="docdb">PL</country><country mxw-id="DS548961939" load-source="docdb">PT</country><country mxw-id="DS548967506" load-source="docdb">RO</country><country mxw-id="DS548961940" load-source="docdb">RS</country><country mxw-id="DS548851058" load-source="docdb">SE</country><country mxw-id="DS548959157" load-source="docdb">SI</country><country mxw-id="DS548970064" load-source="docdb">SK</country><country mxw-id="DS548851059" load-source="docdb">SM</country><country mxw-id="DS548879484" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data></bibliographic-data><description mxw-id="PDES63956949" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">This specification relates to a method and a system for processing image data and a corresponding computer program product. For example, this method may form part of a method of detecting moving objects. Likewise, the system may form part of a system for detecting moving objects.</p><heading id="h0001">BACKGROUND</heading><p id="p0002" num="0002">In the field of automotive safety applications, systems are being developed that detect vehicles in neighboring lanes for issuing a lane change warning. For example, methods have been developed in order to detect moving objects in a blind spot region of a vehicle.</p><p id="p0003" num="0003">For example, <patcit id="pcit0001" dnum="EP1988505A"><text>EP-A-1 988 505</text></patcit> and <patcit id="pcit0002" dnum="EP1988488A"><text>EP-A-1 988 488</text></patcit> relate to methods for detecting moving objects in a blind spot region of a vehicle. Recently, it has become desirable to also detect vehicles far behind the ego-vehicle so that the driver may be warned very early.</p><p id="p0004" num="0004">It is an object of the invention to provide an improved method and system to process image data and to detect moving objects, respectively.</p><p id="p0005" num="0005">The above object is achieved by the subject-matter claimed by the independent claims.</p><p id="p0006" num="0006">Further embodiments are defined in the dependent claims.</p><p id="p0007" num="0007">The accompanying drawings are included to provide a further understanding of embodiments of the invention and are incorporated in and constitute a part of this specification. The drawings illustrate the embodiments of the present invention and together with the description serve to explain the principles. Other embodiments of the invention and many of the intended advantages will be readily appreciated, as they become better understood by reference to the following detailed description. The elements of the drawings are not necessarily to scale relative to each other. Like reference numbers designate corresponding similar parts.</p><heading id="h0002">BRIEF DESCRIPTION OF THE DRAWINGS</heading><!-- EPO <DP n="2"> --><p id="p0008" num="0008"><ul><li><figref idrefs="f0001">Fig. 1</figref> shows a schematic representation of a method and a system according to an embodiment;</li><li><figref idrefs="f0002">Fig. 2</figref> shows a schematic view of various sub-systems of an image processing system;</li><li><figref idrefs="f0003">Fig. 3A and 3B</figref> show a representation of a method and a system for processing image data, respectively;</li><li><figref idrefs="f0004">Figs. 4A and 4B</figref> show a raw image and an image that has been processed;</li><li><figref idrefs="f0004">Fig. 5</figref> shows an example of a transformation;</li><li><figref idrefs="f0005">Fig. 6</figref> illustrates a method and a system for processing image data;</li><li><figref idrefs="f0006">Fig. 7A</figref> illustrates a method for detecting a moving object,</li><li><figref idrefs="f0007">Fig. 7B</figref> illustrates a system for detecting a moving object;</li><li><figref idrefs="f0008">Fig. 8A</figref> shows an example of an object that may be detected;</li><li><figref idrefs="f0008">Fig. 8B</figref> shows an example of a segmentation mask;</li><li><figref idrefs="f0009">Fig. 8C</figref> shows an example of a segmentation mask with a plurality of two-dimensional shape representations overlaid;</li><li><figref idrefs="f0009">Fig. 8D</figref> shows an example of a segmentation mask with an example of a matched two-dimensional shape representation;</li><li><figref idrefs="f0010">Fig. 9</figref> shows a method of detecting multiple objects; and</li><li><figref idrefs="f0011">Fig. 10</figref> shows an example of a system for issuing a warning.</li></ul></p><p id="p0009" num="0009">In the following detailed description reference is made to the accompanying drawings, which form a part hereof and in which are illustrated by way of illustration specific embodiments in which the invention may be practiced. In this regard, directional terminology such as "top", "bottom", "front", "back", "leading", "trailing" etc. is used with reference to the orientation of the Figures being described. Since components of embodiments of the invention can be positioned in a number of<!-- EPO <DP n="3"> --> different orientations, the directional terminology is used for purposes of illustration and is in no way limiting. Is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope defined by the claims.</p><p id="p0010" num="0010"><figref idrefs="f0001">Fig. 1</figref> shows a schematic representation of a method and a system according to an embodiment, that may be used for assessing the driving situation and issue a warning if a collision is likely to occur. In step S101, a sequence of video images is input as a video signal into the system. For example, a camera 110 may be used for taking the images. Images may as well be referred to as "frames" or "fields". The images may be taken for example, by a digital video camera and may be stored pixel-wise in a memory. The camera may be disposed at any appropriate position of a vehicle, for example, at the side mirror in the front portion of the vehicle or even at a rear position of the vehicle. As is clearly to be understood, a set of cameras 110, 111 may be attached to the vehicle. In particular, a first camera 110 may take images from the left-hand lane and a second camera 111 may take images from the right-hand lane with respect to the ego-vehicle. The images are fed to an image processing device that may perform a variety of processing steps, at step S102. At step S 103, an evaluation of the previous image processing S102 is performed and a warning is issued if there is a threat of a collision.</p><p id="p0011" num="0011">The image processing system 112 may comprise a variety of sub-systems that will be explained in the following. As will be described, the image processing system 112 is suited for long distance vehicle detection approaching from behind in a lane change scenario. Accordingly, these vehicles may approach on a neighbouring lane far outside the blind spot area. For example, the system may detect multiple approaching vehicles up to approximately 80 m away, or up to an average of about 7.2 s prior to reaching the collision line in a highway scenario. Accordingly, problems occurring with respect to detection accuracy, false alarm handling and dealing with multiple objects need to be dealt with.</p><p id="p0012" num="0012"><figref idrefs="f0002">Fig. 2</figref> shows a schematic view of the various sub-systems that may form part of the image processing system 112 shown in <figref idrefs="f0001">Fig. 1</figref>. After passing one or more pre-processing module(s) 200, in step S201 an image scaling may be performed by an image scaling system 201. Optionally, in step S202 a motion estimation may be performed by a motion estimation device 202. As a further option, thereafter, in step S203, moving objects may be detected in step S203 by a moving object detection device 203. Then, optionally, multiple objects in a group may be detected in step S203 by a system for detecting multiple objects in a group. During any of the<!-- EPO <DP n="4"> --> steps S202, S203, S204, step S205 for detecting a driving scenario may be performed by the driving scenario detection device 205. Thereafter, optionally further processing steps S206 may be performed. As is clearly to be understood, the method or device according to an embodiment of the present invention may comprise any combination or sub-combination of the above steps S201 to S205 or of the sub-systems 201 to 205. The function of any of the shown sub-systems and steps will be explained in the following.</p><heading id="h0003">I. Image scaling</heading><p id="p0013" num="0013">As is shown in <figref idrefs="f0003">Fig. 3A</figref>, a method for processing image data of an image may comprise
<ul><li>segmenting the image into a plurality of segments;</li><li>applying a scaling factor to the image data that are disposed in a specific segment of the image, wherein different scaling factors are assigned to different segments of the image.</li></ul></p><p id="p0014" num="0014">As is further shown in <figref idrefs="f0003">Fig. 3B</figref>, a system 300 for processing image data of an image may comprise
<ul><li>a first component 301 that is adapted to segment the image into a plurality of segments; and</li><li>a second component 302 that is adapted to apply a scaling factor to the image data that are disposed in a specific segment of the image, wherein different scaling factors are assigned to different segments of the image.</li></ul></p><p id="p0015" num="0015">Moreover, the embodiment provides a computer program product that is adapted to perform the above method.</p><p id="p0016" num="0016"><figref idrefs="f0004">Figs. 4A and 4B</figref> show a raw image (<figref idrefs="f0004">Fig. 4A</figref>) and an image that has been processed according to this method (<figref idrefs="f0004">Fig. 4B</figref>). As is shown, an image 401a that has a short distance to the camera is reduced in size to result in the transformed object 401b. Moreover, an object 402a that has a large distance with respect to the camera, is enlarged in size to result in the transformed object 402b. For example, the box 403a of the close object 401a is reduced in size due to this processing, resulting in the transformed box 403b, and the box 404a enclosing the distant object is enlarged due to this processing, resulting in the transformed box 404b.<!-- EPO <DP n="5"> --></p><p id="p0017" num="0017"><figref idrefs="f0004">Fig. 5</figref> shows an example how this processing may be implemented. The image may be segmented along a first direction, for example, the x-direction into a plurality of segments 50<sub>1</sub> ... 50<sub>n</sub>. Thereafter, a varying scaling factor is assigned to each of these segments. Then, the pixel sizes are multiplied by these specific scaling factors. According to an embodiment, the segments may be arranged along one axis. As a modification, the segments may be disposed along two different axes that are orthogonal to each other. For example, the scaling factor may decrease in a monotonic manner along a direction, for example, the x-direction or the y-direction, of the image. Moreover, the scaling factor may decrease from a value larger than one to a value smaller than one. In the shown example, the scaling factor decreases from a value above 1 to a value below 1 in a direction from left to right. For example, the scaling factor may decrease in a linear manner with respect to the distance of the moving object to the ego-vehicle.</p><p id="p0018" num="0018">The described method performs a bilinear interpolation with a constantly changing scaling factor along each of the axes. Accordingly, a single image representation of multiple scales is obtained. Thereby, the items that are far away are enlarged in size whereas the nearby-portions shrink. Accordingly, a representation of all the portions of the image may be obtained so that the objects and the motion thereof may be detected without the need of too much computational cost. In particular, as can be taken from <figref idrefs="f0004">Fig. 4B</figref>, due to the presence of various scales, all relevant portions of the image may be detected. As a result, when the described method is applied to vehicle detection, approaching vehicles may be detected very early or at a large distance.</p><p id="p0019" num="0019">As becomes apparent from the above description, the system may be implemented as a processing device that is adapted to process the image data that have been received by the camera. Alternatively, the camera may comprise a special optical system that is adapted to perform the above method. Still further, there may be a special optical system that does not form part of the camera such as a mirror or the like, the special optical system being adapted to perform the above method.</p><heading id="h0004">II. Motion Estimation</heading><p id="p0020" num="0020">In conventional systems, the motion field is calculated between consequent frames. Accordingly, a motion of at least one pixel per frame should occur so as to be detected. Based on the image resolution and camera orientation, this motion may be transformed into the actual motion of the world object. For example, if a vehicle approaching from behind (at a specific track, for instance), is considered,<!-- EPO <DP n="6"> --> the image motion at (x, y) with velocity of one pixel can be transformed to the real world distance S(x,y) in meters and velocity v(x) = dS(x)/dx in the x-direction. V(x) is the minimal detectable velocity in meters per frame. Accordingly, increasing the capture rate of the video stream will also increase the minimal real world speed of the object in order to be detected. Generally, it is desirable that the capture rate of the video stream is adapted to the minimal real world speed of the object to be detected. However, if the objects to be detected are disposed in a large range of distances and there may be a large range of real world speeds. For example, a high capture rate is desirable for such regions of the image where the real world high-speed motion is close to the camera, such as the blind spot, where the motion estimator might have problems to find the match due to the high x motion in the image plane. In all other cases the motion detection improves with a lower frame rate. For example, slower objects may be detected earlier with a lower frame rate and thus an increased detection distance. As it was found out, this is very helpful for the detection of vehicles at long distances.</p><p id="p0021" num="0021">As is shown in <figref idrefs="f0005">Fig. 6</figref>, a method for processing image data may comprise supplying the image data concurrently to
<ul><li>a first motion estimator 62, the first motion estimator 62 being adapted to identify moving blocks which have performed a movement between consecutive images of a first sequence, the first sequence having a first frame rate; and</li><li>a second motion estimator 63, the second motion estimator being adapted to identify moving blocks which have performed a movement between consecutive images of a second sequence, the second sequence having a second frame rate.</li></ul></p><p id="p0022" num="0022">Likewise, a system for processing image data comprises a first motion estimator 62, the first motion estimator 62 being adapted to identify moving blocks which have performed a movement between consecutive images of a first sequence, the first sequence having a first frame rate; and a second motion estimator 63, the second motion estimator 63 being adapted to identify moving blocks which have performed a movement between consecutive images of a second sequence, the second sequence having a second frame rate.</p><p id="p0023" num="0023">Moreover, the embodiment provides a computer program product that is adapted to perform the above method.<!-- EPO <DP n="7"> --></p><p id="p0024" num="0024">In the example shown in <figref idrefs="f0005">Fig. 6</figref>, the image data are first processed by a pre-processing block 61. For example, the pre-processing block 61 may include various processing devices such as filtering devices, image correction devices and others. Thereafter, the image data are supplied to the first motion estimator 62 as well as to the second motion estimator 63. Optionally, the image data may as well be supplied to a third motion estimator 64 that identifies moving items that have performed a movement between consecutive images of the third sequence wherein the third sequence has a third frame rate. For example, the first frame rate may be a multiple integer of the second frame rate.</p><p id="p0025" num="0025">As an example, the frame rate of the first motion estimator 62 may be 30 fps (frames per second), corresponding to a distance of 33 ms between consecutive images. This frame rate corresponds to the frame rate of a typical camera. Moreover, the second frame rate may be 3 fps so that the first frame rate corresponds to the 10-fold of the second frame rate. Accordingly, when analyzing consecutive images at the first and the second frame rates, the second motion estimator 63 analyses every tenth frame of the sequence, whereas the first motion estimator 62 analyses every single image of the sequence. Accordingly, this arrangement covers a higher range of detectable speeds. To be more specific, the motion estimator shown in <figref idrefs="f0005">Fig. 6</figref>, detects low speeds as well as high speeds. The first motion estimator 62 still ensures the detection of high-speed approaching vehicles, which are the most interesting and keeps their response delay to a minimum. Moreover, the second motion estimator 63 detects slow pixel motion of slow vehicles and all distant vehicles. The close slow vehicles generate a slow pixel motion due to the camera perspective and their driving direction towards the camera. The estimated motion fields of each of the motion estimators are then normalized and combined to a final result.</p><p id="p0026" num="0026">Optionally, a third motion estimator 64 may be present so as to be adapted to a specific camera or lens setup. As is clearly to be understood, additional motion estimators may be present in the system according to the described embodiment.</p><heading id="h0005">III. A template matching technique</heading><p id="p0027" num="0027">The template matching technique aims at detecting moving objects from an image sequence so that the distance of the moving objects from the camera may be detected. In order to obtain reliable results, it is desirable to accurately determine the moving objects from a sequence of images.<!-- EPO <DP n="8"> --></p><p id="p0028" num="0028">The mono-ocular camera that is usually used for taking the images, is a 2D sensor that is not configured to deliver depth information. Accordingly, additional processing is needed in order to assess the distance information of the detected objects. The detected objects are normally represented by a segmentation mask that describes which pixels of the image belong to a potential object.</p><p id="p0029" num="0029">As is shown in <figref idrefs="f0006">Fig. 7A</figref>, a method for detecting a moving object may comprise:
<ul><li>Generating two-dimensional shape representations of a three-dimensional object on the basis of a plurality of parameter sets (S710); and</li><li>matching motion blocks of a segmentation mask with the two-dimensional shape representations (S712) to obtain a best fit parameter set.</li></ul></p><p id="p0030" num="0030">For example, the two-dimensional shape representations of the three-dimensional object may be generated using a plurality of parameter sets comprising the distance of the three-dimensional object and a viewing perspective, e.g. viewing angle. From the matching of the motion blocks of the segmentation mask with these two-dimensional shape representations, the best matching shape representation and, consequently, the best fit parameter set may be obtained. By way of example, the distance of the three-dimensional object may be assessed from this best fit parameter set (S714). Nevertheless, as is readily to be understood, different parameters such as vehicle modell or object size could as well be taken as parameters and be determined based on this matching method.</p><p id="p0031" num="0031">According to an embodiment, the segmentation mask including motion blocks may be generated by:
<ul><li>taking a sequence of images (S700);</li><li>partitioning each of the images of the sequence into blocks (S702); and</li><li>identifying motion blocks from the sequence.</li></ul></p><p id="p0032" num="0032">According to an embodiment, identifying motion blocks may comprise:
<ul><li>identifying a moving block;</li><li>determining direction and distance of movement of the moving block;; and</li><li>grouping adjacent moving blocks with directions and distances within predetermined intervals to form the motion blocks.</li></ul></p><p id="p0033" num="0033">In step S700, a sequence of images is taken.<!-- EPO <DP n="9"> --></p><p id="p0034" num="0034">For example, when identifying moving blocks, each block may comprise 8 x 8 or 16 x 16 pixels, but not only square blocks but blocks of other shapes or with another number of pixels are also possible.Further, blocks that have moved between consecutive images of the sequence may be identified. For example, this may be accomplished using a motion estimator as has been described above under section II. A movement of a block may be detected by calculating an error criterion for possible block displacements. The sum of absolute differences, the correlation products or other methods may be used as match criteria, for example.</p><p id="p0035" num="0035">Then, a distance and a direction of the movement may be determined, for example, by comparing the position of the moving blocks in consecutive images. The movement in a "similar" direction by a "similar" distance may also be determined by calculating a motion vector between blocks of consecutive images and comparing the motion vector. In step S702, adjacent blocks, for which direction within a predetermined direction interval and distance with a predetermined distance interval have been determined, are grouped in motion blocks. The predetermined direction interval and the predetermined distance interval are used to detect movement in basically the same direction by basically a same distance. The predetermined intervals may be fixed or may be adaptive, e.g. to the actual directions or distances of the moving blocks. Accordingly, moving blocks which basically move in the same direction by a same distance are grouped together to form motion blocks constituting a segmentation mask.</p><p id="p0036" num="0036">Usually, the segmentation mask represents the detected objects, describing which pixels of the image belong to a potential object such as a vehicle to be detected. The segmentation mask may contain multiple detected objects as well as false alarms. The non-connected, contiguous blocks may be isolated and processed alone as possible vehicle candidates. For improving the accuracy of the detection, a projection of the real world objects into the pixel coordinates is accomplished. Accordingly, valid objects are transformed into a 2-dimensional representation.</p><p id="p0037" num="0037">In step S710, two-dimensional shape representations of the moving object are generated. For example, this may be accomplished by calculating a plurality of images of the three-dimensional object on the basis of a plurality of parameter sets. For calculating the image, for example, a perspective transformation may be employed, utilizing constant camera calibration parameters, any information about the optical imaging onto the sensor, the height of the camera as well as variable parameters such as the distance of the object and the camera perspective. Accordingly, for a plurality of sets of parameters a plurality of two-dimensional<!-- EPO <DP n="10"> --> shape representations are generated. Accordingly, it is attempted to project the real world as well as the moving objects to the pixel coordinates, utilizing various specific perspectives from which the images were taken.</p><p id="p0038" num="0038">Thereafter, in step S712, the motion block of the segmentation mask is matched with the two-dimensional shape representations, to obtain the best fit. For example, the best fit may be assessed utilizing a matching score that is computed as a ratio of three areas. <maths id="math0001" num=""><math display="block"><mi>Score</mi><mo>=</mo><mfenced separators=""><mi mathvariant="normal">A</mi><mo>-</mo><msub><mi mathvariant="normal">w</mi><mn mathvariant="normal">1</mn></msub><mo>⁢</mo><mi mathvariant="normal">B</mi></mfenced><mo>/</mo><mfenced separators=""><mi mathvariant="normal">A</mi><mo>+</mo><msub><mi mathvariant="normal">w</mi><mn mathvariant="normal">2</mn></msub><mo>⁢</mo><mi mathvariant="normal">C</mi></mfenced><mn mathvariant="normal">.</mn></math><img id="ib0001" file="imgb0001.tif" wi="57" he="11" img-content="math" img-format="tif"/></maths></p><p id="p0039" num="0039">In the above formula, area A is covered by both the motion block under consideration and the shape representation. Area B is inside the shape representation but not covered by the motion block. Area C is inside the motion block but outside of the shape representation. w<sub>1</sub> and w<sub>2</sub> are the weights. In case of a perfect match, B and C would be zero resulting in a score of 1.0. In all other cases, the final score is below 1.0 and depending on the weight it can be prioritized to select either maximal inner overlapping of the motion block and the shape representation or the minimal outer non-overlapping areas. This method takes into account that the determination of the motion block may be inaccurate and spread out to some neighboring image regions that are not a vehicle. After finding the best-matching two-dimensional shape representation, the best-fit parameter set, for example including distance and viewing perspective, is obtained. <figref idrefs="f0007">Figure 7B</figref> shows an embodiment of a system for detecting moving objects. The system may comprise a camera 702, a processor or processing device 704 and an indicator 706. The camera 702 is configured to take a sequence of images as has been explained above. For example, the camera 702 may be a monocular camera. The processor 704 may be configured to partition each of the images of said sequence into blocks, to identify moving blocks that have performed a movement between consecutive images of the sequence, to determine a direction and a distance of the movement of the moving blocks, and to group adjacent moving blocks, for which directions within a predetermined direction interval and distances within a predetermined distance interval has been determined, to form motion blocks. Moreover, the processor may be configured to generate two-dimensional shape representations of a three-dimensional object in a manner as has been explained above with reference to step S710. The processor 704 is further adapted to match the motion block with the two-dimensional shape representations to obtain a best fit parameter set as has been explained above. The present embodiment also relates to a computer program product that is adapted to perform any of the functions as has been explained<!-- EPO <DP n="11"> --> above. For example, the computer program product may be adapted to perform the steps S702 to S714 as has been explained above. Moreover, there may be several computer program products that are adapted to generate a segmentation mask, for example, in a manner as has been explained above and the steps S710 to S714. Nevertheless, it is clearly to be understood, any combinations of steps may be implemented in different computer program products.</p><p id="p0040" num="0040"><figref idrefs="f0008 f0009">Fig. 8</figref> shows an example of an image having different areas to which the above method is applied.</p><p id="p0041" num="0041"><figref idrefs="f0008">Fig. 8A</figref> shows an example of a vehicle 810 driving on a road 811. <figref idrefs="f0008">Fig. 8B</figref> shows a segmentation mask that may be obtained from a sequence of images that are taken of the vehicle 810. The different portions 803, 804, and 805 shown in <figref idrefs="f0008">Fig. 8B</figref> illustrate the various blocks that may be obtained from an evaluation of the sequence of images. For example, there may be a motion block 804 corresponding to the surroundings of the road 811, a motion block 805 corresponding to the road, and a motion block 803, corresponding to a potential vehicle to be detected. 806 refers to a non-moving portion on the left-hand portion of the road.</p><p id="p0042" num="0042">According to the embodiment illustrated in <figref idrefs="f0009">Fig. 8C</figref>, a plurality of two-dimensional shape representations 802a, 802b, 802c of a vehicle are generated. As has been described above, when generating this shape representation 802, the specific parameters that have been used when capturing the images for forming the segmentation mask have to be taken into account. Moreover, different parameter sets including, for example, distance and viewing perspective are taken into account. Accordingly, various shape representations 802a, 802b, 802c having a shape of a box are generated. According to the method as described above, the motion block 803 is matched with each of the shape representations 802a, 802b, 802c. As a result of this matching procedure, the best matching shape representation 802a is obtained. Accordingly, as is shown in <figref idrefs="f0009">Fig. 8D</figref>, the actual position of the vehicle is as illustrated by shape representation 802a. From the specific parameter set that has been used for generating the shape representation 802a, distance and viewing perspective for this specific object may be assessed. Hence, the remaining portions of area 803 are assumed to not to belong to the object to be detected. Accordingly, as becomes apparent from <figref idrefs="f0009">Fig. 8D</figref>, there are many invalid portions of the image belonging to the block 803 that represents the potential vehicle to be detected. Assessing the position of the moving vehicle from such an invalid portion would result in false results.<!-- EPO <DP n="12"> --></p><heading id="h0006">IV. Detection of grouped vehicles</heading><p id="p0043" num="0043">With increasing detection range, new challenges arise to deal with traffic on the neighboring lane. In particular, single objects have to be detected from a group of multiple objects. Moreover, in a typical side-view perspective the pose and appearance of vehicles changes across the detectable range. For example, approaching vehicles may occlude following vehicles or appear as a long vehicle when driving close to each other. To be more specific, when generating a segmentation mask, motion blocks may be determined that comprise multiple vehicles at a short distance.</p><p id="p0044" num="0044"><figref idrefs="f0010">Fig. 9</figref> schematically illustrates a method of detecting multiple objects in a group.</p><p id="p0045" num="0045">For example, the method may comprise:
<ul><li>Generating a segmentation mask comprising motion blocks;</li><li>detecting a front most object from the motion block; and</li><li>subtracting a shape of the front most object from the motion block.</li></ul></p><p id="p0046" num="0046">Moreover, a system for processing image data may comprise components that are adapted to perform the method as described above. For example, the system may have a construction as is shown in <figref idrefs="f0007">Fig. 7B</figref>.</p><p id="p0047" num="0047">With reference to <figref idrefs="f0010">Fig. 9</figref>, for example, for generating a segmentation mask, in step S900, a sequence of images may be taken, in a similar manner, as has been described above with reference to <figref idrefs="f0006 f0007">Fig. 7</figref>. In step S904, motion blocks may be identified from the sequence. For example, this may be accomplished by performing the steps as described above with reference to <figref idrefs="f0006 f0007">Fig. 7</figref>. To this end, according to an embodiment, each of the images of a sequence may be partitioned into blocks in a similar manner as described above. Moreover,blocks that have been moved between consecutive images of the sequence may be identified in a similar manner as has been described with reference to <figref idrefs="f0006 f0007">Fig. 7</figref>. Then, a distance and a direction of the movement may be determined in a similar manner as in <figref idrefs="f0006 f0007">Fig. 7</figref>. Adjacent blocks for which directions within a predetermined direction interval and distances within a predetermined distance interval have been determined may be grouped to form motion blocks in a similar manner as has been described with reference to <figref idrefs="f0006 f0007">Fig. 7</figref>. Then in step S910, the front most object may be detected from the motion block. For example, this may be accomplished by any suitable method. As an example, the front most object may be detected by performing the matching method as has been described above under section III. Thereafter, in step S912,<!-- EPO <DP n="13"> --> the front most object is subtracted from the image. For example, the pixels relating to the front most objects may be removed from the image, e.g. the corresponding motion block. This creates one or multiple residual motion blocks which are again validated to be a potential object or not (step S904). According to an embodiment, the steps S904 to S912 are repeated until no further blocks are present in the image. Accordingly, a group of vehicles is detected as a closed segmented region from the segmentation mask based on the motion vectors. If the motion blocks are not well segmented, the residual ones may create false alarms or inaccurate detection. Accordingly, it is desirable to have a reliable segmentation and distance estimation method. Hence, the methods as have been described above are suitable for processing the image data prior to performing the present method.</p><heading id="h0007">V. Driving scenario detection</heading><p id="p0048" num="0048">According to this embodiment the parameters of any of the methods as have been explained above may be adjusted according to the specific driving scenario of the vehicle. For example, based on the information available on the vehicles CAN-bus such as speed, steering angle, current gear, the system may distinguish between different driving scenarios such as:
<ul><li>parking/off</li><li>urban/traffic jam</li><li>country side</li><li>highway</li></ul></p><p id="p0049" num="0049">According to the embodiment, the mode selection may be based on the gathered statistics (average, variance, maxima) of the CAN data over a specific time interval in combination with the velocity ranges for each category.
<tables id="tabl0001" num="0001"><table frame="all"><tgroup cols="4"><colspec colnum="1" colname="col1" colwidth="32mm"/><colspec colnum="2" colname="col2" colwidth="31mm"/><colspec colnum="3" colname="col3" colwidth="39mm"/><colspec colnum="4" colname="col4" colwidth="14mm"/><thead><row><entry align="center" valign="top"><b>Mode</b></entry><entry align="center" valign="top"><b>Velocity</b></entry><entry align="center" valign="top"><b>Steering Angle</b></entry><entry align="center" valign="top"><b>Gear</b></entry></row></thead><tbody><row><entry>Parking / Off</entry><entry align="center">&lt;10 km/h</entry><entry align="center">σ<sup>2</sup> high, <i>max</i> high</entry><entry align="center">N, R</entry></row><row><entry>Urban / Traffic Jam</entry><entry align="center">0-70 km/h, σ<sup>2</sup> high</entry><entry align="center">σ<sup>2</sup> high, <i>max</i> low-mid</entry><entry align="center">N, 1-5</entry></row><row><entry>Country-side</entry><entry align="center">50-100 km/h</entry><entry align="center">σ<sup>2</sup> low, <i>max</i> low</entry><entry align="center">&gt;4</entry></row><row><entry>Highway</entry><entry align="center">&gt;60 km/h</entry><entry align="center">σ<sup>2</sup> very low, <i>max</i> very low</entry><entry align="center">&gt;4</entry></row></tbody></tgroup></table></tables></p><p id="p0050" num="0050">In each mode the parameters used for detection and candidate tracking may be modified to correspond to the actual traffic behavior, for example, the linearity of vehicles motion, valid detection range, criteria of four candidate creation, validation and removal, false alarm probability, etc.<!-- EPO <DP n="14"> -->
<tables id="tabl0002" num="0002"><table frame="all"><tgroup cols="5"><colspec colnum="1" colname="col1" colwidth="30mm"/><colspec colnum="2" colname="col2" colwidth="33mm"/><colspec colnum="3" colname="col3" colwidth="35mm"/><colspec colnum="4" colname="col4" colwidth="34mm"/><colspec colnum="5" colname="col5" colwidth="34mm"/><thead><row><entry align="center" valign="top"><b>Mode</b></entry><entry align="center" valign="top"><b>Lanes / candidate probability</b></entry><entry align="center" valign="top"><b>False alarm threshold</b></entry><entry align="center" valign="top"><b>Distance new candidates</b></entry><entry align="center" valign="top"><b>Time before Removal</b></entry></row></thead><tbody><row><entry>Parking / Off</entry><entry align="center">-</entry><entry align="center">-</entry><entry align="center">-</entry><entry align="center">-</entry></row><row><entry>Urban / Traffic Jam</entry><entry align="center">&gt;=1, high</entry><entry align="center">High</entry><entry align="center">middle</entry><entry align="center">middle</entry></row><row><entry>Country-side</entry><entry align="center">1, low</entry><entry align="center">Low</entry><entry align="center">far</entry><entry align="center">short</entry></row><row><entry>Highway</entry><entry align="center">&gt;1, high</entry><entry align="center">high</entry><entry align="center">far</entry><entry align="center">long</entry></row></tbody></tgroup></table></tables></p><p id="p0051" num="0051">Thereby, the probability of false alarms may be drastically decreased.</p><p id="p0052" num="0052"><figref idrefs="f0011">Figure 10</figref> shows an overview of the described method and system. At 1001 a sequence of images, for example a video sequence is input into the system. At 1002 an image correction as has been described above under I is performed. For example, the system 1002 for processing image data of an image may comprise a first component that is adapted to segment the image into a plurality of segments; and a second component that is adapted to apply a scaling factor to the image data that are disposed in a specific segment of the image, wherein different scaling factors are assigned to different segments of the image. Likewise, the method that is performed in the system 1002 may comprise segmenting the image into a plurality of segments, and applying a scaling factor to the image data that are disposed in a specific segment of the image, wherein different scaling factors are assigned to different segments of the image. Accordingly, from the image data that have been processed by the system 1002, moving objects at a large distance may be enlarged and, consequently, be detected very early.</p><p id="p0053" num="0053">Thereafter, according to a first option, the data may be passed to a low pass filter 1003. Then the data are transferred to a system 1004 for performing motion estimation. For example, the system 1004 may comprise a first motion estimator, the first motion estimator being adapted to identify moving blocks which have performed a movement between consecutive images of a first sequence, the first sequence having a first frame rate; and a second motion estimator, the second motion estimator being adapted to identify moving blocks which have performed a movement between consecutive images of a second sequence, the second sequence having a second frame rate. Likewise, the method performed by the system 1004 may comprise supplying the image data concurrently to a first motion estimator, the first motion estimator being adapted to identify moving blocks which have performed a movement between consecutive images of a first sequence, the first sequence having a first frame rate, and to a second motion estimator, the second motion estimator being adapted to identify moving blocks which have performed a<!-- EPO <DP n="15"> --> movement between consecutive images of a second sequence, the second sequence having a second frame rate.</p><p id="p0054" num="0054">The information acquired during the motion estimation 1004 is fused at 1005 and then at 1006 the false moving vehicles may be eliminated from the data. Optionally this step may be performed immediately after the image correction at 1002. At 1007 a segmentation mask is generated from the obtained data and closed regions are extracted at 1008. Then, for each region a template matching as has been described under III may be performed at 1009. For this template matching the real existing cars and world may be modeled and transferred by a shape representation as has been explained above. For example, such a method for detecting a moving object may comprise generating two-dimensional shape representations of a three-dimensional object on the basis of a plurality of parameter sets; and matching motion blocks of the segmentation mask with the two-dimensional shape representations to obtain a best fit parameter set. A system 1009 for performing the template matching may be adapted to match motion blocks with the two-dimensional shape representations to obtain a best fit parameter set. The two-dimensional shape representations have been previously generated on the basis of car and world models as well as the camera calibration.</p><p id="p0055" num="0055">The matching as has been explained above may comprise a candidate assignment and an estimation of the velocity at 1010. Optionally, during all steps, the CAN information about the specific traffic situation may be utilized in order to further improve the accuracy of detection. Further, the grouped vehicles may be extracted as has been explained above under IV so as to determine each single vehicle of a group of moving vehicles. Accordingly, a method that is performed by the system 1111 may comprise generating a segmentation mask comprising motion blocks; detecting a front most object from the motion block; and subtracting a shape of the front most object from the motion block.</p><p id="p0056" num="0056">During any stage, the threat is estimated (for example, at 1112) and, depending on the danger of a collision a warning is given under 1113. Due to the combination of the components as shown in <figref idrefs="f0011">Fig. 10</figref>, also vehicles at long distances may be early recognized and the traffic may be reliably monitored.</p></description><claims mxw-id="PCLM56978336" lang="DE" load-source="patent-office"><!-- EPO <DP n="19"> --><claim id="c-de-01-0001" num="0001"><claim-text>Verfahren zum Detektieren von sich bewegenden Objekten in eine Segmentierungsmaske darstellenden Bilddaten, wobei die Segmentierungsmaske aus einer Sequenz von Bildern generiert wird, die von einer Kamera unter spezifischen Kameraparametern aufgenommen worden sind, wobei die Segmentierungsmaske aus Bewegungsblöcken besteht, wobei das Verfahren Folgendes umfasst:
<claim-text>Berechnen mehrerer zweidimensionalen Formdarstellungen eines dreidimensionalen Objekts auf der Basis mehrerer Parametersätze, wobei jeder der Parametersätze bezüglich des dreidimensionalen Objekts einen variablen Abstand und variable Perspektive und die spezifischen Kameraparameter umfasst;</claim-text>
<claim-text>Abgleichen von Bewegungsblöcken der Segmentierungsmaske mit den zweidimensionalen Formdarstellungen, um einen Best-Fit-Parametersatz zu erhalten, der zu der am besten abgeglichenen zweidimensionalen Formdarstellung führt, wodurch ein vorderstes Objekt aus dem Bewegungsblock detektiert wird;</claim-text>
<claim-text>Bestimmen eines Abstands zwischen dem dreidimensionalen Objekt und einer Kameraposition aus dem Best-Fit-Parametersatz;</claim-text>
<claim-text>Subtrahieren des dem vordersten Objekt entsprechenden Bewegungsblocks von der Segmentierungsmaske und<!-- EPO <DP n="20"> --></claim-text>
<claim-text>Wiederholen der Prozesse des Abgleichens von Bewegungsblöcken, Bestimmen eines Abstands und Subtrahieren des Bewegungsblocks, bis keine weiteren Blöcke in dem Bild vorliegen.</claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Verfahren nach Anspruch 1, wobei die spezifischen Kameraparameter aus der Gruppe gewählt werden, die aus Kamerakalibrierungsparametern, Informationen über die optische Bildgebung auf den Sensor und die Höhe der Kamera besteht.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Verfahren nach Anspruch 1 oder 2, wobei die Segmentierungsmaske durch ein Verfahren generiert wird, das Folgendes umfasst:
<claim-text>Partitionieren jedes Bilds der Sequenz von Bildern in Blöcke;</claim-text>
<claim-text>Identifizieren eines sich bewegenden Blocks;</claim-text>
<claim-text>Bestimmen der Richtung und des Abstands der Bewegung des sich bewegenden Blocks;</claim-text>
<claim-text>Gruppieren benachbarter, sich bewegender Blöcke mit Richtungen und Abständen innerhalb vorbestimmter Intervalle, um Bewegungsblöcke auszubilden.</claim-text></claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Verfahren nach einem der Ansprüche 1 bis 3, wobei die Bilddaten aus einer Sequenz von Bildern generiert werden, wobei das Verfahren weiterhin Folgendes umfasst:
<claim-text>Segmentieren eines der Bilder in mehrere Segmente und</claim-text>
<claim-text>Anwenden eines Skalierungsfaktors auf die Bilddaten, die sich in einem spezifischen Segment des Bilds befinden, wobei verschiedene Skalierungsfaktoren verschiedenen Segmenten des Bilds zugewiesen werden.</claim-text></claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Verfahren nach Anspruch 4, wobei sich die Segmente entlang einer Achse befinden.<!-- EPO <DP n="21"> --></claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Verfahren nach Anspruch 4, wobei sich die Segmente entlang zweier Achsen befinden, wobei die beiden Achsen orthogonal zueinander verlaufen.</claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Verfahren nach Anspruch 4, wobei der Skalierungsfaktor von einem Wert größer als 1 zu einem Wert kleiner als 1 entlang einer Achse abnimmt.</claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Verfahren zum Detektieren eines sich bewegenden Objekts, das Folgendes umfasst:
<claim-text>Generieren von Bilddaten durch Aufnehmen einer Sequenz von Bildern und</claim-text>
<claim-text>Durchführen des Verfahrens nach einem der Ansprüche 1 bis 7.</claim-text></claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Computerprogrammprodukt,<br/>
umfassend ein Computerprogrammmittel, das ausgelegt ist zum Ausführen des Verfahrens nach einem der Ansprüche 1 bis 7 und der Schritte davon, wenn es auf einer Datenverarbeitungseinrichtung ausgeführt wird.</claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Computerlesbares Speichermedium,<br/>
umfassend das Computerprogrammprodukt nach Anspruch 9.</claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>System zum Detektieren von sich bewegenden Objekten in eine Segmentierungsmaske darstellenden Bilddaten, wobei die Segmentierungsmaske aus einer Sequenz von Bildern generiert wird, die von einer Kamera unter spezifischen Kameraparametern aufgenommen worden sind, wobei die Segmentierungsmaske aus Bewegungsblöcken besteht, wobei das System Komponenten umfasst, die ausgelegt sind zum<br/>
Berechnen mehrerer zweidimensionaler Formdarstellungen eines dreidimensionalen Objekts auf der Basis mehrerer Parametersätze, wobei jeder der Parametersätze bezüglich des dreidimensionalen Objekts einen variablen<!-- EPO <DP n="22"> --> Abstand und variable Perspektive und die spezifischen Kameraparameter umfasst;<br/>
Abgleichen von Bewegungsblöcken der Segmentierungsmaske mit den zweidimensionalen Formdarstellungen, um einen Best-Fit-Parametersatz zu erhalten, der zu der am besten abgeglichenen zweidimensionalen Formdarstellung führt, wodurch ein vorderstes Objekt aus dem Bewegungsblock detektiert wird;<br/>
Bestimmen eines Abstands zwischen dem dreidimensionalen Objekt und einer Kameraposition aus dem Best-Fit-Parametersatz;<br/>
Subtrahieren des dem vordersten Objekt entsprechenden Bewegungsblocks von der Segmentierungsmaske und<br/>
Wiederholen der Prozesse des Abgleichens von Bewegungsblöcken, Bestimmen eines Abstands und Subtrahieren des Bewegungsblocks, bis keine weiteren Blöcke in dem Bild vorliegen.</claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>System nach Anspruch 11, wobei die spezifischen Kameraparameter aus der Gruppe gewählt werden, die aus Kamerakalibrierungsparametern, Informationen über die optische Bildgebung auf den Sensor und die Höhe der Kamera besteht.</claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>System zum Detektieren eines sich bewegenden Objekts, das Folgendes umfasst:
<claim-text>eine Einrichtung, die ausgelegt ist zum Generieren von Bilddaten durch Aufnehmen einer Sequenz von Bildern; und</claim-text>
<claim-text>das System nach Anspruch 11 oder 12.</claim-text></claim-text></claim></claims><claims mxw-id="PCLM56978337" lang="EN" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-en-01-0001" num="0001"><claim-text>A method for detecting moving objects in image data representing a segmentation mask, wherein the segmentation mask is generated from a sequence of images that have been taken by a camera, under specific camera parameters, the segmentation mask consisting of motion blocks, the method comprising:
<claim-text>calculating a plurality of two-dimensional shape representations of a three-dimensional object on the basis of a plurality of parameter sets, each of the parameter sets comprising variable distance and variable perspective with respect to the three-dimensional object and the specific camera parameters;</claim-text>
<claim-text>matching motion blocks of the segmentation mask with the two-dimensional shape representations to obtain a best fit parameter set resulting in the best-matching two-dimensional shape representation, thereby detecting a front most object from the motion block;</claim-text>
<claim-text>determining a distance between the three-dimensional object and a camera position from the best fit parameter set;</claim-text>
<claim-text>subtracting the motion block corresponding to the front most object from the segmentation mask; and</claim-text>
<claim-text>repeating the processes of matching motion blocks, determining a distance, and subtracting the motion block until no further blocks are present in the image.</claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>The method of claim 1, wherein the specific camera parameters are selected from the group consisting of camera calibration parameters, information about the optical imaging onto the sensor and the height of the camera.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>The method of claims 1 or 2, wherein the segmentation mask is generated by a method comprising:
<claim-text>partitioning each image of the sequence of images into blocks;</claim-text>
<claim-text>identifying a moving block;</claim-text>
<claim-text>determining direction and distance of movement of the moving block;</claim-text>
<claim-text>grouping adjacent moving blocks with directions and distances within predetermined intervals to form motion blocks.</claim-text></claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>The method of any of claims 1 to 3, wherein the image data are generated from a sequence of images, the method further comprising:
<claim-text>segmenting one of the images into a plurality of segments; and<!-- EPO <DP n="17"> --></claim-text>
<claim-text>applying a scaling factor to the image data that are disposed in a specific segment of the image, wherein different scaling factors are assigned to different segments of the image.</claim-text></claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>The method of claim 4, wherein the segments are disposed along one axis.</claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>The method of claim 4, wherein the segments are disposed along two axes, the two axes being orthogonal to each other.</claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>The method of claim 4, wherein the scaling factor decreases from a value larger than 1 to a value smaller than 1 along an axis.</claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>A method for detecting a moving object, comprising<br/>
generating image data by taking a sequence of images; and<br/>
performing the method of any of claims 1 to 7.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>A computer program product,<br/>
comprising a computer program means that is adapted to carry out the method according to any of claims 1 to 7 and the steps thereof when it is carried out on a data processing device.</claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>A computer readable storage medium,<br/>
comprising the computer program product according to claim 9.</claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>A system for detecting moving objects in image data representing a segmentation mask, wherein the segmentation mask is generated from a sequence of images that have been taken by a camera under specific camera parameters, the segmentation mask consisting of motion blocks, the system comprising components that are adapted<br/>
to calculate a plurality of two-dimensional shape representations of a three-dimensional object on the basis of a plurality of parameter sets, each of the parameter sets comprising variable distance and variable perspective with respect to the three-dimensional object and the specific camera parameters;<br/>
to match motion blocks of the segmentation mask with the two-dimensional shape representations to obtain a best fit parameter set resulting in the best-matching two-dimensional shape representation, thereby detecting a front most object from the motion block;<br/>
<!-- EPO <DP n="18"> -->to determine a distance between the three-dimensional object and a camera position from the best fit parameter set;<br/>
to subtract the motion block corresponding to the front most object from the segmentation mask; and<br/>
to repeat the processes of matching motion blocks, determining a distance, and subtracting the motion block until no further blocks are present in the image.</claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>The system of claim 11, wherein the specific camera parameters are selected from the group consisting of camera calibration parameters, information about the optical imaging onto the sensor and the height of the camera.</claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>A system for detecting a moving object, comprising<br/>
a device that is adapted to generate image data by taking a sequence of images; and<br/>
the system of claim 11 or 12.</claim-text></claim></claims><claims mxw-id="PCLM56978338" lang="FR" load-source="patent-office"><!-- EPO <DP n="23"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Procédé de détection d'objets en mouvement dans des données d'images représentant un masque de segmentation, dans lequel le masque de segmentation est généré à partir d'une séquence d'images ayant été acquises par une caméra conformément à des paramètres de caméra spécifiques, le masque de segmentation étant constitué de blocs de mouvement, le procédé consistant à :
<claim-text>calculer une pluralité de représentations de formes bidimensionnelles d'un objet tridimensionnel sur la base d'une pluralité de jeux de paramètres, chacun des jeux de paramètres comprenant une distance variable et une perspective variable par rapport à l'objet tridimensionnel et aux paramètres de caméra spécifiques ;</claim-text>
<claim-text>apparier des blocs de mouvement du masque de segmentation avec les représentations de formes bidimensionnelles afin d'obtenir un jeu de paramètres de meilleur ajustement conduisant à la représentation de forme bidimensionnelle la mieux appariée pour ainsi détecter un objet le plus en avant dans le bloc de mouvement ;</claim-text>
<claim-text>déterminer une distance entre l'objet tridimensionnel et une position de caméra à partir du jeu de paramètres de meilleur ajustement ;</claim-text>
<claim-text>soustraire le bloc de mouvement correspondant à l'objet<!-- EPO <DP n="24"> --> le plus en avant au masque de segmentation ; et</claim-text>
<claim-text>répéter les processus d'appariement de blocs de mouvement, déterminer une distance et soustraire le bloc de mouvement jusqu'à ce qu'il n'y ait plus d'autres blocs de mouvement dans l'image.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Procédé selon la revendication 1, dans lequel les paramètres de caméra spécifiques sont sélectionnés dans le groupe constitué de paramètres d'étalonnage de caméra, d'informations concernant la formation d'image optique sur le capteur et la hauteur de la caméra.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Procédé selon la revendication 1 ou 2, dans lequel le masque de segmentation est généré par un procédé consistant à :
<claim-text>partitionner chaque image de la séquence d'images en des blocs ;</claim-text>
<claim-text>identifier un bloc de mouvement ;</claim-text>
<claim-text>déterminer la direction et la distance de mouvement du bloc de mouvement ;</claim-text>
<claim-text>regrouper des blocs de mouvement adjacents ayant des directions et des distances se situant dans des intervalles prédéterminés afin de former des blocs de mouvement.</claim-text></claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Procédé selon l'une quelconque des revendications 1 à 3, dans lequel les données d'images sont générées à partir d'une séquence d'images, le procédé consistant en outre à :
<claim-text>segmenter l'une des images en une pluralité de segments ; et</claim-text>
<claim-text>appliquer un facteur d'échelle aux données d'images qui sont disposées dans un segment spécifique de l'image, dans lequel des facteurs d'échelle différents sont affectés à des segments différents de l'image.</claim-text></claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Procédé selon la revendication 4, dans lequel les segments sont disposés le long d'un axe.<!-- EPO <DP n="25"> --></claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Procédé selon la revendication 4, dans lequel les segments sont disposés le long de deux axes, les deux axes étant orthogonaux l'un à l'autre.</claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Procédé selon la revendication 4, dans lequel le facteur d'échelle décroît d'une valeur supérieure à 1 à une valeur inférieure 1 le long d'un axe.</claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Procédé de détection d'un objet en mouvement, consistant à :
<claim-text>générer des données d'images par acquisition d'une séquence d'images ; et</claim-text>
<claim-text>exécuter le procédé selon l'une quelconque des revendications 1 à 7.</claim-text></claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Produit de programme informatique,<br/>
comprenant un moyen de programme informatique qui est apte à mettre en oeuvre le procédé selon l'une quelconque des revendications 1 à 7 et les étapes correspondantes lorsqu'il est mis en oeuvre sur un dispositif de traitement de données.</claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Support de stockage lisible par ordinateur, comprenant le produit de programme informatique selon la revendication 9.</claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Système destiné à détecter des objets en mouvement dans des données d'images représentant un masque de segmentation, dans lequel le masque de segmentation est généré à partir d'une séquence d'images ayant été acquises par une caméra conformément à des paramètres de caméra spécifiques, le masque de segmentation étant constitué de blocs de mouvement, le système comprenant des composants qui sont aptes :
<claim-text>à calculer une pluralité de représentations de formes bidimensionnelles d'un objet tridimensionnel sur la base d'une pluralité de jeux de paramètres, chacun des jeux de paramètres comprenant une distance variable et une perspective variable par rapport à l'objet<!-- EPO <DP n="26"> --> tridimensionnel et aux paramètres de caméra spécifiques ;</claim-text>
<claim-text>à apparier des blocs de mouvement du masque de segmentation avec les représentations de formes bidimensionnelles afin d'obtenir un jeu de paramètres de meilleur ajustement conduisant à la représentation de forme bidimensionnelle la mieux appariée pour ainsi détecter un objet le plus en avant dans le bloc de mouvement ;</claim-text>
<claim-text>à déterminer une distance entre l'objet tridimensionnel et une position de caméra à partir du jeu de paramètres de meilleur ajustement ;</claim-text>
<claim-text>à soustraire le bloc de mouvement correspondant à l'objet le plus en avant au masque de segmentation ; et</claim-text>
<claim-text>à répéter les processus d'appariement de blocs de mouvement, de détermination d'une distance et de soustraction du bloc de mouvement jusqu'à ce qu'il n'y ait plus d'autres blocs de mouvement dans l'image.</claim-text></claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Système selon la revendication 11, dans lequel les paramètres de caméra spécifiques sont sélectionnés dans le groupe constitué de paramètres d'étalonnage de caméra, d'informations concernant la formation d'image optique sur le capteur et la hauteur de la caméra.</claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Système destiné à détecter un objet en mouvement, comprenant :
<claim-text>un dispositif qui est apte à générer des données d'images par acquisition d'une séquence d'images ; et le système selon la revendication 11 ou 12.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW16668551" load-source="patent-office"><!-- EPO <DP n="27"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="132" he="170" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="28"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="154" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="29"> --><figure id="f0003" num="3A,3B"><img id="if0003" file="imgf0003.tif" wi="123" he="180" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="30"> --><figure id="f0004" num="4A,4B,5A,5B"><img id="if0004" file="imgf0004.tif" wi="165" he="215" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="31"> --><figure id="f0005" num="6"><img id="if0005" file="imgf0005.tif" wi="133" he="82" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> --><figure id="f0006" num="7A"><img id="if0006" file="imgf0006.tif" wi="116" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> --><figure id="f0007" num="7B"><img id="if0007" file="imgf0007.tif" wi="106" he="139" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> --><figure id="f0008" num="8A,8B"><img id="if0008" file="imgf0008.tif" wi="123" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> --><figure id="f0009" num="8C,8D"><img id="if0009" file="imgf0009.tif" wi="118" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> --><figure id="f0010" num="9"><img id="if0010" file="imgf0010.tif" wi="139" he="211" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0011" num="10"><img id="if0011" file="imgf0011.tif" wi="156" he="233" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
