<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2095296-B1" country="EP" doc-number="2095296" kind="B1" date="20140101" family-id="38862109" file-reference-id="318248" date-produced="20180822" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="146554145" ucid="EP-2095296-B1"><document-id><country>EP</country><doc-number>2095296</doc-number><kind>B1</kind><date>20140101</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-07728354-A" is-representative="NO"><document-id mxw-id="PAPP154828068" load-source="docdb" format="epo"><country>EP</country><doc-number>07728354</doc-number><kind>A</kind><date>20070420</date><lang>EN</lang></document-id><document-id mxw-id="PAPP220212977" load-source="docdb" format="original"><country>EP</country><doc-number>07728354.7</doc-number><date>20070420</date></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC140451570" ucid="EP-2007053895-W" linkage-type="A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>2007053895</doc-number><kind>W</kind><date>20070420</date></document-id></priority-claim></priority-claims><dates-of-public-availability><intention-to-grant-date><date>20130620</date></intention-to-grant-date></dates-of-public-availability><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL1988130551" load-source="docdb">G06K   9/20        20060101AFI20081110BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL1988125607" load-source="docdb" scheme="CPC">G06K   9/00201     20130101 LI20130101BHEP        </classification-cpc><classification-cpc mxw-id="PCL1988131181" load-source="docdb" scheme="CPC">G06K   9/00362     20130101 FI20130101BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT132194919" lang="DE" load-source="patent-office">Verfahren und System zum Bereitstellen eines dreidimensionalen Modells eines Interessenobjekts</invention-title><invention-title mxw-id="PT132194921" lang="FR" load-source="patent-office">Procédé et système pour fournir un modèle tri-dimensionel d'un objet d'interêt</invention-title><invention-title mxw-id="PT132357742" lang="EN" load-source="patent-office">A method and system  for providing a three-dimensional model of an object of interest.</invention-title><citations><patent-citations><patcit mxw-id="PCIT370702402" load-source="docdb" ucid="US-20020050924-A1"><document-id format="epo"><country>US</country><doc-number>20020050924</doc-number><kind>A1</kind><date>20020502</date></document-id><sources><source name="EXA" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT370697203" load-source="docdb" ucid="WO-2002007839-A2"><document-id format="epo"><country>WO</country><doc-number>2002007839</doc-number><kind>A2</kind><date>20020131</date></document-id><sources><source name="EXA" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>K.H. LEE ET AL: "Data Reduction Methods for Reverse Engineering", THE INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY, vol. 17, no. 10, 3 May 2001 (2001-05-03), pages 735 - 743, XP055051129, ISSN: 0268-3768, DOI: 10.1007/s001700170119</text><sources><source mxw-id="PNPL66997384" load-source="docdb" name="EXA"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR918133717" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SOFTKINETIC SOFTWARE</last-name><address><country>BE</country></address></addressbook></applicant><applicant mxw-id="PPAR918148403" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SOFTKINETIC SOFTWARE</last-name></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR918161623" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>PINAULT GILLES</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918152370" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>PINAULT, GILLES</last-name></addressbook></inventor><inventor mxw-id="PPAR918998936" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>PINAULT, GILLES</last-name><address><street>44, rue Emile Wittmann</street><city>B-1030 Bruxelles</city><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918157171" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>ROY JEREMIE</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918144146" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>ROY, JEREMIE</last-name></addressbook></inventor><inventor mxw-id="PPAR918998937" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>ROY, Jérémie</last-name><address><street>34, rue Augustin Delporte</street><city>B-1050 Bruxelles</city><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918135737" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>DESMECHT LAURENT</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918173126" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>DESMECHT, LAURENT</last-name></addressbook></inventor><inventor mxw-id="PPAR918998935" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>DESMECHT, LAURENT</last-name><address><street>68, rue de l'Arsenal</street><city>B-6230 Pont-à-Celles</city><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918161689" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>BAELE XAVIER</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR918160292" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>BAELE, XAVIER</last-name></addressbook></inventor><inventor mxw-id="PPAR918998934" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>BAELE, XAVIER</last-name><address><street>70/5, rue de la Rive</street><city>B-1200 Bruxelles</city><country>BE</country></address></addressbook></inventor></inventors><assignees><assignee mxw-id="PPAR918998938" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SOFTKINETIC SOFTWARE</last-name><iid>101291579</iid><address><street>Boulevard de la Plaine, 15</street><city>1050 Brussels</city><country>BE</country></address></addressbook></assignee></assignees><agents><agent mxw-id="PPAR918998939" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Mackett, Margaret Dawn</last-name><suffix>et al</suffix><iid>101412112</iid><address><street>Gevers Patents Intellectual Property House Holidaystraat 5</street><city>1831 Diegem</city><country>BE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="EP-2007053895-W"><document-id><country>EP</country><doc-number>2007053895</doc-number><kind>W</kind><date>20070420</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2008128568-A1"><document-id><country>WO</country><doc-number>2008128568</doc-number><kind>A1</kind><date>20081030</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS548990865" load-source="docdb">AT</country><country mxw-id="DS548887334" load-source="docdb">BE</country><country mxw-id="DS548874922" load-source="docdb">BG</country><country mxw-id="DS548988056" load-source="docdb">CH</country><country mxw-id="DS548992543" load-source="docdb">CY</country><country mxw-id="DS548990866" load-source="docdb">CZ</country><country mxw-id="DS548887335" load-source="docdb">DE</country><country mxw-id="DS548992544" load-source="docdb">DK</country><country mxw-id="DS548992545" load-source="docdb">EE</country><country mxw-id="DS548985861" load-source="docdb">ES</country><country mxw-id="DS548874923" load-source="docdb">FI</country><country mxw-id="DS548874924" load-source="docdb">FR</country><country mxw-id="DS548887336" load-source="docdb">GB</country><country mxw-id="DS548992546" load-source="docdb">GR</country><country mxw-id="DS548990867" load-source="docdb">HU</country><country mxw-id="DS548988057" load-source="docdb">IE</country><country mxw-id="DS548992547" load-source="docdb">IS</country><country mxw-id="DS548874925" load-source="docdb">IT</country><country mxw-id="DS548992548" load-source="docdb">LI</country><country mxw-id="DS548984984" load-source="docdb">LT</country><country mxw-id="DS548990868" load-source="docdb">LU</country><country mxw-id="DS548984985" load-source="docdb">LV</country><country mxw-id="DS548984986" load-source="docdb">MC</country><country mxw-id="DS548832199" load-source="docdb">MT</country><country mxw-id="DS548990869" load-source="docdb">NL</country><country mxw-id="DS548988058" load-source="docdb">PL</country><country mxw-id="DS548984987" load-source="docdb">PT</country><country mxw-id="DS548990870" load-source="docdb">RO</country><country mxw-id="DS548988059" load-source="docdb">SE</country><country mxw-id="DS548874930" load-source="docdb">SI</country><country mxw-id="DS548985866" load-source="docdb">SK</country><country mxw-id="DS548832200" load-source="docdb">TR</country></ep-contracting-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><description mxw-id="PDES63957630" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">The present invention relates to a method and system for enabling interaction of a human user with a data processing system.</p><p id="p0002" num="0002">Interaction with data processing systems, and in particular the input of data and commands, is a generally known issue. Conventionally, such interaction takes place through physical input devices such as keyboards, mice, scroll wheels, pens, touchscreens, joysticks, gamepads, etc. which produce signals in response to a physical action of the user on them. However, such physical input devices have many drawbacks. For instance, they can only offer a limited amount of different input signals, which in some applications such as three-dimensional "virtual reality" environments will feel awkward and lack realism. Moreover, they are susceptible to wear and their continued use may even have negative consequences for the user's health, such as Repetitive Strain Injury.</p><p id="p0003" num="0003">Alternative input devices and methods are also known. For instance, practical systems for voice recognition are available. However, voice recognition is not a practical alternative for some applications, such as action games, where rapid, precise and repetitive inputs by the user are required. Moreover, their effectiveness is adversely affected by background noise, and they generally require a learning period to recognise a particular user's voice commands. Another alternative is image recognition. In their simplest form, image recognition systems recognise binary patterns in contrasting colours, such as barcodes, and convert these patterns into binary signals for processing. More advanced image recognition systems can recognise more complex patterns in images and produce a large variety of signals in response. Such image recognition systems have been proposed, for instance, in <patcit id="pcit0001" dnum="US6256033B"><text>US Patent 6256033</text></patcit>, for recognising the gestures of a user in range of an imaging system. However, conventional imaging systems have no perception of<!-- EPO <DP n="2"> --> depth and can produce merely a two-dimensional (2D) projection of said user. As a result, the recognition of the user's gestures is inherently flawed, limited in the range of possible inputs and riddled with possible recognition mistakes. In particular, such systems have problems separating the user from its background.</p><p id="p0004" num="0004">The development of three-dimensional (3D) imaging systems, however, offers the possibility to develop shape recognition methods and devices allowing, for instance, better user gesture recognition. One such 3D imaging system was disclosed in G. Yahav, G. J. Iddam and D. Mandelboum, "3D Imaging Camera for Gaming Application". The 3D imaging system disclosed in this paper is of the so-called "Time-Of-Flight" or TOF type, in which a depth perception is obtained from the shape of a wavefront of light reflected from objects in range of the 3D imaging system. However, other types of imaging systems, such as stereo cameras, LIDAR, radar, sonar, etc. have also been proposed.</p><p id="p0005" num="0005">A gesture recognition method and system using such a 3D imaging system was disclosed in the International Patent Application <patcit id="pcit0002" dnum="WO0030023A1"><text>WO 00/30023 A1</text></patcit>. However, because this method does not recognise volumes as such, but merely responds to the presence of points of a subject in certain regions of interest and their movement therein, it can only recognise the simplest of gestures and remains inappropriate for more complicated applications. An even more basic input method was disclosed in <patcit id="pcit0003" dnum="WO2004064022A1"><text>WO 2004/064022 A1</text></patcit>.</p><p id="p0006" num="0006">United States Patent Application Publication <patcit id="pcit0004" dnum="US2006023558A1"><text>US 2006/023558 A1</text></patcit> discloses a shape recognition method using a 3D imaging system. In this method, the points of the 3D image are grouped in clusters or "blobs" according to their perceived depth. Primitives of different shapes of pre-defined objects can then be associated to these "blobs". While this volume recognition method allows more accurate modelling of objects within range of the 3D imaging system, it still has<!-- EPO <DP n="3"> --> significant drawbacks. As all the objects in the image are allocated a "blob", their number and complexity will be limited by the data processing capabilities available. In practice, this limits this shape recognition method to applications requiring only crude models of objects, such as car collision warning and avoidance systems. It will remain impractical in applications requiring finer object modelling, such as, gesture recognition systems.</p><p id="p0007" num="0007">US Patent Application Publication <patcit id="pcit0005" dnum="US20030113018A1"><text>US 2003/0113018 A1</text></patcit> and International Patent Application <patcit id="pcit0006" dnum="WO03071410A2"><text>WO 03/071410 A2</text></patcit> both disclose shape recognition methods more suitable for gesture recognition.</p><p id="p0008" num="0008">In the method disclosed in <patcit id="pcit0007" dnum="US20030113018A1"><text>US 2003/0113018 A1</text></patcit>, a user is the closest object to the 3D imaging system and, to disregard the background, the points of the 3D image are selected which are closer than a predetermined depth threshold. The selected points are then grouped in five clusters, representing the torso, head, arms and hands, according to several different criteria and grouping algorithms. The torso and arms are then associated to planar shapes and the head and hands to three-dimensional volumes. While this method allows more advanced gesture recognition, the object modelling remains relatively crude, especially as the torso and arms are recognised as planar, rather than three-dimensional elements.</p><p id="p0009" num="0009">In the method disclosed in <patcit id="pcit0008" dnum="WO03071410A2"><text>WO 03/071410 A2</text></patcit>, points of a 3D image are grouped in clusters according to their perceived depth, as in <patcit id="pcit0009" dnum="US2006023558A1"><text>US 2006/023558 A1</text></patcit>, and one of those clusters, representing an object of interest, such as a hand, is selected. A gesture is then recognised by statistical analysis of the characteristics of the points of said selected cluster and comparison with pre-established patterns. Although this method is more powerful than the above-mentioned other prior art methods, it will require a substantial library of patterns for seamless recognition.<!-- EPO <DP n="4"> --></p><p id="p0010" num="0010"><patcit id="pcit0010" dnum="US6771818B1"><text>US 6 771 818 B1</text></patcit>, and <patcit id="pcit0011" dnum="US2006274947A1"><text>US 2006/274947 A1</text></patcit> disclosed other prior art methods for identifying and modelling objects of interest</p><p id="p0011" num="0011"><patcit id="pcit0012" dnum="US677181881B"><text>US 6 771 818 81</text></patcit> discloses a method for continuously identifying and locating people and objects of interest in a scene by selectively clustering distinct three-dimensional regions or blobs within the scene to generate candidate blob clusters and by <u>comparing</u> these blob clusters to at least one model to perform object recognition. An initial three-dimensional depth image of a scene of interest is generated from which the identifcation and location of objects is determined by processing a working image obtained from a background subtraction process using the initial three-dimensional depth image and a live depth image to create a depth image containing a number of distinct three-dimensional regions or blobs. A clustering process is used selectively on distinct blobs to generate the distinct candidate blob clusters. The distinct candidate blob clusters are then processed and analysed to determine the closest match or matches to a model representing the people or objects of interest which are to be identified and localised.</p><p id="p0012" num="0012"><patcit id="pcit0013" dnum="US2006274947A1"><text>US 2006/274947 A1</text></patcit> discloses a method and system for estimating a pose of a subject. The subject can be a human, an animal, a robot, or the like. A camera receives depth information associated with a subject and a pose estimation module is used to determine the pose or action of the subject from the images by separating portions of the image containing the subject into classified and unclassified portions. The portions can be segmented using K-means clustering. The classified portions can be known objects, such as a head and a torso, that are tracked across the images. The unclassified portions are swept across an x and y axis to identify local minima and maxima from which critical points are derived. Potential joint sections are identified by connecting various critical points, and joint sections having sufficient probability of corresponding to an object on the subject are selected.<!-- EPO <DP n="5"> --></p><p id="p0013" num="0013"><patcit id="pcit0014" dnum="WO0207839A1"><text>WO 02/07839 A1</text></patcit> discloses the use of stereo vision to determine position information of an object of interest in a scene for enabling interaction of the object of interest with a computer using gesture recognition. The object of interest may be controlled by a user. The use of stereo vision comprises processing the stereo image to produce a scene description by calculating disparity information, identifying best matching pairs of features in the stereo images, identifying an object of interest from feature information, and analysing the position information of an object of interest over time to determine the gesture performed for interaction with the computer. A region of interest is identified and features falling outside that region are excluded as being unlikely to belong to the object of interest. Interaction with the computer comprises mapping the position information from position coordinates associated with the object of interest to screen coordinates associated with a computer application.</p><p id="p0014" num="0014">In an article entitled "<nplcit id="ncit0001" npl-type="s"><text>Data Reduction Methods for Reverse Engineering" by K. H. Lee et al, The International Journal of Advance Manufacturing Technology, 2001, Vol. 17, pages 735 to 743</text></nplcit>, a method of reverse engineering a three-dimensional object using a scanning laser in which data reduction using median filtering is employed. A grid plane is created consisting of same-sized grids perpendicular to the scanning direction. A data reduction is determined by the size of a grid within the grid plane. Points within each grid are sorted with respect to their distances from the grid plane and a middle point is chosen to represent all points in that grid.</p><p id="p0015" num="0015">The problem addressed by the present invention is therefore that of providing a method and system for quickly modelling a volume of an object of interest, which corresponds to at least part of the body of a human user, within range of a three-dimensional (3D) imaging system with comparatively fine detail, so as to allow easier and more<!-- EPO <DP n="6"> --> accurate interaction with a data processing system, eventually through gesture recognition.</p><p id="p0016" num="0016">The modelling method of the present invention addresses this problem by the steps of: a) capturing three-dimensional image data using a three-dimensional imaging system, said image data representing a plurality of points each of which having at least a set of coordinates in three-dimensional space; b) grouping at least some of the points to form a set of clusters; and c) selecting a cluster corresponding to an object of interest located in range of said three-dimensional imaging system according to a first set of parameters comprising at least one of: position, shape and size; d) grouping at least some of the points of the selected cluster into a set of sub-clusters using a K-means algorithm to group said points of the selected cluster into a predetermined number K of sub-clusters according to a second set of parameters comprising their positions in the three-dimensional space, K initial sub-clusters being determined randomly or according to certain parameters of the cluster, each sub-cluster having a centroid in the three-dimensional space; and e) associating and fixing a volume to each of the centroids of each sub-cluster of said set of sub-clusters, the volume associated with a sub-cluster being a sphere of a predetermined radius, said three-dimensional model of said object of interest being expressed by positions of the centroids of the sub-clusters and the predetermined radius of the spheres.</p><p id="p0017" num="0017">By these steps, the modelling method of the present invention provides, without having recourse to great processing power, a comparatively accurate three-dimensional model of the object of interest formed by the volumes associated with said sub-clusters. This three-dimensional model, while comparatively accurate, can nevertheless be expressed using just the positions of the centroids of the sub-clusters and the dimensions of the associated volumes, thus facilitating the further<!-- EPO <DP n="7"> --> processing of the three-dimensional model for interaction with a data processing system, for instance through gesture recognition.</p><p id="p0018" num="0018">Advantageously, each sphere is centred on the centroid of said sub-cluster. This shape, while allowing good volume modelling, can be characterised using the radius as sole parameter, thus further reducing the size of a dataset expressing the three-dimensional model of the object of interest. Here, K may be 150.</p><p id="p0019" num="0019">Also advantageously, step b) further comprises the following steps:
<ul><li>b1) creating a first cluster comprising a first point; and</li><li>b2) executing the following operations for each other point:
<ol><li>(i) finding the cluster whose centroid is closest to said other point in the three-dimensional space; and</li><li>(ii) creating an additional cluster comprising said other point if the absolute distance in the three-dimensional space between said other point and said closest cluster centroid is higher than a predetermined threshold θ, and the number of clusters is still under a predetermined maximum q; or</li><li>(iii) adding said other point to the cluster whose centroid is closest to said other point if said absolute distance is not higher than the predetermined threshold θ, or the number of clusters has already reached said predetermined maximum q.</li></ol></li></ul></p><p id="p0020" num="0020">This method ensures a quick and efficient method of grouping the points of the image data in a set of clusters, each one corresponding to an object distinct in the three-dimensional space, including the object of interest. By grouping the points by this method according to their position in the three-dimensional space, the objects represented in the three-dimensional image can be more reliably differentiated than by a simple selection according to depth, as in the prior art. This eventually will allow the selection of the cluster<!-- EPO <DP n="8"> --> corresponding to the object of interest even in the presence of several candidates in a tracking area.</p><p id="p0021" num="0021">Particularly advantageously, step b) further comprises the steps of: b3) determining whether two of said clusters are connected, and b4) merging connected clusters. This will avoid the potential problem of grouping the points of the object of interest into several clusters, of which only one would then be selected.</p><p id="p0022" num="0022">Even more advantageously, to determine whether two of said clusters are connected, the following steps can be followed:
<ol><li>(i) calculating the standard deviation of the distribution along an axis linking the centroids of the two clusters of the projections of the points of each one of said two clusters; and</li><li>(ii) checking whether the sum of the standard deviations multiplied by a predetermined factor S is higher than the absolute distance between the centroids of the two clusters.</li></ol></p><p id="p0023" num="0023">By these steps, an efficient determination of connections between adjacent clusters can be carried out in order to eventually merge connecting clusters.</p><p id="p0024" num="0024">Advantageously, said imaging system may comprise a time-of-flight 3D camera, a stereo camera, a plurality of cameras located in different positions in the three-dimensional space, or a LIDAR, sonar or radar system. Any one of these imaging systems may provide three-dimensional image data suitable for modelling an object of interest.</p><p id="p0025" num="0025">Advantageously, said image data may comprise said at least depth and zenith and azimuth angles and a depth value of each point, the method further comprising a step of transforming zenith and azimuth angles and depth values of at least some of these points into three-dimensional Cartesian coordinates. This allows easier handling of depth images provided by a three-dimensional imaging system in this modelling method.</p><p id="p0026" num="0026">Advantageously, said human user can be standing.<!-- EPO <DP n="9"> --></p><p id="p0027" num="0027">Particularly advantageously, said method may further comprise the step of: f) calculating approximated centre of mass and main axis of the torso of said body. Since the position, orientation and movement of a torso of a user can be particularly useful for interacting with a data processing system, for instance for "virtual reality" applications, calculating its approximated centre of mass and main axis, independence of the position and motion of any spread extremities, can be particularly advantageous.</p><p id="p0028" num="0028">Step f) may comprise analysing the shape of the cluster corresponding to the human user to extract parameters of the cluster. Step f) may further comprise identifying and discounting points corresponding to arms of the human user.</p><p id="p0029" num="0029">Even more advantageously, said approximated centre of mass and main axis of the torso may be calculated by executing the following steps:
<ul><li>f1) calculating the centroid and main axis of said selected cluster;</li><li>f2) calculating the distribution curve of the distances of the points of the selected cluster with respect to said main axis of the selected cluster;</li><li>f3) calculating an inflection point in said distribution curve;</li><li>f4) selecting the points with distances with respect to said main axis of the selected cluster inferior to D·s, wherein s is the distance of said inflection point to said main axis of the selected cluster and D is a factor of at most 1.25; and</li><li>f5) calculating said centre of mass and main axis of the torso as the centroid and main axis of the selected points.</li></ul></p><p id="p0030" num="0030">In one embodiment, the factor is at most 1.</p><p id="p0031" num="0031">As in a cluster corresponding to a human body the points corresponding to any spread extremity will usually be clearly detached<!-- EPO <DP n="10"> --> from the area of biggest density of points, which will correspond to the torso, this steps will allow to discount them in the calculation of the approximated centre of mass and main axis of the torso.</p><p id="p0032" num="0032">Particularly advantageously, signals may be transmitted to a data processing system according to the position of the centre of mass of said torso and/or its main axis and/or the orientation of said main axis of said torso. As stated above, this will allow a particularly natural interaction of the user with, for instance, a "virtual reality" application.</p><p id="p0033" num="0033">Particularly advantageously, said method may further comprise the step of measuring the height of the human user.</p><p id="p0034" num="0034">Even more advantageously, a particularly accurate measure of the height of the human user may be obtained by calculating the heights of the points among those of said selected cluster that are closer than a predetermined distance to the main axis of the torso, filtering said heights, preferably by median filtering, and selecting the maximum value of said heights after filtering. A height measurement obtained by these steps will usually not be influenced by the position of any stretched arm, so that it can reliably be used for purposes such as that of determining the position of the head of the human user.</p><p id="p0035" num="0035">Even more advantageously, said measure of the height of the human user may be only considered as valid if a set of conditions is met, such as said main axis of the torso being substantially vertical.</p><p id="p0036" num="0036">If the centre of mass of the torso etc. is used, the value of K may be 25.</p><p id="p0037" num="0037">Advantageously, the volumes associated with said set of sub-clusters may be represented in a virtual environment generated by a data processing system. This would allow a comparatively realistic representation of the object of interest in a chosen virtual environment with a relatively small processing effort. The volumes could, for example, serve as an avatar of a user, if said user's body is the object of interest.<!-- EPO <DP n="11"> --></p><p id="p0038" num="0038">Even more advantageously, there may be a collision and/or proximity check between the representation of the volumes of said set of sub-clusters and a set of elements of said virtual environment, so as to interact with said set of elements of the virtual environment. Thus, a human user could for instance push, grip, activate or pull an element of the virtual environment by moving so that said representation touches said element.</p><p id="p0039" num="0039">Advantageously, a set of links between sub-clusters may be established using criteria such as absolute distance between the centroids of the sub-clusters, the presence of points between sub-clusters, etc. In this way, the underlying structure of the object of interest may be recognised, thus facilitating eventual interactions and possibly allowing the creation of an accurate three-dimensional model of the object of interest with a further reduced dataset.</p><p id="p0040" num="0040">Even more advantageously, a set of extremities of said object of interest may be identified according to said links. Different signals could thus be assigned to movements or positions of extremities, or even to the relative movements or positions between extremities, thus increasing the versatility of an input interface using this modelling method.</p><p id="p0041" num="0041">Even more advantageously, at least one of said extremities is labelled according to a predetermined pattern, for example that of a human user. Different signals could thus be assigned to the movements or positions of different extremities, thus further increasing the versatility of an input interface using this modelling method.</p><p id="p0042" num="0042">Even more advantageously, signals can be transmitted to a data processing system according to an absolute and/or relative position and/or movement of at least one of said extremities. This would provide a particularly versatile interaction method.<!-- EPO <DP n="12"> --></p><p id="p0043" num="0043">Said interaction may be carried out through gesture recognition and/or through proximity and/or collision checks of said volumes with elements of the virtual environment.</p><p id="p0044" num="0044">The present invention also relates to a system for providing a three-dimensional model of an object of interest, the system comprising a three-dimensional imaging system for capturing three-dimensional image data representing a plurality of points, each point having at least a set of coordinates in a three-dimensional space, and at least some of said points corresponding to an object of interest located in range of said three-dimensional imaging system, and a data processing system connected to said three-dimensional imaging system and programmed for carrying out, in cooperation with said three-dimensional imaging system, the modelling method of the invention.</p><p id="p0045" num="0045">Several preferred embodiments of the invention will be described illustratively, but not restrictively with reference to the accompanying figures, in which:
<ul><li><figref idrefs="f0001">Fig. 1</figref> shows a room with a user standing in front of a 3D imaging system for interaction with a data processing system using a modelling system and method according to an embodiment of the present invention;</li><li><figref idrefs="f0002">Fig. 2</figref> shows three-dimensional image data of the same<br/>
room, in the form of points distributed in the three-dimensional space, as captured by the 3D imaging system;</li><li><figref idrefs="f0003">Fig. 3</figref> shows how points are grouped into clusters according to their respective positions;</li><li><figref idrefs="f0004">Fig. 4</figref> shows how neighbouring clusters are checked for connections;</li><li><figref idrefs="f0005">Fig. 5</figref> shows the same three-dimensional image data of <figref idrefs="f0002">Fig. 2</figref>, wherein the points have been grouped in clusters, one of said clusters corresponding to the user;<!-- EPO <DP n="13"> --></li><li><figref idrefs="f0006">Fig. 6a</figref> shows the centroids of 150 sub-clusters of the cluster corresponding to the user;</li><li><figref idrefs="f0007">Fig. 6b</figref> shows 150 spheres, each centred in one of the centroids of <figref idrefs="f0006">Fig. 6a</figref>;</li><li><figref idrefs="f0008">Fig. 6c</figref> shows the 150 spheres of <figref idrefs="f0007">Fig. 6b</figref> representing the user in a virtual environment;</li><li><figref idrefs="f0009">Fig. 7a</figref> shows the centroids of 25 sub-clusters of the cluster corresponding to the user;</li><li><figref idrefs="f0010">Fig. 7b</figref> shows a network linking the centroids of <figref idrefs="f0009">Fig. 7a</figref>;</li><li><figref idrefs="f0011">Fig. 7c</figref> shows a virtual body structure based on the network of <figref idrefs="f0010">Fig. 7b</figref>;</li><li><figref idrefs="f0012">Fig. 7d</figref> shows a user avatar based on the virtual body structure of <figref idrefs="f0011">Fig. 7c</figref>;</li><li><figref idrefs="f0013">Fig. 8a</figref> shows a view of the user with the right arm extended, and the centroid and main axis of the cluster representing the user, as well as the centre of mass and main axis of the torso of the user; and</li><li><figref idrefs="f0013">Fig. 8b</figref> shows a distribution curve of the points in <figref idrefs="f0013">Fig. 8a</figref>;</li><li><figref idrefs="f0014">Fig. 9</figref> shows the user in an initialisation position, facing a screen of a data processing device with extended arms.</li></ul></p><p id="p0046" num="0046">One of the possible uses of an embodiment of the modelling method and system is illustrated in <figref idrefs="f0001">Fig. 1</figref>. In this application, this system and method are used for the recognition of the gestures of an object of interest, in this case a human user 1, in order to interact with a data processing device 2 generating a virtual environment displayed to the human user 1.</p><p id="p0047" num="0047">The modelling system comprises a three-dimensional (3D) imaging system, in this particular embodiment a time-of-flight (TOF) 3D camera 3. This TOF 3D camera 3 is connected to the data processing device 2 with which the human user 1 is to interact. In this embodiment, this data processing device 2 is itself programmed to carry out, in<!-- EPO <DP n="14"> --> cooperation with the TOF 3D camera 3, the modelling method of the invention. Alternatively, a separate data processing device or system programmed to carry out said method could be connected between the TOF 3D camera and the data processing system 2 so as to enable the human user to interact with said data processing system 2.</p><p id="p0048" num="0048">The TOF 3D camera 3 captures 3D image data of the room 4 in which the human user 1 stands, comprising a 2D image of the room with a plurality of pixels and a depth value for each pixel corresponding the distance to the TOF 3D camera 3 of the point imaged by that pixel. Since the X and Y positions of the pixels in the 2D image themselves correspond to zenith and azimuth angles of the points they represent with respect to the TOF 3D camera 3, these 3D image data can be illustrated as in <figref idrefs="f0002">Fig. 2</figref> by a three-dimensional cloud of points 5 corresponding to visible points of the objects in range of the TOF 3D camera 3. For ease of processing, the depth and the zenith and azimuth angles of each point 5 with respect to the TOF 3D camera 3 can be converted into Cartesian coordinates.</p><p id="p0049" num="0049">In the next step of the modelling method of the invention, these points 5 are grouped into clusters 6. A cluster 6 will contain neighbouring points 5, as illustrated in <figref idrefs="f0003">Fig. 3</figref>. This clustering is carried out using a BSAS algorithm, such as was described in <nplcit id="ncit0002" npl-type="b"><text>Chapter 12 of "Pattern Recognition" by Sergios Theodoridis, Konstantinos Koutroumbas and Ricky Smith, published by Academic Press in 1998</text></nplcit>, which has the advantage of speed, as it will perform this clustering in a single pass, not needing a plurality of iterations to provide adequate results.</p><p id="p0050" num="0050">To carry out this clustering, a first cluster 6 comprising a first point 5 is created, and then the following operations are carried out for each other point 5:
<ul><li>i) finding the cluster 6 whose centroid 7 is closest to said other point 5 in the three-dimensional space; and<!-- EPO <DP n="15"> --></li><li>ii) creating an additional cluster 6 comprising said other point 5 if the absolute distance in the three-dimensional space between said other point 5 and said closest cluster centroid 7 is higher than a predetermined threshold θ, and the number of clusters 6 is still under a predetermined maximum q; or</li><li>iii) adding said other point 5 to the cluster 6 whose centroid 7 is closest to said other point 5 if said absolute distance is not higher than the predetermined threshold θ, or the number of clusters has already reached said predetermined maximum q.</li></ul></p><p id="p0051" num="0051">This clustering step will result in a plurality of clusters 6 comprising the points 5. However, the use of this algorithm may result in several of the clusters 6 actually being connected. To properly group the points 5, such connected clusters 6 will be detected and merged as depicted in <figref idrefs="f0004">Fig. 4</figref>.</p><p id="p0052" num="0052">To determine whether two clusters 6 are connected, the points 5 of these two clusters 6 are first projected onto an axis 8 linking the centroids 7 of the two clusters 6. Then the standard deviation of the distribution of the resulting projections along the axis 8 is calculated for each of the clusters 6. The two clusters 6 will be determined to be connected if the sum of these standard deviations, multiplied by a predetermined factor S, which in this particular embodiment is 2, is found to be higher than the absolute distance between the centroids 7 of the two clusters 6. In this case the two clusters 6 will be merged to form a single one.</p><p id="p0053" num="0053">The result of this clustering and merging will be a set of clusters 6 roughly representing the various objects in range of the TOF 3D camera 3, as illustrated in <figref idrefs="f0005">Fig. 5</figref>. Of these clusters 6, one will represent the human user 1. This cluster 6 representing the human user 1 can be identified by a variety of means. For instance, a cluster 6 will be recognised as representing the human user 1 if it is in a determined tracking area where the human user 1 should stand to interact with the<!-- EPO <DP n="16"> --> data processing system 2 and if it comprises a minimum number of points 5. If several clusters 6 fulfil these criteria, the cluster 6 closest to the TOF 3D camera 3 can be chosen as representing the human user 1. Another criterion for identifying the cluster 6 representing the human user 1 can be conformity of the distribution of the points 5 of that cluster to a predetermined pattern consistent with a human user. For instance, if in an initialisation sequence the human user 1 stands with extended arms as illustrated in <figref idrefs="f0014">Fig. 9</figref>, the points 5 of the cluster 6 representing the human user 1 will be distributed according to a characteristic and easily recognised pattern. When the TOF 3D camera 3 is a moving picture camera capturing a series of 3D image data frames at successive moments, another possible criterion for identifying the cluster 6 corresponding to the human user 1 can be proximity with the cluster 6 which was identified as representing the human user 1 in previous frames. In this way, for instance, the modelling system could continue to track the human user 1 first recognised during the initialisation sequence as described above even after he takes a posture less easily recognised as human or even after other people enter the tracking area.</p><p id="p0054" num="0054">Hence, it would be possible to interact with the data processing device 2 through, for example:
<ul><li>the presence or absence of a human user 1 within range of the TOF 3D camera 3;</li><li>the number of clusters 6 recognisable as corresponding to human users 1; and/or</li><li>the general disposition of the room 4.</li></ul></p><p id="p0055" num="0055">The human user 1 would also be able to interact with the data processing device 2 through characteristics of the cluster 6 representing the human user 1, such as:
<ul><li>the symmetry of at least part of the cluster 6;<!-- EPO <DP n="17"> --></li><li>the distribution of at least part of the cluster 6 in space;</li><li>the dispersion of the points 5 in at least part of the cluster 6;</li><li>the centroid 7 of at least part of the cluster 6; and/or</li><li>the main axes of at least part of the cluster 6.</li></ul></p><p id="p0056" num="0056">Once the cluster 6 representing the human user 1 is identified, it is subdivided into a set of K sub-clusters The points 5 of the cluster 6 are grouped into these K sub-clusters using a K-means algorithm.</p><p id="p0057" num="0057">The K-means algorithm starts by partitioning the points 5 into K initial sub-clusters. It then calculates the centroid 11 of each initial sub-cluster. It constructs a new partition in K sub-clusters by associating each point 5 with centroid 11 which is closest in the three-dimensional space, although additional parameters, such as colour, may be used. Then the centroids 11 are recalculated for the new sub-clusters This process can be iterated until the points 5 no longer switch sub-clusters, or until the positions of the centroids 11 stabilise. In practice, good results can be attained with a single iteration.</p><p id="p0058" num="0058">In a first embodiment, the K initial sub-clusters are determined randomly or according to certain parameters of the cluster 6, such as height of the cluster 6 or distribution of the points 5 in the cluster 6, and K is a comparatively high number, such as 150. Using this K-means algorithm then results in a set of 150 sub-clusters, each with a centroid 11, as represented in <figref idrefs="f0006">Fig. 6a</figref>. Associating a sphere 12 of a predetermined radius to each one of the 150 sub-clusters then results in a model 13 of the human user 1, as represented in <figref idrefs="f0007">Fig. 6b</figref>. This model 13 represents the volume occupied by the human user 1 with good accuracy.<!-- EPO <DP n="18"> --></p><p id="p0059" num="0059"><figref idrefs="f0008">Fig. 6c</figref> illustrates the model 13 represented in a virtual environment generated by the data processing device 2. The human user 1 can then interact with elements 14 of this virtual environment through simple collision and/or proximity checks between the representation of the spheres 12 in the virtual environment and the elements 14 of the virtual environment. Hence, the human user 1 would also be able to interact with the data processing device 2 through, for example:
<ul><li>the collision or proximity in one or several dimensions of the representation of at least one sphere 12 with at least one element 14 of a virtual environment generated by the data processing device 2, wherein said element 14 can be punctual, mono-, bi- or three-dimensional;</li><li>the collision or proximity in one or several dimensions of the representation of at least one sphere 12 with a representation of at least one real object of the real environment of the human user 1 in the virtual environment generated by the data processing device 2;</li><li>the position and/or movement of one or several of the centroids 11 of the sub-clusters ; and/or</li><li>the position, movement and/or shape of the volume formed by the spheres 12 associated with at least one of the sub-clusters, for example those sub-clusters whose centroids 11 show substantial movement.</li></ul></p><p id="p0060" num="0060">In a second embodiment, the shape of the cluster 6 corresponding to the human user 1 is additionally analysed so as to extract characteristics of the body of the human user 1, such as the centre of mass, the general orientation, the position of the head, the<!-- EPO <DP n="19"> --> position and orientation of the shoulders and the height. While several of these characteristics, such as centre of mass or general orientation, could be calculated from the points 5 of the whole cluster 6, the results would be exaggeratedly influenced by the position of the arms 15 of the human user 1, as illustrated in <figref idrefs="f0013">Fig. 8a</figref>, wherein the centroid 7 and the main axis 16 of the cluster 6 representing the human user 1 with the right arm 15 extended is represented superposed with the body of the human user 1. For this reason, in this particular embodiment, the points 5 corresponding to the arms 15 are identified and discounted first, so as to be enable the calculation of the centre of mass 17 and main axis 18 of the torso 19 of the human user 1, wherein as torso 19 we understand the whole body of the user 1 with exception of the arms 15. For this purpose, the following steps are executed:
<ol><li>a) calculating the centroid 7 and main axis 16 of said selected cluster 6;</li><li>b) calculating the distribution curve 20, as represented in <figref idrefs="f0013">Fig. 8b</figref> of the distances of the points 5 of the selected cluster 6 with respect to said main axis 16;</li><li>c) calculating an inflection point 21 in said distribution curve 20;</li><li>d) selecting the points 5' with distances with respect to said main axis 16 of the selected cluster 6 inferior to D·s, wherein s is the distance of said inflection point 21 to said main axis 16 of the selected cluster 6 and D is a factor of at most 1.5, preferably at most 1.25; and</li><li>e) calculating said centre of mass 17 and main axis 18 of the torso 1 as the centroid and main axis of the set of selected points 5.</li></ol></p><p id="p0061" num="0061">This process can be carried out iteratively, but usually a single pass can achieve already good results.</p><p id="p0062" num="0062">The position of the head 22 and the shoulders 23 in the cluster 6 can be identified by the characteristic angles 24 formed by the<!-- EPO <DP n="20"> --> neck 25 and the shoulders 26. From the positions of the two shoulders 26, their orientation can also be inferred. In the initialisation sequence illustrated in <figref idrefs="f0014">Fig. 9</figref>, the human user 1 may be asked to face an output display screen, so that the orientation of the shoulders 26 can be considered to be parallel to that of the output display screen, which will provide a reference value for later use. This initialisation sequence thus can provide at least a reference for the orientation of the output display screen, as well as a reference for the initial position of the human user 1 with respect to the TOF 3D camera 3. Some later interactions of the human user 1 with the data processing device 2 may relate to the relative position of at least part of the human user 1 with respect to said initial position.</p><p id="p0063" num="0063">The height of the human user 1 is also calculated using only the selected points 5. For better accuracy, a mean filtering of the selected points 5 is carried out, and the height of the highest remaining selected point 5 is identified as the height of the human user 1. This height measurement will only be considered valid if a set of conditions is met, such as said main axis 18 of the torso 19 being substantially vertical or said highest remaining selected point 5 being in or near the region of the cluster 26 identified as representing the head 22.</p><p id="p0064" num="0064">If the TOF 3D camera 3 is a moving picture camera, the height measurements for several frames are sent to a Gaussian mixture model, so as to take into account possible noise and temporary low positions of the human user 1. The Gaussian with the maximum average having a sufficient weight will provide a robust value of the height of the human user 1.</p><p id="p0065" num="0065">In this second embodiment, the parameters obtained from this analysis of the shape of the cluster 6, such as height, centre of mass 17 and main axis 18 of the torso 19, position of the head 22 and position and orientation of the shoulders 26 can be used in the partition of the cluster 6 into K sub-clusters using the K-means algorithm. For instance,<!-- EPO <DP n="21"> --> one of the K initial sub-clusters may comprise at least some of the points 5 identified as corresponding to the head 22. The cluster 6 can thus be partitioned into a lower K number of sub-clusters, such as 25, that however follow a pattern corresponding to the structure of a human user. The centroids 11 of 25 such sub-clusters are represented in <figref idrefs="f0009">Fig. 7a</figref>.</p><p id="p0066" num="0066">It is then possible to determine which sub-clusters are connected, using criteria such as absolute distance between the centroids 11 of the sub-clusters, the presence of points 5 between sub-clusters, etc. The purpose of determining these connections between sub-clusters is that of generating a network 27 of links 28 between the centroids 11 of sub-clusters, as represented in <figref idrefs="f0010">Fig. 7b</figref>. From such a network 27 it can then be inferred which sub-clusters form extremities 29, as they should have fewer links 28 to other sub-clusters</p><p id="p0067" num="0067">Hence, the human user 1 will be able to interact with the data processing device 2 through, for example:
<ul><li>the position and/or movement of the centre of mass 17;</li><li>the position, orientation and/or movement of the main axis 18;</li><li>the position, orientation and/or movement of the shoulders 26;</li><li>the position and/or movement of the head 22;</li><li>the position, orientation, movement and/or shape of one or several extremities 29.</li></ul></p><p id="p0068" num="0068">Absolute as well as relative positions and movements can be used for these interactions. For example, the human user 1 may interact with the data processing device 2 through the relative positions and movements of extremities 29 with respect to each other, to the main axis 18, shoulders 26, head 22 and/or at least one element 14 of a virtual<!-- EPO <DP n="22"> --> environment generated by the data processing device 2 can be the source of interactions.</p><p id="p0069" num="0069">As illustrated in <figref idrefs="f0011">Fig. 7c</figref>, the network 27 can be used to generate a structure 28 following a predetermined pattern, such as that of a human user. Thus, extremities 2 are not just identified as extremities in general, but also labelled as being, for example, the right arm 30 or the left leg 31 in particular. This further increases the possibilities of interaction. It also allows the generation of a voluminous avatar 32, as shown in <figref idrefs="f0012">Fig. 7d</figref>, to represent the human user 1 in a virtual environment.</p><p id="p0070" num="0070">All the above-mentioned interactions can take place separately or in a combined manner. It is, for example, possible to allow a human user 1 to interact with the data processing device 2 both through the volume occupied by 150 spheres 12 and by the relative movement of his extremities 2.</p><p id="p0071" num="0071">Likewise, the modelling system and method of the present invention can be used alone or in combination with other user interfaces suitable for communication with a data processing system 2, such as: switch, keyboard, mouse, trackball, tablet, touchpad, touchscreen, 6-DOF peripheral, joystick, gamepad, motion tracking system, eye tracking device, dataglove, 3D mouse, voice recognition, bioelectric sensor, neuronal interface, treadmill, static bicycle, rowing machine, or any other sensor or interface suitable for providing input to a data processing system 2.</p><p id="p0072" num="0072">Among the commands and inputs that may be provided to a data processing device 2 through the modelling system and method of the present invention, there are:
<ul><li>2D and/or 3D navigation, such as point of view rotation, translation, positioning and/or orientation, as well as other vision parameters, such as perspective, range, colour, exposition, etc.<!-- EPO <DP n="23"> --></li><li>Interface element navigation, comprising i.a.<br/>
navigations within menus, lists, parameter choices, and/or input fields.</li><li>Manipulation, comprising i.a. avatar control, control of application object parameters, such as position, orientation, translation; rotation, appearance, shape and/or function and/or control of system parameters.</li><li>Triggering, such as validation of i.a. action commands, parameter change commands and/or change of state commands, action commands and/or commands to change the state of an application object, a control parameter and/or other.</li><li>Selection of i.a. interface elements, application objects, real environment objects, etc.</li><li>Force input, for instance in physical simulations.</li><li>Output parameter adjustment, for instance for sound volume, appearance of application objects, presentation of application objects.</li></ul></p><p id="p0073" num="0073">The data processing system 2 can in turn be connected to any of a variety of output devices, such as, for example:
<ul><li>Computer output devices, such as a 2D or 3D display devices, loudspeakers, headphones, printers, haptic output devices, ventilators and/or background lighting.</li><li>Virtual reality output devices, such as virtual reality goggles, portable display devices, multiple display devices such as Cave®, large display devices such as Reality Center ®, stereoscopic screens, force return devices, 3D display devices, smoke machines, and/or sprinklers.<!-- EPO <DP n="24"> --></li><li>Home automation devices, such as window shutter control devices, heating control devices and/or lighting control devices.</li><li>Home entertainment devices, such as TVs and/or music systems.</li><li>Portable devices, such as portable music and/or video players, positioning systems, personal digital assistants, portable computers and/or mobile telephones.</li><li>Other devices connectable to a data processing system 2, such as valves, treadmills, etc.</li></ul></p><p id="p0074" num="0074">Although the present invention has been described with reference to specific exemplary embodiments, it will be evident that various modifications and changes may be made to these embodiments without departing from the broader scope of the invention as set forth in the claims. Accordingly, the description and drawings are to be regarded in an illustrative sense rather than a restrictive sense.</p></description><claims mxw-id="PCLM56980379" lang="DE" load-source="patent-office"><!-- EPO <DP n="31"> --><claim id="c-de-01-0001" num="0001"><claim-text>Verfahren zum Bereitstellen eines dreidimensionalen Modells (13) eines interessierenden Objekts in Reichweite eines dreidimensionalen Bildgebungssystems (3), um eine Interaktion mit einem Datenverarbeitungssystem (2) zu ermöglichen, wobei das interessierende Objekt ein menschlicher Benutzer (1) ist, wobei das Verfahren folgende Schritte umfasst:
<claim-text>a) Aufnehmen von dreidimensionalen Bilddaten unter Verwendung eines dreidimensionalen Bildgebungssystems (3), wobei die Bilddaten eine Vielzahl von Punkten (5) darstellen, von denen jeder mindestens einen Satz Koordinaten in einem dreidimensionalen Raum aufweist;</claim-text>
<claim-text>b) Gruppieren mindestens einiger der Punkte (5), um einen Satz Cluster (6) zu bilden; und</claim-text>
<claim-text>c) Auswählen eines Clusters (6), das einem interessierenden Objekt (1) entspricht, das sich in Reichweite des dreidimensionalen Bildgebungssystems (3) befindet, gemäß einem ersten Satz Parameter, der mindestens eines von Position, Form und Größe umfasst;<br/>
und <b>dadurch gekennzeichnet, dass</b> das Verfahren ferner folgende Schritte umfasst:</claim-text>
<claim-text>d) Gruppieren mindestens einiger der Punkte (5) des ausgewählten Clusters (6) in einen Satz Subcluster unter Verwendung eines K-Means-Algorithmus, um die Punkte (5) des ausgewählten Clusters (6) in eine vorherbestimmte Anzahl K von Subclustern zu gruppieren, gemäß einem zweiten Satz Parameter, der ihre Positionen im dreidimensionalen Raum umfasst, wobei K anfängliche Subcluster willkürlich oder gemäß gewissen Parametern<!-- EPO <DP n="32"> --> des Clusters (6) bestimmt werden, wobei jedes Subcluster ein Zentroid (11) im dreidimensionalen Raum aufweist; und</claim-text>
<claim-text>e) Verknüpfen mit und Fixieren eines Volumens (12) an jedem der Zentroide (11) jedes Subclusters des Satzes Subcluster, wobei das Volumen (12), das mit einem Subcluster verknüpft ist, eine Kugel mit einem vorherbestimmten Radius ist, wobei das dreidimensionale Modell des interessierenden Objekts durch Positionen der Zentroide der Subcluster und durch den vorherbestimmten Radius der Kugeln ausgedrückt ist.</claim-text></claim-text></claim><claim id="c-de-01-0002" num="0002"><claim-text>Verfahren nach Anspruch 1, wobei jede Kugel um das Zentroid (11) des Subclusters zentriert ist.</claim-text></claim><claim id="c-de-01-0003" num="0003"><claim-text>Verfahren nach Anspruch 1 oder 2, wobei K gleich 150 ist.</claim-text></claim><claim id="c-de-01-0004" num="0004"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei Schritt b) die folgenden Schritte umfasst:
<claim-text>b1) Erstellen eines ersten Clusters (6), das einen ersten Punkt (5) umfasst; und</claim-text>
<claim-text>b2) Ausführen der folgenden Vorgänge für jeden anderen Punkt (5):
<claim-text>(i) Finden des Clusters (6), dessen Zentroid (7) am nächsten an dem anderen Punkt im dreidimensionalen Raum liegt; und</claim-text>
<claim-text>(ii) Erstellen eines zusätzlichen Clusters (6), das den anderen Punkt umfasst, wenn der absolute Abstand im dreidimensionalen Raum zwischen dem anderen Punkt (5) und dem nächstgelegenen Zentroid (7) größer als ein vorherbestimmter Schwellenwert θ ist,<!-- EPO <DP n="33"> --> und die Anzahl der Cluster (6) weiter unter einem vorherbestimmten Maximum q bleibt; oder</claim-text>
<claim-text>(iii) Hinzufügen des anderen Punktes (5) zu dem Cluster (6), dessen Zentroid (7) am nächsten an dem anderen Punkt (5) liegt, wenn der absolute Abstand nicht größer als der vorherbestimmte Schwellenwert θ ist oder die Anzahl der Cluster (6) bereits das vorherbestimmte Maximum q erreicht hat.</claim-text></claim-text></claim-text></claim><claim id="c-de-01-0005" num="0005"><claim-text>Verfahren nach Anspruch 4, wobei Schritt b) ferner folgende Schritte umfasst:
<claim-text>b3) Bestimmen, ob zwei der Cluster (6) verbunden sind; und</claim-text>
<claim-text>b4) Zusammenlegen der verbundenen Cluster (6).</claim-text></claim-text></claim><claim id="c-de-01-0006" num="0006"><claim-text>Verfahren nach Anspruch 5, wobei Schritt b3) folgende Schritte umfasst:
<claim-text>(i) Berechnen der Standardabweichung der Verteilung der Projektionen der Punkte (5) jedes der beiden Cluster (6) entlang einer Achse (8), welche die Zentroide (7) der beiden Cluster (6) verbindet; und</claim-text>
<claim-text>(ii) Überprüfen, ob die Summe der Standardabweichungen, die mit einem vorherbestimmten Faktor S multipliziert werden, größer als der absolute Abstand zwischen den Zentroiden (7) der beiden Cluster (6) ist.</claim-text></claim-text></claim><claim id="c-de-01-0007" num="0007"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei das Bildgebungssystem (3) eines umfasst von: einer dreidimensionalen Laufzeitkamera, einer Stereokamera, einer Vielzahl von Kameras, die sich in verschiedenen Positionen im dreidimensionalen Raum befinden, einem LIDAR-System, einem Sonarsystem oder einem Radarsystem.<!-- EPO <DP n="34"> --></claim-text></claim><claim id="c-de-01-0008" num="0008"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei die Bilddaten mindestens Zenit- und Azimut-Winkel und einen Tiefenwert jedes Punktes (5) umfassen, wobei das Verfahren ferner einen Schritt des Umwandelns der Zenit- und Azimut-Winkel und der Tiefenwerte von mindestens einigen dieser Punkte (5) in dreidimensionale kartesische Koordinaten umfasst.</claim-text></claim><claim id="c-de-01-0009" num="0009"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei sich der menschliche Benutzer in einer stehenden Position befindet.</claim-text></claim><claim id="c-de-01-0010" num="0010"><claim-text>Verfahren nach Anspruch 9, ferner umfassend folgenden Schritt:
<claim-text>f) Berechnen des genäherten Massenschwerpunktes (17) und der Hauptachse (18) des Oberkörpers (19) des menschlichen Benutzers.</claim-text></claim-text></claim><claim id="c-de-01-0011" num="0011"><claim-text>Verfahren nach Anspruch 10, wobei Schritt f) das Analysieren der Form des Clusters (6) umfasst, das dem menschlichen Benutzer (1) entspricht, um die Parameter des Clusters (6) zu entnehmen.</claim-text></claim><claim id="c-de-01-0012" num="0012"><claim-text>Verfahren nach Anspruch 11, wobei Schritt f) ferner das Identifizieren und Abziehen von Punkten umfasst, die den Armen (15) des menschlichen Benutzers (1) entsprechen.</claim-text></claim><claim id="c-de-01-0013" num="0013"><claim-text>Verfahren nach einem der Ansprüche 10 bis 12, wobei Schritt f) die folgenden Schritte umfasst:
<claim-text>f1) Berechnen des Zentroids (7) und der Hauptachse (16) des<br/>
ausgewählten Clusters (6);</claim-text>
<claim-text>f2) Berechnen der Verteilungskurve (20) der Abstände der<br/>
Punkte (5) des ausgewählten Clusters (6) im Verhältnis zu der Hauptachse (16) des ausgewählten Clusters (6);</claim-text>
<claim-text>f3) Berechnen eines Wendepunktes (21) in der<br/>
Verteilungskurve (20);<!-- EPO <DP n="35"> --></claim-text>
<claim-text>f4) Auswählen der Punkte (5) mit Abständen im Verhältnis zur<br/>
Hauptachse (16) des ausgewählten Clusters (6), die kleiner als D·s sind, wobei s der Abstand vom Wendepunkt (21) zur Hauptachse (16) des ausgewählten Clusters (6) und D ein Faktor von höchstens 1,25 ist; und</claim-text>
<claim-text>f5) Berechnen des Massenschwerpunktes (17) und der<br/>
Hauptachse (18) des Oberkörpers (19) als Zentroid und Hauptachse der ausgewählten Punkte (5).</claim-text></claim-text></claim><claim id="c-de-01-0014" num="0014"><claim-text>Verfahren nach Anspruch 13, wobei der Faktor höchstens gleich 1 ist.</claim-text></claim><claim id="c-de-01-0015" num="0015"><claim-text>Verfahren nach einem der Ansprüche 10 bis 14, wobei die Signale an ein Datenverarbeitungssystem (2) gemäß der Position des Massenschwerpunktes (17) des Oberkörpers (19) und/oder seiner Hauptachse (18) und/oder der Orientierung der Hauptachse (18) des Oberkörpers (1) übertragen werden.</claim-text></claim><claim id="c-de-01-0016" num="0016"><claim-text>Verfahren nach einem der Ansprüche 10 bis 15, ferner umfassend den Schritt des Messens der Größe des menschlichen Benutzers.</claim-text></claim><claim id="c-de-01-0017" num="0017"><claim-text>Verfahren nach Anspruch 16, wobei die Größe des menschlichen Benutzers gemessen wird, indem die Höhen der Punkte (5) aus denen des ausgewählten Clusters (6) berechnet werden, die näher als in einem vorherbestimmten Abstand von der Hauptachse (18) des Oberkörpers (19) liegen, indem die Höhen gefiltert werden, bevorzugt durch Medianfiltern, und indem der Höchstwert der Höhen nach dem Filtern ausgewählt wird.</claim-text></claim><claim id="c-de-01-0018" num="0018"><claim-text>Verfahren nach Anspruch 17, wobei die Messung der Größe des menschlichen Benutzers nur als gültig angesehen wird, wenn ein Satz Bedingungen erfüllt ist, wie etwa dass die Hauptachse (18) des Oberkörpers (19) im Wesentlichen senkrecht ist.<!-- EPO <DP n="36"> --></claim-text></claim><claim id="c-de-01-0019" num="0019"><claim-text>Verfahren nach einem der Ansprüche 10 bis 18, wobei K gleich 25 ist.</claim-text></claim><claim id="c-de-01-0020" num="0020"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei die Volumen (12), die mit dem Satz Subcluster verknüpft sind, in einer virtuellen Umgebung dargestellt werden, die durch das Datenverarbeitungssystem (2) erzeugt wird.</claim-text></claim><claim id="c-de-01-0021" num="0021"><claim-text>Verfahren nach Anspruch 20, wobei es eine Kollisions- und/oder Näherungskontrolle zwischen der Darstellung der Volumen (12) des Satzes Subcluster und einem Satz Elemente (14) der virtuellen Umgebung gibt, um mit dem Satz Elemente (14) der virtuellen Umgebung zu interagieren.</claim-text></claim><claim id="c-de-01-0022" num="0022"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei ein Satz Verbindungen (28) zwischen Subclustern unter Verwendung von Kriterien, wie etwa dem absoluten Abstand zwischen den Zentroiden (11) der Subcluster, dem Vorliegen von Punkten (5) zwischen den Subclustern, usw. ermittelt wird.</claim-text></claim><claim id="c-de-01-0023" num="0023"><claim-text>Verfahren nach Anspruch 22, wobei ein Satz Extremitäten (29) des interessierenden Objekts (1) gemäß den Verbindungen (28) identifiziert wird.</claim-text></claim><claim id="c-de-01-0024" num="0024"><claim-text>Verfahren nach Anspruch 23, wobei mindestens eine der Extremitäten (29) gemäß einem vorherbestimmten Muster, beispielsweise dem eines menschlichen Benutzers, bezeichnet ist.</claim-text></claim><claim id="c-de-01-0025" num="0025"><claim-text>Verfahren nach Anspruch 23 oder 24, wobei Signale an ein Datenverarbeitungssystem (2) gemäß einer absoluten und/oder relativen Position und/oder einer Bewegung mindestens einer der Extremitäten (29) übertragen werden.</claim-text></claim><claim id="c-de-01-0026" num="0026"><claim-text>Verfahren nach einem der vorhergehenden Ansprüche, wobei die Interaktion durch Gestenerkennung und/oder durch Näherungs-und/oder<!-- EPO <DP n="37"> --> Kollisionskontrollen der Volumen (12) mit Elementen einer virtuellen Umgebung ausgeführt wird.</claim-text></claim><claim id="c-de-01-0027" num="0027"><claim-text>System zum Bereitstellen eines dreidimensionalen Modells (13) eines interessierenden Objekts, wobei das System ein dreidimensionales Bildgebungssystem (3) umfasst, um dreidimensionale Bilddaten aufzunehmen, die eine Vielzahl von Punkten (5) darstellen, wobei jeder Punkt (5) mindestens einen Satz Koordinaten in einem dreidimensionalen Raum aufweist, und mindestens einige der Punkte (5) einem interessierenden Objekt (1) entsprechen, das sich in Reichweite des dreidimensionalen Bildgebungssystems (3) befindet, und ein Datenverarbeitungssystem (2), das an das dreidimensionale Bildgebungssystem (3) angeschlossen ist und programmiert ist, um in Zusammenarbeit mit dem dreidimensionalen Bildgebungssystem ein Verfahren nach einem der vorhergehenden Ansprüche auszuführen.</claim-text></claim></claims><claims mxw-id="PCLM56980380" lang="EN" load-source="patent-office"><!-- EPO <DP n="25"> --><claim id="c-en-01-0001" num="0001"><claim-text>A method of providing a three-dimensional model (13) of an object of interest within range of a three-dimensional imaging system (3) for enabling interaction with a data processing system (2), the object of interest corresponding to a human user (1), the method comprising the steps of:-
<claim-text>a) capturing three-dimensional image data using a three-dimensional imaging system (3), said image data representing a plurality of points (5) each of which having at least a set of coordinates in a three-dimensional space;</claim-text>
<claim-text>b) grouping at least some of the points (5) to form a set of clusters (6); and</claim-text>
<claim-text>c) selecting a cluster (6) corresponding to an object of interest (1) located in range of said three-dimensional imaging system (3) according to a first set of parameters comprising at least one of position, shape and<br/>
size;<br/>
and <b>characterised in that</b> the method further comprises the steps of:-</claim-text>
<claim-text>d) grouping at least some of the points (5) of the selected cluster (6) into a set of sub-clusters using a K-means algorithm to group said points (5) of the selected cluster (6) into a predetermined number K of sub-clusters according to a second set of parameters comprising their positions in three-dimensional space, K initial sub-clusters being determined randomly or according to certain parameters of the cluster (6) each sub-cluster having a centroid (11) in said three-dimensional space;<br/>
and</claim-text>
<claim-text>e) associating and fixing a volume (12) each of the centroids (11) of each sub-cluster of said set of sub-clusters, the volume (12) associated with a sub-cluster being a sphere of predetermined radius,<br/>
said three-dimensional model of said object of interest being expressed<!-- EPO <DP n="26"> --> by positions of the centroids of the sub-clusters and the predetermined radius of the spheres.</claim-text></claim-text></claim><claim id="c-en-01-0002" num="0002"><claim-text>A method according to claim 1, wherein each sphere is centred on the centroid (11) of said sub-cluster.</claim-text></claim><claim id="c-en-01-0003" num="0003"><claim-text>A method according to claim 1 or 2, wherein K is 150.</claim-text></claim><claim id="c-en-01-0004" num="0004"><claim-text>A method according to any one of the preceding claims,<br/>
wherein step b) comprises the following steps:-
<claim-text>b1) creating a first cluster (6) comprising a first point (5); and</claim-text>
<claim-text>b2) executing the following operations for each other point (5):
<claim-text>(i) finding the clusters (6) whose centroid (7) is closest to said other point in the three-dimensional space; and</claim-text>
<claim-text>(ii) creating an additional cluster (6) comprising said other point if the absolute distance in the three-dimensional space between said other point (5) and said closest centroid (7) is higher than a predetermined threshold θ, and the number of clusters (6) is still under a predetermined maximum q; or</claim-text>
<claim-text>(iii) adding said other point (5) to the cluster (6) whose centroid (7) is closest to said other point (5) if said absolute distance is not higher than the predetermined threshold θ, or the number of clusters (6) has already reached said predetermined maximum q.</claim-text></claim-text></claim-text></claim><claim id="c-en-01-0005" num="0005"><claim-text>A method according to claim 4, wherein step b) further comprises the steps of:-
<claim-text>b3) determining whether two of said clusters (6) are connected; and</claim-text>
<claim-text>b4) merging connected clusters (6).</claim-text></claim-text></claim><claim id="c-en-01-0006" num="0006"><claim-text>A method according to claim 5, wherein step b3) comprises the steps of:
<claim-text>(i) calculating the standard deviation of the distribution of the projections of the points (5) of each one of said two clusters (6) along an axis (8) linking the centroids (7) of the two clusters (6); and<!-- EPO <DP n="27"> --></claim-text>
<claim-text>(ii) checking whether the sum of the standard deviations multiplied by a predetermined factor S is higher than the absolute distance between the centroids (7) of the two clusters (6).</claim-text></claim-text></claim><claim id="c-en-01-0007" num="0007"><claim-text>A method according to any one of the preceding claims,<br/>
wherein said imaging system (3) comprises any one of: a time-of-flight three-dimensional camera, a stereo camera, a plurality of cameras located in different positions in the three-dimensional space, a LIDAR system, a sonar system or a radar system.</claim-text></claim><claim id="c-en-01-0008" num="0008"><claim-text>A method according to any one of the preceding claims,<br/>
wherein said image data comprise at least zenith and azimuth angles and<br/>
a depth value of each point (5), the method further comprising a step of transforming zenith and azimuth angles and depth values of at least some of these points (5) into three-dimensional Cartesian coordinates.</claim-text></claim><claim id="c-en-01-0009" num="0009"><claim-text>A method according to any one of the preceding claims,<br/>
wherein said human user is in a standing position.</claim-text></claim><claim id="c-en-01-0010" num="0010"><claim-text>A method according to claim 9, further comprising the step of:- f) calculating the approximated centre of mass (17) and main axis (18) of the torso (19) of said human user.</claim-text></claim><claim id="c-en-01-0011" num="0011"><claim-text>A method according to claim 10, wherein step f) comprises analysing the shape of the cluster (6) corresponding to the human user (1) to extract parameters of the cluster (6).</claim-text></claim><claim id="c-en-01-0012" num="0012"><claim-text>A. method according to claim 11, wherein step f) further comprises identifying and discounting points corresponding to arms (15) of the human user (1).</claim-text></claim><claim id="c-en-01-0013" num="0013"><claim-text>A method according to any one of claims 10 to 12,<br/>
wherein step f) comprises the following steps:
<claim-text>f1) calculating the centroid (7) and main axis (16) of said selected cluster (6);</claim-text>
<claim-text>f2) calculating the distribution curve (20) of the distances of the points (5) of the selected cluster (6) with respect to said main axis (16) of the selected cluster (6);<!-- EPO <DP n="28"> --></claim-text>
<claim-text>f3) calculating an inflection point (21) in said distribution curve (20);</claim-text>
<claim-text>f4) selecting the points (5) with distances with respect to said main axis (16) of the selected cluster (6) inferior to D·s, wherein s is the distance of said inflection point (21) to said main axis (16) of the selected cluster (6) and D is a factor of at most 1.25; and</claim-text>
<claim-text>f5) calculating said centre of mass (17) and main axis (18) of the torso (19) as the centroid and main axis of the selected points (5).</claim-text></claim-text></claim><claim id="c-en-01-0014" num="0014"><claim-text>A method according to claim 13, wherein the factor is at most 1.</claim-text></claim><claim id="c-en-01-0015" num="0015"><claim-text>A method according to any one of claims 10 to 14,<br/>
wherein signals are transmitted to a data processing system (2) according to the position of the centre of mass (17) of said torso (19) and/or its main axis (18) and/or the orientation of said main axis (18) of said torso (1).</claim-text></claim><claim id="c-en-01-0016" num="0016"><claim-text>A method according to any one of claims 10 to 15,<br/>
further comprising the step of measuring the height of the human user.</claim-text></claim><claim id="c-en-01-0017" num="0017"><claim-text>A method according to claim 16, wherein said height of the human user is measured by calculating the heights of the points (5) among those of said selected cluster (6) that are closer than a predetermined distance to the main axis (18) of the torso (19), filtering said heights, preferably by median filtering, and selecting the maximum value of said heights after filtering.</claim-text></claim><claim id="c-en-01-0018" num="0018"><claim-text>A method according to claim 17 wherein said measure of the height of the human user is only considered as valid if a set of conditions are met, such as said main axis (18) of the torso (19) being subsantially vertical.</claim-text></claim><claim id="c-en-01-0019" num="0019"><claim-text>A method according to any one of claims 10 to 18,<br/>
wherein K is 25.</claim-text></claim><claim id="c-en-01-0020" num="0020"><claim-text>A method according to any one of the preceding claims,<br/>
wherein the volumes (12) associated with said set of sub-clusters are<!-- EPO <DP n="29"> --> represented in a virtual environment generated by the data processing system (2).</claim-text></claim><claim id="c-en-01-0021" num="0021"><claim-text>A method according to claim 20, wherein there is a collision and/or proximity check between the representation of the volumes (12) of said set of sub-clusters and a set of elements (14) of said virtual environment, so as to interact with said set of elements (14) of the virtual environment.</claim-text></claim><claim id="c-en-01-0022" num="0022"><claim-text>A method according to any one of the preceding claims,<br/>
wherein a set of links (28) between sub-clusters is established using criteria such as absolute distance between the centroids (11) of the sub-clusters, the presence of points (5) between sub-clusters, etc.</claim-text></claim><claim id="c-en-01-0023" num="0023"><claim-text>A method according to claim 22, where a set of extremities (29) of said object of interest (1) is identifed according to said links (28).</claim-text></claim><claim id="c-en-01-0024" num="0024"><claim-text>A method according to claim 23, wherein at least one of said extremities (29) is labelled according to a predetermined pattern, for example that of a human user.</claim-text></claim><claim id="c-en-01-0025" num="0025"><claim-text>A method according to claim 23 or 24, wherein signals are transmitted to a data processing system (2) according to an absolute and/or relative position and/or movement of at least one of said extremities (29).</claim-text></claim><claim id="c-en-01-0026" num="0026"><claim-text>A method according to any one of the preceding claims,<br/>
wherein said interaction is carried out through gesture recognition and/or through proximity and/or collision checks of said volumes (12) with elements of a virtual environment.</claim-text></claim><claim id="c-en-01-0027" num="0027"><claim-text>A system for providing a three-dimensional model (13) of an object of interest, the system comprising a three-dimensional imaging<br/>
system (3) for capturing three-dimensional image data representing a plurality of points (5), each point (5) having at least a set of coordinates in a three-dimensional space, and at least some of said points (5) corresponding to an object of interest (1) located in range of said three-dimensional<!-- EPO <DP n="30"> --> imaging system (3), and a data processing system (2) connected to said three-dimensional imaging system (3) and programmed for carrying out, in cooperation with said three-dimensional imaging system, a method according to any one of the preceding claims.</claim-text></claim></claims><claims mxw-id="PCLM56980381" lang="FR" load-source="patent-office"><!-- EPO <DP n="38"> --><claim id="c-fr-01-0001" num="0001"><claim-text>Procédé pour fournir un modèle tridimensionnel (13) d'un objet d'intérêt à portée d'un système d'imagerie en trois dimensions (3) pour permettre une interaction avec un système de traitement de données (2), l'objet d'intérêt correspondant à un utilisateur humain (1), le procédé comprenant les étapes consistant à :
<claim-text>a) capter des données d'image en trois dimensions en utilisant un système d'imagerie en trois dimensions (3), lesdites données d'image représentant une pluralité de points (5), chacun d'eux ayant au moins un jeu de coordonnées dans un espace tridimensionnel ;</claim-text>
<claim-text>b) grouper au moins certains des points (5) pour former un ensemble de grappes (6) ; et</claim-text>
<claim-text>c) sélectionner une grappe (6) correspondant à un objet d'intérêt (1) situé à portée dudit système d'imagerie en trois dimensions (3) selon un premier ensemble de paramètres en comprenant au moins un de position, de forme et de taille ;<br/>
et <b>caractérisé en ce que</b> le procédé comprend en outre les étapes consistant à :</claim-text>
<claim-text>d) grouper au moins certains des points (5) de la grappe (6) sélectionnée en un ensemble de sous-grappes en utilisant un algorithme des K-moyennes pour grouper lesdits points (5) de la grappe (6) sélectionnée en un nombre K prédéterminé de sous-grappes selon un deuxième ensemble de paramètres comprenant leurs positions dans l'espace tridimensionnel, K sous-grappes initiales étant déterminées de manière aléatoire ou selon certains paramètres de la grappe (6), chaque sous-grappe ayant un centroïde (11) dans ledit espace tridimensionnel ; et</claim-text>
<claim-text>e) associer et fixer un volume (12) à chacun des centroïdes (11) de chaque sous-grappe dudit ensemble de sous-grappes, le volume (12) associé à une sous-grappe étant une sphère de rayon déterminé,<!-- EPO <DP n="39"> --> ledit modèle tridimensionnel dudit objet d'intérêt étant exprimé par des positions des centroïdes des sous-grappes et le rayon prédéterminé des sphères.</claim-text></claim-text></claim><claim id="c-fr-01-0002" num="0002"><claim-text>Procédé selon la revendication 1, dans lequel chaque sphère est centrée sur le centroïde (11) de ladite sous-grappe.</claim-text></claim><claim id="c-fr-01-0003" num="0003"><claim-text>Procédé selon la revendication 1 ou 2, dans lequel K est 150.</claim-text></claim><claim id="c-fr-01-0004" num="0004"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel l'étape b) comprend les étapes suivantes :
<claim-text>b1) créer une première grappe (6) comprenant un premier point (5) ; et</claim-text>
<claim-text>b2) exécuter les opérations suivantes pour chaque point (5) :
<claim-text>(i) trouver la grappe (6) dont le centroïde (7) est le plus proche dudit autre point dans l'espace tridimensionnel ; et</claim-text>
<claim-text>(ii) créer une grappe (6) supplémentaire comprenant ledit autre point si la distance absolue dans l'espace tridimensionnel entre ledit autre point (5) et ledit centroïde (7) le plus proche est supérieure à un seuil θ prédéterminé, et le nombre de grappes (6) est encore en dessous d'un maximum q prédéterminé ; ou</claim-text>
<claim-text>(iii) ajouter ledit autre point (5) à la grappe (6) dont le centroïde (7) est le plus proche dudit autre point (5) si ladite distance absolue n'est pas supérieure audit seuil θ prédéterminé, ou le nombre de grappes (6) a déjà atteint ledit maximum q prédéterminé.</claim-text></claim-text></claim-text></claim><claim id="c-fr-01-0005" num="0005"><claim-text>Procédé selon la revendication 4, dans lequel l'étape b) comprend en outre les étapes consistant à :
<claim-text>b3) déterminer si deux desdites grappes (6) sont connectées ;<br/>
et</claim-text>
<claim-text>b4) fusionner des grappes (6) connectées.</claim-text></claim-text></claim><claim id="c-fr-01-0006" num="0006"><claim-text>Procédé selon la revendication 5, dans lequel l'étape b3) comprend les étapes consistant à :<!-- EPO <DP n="40"> -->
<claim-text>(i) calculer l'écart type de la distribution des projections des points (5) de chacune desdites deux grappes (6) le long d'un axe (8) reliant les centroïdes (7) des deux grappes (6) ; et</claim-text>
<claim-text>(ii) contrôler si la somme des écarts types multipliée par un facteur S prédéterminé est supérieure à la distance absolue entre les centroïdes (7) des deux grappes (6).</claim-text></claim-text></claim><claim id="c-fr-01-0007" num="0007"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel ledit système d'imagerie (3) comprend un quelconque de : une caméra en trois dimensions temps de vol, une caméra stéréo, une pluralité de caméras situées dans des positions différentes dans l'espace tridimensionnel, un système LIDAR, un système sonar ou un système radar.</claim-text></claim><claim id="c-fr-01-0008" num="0008"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel lesdites données d'image comprennent au moins des angles zénithaux et azimutaux et une valeur de profondeur de chaque point (5), le procédé comprenant en outre une étape consistant à transformer des angles zénithaux et azimutaux et des valeurs de profondeur d'au moins certains de ces points (5) en coordonnées cartésiennes en trois dimensions.</claim-text></claim><claim id="c-fr-01-0009" num="0009"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel l'utilisateur humain est dans une position debout.</claim-text></claim><claim id="c-fr-01-0010" num="0010"><claim-text>Procédé selon la revendication 9, comprenant en outre l'étape consistant à :
<claim-text>f) calculer le centre de masse (17) approximé et l'axe principal (18) du torse (19) dudit utilisateur humain.</claim-text></claim-text></claim><claim id="c-fr-01-0011" num="0011"><claim-text>Procédé selon la revendication 10, dans lequel l'étape f) comprend l'analyse de la forme de la grappe (6) correspondant à l'utilisateur humain (1) pour extraire des paramètres de la grappe (6).</claim-text></claim><claim id="c-fr-01-0012" num="0012"><claim-text>Procédé selon la revendication 11, dans lequel l'étape f) comprend en outre l'identification et la soustraction des points correspondant aux bras (15) de l'utilisateur humain (1).<!-- EPO <DP n="41"> --></claim-text></claim><claim id="c-fr-01-0013" num="0013"><claim-text>Procédé selon l'une quelconque des revendications 10 à 12, dans lequel l'étape f) comprend les étapes suivantes :
<claim-text>f1) calculer le centroïde (7) et l'axe principal (16) de ladite grappe (6) sélectionnée ;</claim-text>
<claim-text>f2) calculer la courbe de distribution (20) des distances des points (5) de la grappe (6) sélectionnée par rapport audit axe principal (16) de la grappe (6) sélectionnée ;</claim-text>
<claim-text>f3) calculer un point d'inflexion (21) dans ladite courbe de distribution (20) ;</claim-text>
<claim-text>f4) sélectionner les points (5) avec des distances par rapport audit axe principal (16) de la grappe (6) sélectionnée inférieures à D•s, s étant la distance dudit point d'inflexion (21) audit axe principal (16) de la grappe (6) sélectionnée et D étant un facteur d'au maximum 1,25 ; et</claim-text>
<claim-text>f5) calculer ledit centre de masse (17) et l'axe principal (18) du torse (19) comme le centroïde et axe principal des points (5) sélectionnés.</claim-text></claim-text></claim><claim id="c-fr-01-0014" num="0014"><claim-text>Procédé selon la revendication 13, dans lequel le facteur est au maximum 1.</claim-text></claim><claim id="c-fr-01-0015" num="0015"><claim-text>Procédé selon l'une quelconque des revendications 10 à 14, dans lequel des signaux sont transmis à un système de traitement de données (2) selon la position du centre de masse (17) dudit torse (19) et/ou de son axe principal (18) et/ou l'orientation dudit axe principal (18) dudit torse (1).</claim-text></claim><claim id="c-fr-01-0016" num="0016"><claim-text>Procédé selon l'une quelconque des revendications 10 à 15, comprenant en outre l'étape consistant à mesurer la hauteur de l'utilisateur humain.</claim-text></claim><claim id="c-fr-01-0017" num="0017"><claim-text>Procédé selon la revendication 16, dans lequel ladite hauteur de l'utilisateur humain est mesurée en calculant les hauteurs des points (5) parmi ceux de ladite grappe (6) sélectionnée qui sont plus proches qu'une distance prédéterminée de l'axe principal (18) du torse (19), en filtrant lesdites hauteurs, de préférence par filtrage médian, et en sélectionnant la valeur maximale desdites hauteurs après filtrage.<!-- EPO <DP n="42"> --></claim-text></claim><claim id="c-fr-01-0018" num="0018"><claim-text>Procédé selon la revendication 7, dans lequel ladite mesure de la hauteur de l'utilisateur humain n'est considérée comme valable que si un ensemble de conditions sont satisfaites, comme ledit axe principal (18) du torse (19) étant sensiblement vertical.</claim-text></claim><claim id="c-fr-01-0019" num="0019"><claim-text>Procédé selon l'une quelconque des revendications 10 à 18, dans lequel K est 25.</claim-text></claim><claim id="c-fr-01-0020" num="0020"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel les volumes (12) associés audit ensemble de sous-grappes sont représentés dans un environnement virtuel généré par le système de traitement de données (2).</claim-text></claim><claim id="c-fr-01-0021" num="0021"><claim-text>Procédé selon la revendication 20, dans lequel il y a un contrôle de collision et/ou de proximité entre la représentation des volumes (12) dudit ensemble de sous-grappes et un ensemble d'éléments (14) dudit environnement virtuel, de manière à interagir avec ledit ensemble d'éléments (14) de l'environnement virtuel.</claim-text></claim><claim id="c-fr-01-0022" num="0022"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel un ensemble de liens (28) entre sous-grappes est établi en utilisant des critères tels que distance absolue entre les centroïdes (11) des sous-grappes, la présence de points (5) entre sous-grappes, et.</claim-text></claim><claim id="c-fr-01-0023" num="0023"><claim-text>Procédé selon la revendication 22, dans lequel un ensemble d'extrémités (29) dudit objet d'intérêt (1) est identifié d'après lesdits liens (28).</claim-text></claim><claim id="c-fr-01-0024" num="0024"><claim-text>Procédé selon la revendication 23, dans lequel au moins une desdites extrémités (29) est étiquetée selon un motif prédéterminé, par exemple celui d'un utilisateur humain.</claim-text></claim><claim id="c-fr-01-0025" num="0025"><claim-text>Procédé selon la revendication 23 ou 24, dans lequel des signaux sont transmis à un système de traitement des données (2) en fonction d'une position absolue et / ou relative et/ou d'un mouvement d'au moins une desdites extrémités (29).</claim-text></claim><claim id="c-fr-01-0026" num="0026"><claim-text>Procédé selon l'une quelconque des revendications précédentes, dans lequel ladite interaction est effectuée par<!-- EPO <DP n="43"> --> reconnaissance de gestes et / ou par des contrôles de proximité et / ou de collision desdits volumes (12) avec des éléments d'un environnement virtuel.</claim-text></claim><claim id="c-fr-01-0027" num="0027"><claim-text>Système pour fournir un modèle tridimensionnel (13) d'un objet d'intérêt, le système comprenant un système d'imagerie en trois dimensions (3) pour capturer des données d'image en trois dimensions représentant une pluralité de points (5), chaque point (5) ayant au moins un jeu de coordonnées dans un espace tridimensionnel, et au moins certains desdits points (5) correspondant à un objet d'intérêt (1) situé à portée dudit système d'imagerie en trois dimensions (3), et un système de traitement de données (2) connecté audit système d'imagerie en trois dimensions (3) et programmé pour mettre en oeuvre, en coopération avec ledit système d'imagerie en trois dimensions, un procédé selon l'une quelconque des revendications précédentes.</claim-text></claim></claims><drawings mxw-id="PDW16669141" load-source="patent-office"><!-- EPO <DP n="44"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="154" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="45"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="156" he="224" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="46"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="165" he="157" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="47"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="165" he="128" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="48"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="163" he="224" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="49"> --><figure id="f0006" num="6a"><img id="if0006" file="imgf0006.tif" wi="150" he="208" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="50"> --><figure id="f0007" num="6b"><img id="if0007" file="imgf0007.tif" wi="165" he="209" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="51"> --><figure id="f0008" num="6c"><img id="if0008" file="imgf0008.tif" wi="155" he="222" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="52"> --><figure id="f0009" num="7a"><img id="if0009" file="imgf0009.tif" wi="160" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="53"> --><figure id="f0010" num="7b"><img id="if0010" file="imgf0010.tif" wi="165" he="223" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="54"> --><figure id="f0011" num="7c"><img id="if0011" file="imgf0011.tif" wi="151" he="214" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="55"> --><figure id="f0012" num="7d"><img id="if0012" file="imgf0012.tif" wi="161" he="214" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="56"> --><figure id="f0013" num="8a,8b"><img id="if0013" file="imgf0013.tif" wi="153" he="229" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="57"> --><figure id="f0014" num="9"><img id="if0014" file="imgf0014.tif" wi="157" he="213" img-content="drawing" img-format="tif"/></figure></drawings><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
