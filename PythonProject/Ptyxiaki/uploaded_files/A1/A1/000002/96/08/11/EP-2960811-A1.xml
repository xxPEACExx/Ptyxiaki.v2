<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960811-A1" country="EP" doc-number="2960811" kind="A1" date="20151230" family-id="51211152" file-reference-id="253942" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451466" ucid="EP-2960811-A1"><document-id><country>EP</country><doc-number>2960811</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14306009-A" is-representative="YES"><document-id mxw-id="PAPP193865900" load-source="patent-office" format="original"><country>EP</country><doc-number>14306009.3</doc-number><date>20140626</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865901" load-source="docdb" format="epo"><country>EP</country><doc-number>14306009</doc-number><kind>A</kind><date>20140626</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162031026" ucid="EP-14306009-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>14306009</doc-number><kind>A</kind><date>20140626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988521367" load-source="docdb">G06K   9/00        20060101ALI20141202BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988523804" load-source="docdb">G06F  17/30        20060101AFI20141202BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1981011538" load-source="docdb" scheme="CPC">G06K   9/00751     20130101 LI20160122BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987763680" load-source="docdb" scheme="CPC">G06F  17/30837     20130101 LI20141121BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987800538" load-source="docdb" scheme="CPC">G06F  17/3079      20130101 FI20141121BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545864" lang="DE" load-source="patent-office">Verfahren und Vorrichtung zur Analyse einer Datenbank mit einer Vielzahl von Videobildern</invention-title><invention-title mxw-id="PT165545865" lang="EN" load-source="patent-office">Method and device for analyzing a database comprising a large number of video images</invention-title><invention-title mxw-id="PT165545866" lang="FR" load-source="patent-office">Procédé et dispositif d'analyse d'une base de données comprenant un grand nombre d'images vidéo</invention-title><citations><patent-citations><patcit mxw-id="PCIT335962070" load-source="docdb" ucid="US-20070162873-A1"><document-id format="epo"><country>US</country><doc-number>20070162873</doc-number><kind>A1</kind><date>20070712</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962071" load-source="docdb" ucid="US-20100092037-A1"><document-id format="epo"><country>US</country><doc-number>20100092037</doc-number><kind>A1</kind><date>20100415</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>None</text><sources><source mxw-id="PNPL57937299" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>YONG JAE LEE ET AL:  "Discovering important people and objects for egocentric video summarization", IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR 2012), 16 June 2012 (2012-06-16), pages 1346-1353, XP032232219, DOI: 10.1109/CVPR.2012.6247820 ISBN: 978-1-4673-1226-4</text><sources><source mxw-id="PNPL57937300" load-source="docdb" name="SEA" category="I"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103323532" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>THOMSON LICENSING</last-name><address><country>FR</country></address></addressbook></applicant><applicant mxw-id="PPAR1103309306" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>THOMSON LICENSING</last-name></addressbook></applicant><applicant mxw-id="PPAR1101642131" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Thomson Licensing</last-name><iid>101463287</iid><address><street>1-5 Rue Jeanne d'Arc</street><city>92130 Issy-Les-Moulineaux</city><country>FR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103316489" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SHARMA GAURAV</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103333827" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SHARMA, GAURAV</last-name></addressbook></inventor><inventor mxw-id="PPAR1101643172" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SHARMA, GAURAV</last-name><address><street>TECHNICOLOR R&amp;D FRANCE 975 AVENUE DES CHAMPS BLANCS</street><city>35576 Cesson Sevigne Cedex</city><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103325019" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>PEREZ PATRICK</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103313497" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>PEREZ, PATRICK</last-name></addressbook></inventor><inventor mxw-id="PPAR1101651112" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>PEREZ, PATRICK</last-name><address><street>TECHNICOLOR R&amp;D FRANCE 975 AVENUE DES CHAMPS BLANCS</street><city>35576 Cesson Sevigne Cedex</city><country>FR</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101642157" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Huchet, Anne</last-name><iid>101463286</iid><address><street>Technicolor 1-5 rue Jeanne d'Arc</street><city>92130 Issy-Les-Moulineaux</city><country>FR</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660610439" load-source="docdb">AL</country><country mxw-id="DS660783450" load-source="docdb">AT</country><country mxw-id="DS660610440" load-source="docdb">BE</country><country mxw-id="DS660685049" load-source="docdb">BG</country><country mxw-id="DS660690350" load-source="docdb">CH</country><country mxw-id="DS660610441" load-source="docdb">CY</country><country mxw-id="DS660606545" load-source="docdb">CZ</country><country mxw-id="DS660610581" load-source="docdb">DE</country><country mxw-id="DS660610442" load-source="docdb">DK</country><country mxw-id="DS660610451" load-source="docdb">EE</country><country mxw-id="DS660687737" load-source="docdb">ES</country><country mxw-id="DS660685050" load-source="docdb">FI</country><country mxw-id="DS660685055" load-source="docdb">FR</country><country mxw-id="DS660610582" load-source="docdb">GB</country><country mxw-id="DS660610452" load-source="docdb">GR</country><country mxw-id="DS660610591" load-source="docdb">HR</country><country mxw-id="DS660606546" load-source="docdb">HU</country><country mxw-id="DS660690359" load-source="docdb">IE</country><country mxw-id="DS660610453" load-source="docdb">IS</country><country mxw-id="DS660685056" load-source="docdb">IT</country><country mxw-id="DS660610454" load-source="docdb">LI</country><country mxw-id="DS660610592" load-source="docdb">LT</country><country mxw-id="DS660783451" load-source="docdb">LU</country><country mxw-id="DS660685057" load-source="docdb">LV</country><country mxw-id="DS660685058" load-source="docdb">MC</country><country mxw-id="DS660783452" load-source="docdb">MK</country><country mxw-id="DS660783453" load-source="docdb">MT</country><country mxw-id="DS660783454" load-source="docdb">NL</country><country mxw-id="DS660606547" load-source="docdb">NO</country><country mxw-id="DS660783455" load-source="docdb">PL</country><country mxw-id="DS660610463" load-source="docdb">PT</country><country mxw-id="DS660685449" load-source="docdb">RO</country><country mxw-id="DS660610464" load-source="docdb">RS</country><country mxw-id="DS660783456" load-source="docdb">SE</country><country mxw-id="DS660610594" load-source="docdb">SI</country><country mxw-id="DS660606548" load-source="docdb">SK</country><country mxw-id="DS660606549" load-source="docdb">SM</country><country mxw-id="DS660687738" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479836" lang="EN" load-source="patent-office"><p id="pa01" num="0001">The present disclosure relates to a method for analyzing a database comprising a plurality of video images, characterized in that the method comprises the steps of:<br/>
- sampling (S1) each video image of at least a part of the plurality of video images of the database in order to get a plurality of image patches for said video image, each image patch being a portion of said video image;<br/>
- filtering (S2) said plurality of image patches based on at least one predetermined criterion in order to remove image patches having redundant visual content and to keep a limited group of visually non-similar image patches and/or visually non-similar image patches representative of said plurality of image patches ; and<br/>
- clustering (S2) image patches of said limited group of image patches into a plurality of clusters according to a visual similarity criterion, each cluster including image patches including visually and/or semantically similar visual motifs.
<img id="iaf01" file="imgaf001.tif" wi="93" he="97" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759648" lang="EN" source="EPO" load-source="docdb"><p>The present disclosure relates to a method for analyzing a database comprising a plurality of video images, characterized in that the method comprises the steps of: 
- sampling (S1) each video image of at least a part of the plurality of video images of the database in order to get a plurality of image patches for said video image, each image patch being a portion of said video image; 
- filtering (S2) said plurality of image patches based on at least one predetermined criterion in order to remove image patches having redundant visual content and to keep a limited group of visually non-similar image patches and/or visually non-similar image patches representative of said plurality of image patches ; and 
- clustering (S2) image patches of said limited group of image patches into a plurality of clusters according to a visual similarity criterion, each cluster including image patches including visually and/or semantically similar visual motifs.</p></abstract><description mxw-id="PDES98404537" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b><u>Technical field</u></b></heading><p id="p0001" num="0001">The present disclosure relates to the domain of data mining in database comprising videos and can be used for video summarization, indexing and browsing. More specifically, the present disclosure relates to a method and a device for analyzing a database comprising a large number of videos.</p><heading id="h0002"><b><u>Technical background</u></b></heading><p id="p0002" num="0002">With the increasing popularity of affordable imaging devices e.g. digital cameras and video recorders, digital visual data, like images and videos, is growing at a very high rate. Tools for analyzing and extracting information from very big databases of images and videos are critical for future technologies. These tools are used for different applications like video summarization, video indexing or video browsing.</p><p id="p0003" num="0003">Many previous works have addressed the issue of video summarization. While some have addressed low level visual cues for extracting information, some others have exploited both audio and video. Systems and methods have also been proposed to use higher level knowledge e.g. known celebrities and even sources (like the internet) other than only the present video (collection). Video indexing based on both low level features and high level objects is an active topic of research and system development. Similarly browsing video collections based on objects of interest has also been explored.</p><p id="p0004" num="0004">A common point of all these existing systems is that they all work at global frame level or assume the existence/definition of objects of interests.</p><heading id="h0003"><b><u>Summary</u></b></heading><p id="p0005" num="0005">One purpose of the present disclosure is to provide an alternative to such methods.<!-- EPO <DP n="2"> --></p><p id="p0006" num="0006">Another purpose of the present disclosure is to provide an efficient processing of large video databases e.g. archives of movies, TV shows, documentaries, for the related tasks of summarization of the visual content, indexing of and retrieval in the database and efficient browsing of visual content in the database.</p><p id="p0007" num="0007">According to the present disclosure, it is proposed to work on patches or pieces of images and to mine out the interesting visual content of the video database based on said image patches.</p><p id="p0008" num="0008">The present disclosure relates to a method for analyzing a database comprising a plurality of video images, also called video frames, the method comprising:
<ul><li>sampling each video image of at least a part of the plurality of video images of the database in order to get a plurality of image patches for the video image, each image patch being a portion of the video image;</li><li>filtering the image patches, based on at least one predetermined criterion, in order to remove image patches having redundant visual content and to keep a limited group of visually non-similar image patches and/or visually non-similar image patches representative of the plurality of image patches; and</li><li>clustering image patches of the limited group of image patches into a plurality of clusters according to a visual similarity criterion, each cluster including image patches including visually and/or semantically similar visual motifs.</li></ul></p><p id="p0009" num="0009">In a specific embodiment, the plurality of video images belong to a plurality of video streams.</p><p id="p0010" num="0010">In a variant, the plurality of video images belong to one video stream having a plurality of video shots.</p><p id="p0011" num="0011">In a specific embodiment, each video image is sampled randomly.</p><p id="p0012" num="0012">Advantageously, the plurality of images are sampled in such a way that each pixel of each video image is included in at least one image patch of the plurality of image patches.<!-- EPO <DP n="3"> --></p><p id="p0013" num="0013">In a specific embodiment, the size of the image patches varies randomly.</p><p id="p0014" num="0014">In a specific embodiment, the width and/or the height of image patches depends on the image resolution. The smallest patch can be only a few pixels while the larger patch could be almost the full image.</p><p id="p0015" num="0015">In another embodiment, the size of the image patches varies according to their position in the video image.</p><p id="p0016" num="0016">In a specific embodiment, in the filtering step, the image patches are also filtered according to at least one predetermined additional criterion in order to keep (i) the image patches which have non-uniform color or gray intensity and/or (ii) the image patches comprising at least one object and/or (iii) the image patches comprising one object or a group of visually or semantically similar objects.</p><p id="p0017" num="0017">In a specific embodiment, the clustering is unsupervised.</p><p id="p0018" num="0018">In another embodiment, the clustering is semi-supervised.</p><p id="p0019" num="0019">In another embodiment, the clustering is fully supervised.</p><p id="p0020" num="0020">In a summarization application, the method further comprises a step of selecting at least one image patch in each cluster of the plurality of clusters and a step of displaying the selected image patches.</p><p id="p0021" num="0021">In an indexation application, the method further comprises a step of creating an index for each cluster, the index pointing back in the database to at least one video image comprising an image patch of the cluster.</p><p id="p0022" num="0022">In a browsing application, the method further comprises associating links between at least two clusters within the plurality of clusters by defining a visual similarity between different the clusters and/or a visual similarity between the centroids of the clusters.<!-- EPO <DP n="4"> --></p><p id="p0023" num="0023">The present disclosure concerns also a device configured to analyze a database comprising a plurality of video images, characterized in that the device comprises at least one processor configured for:
<ul><li>sampling each video image of at least a part of the plurality of video images of the database in order to get a plurality of image patches for the video image, each image patch being a portion of the video image;</li><li>filtering the image patches based on at least one predetermined criterion in order to remove image patches having redundant visual content and to keep a limited group of visually non-similar image patches and/or visually non-similar image patches representative of the plurality of image patches; and</li><li>clustering image patches of the limited group of image patches into a plurality of clusters according to a visual similarity criterion, each cluster including image patches including visually and/or semantically similar visual motifs.</li></ul></p><p id="p0024" num="0024">In a specific embodiment, the processor is further configured for creating an index for each cluster, the index pointing back in the database to at least one video image comprising an image patch of the cluster.</p><p id="p0025" num="0025">In a specific embodiment, the processor is further configured for associating a link between two clusters within the plurality of clusters by defining a visual similarity between different the clusters and/or a visual similarity between the centroids of the clusters.</p><p id="p0026" num="0026">The present disclosure also relates to a computer program product comprising instructions of program code for executing steps of the above-mentioned method, when the program is executed on a computer.</p><heading id="h0004"><b><u>List of figures</u></b></heading><p id="p0027" num="0027">The present disclosure will be better understood, and other specific features and advantages will emerge upon reading the following description, the description making reference to the annexed drawings wherein:
<ul><li><figref idrefs="f0001">Fig.1</figref> shows a flow chart of the basic steps of the method according to the present disclosure;<!-- EPO <DP n="5"> --></li><li><figref idrefs="f0002">Fig.2</figref> shows results of the method of the present disclosure applied to a video collection;</li><li><figref idrefs="f0001">Fig.3</figref> a block diagram of the method according to the present disclosure; and</li><li><figref idrefs="f0003">Fig.4</figref> shows an example of device implementing the method of the present disclosure.</li></ul></p><heading id="h0005"><b><u>Detailed description of embodiments</u></b></heading><p id="p0028" num="0028">The present disclosure will be described in reference to a particular embodiment of a method of processing a large video collection (or video database) comprising a large number of video content like movies, TV shows, documentaries. Of course, the video collection may comprise any kind of video images. The terms "image" or "frame" will be used interchangeable in the present description.</p><p id="p0029" num="0029"><figref idrefs="f0001"><b>Fig.1</b></figref> shows a basic flow chart of the steps of the method according to a non limitative embodiment of the present disclosure, <figref idrefs="f0002"><b>Fig.2</b></figref> shows the results of these steps applied to a given video collection and <figref idrefs="f0001"><b>Fig.3</b></figref> is a block diagram of the method according to a specific embodiment of the present disclosure with additional and optional steps for summarization, indexation and browsing.</p><p id="p0030" num="0030">According to a first step S1, each video image of a video collection is sampled in order to get a plurality of image patches for said video image, each image patch being a portion of said video image. The function of this step is to generate a large number of candidate patches, each candidate patch being likely to include interesting visual motifs which can help for summarization and/or indexation of the video collection. According to the present disclosure, it is considered that the visual entities of interest of a video database are the visual motifs which appear frequently in the database. The definition of interesting visual motifs here is quite generic i.e. these visual motifs of interest could be objects in the traditional sense e.g. cars, trees, people, or they could be scenes like beach, forest, or they could even be parts like human faces, car wheels or windows etc.<!-- EPO <DP n="6"> --></p><p id="p0031" num="0031">In <figref idrefs="f0002">figure 2</figref>, the video collection comprises a plurality of video streams V<sub>i</sub> with <i>i</i> ∈ [1..<i>n</i>] and these video streams V<sub>i</sub> are sampled in order to get a set S<sub>i</sub> of image samples for each video stream Vi.</p><p id="p0032" num="0032">In this example, the video streams Vi show people skateboarding in a public place. The visual motifs of interest or recurring motifs in these videos may be, but not limited to, persons, skateboards or skate rings.</p><p id="p0033" num="0033">As mentioned before, the purpose of the step S1 is to generate image patches which might contain interesting visual motifs which will help in summarization and indexation of the video database. The idea is to generate an over-complete set of image patches comprising all possible motif candidates. That is a reason why a large number of patches is advantageously generated at many different scales or sizes and at many locations inside the video images.</p><p id="p0034" num="0034">The number and the size of samples generated for each video stream Vi of the video collection may vary. They may vary randomly or depend on the size and the number of video images of the video stream. In large format, the width and/or the height of image patches depends on the image resolution. The smallest patch can be only a few pixels while the larger patch could be almost the full image.</p><p id="p0035" num="0035">Advantageously, the images of the video streams are sampled in such a way that each pixel of each video image is included in at least one image patch.</p><p id="p0036" num="0036">If the user knows about the type of visual motifs which might be interesting, the present disclosure allows for specification of parameters on the location and scale/size of such motifs i.e. prior information on the occurrence of interesting visual motifs. For example, if the user is interested in faces of persons and the videos are talk shows, the user might specify that the interesting patches occur generally in the center of the images and usually around 60 pixels square. In this case, it will be required to generate more patches of sizes around 60 pixels square in the central locations in the images. In this specific embodiment, the size of the image patches varies according to their position in the video image.<!-- EPO <DP n="7"> --></p><p id="p0037" num="0037">The sampling step S1 generates potentially millions of image patches of different sizes which make the process very expensive. So, according to the present disclosure, the step S1 is followed by a step S2 for filtering these image patches based on a determined criterion in order to remove image patches having redundant visual content and to keep only a limited group of non visually similar image patches and/or a restricted group of non visually similar of image patches representative of the plurality of image patches generated by S1.</p><p id="p0038" num="0038">The purpose of the step S2 is to remove redundant image patches because the oversampling of the video images of step S1 generates a big number of image patches some of which are redundant in the sense that they contain the same visual data.</p><p id="p0039" num="0039">To remove redundant patches, the method of the present disclosure uses for example an approximate nearest neighbor algorithm to find similar image patches in terms of distance between the patches. The said distance between the patches could be based on pixelwise gray level comparison of intensity normalized patches or based on some standard feature detector and descriptor like SIFT (Scale Invariant Feature Transform) and/or HOG (Histogram of Oriented Gradients). Only one single image patch is kept for a set of visually similar image patches. This single patch can be a patch from the set of similar image patches or a synthetic patch issued from a group of image patches. This patch is for example the mean or medoid patch of the set of similar image patches.</p><p id="p0040" num="0040">Advantageously, this step S2 is also used to remove non interesting image patches according to other determined criteria. To remove the non interesting image patches, the method enters additional criteria like color non-uniformity, presence of at least one object in the patch, sum of edge magnitude for all pixels in the patch above a threshold, in order to keep only the image patches having a non-uniform color/intensity and/or the image patches comprising at least one object or a group of similar objects. In the latter case, an object detection algorithm is applied to the image patches during the filtering step.<!-- EPO <DP n="8"> --></p><p id="p0041" num="0041">After redundant and non interesting image patch removal, the inventive method comprises a step S3 of clustering the remaining image patches into a plurality of clusters, each cluster including image patches having similar appearance and/or belonging to the same semantic field (like face, car, window etc.). Each cluster corresponds to a given category of recurring visual motifs and includes patch images including visually and/or semantically similar visual motifs.</p><p id="p0042" num="0042">The purpose of the step S3 is to perform an automatic discovery of recurring visual motifs and to generate clusters each including image patches having the same visual and/or semantic content. For example, in the case of <figref idrefs="f0002">figure 2</figref>, all the image patches issued from the step S2 containing a person are clustered in a cluster C1, all the image patches containing a skateboard are clustered in a cluster C2 and all the remaining image patches including a skate ring are clustered in a cluster C3.</p><p id="p0043" num="0043">The clustering step can be done in three modes. First, in a fully unsupervised mode, no annotation of any kind is provided to the process. The clustering step S3 is made based on the distances, in appropriate feature space, between the image patches using unsupervised learning algorithms like for example a k-means method, Gaussian Mixture Models (GMM), Latent Semantic Analysis (LSA).</p><p id="p0044" num="0044">In a second mode, called semi-supervised mode, a small number of annotated examples of a few interesting visual motif categories, like people or cars, is provided. These examples are used by semi supervised learning algorithm like distance metric learning algorithm with side information to find the occurrences of such categories along with other recurring motifs.</p><p id="p0045" num="0045">In a third mode, called supervised mode, the method is provided with predetermined information for some interesting visual motif categories. It may be provided with discriminative classifiers predicting if a given example patch belongs to the category or not or it may be provided with a learnt metric for computing the distance or closeness of two examples patches in the semantic visual sense. In this mode, like the second mode, the inventive method incorporates this information and performs transfer learning to get the final output.<!-- EPO <DP n="9"> --></p><p id="p0046" num="0046">At the end of step S3, a plurality of clusters Cj of image patches have been generated.</p><p id="p0047" num="0047">As illustrated by <figref idrefs="f0001">fig.3</figref>, these clusters may be used for summarization, index creation or browsing.</p><p id="p0048" num="0048">For summarization, the inventive method comprises the additional steps:
<ul><li>selecting at least one image patch in each cluster of the plurality of clusters;</li><li>displaying the selected image patches.</li></ul></p><p id="p0049" num="0049">The summarization consists in presenting selected image patches, corresponding to the discovered visual motifs, which are representative of the visual content of the collection. If the collection is about skateboarding, the summarization may display image patches of people, skateboards, and cemented skating rings which were automatically discovered by the method.</p><p id="p0050" num="0050">For index creation, the method further comprises creating an index for each cluster, said index pointing back in the video collection to at least one video image comprising a image patch of said cluster. The index points back advantageously to all images of the video collection including an image patch of the cluster.</p><p id="p0051" num="0051">For browsing, the method proposes to link together the visual motifs, exploiting either user provided or automatically learnt (using potentially other sources for extra information, textual or visual, like the internet) semantics or by gathering user feedback on the semantic relations between discovered visual motifs. For example, the method might request the use to mark similar patches to a given patch, and upon getting the user feedback it would learn to associate more similar patches automatically by analysing the content of the patches marked by the user.</p><p id="p0052" num="0052"><figref idrefs="f0003"><b>Fig.4</b></figref> schematically illustrates an example of a hardware embodiment of a device 1 arranged for implementing the method of the present disclosure. The device 1 comprises the following elements, interconnected by an address and data bus 10:
<ul><li>a microprocessor 11 (or CPU) ;<!-- EPO <DP n="10"> --></li><li>a graphics card 12 including:
<ul><li>a plurality of graphics processors 120 (or GPUs) ;</li><li>a Graphical Random Access Memory (or GRAM) 121;</li></ul></li><li>a Read Only Memory (ROM) 13;</li><li>a Random Access Memory (RAM) 14;</li><li>a Flash memory or a Hard Disk Drive 15; and</li><li>Input/Output devices (I/O) 16, such as for example a keyboard or a mouse.</li></ul></p><p id="p0053" num="0053">The device 1 also includes a display device 17 connected directly to the graphics card 12 for displaying the video collection and/or the images patches of the cluster and the results of the inventive method. Alternatively, the display device 17 is external to the device 1.</p><p id="p0054" num="0054">It is observed that the word "register" used in the description of the memories 121, 13, 14 and 15 designates, in each of these memories, an area of low capacity memory (some binary data) as wall as a large memory area (for storing an entire program or all or part of the data representative of data calculated or displayed).</p><p id="p0055" num="0055">At power up, the microprocessor 11 loads a program and different parameters into the RAM 14 and executes instructions of the program. The RAM 14 notably includes:
<ul><li>in a register 140, the program of the microprocessor 11 loaded at the power-up of the device;</li><li>in a register 141, the images of the video collection; and</li><li>in a register 142, algorithms related to the sampling, filtering and clustering steps.</li></ul></p><p id="p0056" num="0056">At power-up and once all needed information are loaded into the RAM 14, the graphics processors 120 load these information in GRAM 121 and execute the instructions of the algorithms related to the sampling, filtering and clustering operations. Intermediary results like candidate patch images or patch images (registers 1211 and 1212) and final results like clusters are stored in the GRAM, or if the space memory is not sufficient, in the flash memory or HDD 16.<!-- EPO <DP n="11"> --></p><p id="p0057" num="0057">Naturally, the present disclosure is not limited to the embodiments previously described. The implementations described herein may be implemented in, for example, a method or a process, an apparatus or a software program. Even if only discussed in the context of a single form of implementation (for example, discussed only as a method or a device), the implementation of features discussed may also be implemented in other forms (for example a program). An apparatus may be implemented in, for example, appropriate hardware, software, and firmware. The methods may be implemented in, for example, an apparatus such as, for example, a processor, which refers to processing devices in general, including, for example, a computer, a microprocessor, an integrated circuit, or a programmable logic device. Processors also include communication devices, such as, for example, Smartphones, tablets, computers, mobile phones, portable/personal digital assistants ("PDAs"), and other devices that facilitate communication of information between end-users.</p><p id="p0058" num="0058">Implementations of the various processes and features described herein may be embodied in a variety of different equipment or applications, particularly, for example, equipment or applications associated with data archiving, data summarization, data indexation and browsing.</p><p id="p0059" num="0059">Additionally, the methods may be implemented by instructions being performed by a processor, and such instructions (and/or data values produced by an implementation) may be stored on a processor-readable medium such as, for example, an integrated circuit, a software carrier or other storage device such as, for example, a hard disk, a compact diskette ("CD"), an optical disc (such as, for example, a DVD, often referred to as a digital versatile disc or a digital video disc), a random access memory ("RAM"), or a read-only memory ("ROM"). The instructions may form an application program tangibly embodied on a processor-readable medium. Instructions may be, for example, in hardware, firmware, software, or a combination. Instructions may be found in, for example, an operating system, a separate application, or a combination of the two. A processor may be characterized, therefore, as, for example, both a device configured to carry out a process and a device that includes a processor-readable medium (such as a storage device) having instructions for carrying out a process. Further, a processor-readable medium may store, in addition to or in lieu of instructions, data values produced by an implementation.<!-- EPO <DP n="12"> --></p><p id="p0060" num="0060">As will be evident to one of skill in the art, implementations may produce a variety of signals formatted to carry information that may be, for example, stored or transmitted. The information may include, for example, instructions for performing a method, or data produced by one of the described implementations.</p><p id="p0061" num="0061">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made. For example, elements of different implementations may be combined, supplemented, modified, or removed to produce other implementations. Additionally, one of ordinary skill will understand that other structures and processes may be substituted for those disclosed and the resulting implementations will perform at least substantially the same function(s), in at least substantially the same way(s), to achieve at least substantially the same result(s) as the implementations disclosed. Accordingly, these and other implementations are contemplated by this application.</p><p id="p0062" num="0062">The present disclosure has the following advantages.
<ol><li>(i) The method is capable of discovering recurring visual motifs automatically from large collection of videos;</li><li>(ii) The method does not assume knowledge of objects of interest in the video collection; it assumes only that the visual motifs which reoccur often in the video collection are interesting; at the same time, it is capable of incorporating prior knowledge of potentially interesting objects.</li><li>(iii) The method does not require expensive and subjective annotations, in general, but is capable of using annotations if available.</li><li>(iv) The method can also incorporate prebuilt systems e.g. discriminative object classifiers and learnt metrics for comparing patches.</li></ol></p><p id="p0063" num="0063">The applications of the present disclosure are numerous and are for example the following ones:
<ol><li>(i) The method can be used to browse and search in archival contents; after the video collection has been processed by the method, the user would be capable of searching and browsing the collection e.g. look for objects like cars or people, which recur in the collection and hence were automatically discovered by the system.<!-- EPO <DP n="13"> --></li><li>(ii) The method can be used to design recommender systems; given the collection of preferred films for a user, the method could be used to discover the recurring motifs which are expected to be highly correlated to the genre/theme of the films e.g. medieval fantasy movies would have castles, horses, knights etc. recurring; in addition, such recurring motifs could be analyzed or automatically indexed by the system in larger film collection, which would give recommendations of similar type of movies to the user.</li><li>(iii) The system may also be used for example based search in large video collections. E.g. a user provides an image or a video clip as a query and the system returns the list of occurrences of visually closest motifs in the video collection.</li><li>(iv) The mined visual motifs could be interpreted as visual 'attributes' and may be further utilized to design a system for categorizing images, video shots or even complete videos into user interested categories.</li></ol></p></description><claims mxw-id="PCLM90459473" lang="EN" load-source="patent-office"><!-- EPO <DP n="14"> --><claim id="c-en-0001" num="0001"><claim-text>Method of analyzing a database comprising a plurality of video images, <b>characterized in that</b> the method comprises:
<claim-text>- sampling (S1) each video image of at least a part of the plurality of video images of the database in order to get a plurality of image patches for said video image, each image patch being a portion of said video image;</claim-text>
<claim-text>- filtering (S2) said plurality of image patches based on at least one determined criterion in order to remove image patches having redundant visual content and to keep a limited group of visually non-similar image patches and/or visually non-similar image patches representative of said plurality of image patches ; and</claim-text>
<claim-text>- clustering (S2) image patches of said limited group of image patches into a plurality of clusters according to a visual similarity criterion, each cluster including image patches including visually and/or semantically similar visual motifs.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>Method according claim 1, wherein the plurality of video images belong to a plurality of video streams.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>Method according to claim 1 or 2, wherein each video image is sampled randomly.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>Method according to any one of claims 1 to 3, wherein said plurality of images are sampled in such a way that each pixel of each video image is included in at least one image patch of said plurality of image patches.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>Method according to any one of claims 1 to 4, wherein the size of the image patches varies randomly.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>Method according to any one of claims 1 to 4, wherein the size of the image patches depends on the image resolution.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>Method according to any one of claims 1 to 4, wherein the size of the image patches varies according to their position in the video image.<!-- EPO <DP n="15"> --></claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>Method according to any one of claims 1 to 7, wherein, in the filtering step, the image patches are also filtered according to at least one predetermined additional criterion in order to keep (i) the image patches which have non-uniform color or gray intensity and/or (ii) the image patches comprising at least one object and/or (iii) the image patches comprising one object or a group of visually or semantically similar objects.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>Method according to any one of claims 1 to 7, wherein the clustering is unsupervised or semi-supervised or fully supervised.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>Method according to any one of claims 1 to 9, wherein the method further comprises:
<claim-text>- selecting at least one image patch in each cluster of the plurality of clusters;</claim-text>
<claim-text>- displaying the selected image patches.</claim-text></claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>Method according to any one of claims 1 to 10, wherein the method further comprises creating an index for each cluster, said index pointing back in the database to at least one video image comprising an image patch of said cluster.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>Method according to any one of claims 1 to 11, wherein the method further comprises associating a link between two clusters within the plurality of clusters.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>Device configured for analyzing a database comprising a plurality of video images, <b>characterized in that</b> the device comprises at least one processor configured for:
<claim-text>- sampling each video image of at least a part of the plurality of video images of the database in order to get a plurality of image patches for said video image, each image patch being a portion of said video image;</claim-text>
<claim-text>- filtering said image patches based on at least one predetermined criterion in order to remove image patches having redundant visual content and to keep a limited group of visually non-similar image patches and/or visually non-similar image patches representative of said plurality of image patches; and<!-- EPO <DP n="16"> --></claim-text>
<claim-text>- clustering image patches of said limited group of image patches into a plurality of clusters according to a visual similarity criterion, each cluster including image patches including visually and/or semantically similar visual motifs.</claim-text></claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>Device according to claim 13, wherein the at least one processor is further configured for creating an index for each cluster, said index pointing back in the database to at least one video image comprising an image patch of said cluster.</claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>Computer program product, <b>characterized in that</b> it comprises instructions of program code for executing steps of the method according to one of claims 1 to 12, when said program is executed on a computer.</claim-text></claim></claims><drawings mxw-id="PDW20422200" load-source="patent-office"><!-- EPO <DP n="17"> --><figure id="f0001" num="1,3"><img id="if0001" file="imgf0001.tif" wi="153" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="18"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="111" he="232" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="19"> --><figure id="f0003" num="4"><img id="if0003" file="imgf0003.tif" wi="158" he="230" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="159" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="159" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
