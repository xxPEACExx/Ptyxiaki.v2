<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960827-A1" country="EP" doc-number="2960827" kind="A1" date="20151230" family-id="53058993" file-reference-id="304362" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451450" ucid="EP-2960827-A1"><document-id><country>EP</country><doc-number>2960827</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15165780-A" is-representative="YES"><document-id mxw-id="PAPP193865868" load-source="patent-office" format="original"><country>EP</country><doc-number>15165780.6</doc-number><date>20150429</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865869" load-source="docdb" format="epo"><country>EP</country><doc-number>15165780</doc-number><kind>A</kind><date>20150429</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162030384" ucid="DE-102014109063-A" load-source="docdb"><document-id format="epo"><country>DE</country><doc-number>102014109063</doc-number><kind>A</kind><date>20140627</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988521229" load-source="docdb">G06K   9/00        20060101AFI20151120BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861378413" load-source="docdb" scheme="CPC">G06T   7/75        20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1946264850" load-source="docdb" scheme="CPC">G06T2207/20061     20130101 LA20160429BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987765432" load-source="docdb" scheme="CPC">G06T2207/30256     20130101 LA20151102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987775873" load-source="docdb" scheme="CPC">G06K   9/00798     20130101 FI20151102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987789790" load-source="docdb" scheme="CPC">G06K   9/00812     20130101 LI20151102BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545816" lang="DE" load-source="patent-office">VERFAHREN ZUR DETEKTION EINES OBJEKTS MIT EINER VORBESTIMMTEN GEOMETRISCHEN FORM IN EINEM UMGEBUNGSBEREICH EINES KRAFTFAHRZEUGS</invention-title><invention-title mxw-id="PT165545817" lang="EN" load-source="patent-office">METHOD FOR DETECTING AN OBJECT WITH A PREDETERMINED GEOMETRIC SHAPE IN AN ENVIRONMENTAL REGION OF A MOTOR VEHICLE</invention-title><invention-title mxw-id="PT165545818" lang="FR" load-source="patent-office">PROCÉDÉ POUR DÉTECTER UN OBJET AU MOYEN D'UNE FORME GÉOMÉTRIQUE PRÉDÉTERMINÉE DANS L'ENVIRONNEMENT D'UN VÉHICULE AUTOMOBILE</invention-title><citations><patent-citations><patcit mxw-id="PCIT364786743" load-source="docdb" ucid="US-20130100286-A1"><document-id format="epo"><country>US</country><doc-number>20130100286</doc-number><kind>A1</kind><date>20130425</date></document-id><sources><source name="SEA" category="XI" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT364786744" load-source="docdb" ucid="US-20140029852-A1"><document-id format="epo"><country>US</country><doc-number>20140029852</doc-number><kind>A1</kind><date>20140130</date></document-id><sources><source name="SEA" category="XI" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335740579" load-source="docdb" ucid="US-3069654-A"><document-id format="epo"><country>US</country><doc-number>3069654</doc-number><kind>A</kind><date>19621218</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>LEE J W ET AL: "A lane-departure identification based on LBPE, Hough transform, and linear regression", COMPUTER VISION AND IMAGE UNDERSTANDING, ACADEMIC PRESS, US, vol. 99, no. 3, 1 September 2005 (2005-09-01), pages 359 - 383, XP027222017, ISSN: 1077-3142, [retrieved on 20050813]</text><sources><source mxw-id="PNPL57906880" load-source="docdb" name="SEA" category="XI"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103329937" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CONNAUGHT ELECTRONICS LTD</last-name><address><country>IE</country></address></addressbook></applicant><applicant mxw-id="PPAR1103334941" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CONNAUGHT ELECTRONICS LTD.</last-name></addressbook></applicant><applicant mxw-id="PPAR1101640504" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Connaught Electronics Ltd.</last-name><iid>101272453</iid><address><street>Dunmore Road</street><city>Tuam, County Galway</city><country>IE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103306903" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>RAMMOS PERIKLES</last-name><address><country>IE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103322277" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>RAMMOS, Perikles</last-name></addressbook></inventor><inventor mxw-id="PPAR1101651178" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>RAMMOS, Perikles</last-name><address><street>Apt 24, Chamber House, South Circular Road, Islandbridge</street><city>Dublin 8</city><country>IE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103320516" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>VOROS ROBERT</last-name><address><country>HU</country></address></addressbook></inventor><inventor mxw-id="PPAR1103333162" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>VOROS, ROBERT</last-name></addressbook></inventor><inventor mxw-id="PPAR1101652218" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>VOROS, ROBERT</last-name><address><street>Sziv 52/A street,</street><city>Budapest</city><country>HU</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101652081" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Jauregui Urbahn, Kristian</last-name><iid>101286306</iid><address><street>Valeo Schalter und Sensoren GmbH CDA-IP Laiernstraße 12</street><city>74321 Bietigheim-Bissingen</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660685011" load-source="docdb">AL</country><country mxw-id="DS660690074" load-source="docdb">AT</country><country mxw-id="DS660687389" load-source="docdb">BE</country><country mxw-id="DS660783368" load-source="docdb">BG</country><country mxw-id="DS660606470" load-source="docdb">CH</country><country mxw-id="DS660687390" load-source="docdb">CY</country><country mxw-id="DS660690079" load-source="docdb">CZ</country><country mxw-id="DS660685013" load-source="docdb">DE</country><country mxw-id="DS660687395" load-source="docdb">DK</country><country mxw-id="DS660687396" load-source="docdb">EE</country><country mxw-id="DS660610158" load-source="docdb">ES</country><country mxw-id="DS660783369" load-source="docdb">FI</country><country mxw-id="DS660606471" load-source="docdb">FR</country><country mxw-id="DS660685014" load-source="docdb">GB</country><country mxw-id="DS660687397" load-source="docdb">GR</country><country mxw-id="DS660685027" load-source="docdb">HR</country><country mxw-id="DS660690080" load-source="docdb">HU</country><country mxw-id="DS660610167" load-source="docdb">IE</country><country mxw-id="DS660687398" load-source="docdb">IS</country><country mxw-id="DS660783370" load-source="docdb">IT</country><country mxw-id="DS660687407" load-source="docdb">LI</country><country mxw-id="DS660783371" load-source="docdb">LT</country><country mxw-id="DS660684881" load-source="docdb">LU</country><country mxw-id="DS660783372" load-source="docdb">LV</country><country mxw-id="DS660783373" load-source="docdb">MC</country><country mxw-id="DS660684882" load-source="docdb">MK</country><country mxw-id="DS660684887" load-source="docdb">MT</country><country mxw-id="DS660684888" load-source="docdb">NL</country><country mxw-id="DS660690081" load-source="docdb">NO</country><country mxw-id="DS660684889" load-source="docdb">PL</country><country mxw-id="DS660687408" load-source="docdb">PT</country><country mxw-id="DS660610333" load-source="docdb">RO</country><country mxw-id="DS660687409" load-source="docdb">RS</country><country mxw-id="DS660684890" load-source="docdb">SE</country><country mxw-id="DS660685029" load-source="docdb">SI</country><country mxw-id="DS660690082" load-source="docdb">SK</country><country mxw-id="DS660690087" load-source="docdb">SM</country><country mxw-id="DS660610168" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479820" lang="EN" load-source="patent-office"><p id="pa01" num="0001">The invention relates to a method for detecting an object (24), which has a predetermined geometric shape and is located in an environmental region of a motor vehicle (1). The detection is effected by means of a camera system (2) of the motor vehicle (1). An image of the environmental region is provided by means of a camera (3) of the camera system (2). Then, a gradient image (22) is generated from the image by means of an image processing device (4) of the camera system (2), thus in particular an edge image. The image processing device (4) then determines at least one regression function (29) from edges (19) of the gradient image (22), wherein a function is used as the regression function (29), which has a shape corresponding to the geometric shape of the searched object (24). Then, transformation of the regression function (29) from an image space (28) of the gradient image (22) into a parameter space (31) is effected using a predetermined transformation rule of the image processing device (4), for example a Hough transformation. The detection of the object (24) is then effected based on the transformed regression function (29) in the parameter space (31).
<img id="iaf01" file="imgaf001.tif" wi="78" he="77" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759632" lang="EN" source="EPO" load-source="docdb"><p>The invention relates to a method for detecting an object (24), which has a predetermined geometric shape and is located in an environmental region of a motor vehicle (1). The detection is effected by means of a camera system (2) of the motor vehicle (1). An image of the environmental region is provided by means of a camera (3) of the camera system (2). Then, a gradient image (22) is generated from the image by means of an image processing device (4) of the camera system (2), thus in particular an edge image. The image processing device (4) then determines at least one regression function (29) from edges (19) of the gradient image (22), wherein a function is used as the regression function (29), which has a shape corresponding to the geometric shape of the searched object (24). Then, transformation of the regression function (29) from an image space (28) of the gradient image (22) into a parameter space (31) is effected using a predetermined transformation rule of the image processing device (4), for example a Hough transformation. The detection of the object (24) is then effected based on the transformed regression function (29) in the parameter space (31).</p></abstract><description mxw-id="PDES98404521" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">The invention relates to a method for detecting an object with a predetermined geometric shape in an environmental region of a motor vehicle by means of a camera system of the motor vehicle. In the method, at least one image of the environmental region is provided by means of a camera of the camera system, wherein the detection is effected based on the image. In addition, the invention relates to a camera system for a motor vehicle, which is formed for performing such a method, as well as to a motor vehicle with such a camera system.</p><p id="p0002" num="0002">Camera systems for motor vehicles detecting objects with a predetermined geometric shape in the environmental region of the motor vehicle are already known from the prior art. On the one hand, this can be effected with a shape matching method, in which a stored template is compared to an image of the environmental region currently captured by the camera. Herein, predetermined shapes are searched in a camera image - for example by means of cross-correlation. On the other hand, alternatively, the detection can also be effected with a transformation into a parameter space - in particular a Hough transformation - as it is known from the printed matter <patcit id="pcit0001" dnum="US3069654A"><text>US 3 069 654 A</text></patcit>. In this case, first, a gradient image is calculated from the camera image; this implies that pixels belonging to edges are determined and subsequently represented as a binary image. In a further step, then, a transformation of the gradient image into a Hough space, which represents a parameter space, is iteratively effected. This is effected via a voting process, in which each pixel of the gradient image contributes to each line crossing the pixel with each one vote. These lines are present as sinusoidal curves in the Hough space. In order to find the searched object in the Hough space, now, the lines are selected, which have obtained particularly many votes in the voting process. In a slight modification of this method, another geometric object as for example a circle can also be used instead of a line.</p><p id="p0003" num="0003">It is disadvantageous in the mentioned prior art that the shape matching method requires an exact template of the searched object in a database. This results in the method often not robustly operating because for example the scaling of the current camera image does not exactly coincide with that of the template. In addition, a severe restriction with respect to the generality is to be accepted: In case the object to be detected has slightly changed features with respect to the template, an error can occur in the detection. Here, the above mentioned Hough transformation provides a remedy. It is not dependent on an exact<!-- EPO <DP n="2"> --> template of the searched object. For example, a searched line can extend over a length to be determined in a certain range: a fact, which increases the generality of the method. Furthermore, the Hough space is known to be robust. However, a disadvantage of the Hough transformation is the high computational effort due to an extremely iterative procedure. In other words, the Hough transformation is a brute force method.</p><p id="p0004" num="0004">It is the object of the invention to demonstrate a solution, how in a method of the initially mentioned kind the computational effort in detecting the object, in particular in a transformation into a parameter space, can be reduced compared to a conventional Hough transformation.</p><p id="p0005" num="0005">According to the invention, this object is solved by a method, by a camera system as well as by a motor vehicle having the features according to the respective independent claims. Advantageous implementations of the invention are the subject matter of the dependent claims, of the description and of the figures.</p><p id="p0006" num="0006">A method according to the invention serves for detecting an object having a predetermined geometric shape and located in an environmental region of a motor vehicle. The detection is effected by means of a camera system of the motor vehicle. An image of the environmental region is provided by means of a camera of the camera system. Then, a gradient image is generated from the image by means of an image processing device of the camera system, thus in particular an edge image. The image processing device then determines at least one regression function from edges of the gradient image, wherein a function is used as the regression function, which has a shape corresponding to the geometric shape of the searched object. Then, transforming the regression function from an image space of the gradient image into a parameter space is effected using a predetermined transformation rule by the image processing device, for example a Hough transformation. The detection of the object is then effected based on the transformed regression function in the parameter space and thus based on elements of the parameter space.</p><p id="p0007" num="0007">Accordingly, according to the invention, it is proposed not to directly transform the pixels of the gradient image into the parameter space, but first to introduce a pre-processing step. In this pre-processing step, the pixels of the gradient image belonging to edges are approximated by at least one regression function. Subsequently, this regression function itself can be transformed into the parameter space, and it does not have to be iterated over all of the pixels anymore, but only over the regression function.<!-- EPO <DP n="3"> --></p><p id="p0008" num="0008">The method according to the invention has the advantage that by the introduction of the regression function approximating the course of contours of the searched object and representing a so-called skeleton in this manner, a functional representation of the edges of the gradient image is allowed. This functional representation then allows transformation into the parameter space with particularly low computing time. Overall, the detection of the objects can therefore be effected without much computational effort such that an image processing device (DSP) with restricted computational power can also be employed and yet the detection of the object can be performed in real time.</p><p id="p0009" num="0009">Presently, an - in particular color sensitive - image capturing device is understood by a camera, which is formed for capturing light in the visible spectral range and thus is able to provide images of the environmental region. For example, a CCD camera or a CMOS camera can be employed as the camera. In particular, the camera is also a video camera, which provides a temporal sequence of images of the environmental region and communicates it to the image processing device.</p><p id="p0010" num="0010">In an embodiment, a roadway marking applied to a roadway is detected as the object. In this context, it can be provided that a parking space marking delimiting a parking space and/or a roadway longitudinal marking delimiting a lane is detected by the camera system as the object. This is an advantageous application because these roadway markings can be particularly well approximated with a linear regression function. Thus, the camera system can for example particularly advantageously be used as a lane departure warning system and/or as a parking assist system.</p><p id="p0011" num="0011">In an embodiment, an object with at least one rectilinear contour is detected, wherein a rectilinear and thus linear function is used as the regression function. This has the advantage that this rectilinear function can be simply and fast calculated and rectilinear street markings - such as parking space markings and/or roadway longitudinal markings - can be described and detected without much computational effort.</p><p id="p0012" num="0012">Additionally or alternatively, an object with at least one oval, in particular circular, contour can be detected, wherein an oval, in particular circular, function is then used as the regression function. For example, this function can be used to describe a component, in particular a wheel or the like, of another object participating in traffic.<!-- EPO <DP n="4"> --></p><p id="p0013" num="0013">Further additionally or alternatively, an object with a T-shaped and/or L-shaped contour can be detected, wherein a combination of rectilinear functions is used as the regression function in this case. It is advantageous that a majority of the typical parking space markings can be described with these shapes.</p><p id="p0014" num="0014">Preferably, a transformation rule for line elements is used as the transformation rule for transforming the regression function. This means that not separately the pixels of the gradient image, but the entire regression function is taken as a basis for the transformation. This has the advantage that the transformation proceeds more effectively because a vote does not have to be given for each pixel in the parameter space, but a parameter of the functional representation in the image space can be simply transformed or converted into the parameter space.</p><p id="p0015" num="0015">In particular, it is provided that a Hough transformation is used as the transformation rule such that the parameter space corresponds to a Hough space. This has the advantage that it can be resorted to the robustness of a Hough space.</p><p id="p0016" num="0016">Preferably, transforming the regression function includes a kernel density estimation. This means that each transformed pixel does not only contribute to a coordinate or element in the parameter space, but also to a certain area around the element. The area around the element is determined with a kernel underlying the kernel density estimation. Possible kernels are for example a Gaussian kernel, a Cauchy kernel, a Picard kernel or an Epanechnikov kernel. The advantage in this procedure is increased robustness in establishing the Hough space.</p><p id="p0017" num="0017">In a further embodiment it is provided that the detection of the object in the parameter space includes that values of the parameter space are combined to clusters using a clustering method and the clusters are extracted. The combination to clusters can for example be effected with a mean shift algorithm. Herein, a better interpretability of the Hough space is advantageous; this serves for that searched objects can be more precisely and less error-prone extracted from the Hough space.</p><p id="p0018" num="0018">Additionally or alternatively, it can be provided that the detection of the object in the parameter space includes that values of the parameter space are extracted using a non-maximum suppression method. The non-maximum suppression method is often employed in association with edge detection methods. The image is scanned along the gradient direction and, if pixels are not a part of a local maximum, these pixels are set to zero. This<!-- EPO <DP n="5"> --> has the effect that all of the pixels, which are not a part of the local maximum, are suppressed. The better interpretability of the Hough space is advantageous; this in turn serves for that searched objects can be more precisely and less error-prone extracted from the Hough space.</p><p id="p0019" num="0019">Additionally or alternatively, it can be provided that the detection of the object in the parameter space includes that values of the parameter space are compared to a predetermined threshold value and extracted depending on the comparison for detecting the object. The effect is a reduction of the elements in the Hough space. This approach too has the advantage that searched objects can be better extracted from the Hough space.</p><p id="p0020" num="0020">In particular, it is provided that points of intersection of the regression function on the one hand with the edges of the gradient image on the other hand are determined in the image space and transformed into the parameter space. Then, a progression of the transformed regression function in the parameter space can be determined considering the transformed points of intersection. By this approach, the parameter space can be extended with a starting point and an endpoint such that the line can be more precisely determined using the starting point and the endpoint in the Hough space. Hereby, an infinitive straight line for example becomes a limited line segment with a starting point and an endpoint. The advantage is in the additional information content of the Hough space. Hereby, objects can in turn be more precisely extracted from the Hough space.</p><p id="p0021" num="0021">A camera system according to the invention for a motor vehicle is formed for performing a method according to the invention. The camera system for a motor vehicle includes at least a camera for providing an image of an environmental region of the motor vehicle and an image processing device, which is adapted to perform a method according to the invention.</p><p id="p0022" num="0022">A motor vehicle according to the invention, in particular a passenger car, includes a driver assistance system according to the invention.</p><p id="p0023" num="0023">The preferred embodiments presented with respect to the method according to the invention and the advantages thereof correspondingly apply to the camera system according to the invention as well as to the motor vehicle according to the invention.<!-- EPO <DP n="6"> --> Further features of the invention are apparent from the claims, the figures and the description of figures. All of the features and feature combinations mentioned above in the description as well as the features and feature combinations mentioned below in the description of figures and/or shown in the figures alone are usable not only in the respectively specified combination, but also in other combinations or else alone.</p><p id="p0024" num="0024">Now, the invention is explained in more detail based on a preferred embodiment as well as with reference to the attached drawings.</p><p id="p0025" num="0025">There show:
<dl id="dl0001"><dt>Fig. 1</dt><dd>in schematic illustration a plan view of a motor vehicle with a camera system according to an embodiment of the invention, wherein the motor vehicle is parked into a parking space;</dd><dt>Fig. 2</dt><dd>in schematic illustration a plan view of the motor vehicle, which travels on a multi-lane road, wherein the surrounding roadway markings are detected by means of the camera system;</dd><dt>Fig. 3</dt><dd>a flow diagram of a method according to an embodiment of the invention;</dd><dt>Fig. 4</dt><dd>an exemplary image of an environmental region of the motor vehicle provided by a camera of the camera system;</dd><dt>Fig. 5</dt><dd>an exemplary binary image, in which edges in horizontal direction are detected;</dd><dt>Fig. 6</dt><dd>an exemplary binary image, in which edges in vertical direction are detected;</dd><dt>Fig. 7</dt><dd>an exemplary binary image, in which line elements or center lines each calculated by means of a regression function based on the horizontal and the vertical edges are illustrated;</dd><dt>Fig. 8</dt><dd>a binary image of <figref idrefs="f0003">Fig. 4</figref>, in which the line elements and center lines, respectively, which are each calculated by means of a regression function<!-- EPO <DP n="7"> --> based on the horizontal and the vertical edges, are illustrated; and</dd><dt>Fig. 9</dt><dd>an illustration of a parameter space with the origin top left, abscissa ρ and ordinate θ, in which a kernel density estimation was utilized for establishing.</dd></dl></p><p id="p0026" num="0026">In <figref idrefs="f0001">Fig. 1</figref>, a plan view of a motor vehicle 1 with a camera system 2 is schematically illustrated, which includes a camera 3 and an image processing device 4. The motor vehicle 1 is parked into a parking space 5 according to arrow representation 7, which is delimited by roadway markings 6. The roadway markings 6 according to <figref idrefs="f0001">Fig. 1</figref> therefore represent parking space markings. Herein, the roadway markings 6 are detected by the camera system 2, wherein this detection is effected by the image processing device 4 based on an image, which is provided by the camera 3 and communicated to the image processing device 4. For example, the camera system 2 is a part of a driver assistance system, which provides additional information such as the presence and the position of the parking space 5 to a driver of the motor vehicle 1 in order to allow a more precise, more comfortable and more secure parking operation. Optionally, the camera system 2 can also be associated with a parking assist system, which is formed for automatic or semi-autonomous parking of the motor vehicle 1.</p><p id="p0027" num="0027">In <figref idrefs="f0001">Fig. 2</figref>, the motor vehicle 1 is also shown in a schematic plan view. The motor vehicle 1 is on a three-lane roadway or road 8, on which roadway markings 6 are applied as roadway longitudinal markings. A solid line roadway marking 9 indicates the lateral end of the road 8, while dashed line roadway markings 11 delimit lanes 10 from each other. The camera system 2 detects the roadway markings 6 and thus can assist the driver in keeping the motor vehicle 1 in the respective lane 10 and thus possibly avoiding a risk of accident by unintentionally exiting the lane 10.</p><p id="p0028" num="0028">Furthermore, the exemplary procedure of the method is explained based on a program flow diagram 12 shown in <figref idrefs="f0002">Fig. 3</figref>. First, the grey level channel of an input image 13, which is presently captured with a fish eye lens, is converted to a plan view image 15 with the aid of look-up tables 14. The look-up tables 14 include geometric data of the camera 3. The conversion can for example be performed by means of a nearest neighborhood interpolation method. Alternatively to the nearest neighborhood interpolation method, a bilinear interpolation is also possible. The bilinear interpolation incorporates intensity values of surrounding pixels of the input image 13 in the interpolation method in order to<!-- EPO <DP n="8"> --> obtain a correspondingly weighted pixel thereby better matched to the environment of the pixel in the plan view image 15.</p><p id="p0029" num="0029">In the next process step 16, the plan view image 15 is smoothed with a smoothing filter, for example a Gaussian filter or a locally adapted smoothing filter, and the histogram of the plan view image 15 is equalized. The smoothing filter results in small high-frequency areas in the plan view image being eliminated and not remaining as disturbing artifacts for the following steps. The histogram equalization results in the entire range of values of the plan view image 15, for example 8 bits, being uniformly exploited. This is in particular interesting if light conditions exist at the time of image capture, for which the camera is not pre-configured and thus is subjected to under- or overexposure. Subsequently, an adaptive threshold value method is applied to the plan view image 15, which has a binary image 17 as the result. The binary image 17 is denoised with a morphologic method, in particular an opening or closing, to clean it up from small artifacts.</p><p id="p0030" num="0030">Thereafter, in the process step 18, edges 19 are now extracted in this binary image 17. First, this is effected for all of the horizontally extending edges 20, as shown in <figref idrefs="f0003">Fig. 5</figref>, and subsequently for all of the vertically extending edges 21, as shown in <figref idrefs="f0004">Fig. 6</figref>. By combining the horizontal edges 20 and the vertical edges 21, an overall gradient image 22 arises, as it is exemplarily shown in <figref idrefs="f0004">Fig. 7</figref>. The gradient image 22 shows all of the high-frequency areas in the binary image 17, i.e. pixels with a certain change of the intensity compared to adjacent pixels. The gradient image 22 can for example be generated with a Sobel operator, which is applied horizontally and vertically and optionally additionally also diagonally. The Canny edge detector, which also incorporates the Sobel operator, presents a further method to calculate the gradient image 22.</p><p id="p0031" num="0031">Now, the gradient image 22 has contours 23 or outlines of objects 24 (compare also <figref idrefs="f0003">Fig. 4</figref>) in the binary image 17, wherein one of the contours 23 can be composed of multiple edges 19.</p><p id="p0032" num="0032">Also in process step 18, based on the gradient image 22, which is in an image space 28, a regression function 29 is calculated. The contour 23 of the respective object 24 is approximated by at least one such regression function 29. The basic shape of the regression function 29 is determined in advance, thus, for example, a rectilinear function, an elliptical function, a combination of several rectilinear or elliptical functions or the like is possible. As a result of this regression step, one obtains at least one line element 30 in the form of a center line extending through the center of the contour 23, as is seen in <figref idrefs="f0004">Fig.<!-- EPO <DP n="9"> --> 7</figref>. The process is similar to that of a skeletonization, but less computationally intensive than this.</p><p id="p0033" num="0033">The result of the preceding steps is exemplarily shown in <figref idrefs="f0003">Fig. 4</figref> and <figref idrefs="f0005">Fig. 8</figref>. In <figref idrefs="f0003">Fig. 4</figref>, an exemplary input image 13 is shown, which includes objects 24 in the form of a wheel 25 with a rim 26 and a parking space marking 27. The parking space marking 27 has a T shape and delimits the parking space 5 in two directions. The corresponding binary image 17 with extracted line elements 30 is shown in <figref idrefs="f0005">Fig. 8</figref>. The line elements 30 are clearly seen, which originate from the rim 26 or the parking space marking 27.</p><p id="p0034" num="0034">The present regression functions 29 are now transformed into a parameter space 31, which is exemplarily illustrated in <figref idrefs="f0005">Fig. 9</figref>. The parameter space 31 is a Hough space in the embodiment. The transformation can be mathematically represented as follows if the regression function 29 is for example present in the form of a straight line: <maths id="math0001" num=""><math display="block"><mrow><mi>y</mi><mo>=</mo><mi mathvariant="italic">ax</mi><mo>+</mo><mi>b</mi><mo>→</mo><mi>θ</mi><mo>=</mo><msup><mi>cot</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mfenced separators=""><mo>-</mo><mi>a</mi></mfenced><mo>,</mo><mi>ρ</mi><mo>=</mo><mi>b</mi><mo>⋅</mo><mi>sin</mi><mfenced><mi>θ</mi></mfenced><mo>;</mo></mrow></math><img id="ib0001" file="imgb0001.tif" wi="76" he="8" img-content="math" img-format="tif"/></maths> <maths id="math0002" num=""><math display="block"><mrow><mi>x</mi><mo>=</mo><mi mathvariant="italic">ay</mi><mo>+</mo><mi>b</mi><mo>→</mo><mi>θ</mi><mo>=</mo><msup><mi>tan</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mfenced separators=""><mo>-</mo><mi>a</mi></mfenced><mo>,</mo><mi>ρ</mi><mo>=</mo><mi>b</mi><mo>⋅</mo><mi>cos</mi><mfenced><mi>θ</mi></mfenced><mo>;</mo></mrow></math><img id="ib0002" file="imgb0002.tif" wi="78" he="8" img-content="math" img-format="tif"/></maths><br/>
wherein:
<ul><li>x and y define the coordinate in the image coordinate system,</li><li>a denotes the slope of the regression function 29,</li><li>b is the shift constant of the regression function 29, and</li><li>θ and ρ define the coordinate in the parameter space 31.</li></ul></p><p id="p0035" num="0035">On the left side of the arrow shown in the above equation, the regression function 29 in the image space 28 and on the right side of the arrow, the conversion of the parameters a and b of the regression function 29 into the parameter space 31 or the Hough space is described, which is defined by θ and ρ.</p><p id="p0036" num="0036">In a voting process, which proceeds corresponding to the original Hough transformation, now, the votes of the line elements 20 for each element 33 in the parameter space 31 are accumulated in order to be able to evaluate the intensity or importance of an element 33 and thus of an object 24 in the binary image 17. The number of the votes for a certain element 33 is determined depending on a length of the line elements 30, which give votes for the element 33. The length can be determined either via the number of the pixels or via<!-- EPO <DP n="10"> --> the distance between starting and endpoint of the line element 30. The length can also be included in the parameter space 31 as a parameter.</p><p id="p0037" num="0037">The advantage of this approach compared to a conventional Hough transformation is that less computational power has to be expended for the transformation from the image space 28 to the parameter space 31. A reason for this is that not every single pixel of the binary image 17 is transformed into the parameter space 31, but only each single line element 30, i.e. each single regression function 29. In addition, the parameter space 31 is "cleaner" compared to a conventional Hough space, i.e. there are less elements 33 in it, which results in an analysis of the parameter space 31 or extraction of the searched elements 33 being able to proceed less computationally intensive.</p><p id="p0038" num="0038">The transformation into the parameter space 31 can also be effected depending on a kernel density estimation. Herein, not only a vote for a certain element 33 (θ,ρ) in the parameter space 31 is given by the line element 30 or the regression function 29, but a neighborhood 34 around the certain element (θ,ρ) is also taken into account and benefits from the vote albeit optionally only proportionally. The rule, according to which the area of the neighborhood 34 is selected and which weighted proportion of the vote the other elements 33 are awarded, which are located in the area of the neighborhood 34, depends on the selected kernel. Often, the weighted proportion of the vote is made depending with respect to the distance of the transformed element 33 to the adjacent element benefiting from the proportional vote. Possible kernels are for example a Gaussian kernel, a Cauchy kernel, a Picard kernel or an Epanechnikov kernel.</p><p id="p0039" num="0039">In case the length of the line elements 30 of the regression function 29 is included as a parameter in the parameter space 31 and the kernel density estimation is applied, the length can be employed as a multiplier of the kernel.</p><p id="p0040" num="0040">An exemplary parameter space 31 with the axes θ and ρ is shown in <figref idrefs="f0005">Fig. 9</figref>. For the transformation from the image space 28 into the parameter space 31, a parabolic kernel has been used, wherefore circular elements 33 have been arisen. In the present example, each element 33 in the parameter space 31 represents a line and thus a regression function 29 in the image space 28.</p><p id="p0041" num="0041">In a following process step 32, searched or important elements 33 of the parameter space 31, which each represent a searched or important object 24 in the image space 28, in the present example the line element 30, are determined in the parameter space 31. This is<!-- EPO <DP n="11"> --> effected with a clustering method and/or with a non-maximum suppression method and/or with a threshold value method.</p><p id="p0042" num="0042">The clustering method combines certain areas in the parameter space 31. To this, there is a plurality of possibilities such as for example a k means algorithm and/or a BIRCH (balanced iterative reducing and clustering using hierarchies) algorithm. The non-maximum suppression method pursues the goal to only consider elements 33 of the parameter space 31, which belong to a local optimum. A reduction of the parameter space 31 is also achieved with the threshold value method, wherein elements 33 of the parameter space 31, which are below or above a certain threshold value, are excluded from the further procedure. The object of this last mentioned method is a reduction of the parameter space 31 to the essentials in order to simplify and improve a subsequently following analysis.</p><p id="p0043" num="0043">In a parallel process step 35, the line elements 30 of the regression function 29 are matched to the edges 19 of the binary image 17. As a result, one obtains marks 36 in the form of coordinates, which each correspond to the point of intersection of the contour 23 of the object 24 of the binary image 17 with the line element 30.</p><p id="p0044" num="0044">These marks 36 are also transformed into the parameter space 31 in a further process step 37. The purpose of the marks 36 is that elements 33 in the parameter space 31, which are for example present only as a straight line up to now, can now be provided with further parameters such as endpoints 38, and thus are available as a curve bounded at the start and at the end, in the present example as a line segment.</p><p id="p0045" num="0045">The exemplary parameter space 31, which now contains the parameters θ, ρ and two endpoints per element 33, is finally processed with a higher logic in the process step 39 and finally provided via an API (application programming interface) to a card 40.</p><p id="p0046" num="0046">However, the parameter space 31 is not restricted to the mentioned parameters. Thus, for example, in case of a regression function 29, which is of higher order than the rectilinear regression function 29 in the cited example, correspondingly more parameters are also required in order to describe the line element 30 or a curve of the regression function 29.</p></description><claims mxw-id="PCLM90459457" lang="EN" load-source="patent-office"><!-- EPO <DP n="12"> --><claim id="c-en-0001" num="0001"><claim-text>Method for detecting an object (24) with a predetermined geometric shape in an environmental region of a motor vehicle (1) by means of a camera system (2) of the motor vehicle (1), including the steps of:
<claim-text>- providing an image (13) of the environmental region by means of a camera (3) of the camera system (2),</claim-text>
<claim-text>- generating a gradient image (22) from the image (13) by means of an image processing device (4) of the camera system (2),</claim-text>
<claim-text>- determining a regression function (29) from edges (19) of the gradient image (22) by the image processing device (4), wherein the regression function (29) has a shape corresponding to the geometric shape of the object (24),</claim-text>
<claim-text>- transforming the regression function (29) from an image space (28) of the gradient image (22) into a parameter space (31) using a predetermined transformation rule by the image processing device (4), and</claim-text>
<claim-text>- detecting the object (24) based on the transformed regression function (29) in the parameter space (31).</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>Method according to claim 1,<br/>
<b>characterized in that</b><br/>
a roadway marking (6) applied to a roadway (8), in particular a parking space marking (27) delimiting a parking space (5) and/or a roadway longitudinal marking (9, 11) delimiting a lane (10), is detected as the object (24) by the camera system (2).</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>Method according to claim 1 or 2,<br/>
<b>characterized in that</b><br/>
at least one of the following objects (24) is detected as the object (24) by the camera system (2):
<claim-text>- an object (24) with at least one rectilinear edge, wherein a rectilinear function is used as the regression function (29), and/or</claim-text>
<claim-text>- an object (24) with an oval, in particular circular, contour, wherein an oval, in particular circular, function is used as the regression function (29), and/or</claim-text>
<claim-text>- an object (24) with a T shaped and/or L shaped contour.</claim-text><!-- EPO <DP n="13"> --></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>Method according to any one of the preceding claims,<br/>
<b>characterized in that</b><br/>
a Hough transformation is used as the transformation rule such that the parameter space (31) corresponds to a Hough space.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>Method according to any one of the preceding claims,<br/>
<b>characterized in that</b><br/>
the transforming of the regression function (29) includes a kernel density estimation.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>Method according to any one of the preceding claims,<br/>
<b>characterized in that</b><br/>
the detection of the object (24) in the parameter space (31) includes that values of the parameter space (31) are combined to clusters using a clustering method and the clusters are extracted.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>Method according to any one of the preceding claims,<br/>
<b>characterized in that</b><br/>
the detection of the object (24) in the parameter space (31) includes that values of the parameter space (31) are extracted using a non-maximum suppression method.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>Method according to any one of the preceding claims,<br/>
<b>characterized in that</b><br/>
the detection of the object (24) in the parameter space (31) includes that values of the parameter space (31) are compared to a predetermined threshold value and extracted depending on the comparison.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>Method according to any one of the preceding claims,<br/>
<b>characterized in that</b><br/>
in the image space (28), points of intersection (36) of the regression function (29) on the one hand with the edges (19) of the gradient image (22) on the other hand are determined and transformed into the parameter space (31), wherein a course of the transformed regression function (29) in the parameter space (31) is determined considering the transformed points of intersection (36).<!-- EPO <DP n="14"> --></claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>Camera system (2) for a motor vehicle (1) including at least a camera (3) for providing an image of an environmental region of the motor vehicle (1), and including an image processing device (4), which is adapted to perform a method according to any one of the preceding claims.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>Motor vehicle (1), in particular passenger car, with a camera system (2) according to claim 10.</claim-text></claim></claims><drawings mxw-id="PDW20422184" load-source="patent-office"><!-- EPO <DP n="15"> --><figure id="f0001" num="1,2"><img id="if0001" file="imgf0001.tif" wi="135" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="16"> --><figure id="f0002" num="3"><img id="if0002" file="imgf0002.tif" wi="155" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="17"> --><figure id="f0003" num="4,5"><img id="if0003" file="imgf0003.tif" wi="158" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="18"> --><figure id="f0004" num="6,7"><img id="if0004" file="imgf0004.tif" wi="161" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="19"> --><figure id="f0005" num="8,9"><img id="if0005" file="imgf0005.tif" wi="151" he="233" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="159" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="159" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
