<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960858-A1" country="EP" doc-number="2960858" kind="A1" date="20151230" family-id="50982805" file-reference-id="318329" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451823" ucid="EP-2960858-A1"><document-id><country>EP</country><doc-number>2960858</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14173827-A" is-representative="YES"><document-id mxw-id="PAPP193866614" load-source="patent-office" format="original"><country>EP</country><doc-number>14173827.8</doc-number><date>20140625</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193866615" load-source="docdb" format="epo"><country>EP</country><doc-number>14173827</doc-number><kind>A</kind><date>20140625</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162034082" ucid="EP-14173827-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>14173827</doc-number><kind>A</kind><date>20140625</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988526653" load-source="docdb">G06T   7/00        20060101AFI20141203BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1861374449" load-source="docdb" scheme="CPC">G06T   7/564       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1861376880" load-source="docdb" scheme="CPC">G06T   7/593       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1890307732" load-source="docdb" scheme="CPC">G06T2207/10021     20130101 LA20161004BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1890308313" load-source="docdb" scheme="CPC">G06T2207/30252     20130101 LA20161004BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984696776" load-source="docdb" scheme="CPC">G06T2207/10012     20130101 LA20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700970" load-source="docdb" scheme="CPC">G01B  11/026       20130101 FI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984703870" load-source="docdb" scheme="CPC">G06K   9/00791     20130101 LI20151231BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165546935" lang="DE" load-source="patent-office">Sensorsystem zur Bestimmung von Entfernungsinformationen auf Basis von stereoskopischen Bildern</invention-title><invention-title mxw-id="PT165546936" lang="EN" load-source="patent-office">Sensor system for determining distance information based on stereoscopic images</invention-title><invention-title mxw-id="PT165546937" lang="FR" load-source="patent-office">Système de capteur pour déterminer des informations de distance basées sur des images stéréoscopiques</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103309161" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>HONDA RES INST EUROPE GMBH</last-name><address><country>DE</country></address></addressbook></applicant><applicant mxw-id="PPAR1103308526" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>HONDA RESEARCH INSTITUTE EUROPE GMBH</last-name></addressbook></applicant><applicant mxw-id="PPAR1101651180" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Honda Research Institute Europe GmbH</last-name><iid>100141037</iid><address><street>Carl-Legien-Strasse 30</street><city>63073 Offenbach/Main</city><country>DE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103343355" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>EINECKE NILS</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103320513" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>Einecke, Nils</last-name></addressbook></inventor><inventor mxw-id="PPAR1101639501" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Einecke, Nils</last-name><address><street>c/o Honda Research Institute Europe GmbH Carl-Legien-Str. 30</street><city>63073 Offenbach</city><country>DE</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101653106" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Rupp, Christian</last-name><iid>101434913</iid><address><street>Mitscherlich PartmbB Patent- und Rechtsanwälte Sonnenstrasse 33</street><city>80331 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660615107" load-source="docdb">AL</country><country mxw-id="DS660785229" load-source="docdb">AT</country><country mxw-id="DS660615109" load-source="docdb">BE</country><country mxw-id="DS660690978" load-source="docdb">BG</country><country mxw-id="DS660693472" load-source="docdb">CH</country><country mxw-id="DS660615485" load-source="docdb">CY</country><country mxw-id="DS660785230" load-source="docdb">CZ</country><country mxw-id="DS660615110" load-source="docdb">DE</country><country mxw-id="DS660615486" load-source="docdb">DK</country><country mxw-id="DS660615495" load-source="docdb">EE</country><country mxw-id="DS660693354" load-source="docdb">ES</country><country mxw-id="DS660691235" load-source="docdb">FI</country><country mxw-id="DS660691236" load-source="docdb">FR</country><country mxw-id="DS660615119" load-source="docdb">GB</country><country mxw-id="DS660615496" load-source="docdb">GR</country><country mxw-id="DS660615120" load-source="docdb">HR</country><country mxw-id="DS660785231" load-source="docdb">HU</country><country mxw-id="DS660693473" load-source="docdb">IE</country><country mxw-id="DS660615497" load-source="docdb">IS</country><country mxw-id="DS660691237" load-source="docdb">IT</country><country mxw-id="DS660615498" load-source="docdb">LI</country><country mxw-id="DS660702118" load-source="docdb">LT</country><country mxw-id="DS660785232" load-source="docdb">LU</country><country mxw-id="DS660702127" load-source="docdb">LV</country><country mxw-id="DS660702128" load-source="docdb">MC</country><country mxw-id="DS660611200" load-source="docdb">MK</country><country mxw-id="DS660611201" load-source="docdb">MT</country><country mxw-id="DS660785233" load-source="docdb">NL</country><country mxw-id="DS660693359" load-source="docdb">NO</country><country mxw-id="DS660693474" load-source="docdb">PL</country><country mxw-id="DS660702130" load-source="docdb">PT</country><country mxw-id="DS660785234" load-source="docdb">RO</country><country mxw-id="DS660702135" load-source="docdb">RS</country><country mxw-id="DS660693479" load-source="docdb">SE</country><country mxw-id="DS660691238" load-source="docdb">SI</country><country mxw-id="DS660693360" load-source="docdb">SK</country><country mxw-id="DS660693480" load-source="docdb">SM</country><country mxw-id="DS660611202" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166480193" lang="EN" load-source="patent-office"><p id="pa01" num="0001">The invention provides a distance measurement method determining the distance of a sensor system to a physical object, comprising the steps of obtaining, from the sensor system, at least a pair of stereoscopic images including the physical object, applying to each element of at least a portion of a first image of the pair of stereoscopic images and to each element of at least a portion of a second image of the pair of stereoscopic images at least two differently shaped and/or sized filters, respectively, determining correlation values for each filter applied to the first and second image, determining combined correlation values for the applied filters by combining the determined correlation values for each applied filter, evaluating the combined correlation values for different disparities for an extremum value of the combined correlation values, calculating a distance value of the sensor system to the physical object based on a disparity value at which the extremum occurs, and outputting the distance value.
<img id="iaf01" file="imgaf001.tif" wi="118" he="97" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166760005" lang="EN" source="EPO" load-source="docdb"><p>The invention provides a distance measurement method determining the distance of a sensor system to a physical object, comprising the steps of obtaining, from the sensor system, at least a pair of stereoscopic images including the physical object, applying to each element of at least a portion of a first image of the pair of stereoscopic images and to each element of at least a portion of a second image of the pair of stereoscopic images at least two differently shaped and/or sized filters, respectively, determining correlation values for each filter applied to the first and second image, determining combined correlation values for the applied filters by combining the determined correlation values for each applied filter, evaluating the combined correlation values for different disparities for an extremum value of the combined correlation values, calculating a distance value of the sensor system to the physical object based on a disparity value at which the extremum occurs, and outputting the distance value.</p></abstract><description mxw-id="PDES98404894" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><p id="p0001" num="0001">The invention relates to an image processing method for processing preferably stereoscopic images and an optical (visual) sensor system, especially a camera system using this method. Furthermore, the invention relates to a vehicle, especially a ground, air or sea vehicle or a robotic device, comprising the sensor system adapted to determine/calculate the distances from the sensor system to a physical object, and/or may also be used in determining/calculating optical flow from images/an image stream provided by the optical sensor system.</p><p id="p0002" num="0002">The invention especially relates to the field of stereoscopic vision, which is used in many autonomous or semi-autonomous systems including Advanced Driver Assistance Systems (ADAS), such as in-vehicle navigation systems, adaptive cruise control (ACC), lane departure warning systems, lane change assistance, collision avoidance systems (or pre-crash systems), intelligent speed adaptation or intelligent speed advice (ISA), night vision, adaptive light control, pedestrian protection systems, automatic parking, traffic sign recognition, blind spot detection, driver drowsiness detection, vehicular communication systems, and/or hill descent control, etc.</p><p id="p0003" num="0003">Stereoscopic vision allows for the estimation of distances by using two or more sensors and images derived therefrom. Image parts or patches of one camera are correlated with image parts or patches of images of one or more other cameras. The difference in position of the physical object in the correlating image parts directly relates to the distance of the object from the camera. Generally, close objects have a large difference in position in the compared image parts while far away objects have a small difference in position. An advantage over other distance measurement means is that energy efficient sensors such as cameras can be used. Using stereoscopic vision is also beneficial as stereoscopic vision sensor systems allow to scale as stereo cameras can be used for any distance by altering the baseline (i.e. distance between the cameras).</p><p id="p0004" num="0004">The sensor system according to the invention hence comprises at least two optical sensors, such as cameras (CCD, CMOS, ...), laser scanners, infrared sensors, etc. The visual sensor produces images and sends these images to a processing unit, e.g. as a stream of images.<!-- EPO <DP n="2"> --></p><p id="p0005" num="0005">The processing unit processes the images and derives image information from the images provided by the two sensors. The processing unit may be part of the sensor system, but may also be separate from the sensor system. For example, an image stream can be supplied from a camera-based stream recording system to the processing unit for processing.</p><p id="p0006" num="0006">Known image parts or patch-matching stereo methods suffer from bad correlations when the fronto-parallel assumption is violated or when the texture information is low. Two frame stereoscopic correspondence methods usually work with a rectified image pair, and typically exploit the fronto-parallel assumption or frontal parallel plane assumption either explicitly or implicitly.</p><p id="p0007" num="0007">In particular, this assumption assumes that position disparity (or depth) is constant (with respect to the rectified stereo image pair or image part/patch pair) over a region under consideration. However, physical objects may possess surfaces rich in shape, which generically violates the frontal parallel plane assumption. This is explained with reference to <figref idrefs="f0001">Fig. 1</figref>: For a regular surface <i>S <sub>3</sub></i>, the tangent plane <i>T<sub>p</sub>(S)</i> (in solid lines) at a point <i>p S</i> is well defined. Traditional stereoscopic correspondence methods use the frontal parallel plane (in dotted lines) to represent the (local) surface geometry at <i>p</i>, which, however, is incorrect. In <figref idrefs="f0001">Fig. 1</figref>, the sensors <i>C<sub>l</sub></i> and <i>C<sub>r</sub></i> are shown, which refer to a left (<i>l</i>) and right (<i>r</i>) camera.</p><p id="p0008" num="0008">This invention improves block-matching stereo matching by combining the matching value of differently shaped and sized matching filters in a multiplicative manner, where a block-matching method is a way of locating matching blocks in a sequence of digital video image frames, e.g. for the purposes of motion estimation. The purpose of a block-matching method is to find a matching block from a frame <i>i</i> in some other frame <i>j</i>, which may appear before or after <i>i</i>. Block-matching methods make use of an evaluation metric to determine whether a given block in frame <i>j</i> matches the search block in frame <i>i</i>. In the following, the term frame is used analogous with image patch, part, (sub-)window, or portion, where a block is also referred to as a filter of essentially rectangular shape.</p><p id="p0009" num="0009">Known approaches are described e.g. in <patcit id="pcit0001" dnum="EP2386998A1"><text>EP2 386 998 A1</text></patcit>, which describes a robust matching measure: the summed normalize cross-correlation (SNCC), which can be used for patch-matching correlation searches. One application of this is for example the stereoscopic depth computation from stereo images.<!-- EPO <DP n="3"> --></p><p id="p0010" num="0010">The paper "<nplcit id="ncit0001" npl-type="s"><text>A taxonomy and evaluation of dense two-frame stereo correspondence algorithms" by Scharstein and Szeliski (2002, International Journal of Computer Vision, 47(1-3):7-42</text></nplcit>) in an overview shows the most common stereo computation methods used in the art.</p><p id="p0011" num="0011">In "<nplcit id="ncit0002" npl-type="s"><text>Non-parametric Local Transforms for Computing Visual Correspondence" (1994, Proceedings of the third European conference on Computer Vision, Vol. II) Zabih and Woodfill </text></nplcit>introduce the rank and census transform for images in order to improve patch correlation. It is proposed to match rank transformed images with summed absolute or squared difference and census transformed images with the hamming distance.</p><p id="p0012" num="0012">Finally, in "<nplcit id="ncit0003" npl-type="s"><text>Real-Time Correlation-Based Stereo Vision with Reduced Border Errors" (2002, International Journal of Computer Vision) Hirschmüller, Innocent and Garibaldi </text></nplcit>describe a multi-window block-matching stereo approach where a larger correlation window is partitioned into equal-shaped sub-windows. For each disparity the correlation values of the sub-window are sorted and only the n best sub-windows are used for calculating the overall window correlation value in order to reduce border effects. The correlation is computed by summed absolute difference within each sub-window and the overall window cost is computed by summing up the correlation values of the n best sub-windows.</p><p id="p0013" num="0013">The invention hence provides a solution as claimed with the independent claims. Further aspects of the invention are detailed in the dependent claims. In particular, the invention provides a method and a system as set out in the independent claims.</p><p id="p0014" num="0014">In a first aspect, the invention provides a distance measurement method determining the distance of a sensor system to a physical object, comprising the steps of obtaining, from the sensor system, at least a pair of stereoscopic images including the physical object, applying to each element of at least a portion of a first image of the pair of stereoscopic images and to each element of at least a portion of a second image of the pair of stereoscopic images at least two differently shaped and/or sized filters, respectively, determining correlation values for each filter applied to the first and second image, determining combined correlation values for the applied filters by combining the determined correlation values for each applied filter, evaluating the combined correlation values for different disparities for an extremum value of the combined correlation values, calculating a distance value of the sensor system to the physical object based on a disparity value at which the extremum occurs, and outputting the distance value.<!-- EPO <DP n="4"> --></p><p id="p0015" num="0015">The sensor system can comprise at least two visual and/or optical sensors, especially at least one of the sensors being of a camera, a radar sensor, a lidar sensor, an infrared sensor, or a thermal sensor.</p><p id="p0016" num="0016">The filters may be essentially rectangular and in particular can be elongated along one spatial direction/along one dimension of an image, especially vertically or horizontally.</p><p id="p0017" num="0017">The correlation values may be normalized, e.g. the correlation values of each filter for one element may be normalized by the sum of all correlation values of the filter for the element before the combination of the filter correlation values.</p><p id="p0018" num="0018">The correlation values can be computed by means of normalized cross-correlation, summed normalized cross-correlation, hamming distance of census transformed images or absolute difference of rank transform images.</p><p id="p0019" num="0019">The correlation values of the different filters can be weighted, e.g. by means of exponentiation.</p><p id="p0020" num="0020">The elements may be pixels.</p><p id="p0021" num="0021">The sensor system can comprise more than two sensors supplying more than two images and one sensor can be used as a reference sensor.</p><p id="p0022" num="0022">The extremum especially is a maximum.</p><p id="p0023" num="0023">The combination of the filter correlation values can be a multiplicative combination.</p><p id="p0024" num="0024">The sensor may be a passive sensor, especially a optical sensor.</p><p id="p0025" num="0025">The images can be images supplied in a sequence of images provided by the sensor system and wherein the method can be executed for a plurality of images in the sequence. The sequence of images e.g. is an image stream supplied by the sensors of the senor system.</p><p id="p0026" num="0026">In another aspect, the invention provides a sensor system comprising at least an sensor system adapted to supply at least a pair of stereoscopic images, the system furthermore<!-- EPO <DP n="5"> --> comprising means for obtaining, from the sensor system, at least a pair of stereoscopic images including a physical object, means configured for applying to each element of at least a portion of a first image of the pair of stereoscopic images and to each element of at least a portion of a second image of the pair of stereoscopic images at least two differently shaped and/or sized filters, respectively, means configured for determining correlation values for each applied filter to the first and second image, means configured for determining combined correlation values for the applied filters by combining the determined correlation values for each applied filter, means configured for evaluating the combined correlation values for different disparities for an extremum value of the combined correlation values, means configured for calculating a distance value of the sensor system to the physical object based on a disparity value at which the extremum occurs, and means configured for outputting the distance value.</p><p id="p0027" num="0027">In yet another aspect, the invention provides a sensor system as described herein, wherein the sensor system is adapted to perform a method as previously described.</p><p id="p0028" num="0028">Further, the invention provides a land, air, sea or space vehicle equipped with such a sensor system. The vehicle may be a robot or a motorcycle, a scooter, other 2-wheeled vehicle, a passenger car or a lawn mower.</p><p id="p0029" num="0029">In still another aspect, the invention provides a vehicle driver assistance system including the sensor system performing the method as previously described.</p><p id="p0030" num="0030">Finally, the invention provides a computer program product performing, when executed on a computer, the method as previously described.</p><p id="p0031" num="0031">The invention is also described with reference to the figures:
<dl id="dl0001"><dt>Fig. 1</dt><dd>illustrates the fronto-parallel assumption.</dd><dt>Fig. 2</dt><dd>illustrates a block-matching with a square filter.</dd><dt>Fig. 3</dt><dd>illustrates a block-matching with a filter elongated into one spatial direction of the stereoscopic images, e.g. a horizontal filter.<!-- EPO <DP n="6"> --></dd><dt>Fig. 4</dt><dd>illustrates correlation values of a disparity search, i.e. for different disparities.</dd><dt>Fig. 5</dt><dd>illustrates correlation values of a disparity search for a horizontal filter (top), correlation values of a disparity search for a vertical filter (middle), and combined correlation values (bottom).</dd><dt>Fig. 6</dt><dd>schematically shows a vehicle using the disclosed system and method.</dd></dl></p><p id="p0032" num="0032">A filter typically refers to a number and an extension of pixels that is regarded when determining whether a currently regarded pixel (which is the base entity a digital image is composed of) or area in/of one image part/patch is similar/identical to a pixel or area in/of another image part/patch.</p><p id="p0033" num="0033">The core of the invention is to improve the depth estimation performance or depth estimation in stereo or multi-sensor systems or the optical flow in multi-image systems by a multiplicative combination of multiple matching filters of different sizes and/or shapes for the correspondence search.</p><p id="p0034" num="0034">For estimating depth from stereo sensor images it is necessary to find correlating image pixels in at least two sensor images, e.g. the left and right camera image.</p><p id="p0035" num="0035">A multi sensor system is similar in the sense that either each pair of sensors can be used as a stereo sensor or that one sensor is defined as a reference sensor and all other sensors are treated like the second sensor of a stereo sensor system. This means that correlating pixels are either searched in each sensor image pair or between the reference sensor images and the images of all other sensors.</p><p id="p0036" num="0036">After pixel correlations have been found the depth can be easily calculated from the distance between the correlating pixels. The distance between correlating pixels is called disparity and is measured in a number of pixels, i.e. if the correlating pixels are 5 pixels apart they have a disparity of 5.</p><p id="p0037" num="0037">The depth is computed by the simple formula:<!-- EPO <DP n="7"> --> <maths id="math0001" num=""><math display="block"><mi mathvariant="italic">depth</mi><mo>=</mo><mfrac><mi mathvariant="italic">fb</mi><mi mathvariant="italic">disparity</mi></mfrac></math><img id="ib0001" file="imgb0001.tif" wi="46" he="19" img-content="math" img-format="tif"/></maths><br/>
where f is the focal length of the sensor and b is the baseline. The baseline is the (3D) distance between the at least two sensors.</p><p id="p0038" num="0038">Unfortunately, finding single pixel correlations is quite demanding. Therefore an area (patch, part, portion or window) around each pixel is used for finding correlations. If this area has a rectangular shape this approach is called block-matching. Since the patch correlations are typically computed by means of image filtering the patches shape and size are often referred to as filter shape and size, respectively.</p><p id="p0039" num="0039">A major problem in finding correlations of patches is that this constitutes an inherent assumption that the depth (or disparity) values of all pixels within that patch are the same because only pixels from the same depth are depicted in the same spatial arrangement in the stereo images. Since the scene observed by the (stereo) sensor system consists of many surfaces that are not fronto-parallel, the assumption is violated quite often. In these cases the correlations computed with patches are poor and thus are hard to detect.</p><p id="p0040" num="0040"><figref idrefs="f0002">Fig. 3</figref> shows a simple example of stereo images from a car. The street is strongly slanted and thus the spatial arrangement of the pixels changes. Correlating patches from the left image (e.g. a 3x3 patch around the pixel R) will fail as the spatial layout of the pixels is different in the right image.</p><p id="p0041" num="0041">However, if a different filter is used such that the pixels inside the patch do not change their spatial arrangement, the correlation would succeed.</p><p id="p0042" num="0042"><figref idrefs="f0002">Fig. 3</figref> shows another example using a horizontally elongated filter.</p><p id="p0043" num="0043">The downside of a horizontally elongated filter (in particular a filter which extends into on spatial direction more than into others) is that it is less suitable for upright objects. Such objects result in slanted surfaces in the image, like fences, and that it yields noisy results for thin upright objects like trees or traffic sign posts. Such structures are best be matched with a vertically elongated filter.<!-- EPO <DP n="8"> --></p><p id="p0044" num="0044">Hence the invention uses differently shaped (and/or sized) filters, e.g. one square one horizontal and one vertical filter (wherein a horizontal/vertical filter is an essentially rectangular filter with a pronounced extension and into one spacial direction). The goal now is to find out which filter is best suited at each image position.</p><p id="p0045" num="0045">Unfortunately, it is difficult to use the correlation value <i>C<sub>d</sub></i> of the different filters to select the best filter. The correlation values <i>C<sub>d</sub></i> are defined in the following. The correlation values of filters with a different number of pixel-elements are usually not comparable (even after normalization) because filters with a lower number of pixels have a tendency to have better correlation values than filters with a larger number of pixels. The reason for this is pure statistics: The more pixels a filter encompasses the more likely it is that a pixels is wrongly matched, leading to a decrease in correlation.</p><p id="p0046" num="0046">The correlation value of a filter, i.e. of an image patch in one image and another image patch in the other image, is: <maths id="math0002" num=""><math display="block"><msub><mi>C</mi><mi>d</mi></msub><mo>=</mo><mstyle displaystyle="false"><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder></mstyle><msub><mi>c</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub></mstyle></math><img id="ib0002" file="imgb0002.tif" wi="36" he="23" img-content="math" img-format="tif"/></maths></p><p id="p0047" num="0047">Where <i>C<sub>d</sub></i> is the aggregated filter (or patch) matching cost for disparity <i>d</i> and <i>c<sub>i,d</sub></i> is the pixel-level cost of pixel <i>i</i> in the left patch and its corresponding pixel in the right patch (or the other way around). In parallel stereo camera setups corresponding pixels are typically on the same image line: <maths id="math0003" num=""><math display="block"><msub><mi>c</mi><mrow><mi>i</mi><mo>,</mo><mi>d</mi></mrow></msub><mo>=</mo><msub><mi>f</mi><mi>c</mi></msub><mo>⁢</mo><mfenced separators=""><mi>p</mi><mo>⁢</mo><msup><mfenced><msub><mi>x</mi><mi>i</mi></msub><msub><mi>y</mi><mi>i</mi></msub></mfenced><mi>L</mi></msup><mo>,</mo><mi>p</mi><mo>⁢</mo><msup><mfenced separators=""><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mi>d</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mfenced><mi>R</mi></msup></mfenced></math><img id="ib0003" file="imgb0003.tif" wi="86" he="17" img-content="math" img-format="tif"/></maths><br/>
where <i>f<sub>c</sub></i> is the pixel-level matching cost function, <i>p</i>(<i>x<sub>i</sub></i>,<i>y<sub>i</sub></i>)<i><sup>L</sup></i> is pixel <i>i</i> in the left image and <i>p</i>(<i>x<sub>i</sub></i> - <i>d</i>, <i>y<sub>i</sub></i>)<i><sup>R</sup></i> corresponding pixel in the right image.</p><p id="p0048" num="0048">The idea now is to merge the filter correlation values <i>C<sub>d</sub></i> of the differently shaped and sized filters <i>i</i> in a statistical way. When looking at the filter correlations for the whole disparity search range we get a distribution like the one shown in <figref idrefs="f0002">Fig. 4</figref>. This means that the correlation values of differently shaped filters <i>i</i> can be combined by multiplying their<!-- EPO <DP n="9"> --> distributions. Since the single values of the distribution are independent this means that the single correlation values can be multiplied for combination into an overall measure: <maths id="math0004" num=""><math display="block"><msubsup><mi>C</mi><mi>d</mi><mo>*</mo></msubsup><mo>=</mo><mstyle displaystyle="true"><munder><mo>∏</mo><mi>i</mi></munder></mstyle><msubsup><mi>C</mi><mi>d</mi><mi>i</mi></msubsup></math><img id="ib0004" file="imgb0004.tif" wi="36" he="23" img-content="math" img-format="tif"/></maths></p><p id="p0049" num="0049">The advantage of such a combination is that filters having a bad shape for a certain image part typically have a flat distribution for that image part because no good matches can be found. On the other hand filters having a favorable shape will give a strong peak. This is illustrated schematically in <figref idrefs="f0003">Fig. 5</figref> where the correlation distribution of a horizontally elongated filter and a vertically elongated filter are shown for an image position where a vertical structure like a tree trunk is seen. Furthermore, <figref idrefs="f0003">Fig. 5</figref> depicts the multiplicatively combined distribution (bottom).</p><p id="p0050" num="0050">At a vertical structure a horizontal filter (top distribution) will have a very flat distribution because the filter shape does not fit very well to the scene structure while a vertical filter (middle distribution) will have a prominent peak at the corresponding vertical structure position in the other image.</p><p id="p0051" num="0051">Since the distribution of the horizontal filter is very flat, the peak in the distribution of the vertical filter will dominate the combined (multiplicative) result. In general when combining multiple filters this approach leads to an implicit selection of the best filter shape since bad-fitting filters yield a flat distribution and only good fitting filters yield a prominent peak.</p><p id="p0052" num="0052">Another additional or alternative advantageous combination is that of small and large filters. Large filters yield stable results in weakly textured regions due to their larger integration area while small filters give very noisy results, i.e. no clear peak. On the other hand, large filters have very wide and small peaks at small objects which leads to unstable results and a fattening effect (disparity values get smeared to neighboring pixels) while small filter have a very strong, sharp peak for these small objects. Of course it is also advantageous to combine more filters, e.g. a vertical, a horizontal, a small squared and/or a large squared filter. This way multiple scene structures can be robustly correlated.<!-- EPO <DP n="10"> --></p><p id="p0053" num="0053">Depending on the correlation measure used, it might be necessary to normalize the correlation values in order to make them real probabilities. For doing so each correlation value of a distribution may be divided by the sum of the whole distribution. <maths id="math0005" num=""><math display="block"><msubsup><mi>C</mi><mi>d</mi><mo>*</mo></msubsup><mo>=</mo><mstyle displaystyle="true"><munder><mo>∏</mo><mi>i</mi></munder></mstyle><mfenced><mfrac><msubsup><mi>C</mi><mi>d</mi><mi>i</mi></msubsup><mrow><msub><mi mathvariant="normal">Σ</mi><mi>d</mi></msub><mo>⁢</mo><msubsup><mi>C</mi><mi>d</mi><mi>i</mi></msubsup></mrow></mfrac></mfenced></math><img id="ib0005" file="imgb0005.tif" wi="51" he="24" img-content="math" img-format="tif"/></maths></p><p id="p0054" num="0054">Of course this requires calculating the whole distributions before. If, however, corresponding pixels are selected by means of finding the best correlation value then the normalization is not necessary because it does not change the maximum value.</p><p id="p0055" num="0055">On the other hand it is sometimes favorable to weight certain filters over others in order to ensure a good estimation for certain scene structures. For example for the road area detection in car scenarios it is important to capture the (3D) structure of the street. In order to ensure good depth estimation for the street a horizontal filter is weighted more strongly than other filters. Since the filter responses are multiplied, a weighting has to be done by means of exponentiation, <maths id="math0006" num=""><math display="block"><msubsup><mi>C</mi><mi>d</mi><mo>*</mo></msubsup><mo>=</mo><mstyle displaystyle="true"><munder><mo>∏</mo><mi>i</mi></munder></mstyle><msup><mfenced><msubsup><mi>C</mi><mi>d</mi><mi>i</mi></msubsup></mfenced><msub><mi>w</mi><mi>i</mi></msub></msup></math><img id="ib0006" file="imgb0006.tif" wi="45" he="22" img-content="math" img-format="tif"/></maths><br/>
where <i>w<sub>i</sub></i> is the weight that is applied to filter <i>i</i>.</p><p id="p0056" num="0056">In a similar fashion the method can be used for block-matching optical flow. The only difference is that the disparity distributions are two-dimensional because optical flow correlations are searched in both x and y direction.</p><p id="p0057" num="0057">Generally, a computer-implemented method for finding correlations between images is provided, in which at least two images are received from at least one vision or optical sensor, wherein each sensor supplies at least one image. For a set of pixels in one image correlations are computed to find corresponding pixels in the other images, or image parts or patches, respectively. For each of the pixels from the pixel set at least two differently shaped and/or sized filters are used to compute correlations in the other images. Combined correlation values are computed from the at least two differently shaped and sized filters by<!-- EPO <DP n="11"> --> multiplication. Corresponding pixels in the other images are found by analyzing the combined correlation values.</p><p id="p0058" num="0058">The method may be used to calculate stereoscopic depth and/or to calculate optical flow. The analyzing of the combined correlation value is a maximum selection. The at least two differently shaped and/or sized filters can include a horizontally elongated filter, a vertically elongated filter and a square filter. A square filter essentially extends into at least two spatial directions to the same degree. The at least two differently shaped and/or sized filters may include filters with essentially the same shape but different sizes. The correlation values of each filter for one pixel can be normalized by the sum of all correlation values of that filter for that pixel before the multiplicative combination of the filter correlation values. The correlation values may be computed by means of normalized cross-correlation, summed normalized cross-correlation, hamming distance of census transformed images or absolute difference of rank transform images.</p><p id="p0059" num="0059">The correlation values of the different filters can be weighted, e.g. by means of exponentiation.</p><p id="p0060" num="0060">The invention may be employed in a robot, land, air, sea or space vehicle preferably equipped with a system, especially a depth estimation, motion estimation, object detection or object tracking system, performing the method comprising at least one optical or visual sensor, in particular for depth estimation a stereo camera, and a computing unit. The robot can be a robotic lawn mower, a car or a motorcycle.</p><p id="p0061" num="0061">Generally, possible applications are driver assistant systems like collision warning, lane departure warning or cruise control. For example the improvement of the depth perception of the ground allows for using the depth data to detect drivable areas which then can be used as lane information in case no or only partial lane markings are available. Another application field is in robotics systems, where the improved depth estimation is used for object detection. Another application is an autonomous lawn mower. Here the improved depth perception of the ground allows for an accurate obstacle detection which can then be avoided without using the bump sensor.</p><p id="p0062" num="0062">In contrast to the prior art, the invention uses a multiplicative combination of differently shaped and sized filters. Also the invention does not subdivide a correlation window into sub-windows but integrates the correlations of single independent filters or patches. The sub-windows<!-- EPO <DP n="12"> --> in the prior art have strongly different anchor points while according to the invention the independent filters share the same anchor point. Furthermore, the sub-windows in the prior art are equal sized while the invention explicitly uses different sized and shaped filters. The major reason for the equal sized sub-windows is the sorting step used for selecting the n-best sub-windows. This strongly limits the prior art approach. In contrast, the invention overcomes that limitation by using the multiplicative combination without sorting which corresponds to a statistical integration.</p><p id="p0063" num="0063">The invention combines small filters and large filters, which leads to a robust matching in weakly textured regions due to the contribution of the large filters while the fattening effect (spatial depth smearing) is kept at a minimal level due to the contribution of the small filters. Also the additional and/or alternative combination of vertically and horizontally elongated filters lead to a robust matching at vertical structures due to the vertical filter and to a robust matching at horizontal structures due to the horizontal filter.</p><p id="p0064" num="0064"><figref idrefs="f0003">Fig. 6</figref> schematically depicts an autonomous or partially autonomous vehicle 1, which e.g. moves from a starting point to a destination without planned intervention by a passenger of the autonomous vehicle. On the movement path from the starting point to the destination, the vehicle preferably automatically adapts its movement path to traffic conditions en route.</p><p id="p0065" num="0065">In order to perceive its environment, the vehicle 1 typically comprises a number of sensors sensing the environment but at least a visual or optical sensor system 2, which comprises at least a stereoscopic sensor system. Basically, sensing in this case means that the vehicle 1 processes data supplied by the sensors 2 in a processing unit 3 to derive parameters symbolizing aspects of the environment. Together, the derived parameters form a virtual model of the vehicle's view of the environment.</p><p id="p0066" num="0066">The vehicle 1 continuously monitors the parameters and makes decisions based on the parameters, i.e. the result of a calculation or parameter comparison leads to a result which leads to an execution of a specified process. In this case, especially the distance to physical objects in the environment of the vehicle 1 is monitored, and resulting parameters indicative of distance information are evaluated. A decision is made, when specific constraints or thresholds are reached by the parameters.</p><p id="p0067" num="0067">The vehicle 1 typically comprises actuators for actuating steering, for accelerating or decelerating (braking) the vehicle and/or for communicating with the passengers. After a<!-- EPO <DP n="13"> --> decision is made, i.e. a process is started, the autonomous vehicle 1 actuates the actuators in accordance with steps, calculations and/or comparisons specified in the respective process.</p><p id="p0068" num="0068">At least some of the optical sensors can be cameras, which are used to generate the image sequence for calculating the optical flow in order to enhance navigation and to avoid objects/obstacles in the movement path of the autonomous vehicle 1.</p><p id="p0069" num="0069">In order to process obtained information (observations), the inventive method and system may use and include analysis means employing the processing module 3 and/or apply neural networks, which can generally be used to infer functions from observations. Neural networks allow working with none or only little a priori knowledge on a problem to be solved and also show a failure tolerant behavior. Problems that may be addressed relate, e.g., to feature identification, control (vehicle control, process control), decision making, machine vision and/or pattern recognition (facial recognition, object recognition, gesture recognition, speech recognition, character and text recognition), etc. A neural network thereby consists of a set of neurons and a set of synapses. The synapses connect neurons and store information in parameters called weights, which are used in transformations performed by the neural network and learning processes.</p><p id="p0070" num="0070">Typically, to make an observation, an input signal or input pattern, e.g. digital image information, is accepted from the detection means 2 which is then processed using hardware units and/or software components. An output signal or output pattern is obtained, which may serve as input to other systems for further processing, e.g. for visualization purposes. As an output signal, e.g. the distance to an object can be output.</p><p id="p0071" num="0071">The input signal, which may also include information on detected features influencing movement, may be supplied by one or more sensors, e.g. the mentioned visual or optical detecting means 2, but also by a software or hardware interface. The output pattern may as well be output through a software and/or hardware interface or may be transferred to another processing module 3 or actor, e.g. a powered steering control or a brake controller, which may be used to influence the actions or behavior of the vehicle.</p><p id="p0072" num="0072">Computations and transformations required by the invention, necessary for evaluation, processing, maintenance, adjustment, and also execution (e.g. of movement change commands or actuation commands) may be performed by a processing module 3 such as one or more processors (CPUs), signal processing units or other calculation, processing or<!-- EPO <DP n="14"> --> computational hardware and/or software, which might also be adapted for parallel processing. Processing and computations may be performed on standard off the shelf (OTS) hardware or specially designed hardware components. A CPU of a processor may perform the calculations and may include a main memory (RAM, ROM), a control unit, and an arithmetic logic unit (ALU). It may also address a specialized graphic processor, which may provide dedicated memory and processing capabilities for handling the computations needed.</p><p id="p0073" num="0073">Also data memory is usually provided. The data memory is used for storing information and/or data obtained, needed for processing, determination and results. The stored information may be used by other processing means, units or modules required by the invention. The memory also allows storing or memorizing observations related to events and knowledge deducted therefrom to influence actions and reactions for future events.</p><p id="p0074" num="0074">The memory may be provided by devices such as a hard disk (SSD, HDD), RAM and/or ROM, which may be supplemented by other (portable) memory media such as floppy disks, CD-ROMs, tapes, USB drives, smartcards, pendrives etc. Hence, a program encoding a method according to the invention as well as data acquired, processed, learned or needed in/for the application of the inventive system and/or method may be stored in a respective memory medium.</p><p id="p0075" num="0075">In particular, the method described by the invention may be provided as a software program product on a (e.g., portable) physical memory medium which may be used to transfer the program product to a processing system or a computing device in order to instruct the system or device to perform a method according to this invention. Furthermore, the method may be directly implemented on a computing device or may be provided in combination with the computing device.</p><p id="p0076" num="0076">It should be understood that the foregoing relates not only to embodiments of the invention and that numerous changes and modifications made therein may be made without departing from the scope of the invention as set forth in the following claims.<!-- EPO <DP n="15"> --></p><heading id="h0001"><b><u>Nomenclature</u></b></heading><heading id="h0002"><u>Stereo Camera:</u></heading><p id="p0077" num="0077">A stereo camera is a type of camera with two lenses with a separate image sensor for each lens. Often, a stereo camera actually consists of two separate cameras attached to a rig. In this case the cameras might either be fixed or movable. In the fixed case the cameras are usually aligned with image sensors being coplanar (parallel setup). In the movable case such a stereo camera is usually used to mimic the vergence movement of human eyes.</p><heading id="h0003"><u>Fattening effect:</u></heading><p id="p0078" num="0078">This effect occurs when computing disparity (or depth) maps by means of the correspondence search between the two cameras of a stereo camera setup. The most prominent variant is the foreground fattening. In this case disparity values of foreground pixels are smeared over background pixels leading to wrong disparities for the background pixels near occlusion borders. Since objects in a disparity map look like they become bigger this effect is referred to as fattening.</p><heading id="h0004"><u>Matching Window:</u></heading><p id="p0079" num="0079">Also referred to as (matching) patch or (matching) filter. In stereo processing this describes a small subpart (image patch) of an image. Typically, a matching window from one image is compared to a matching window of the same size and shape of another image. Quite often multiple of these comparisons are done together by means of image filtering; hence the windows themselves are often referred to as filters. Consequently, filter size, window size and patch size are also the same, i.e. the size of the matching window.</p></description><claims mxw-id="PCLM90459845" lang="EN" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-en-0001" num="0001"><claim-text>Distance measurement method determining the distance of an sensor system to a physical object, comprising the steps of:
<claim-text>- obtaining, from the sensor system, at least a pair of stereoscopic images including the physical object,</claim-text>
<claim-text>- applying to each element of at least a portion of a first image of the pair of stereoscopic images and to each element of at least a portion of a second image of the pair of stereoscopic images at least two differently shaped and/or sized filters, respectively,</claim-text>
<claim-text>- determining correlation values for each filter applied to the first and second image,</claim-text>
<claim-text>- determining combined correlation values for the applied filters by combining the determined correlation values for each applied filter,</claim-text>
<claim-text>- evaluating the combined correlation values for different disparities for an extremum value of the combined correlation values,</claim-text>
<claim-text>- calculating a distance value of the sensor system to the physical object based on a disparity value at which the extremum occurs, and</claim-text>
<claim-text>- outputting the distance value.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>Method according to claim 1, wherein the sensor system comprises at least two visual and/or optical sensors, especially at least one of the sensors being of a camera, a radar sensor, a lidar sensor, an infrared sensor, or n thermal sensor.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>Method according to claim 1 or 2, wherein<br/>
the filters are essentially rectangular and in particular are elongated along one spatial direction/along one dimension of an image, especially vertically or horizontally.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>Method according to any one of the preceding claims, wherein the correlation values are normalized, e.g. the correlation values of each filter for one element are normalized by the sum of all correlation values of the filter for the element before the combination of the filter correlation values.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>Method according to any one of the preceding claims, wherein the correlation values are computed by means of normalized cross-correlation, summed normalized cross-correlation,<!-- EPO <DP n="17"> --> hamming distance of census transformed images or absolute difference of rank transform images.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>Method according to any one of the preceding claims, wherein the correlation values of the different filters are weighted, e.g. by means of exponentiation.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>Method according to any one of the preceding claims, wherein the elements are pixels.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>Method according to any one of the preceding claims, wherein the sensor system comprises more than two sensors supplying more than two images and wherein one sensor used as a reference sensor.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>Method according to any one of the preceding claims, wherein the extremum is a maximum.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>Method according to any one of the preceding claims, wherein the combination of the filter correlation values is a multiplicative combination.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>Method according to any one of the preceding claims, wherein the sensor is a passive sensor, especially a optical sensor.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>Method according to any one of the preceding claims, wherein the images are images supplied in a sequence of images provided by the sensor system and wherein the method is executed for a plurality of images in the sequence.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>A sensor system comprising at least an sensor system adapted to supply at least a pair of stereoscopic images, the system furthermore comprising:
<claim-text>- means for obtaining, from the sensor system, at least a pair of stereoscopic images including a physical object,</claim-text>
<claim-text>- means configured for applying to each element of at least a portion of a first image of the pair of stereoscopic images and to each element of at least a portion of a second image of the pair of stereoscopic images at least two differently shaped and/or sized filters,</claim-text>
<claim-text>- means configured for determining correlation values for each applied filter to the first and second image,<!-- EPO <DP n="18"> --></claim-text>
<claim-text>- means configured for determining combined correlation values for the applied filters by combining the determined correlation values for each applied filter,</claim-text>
<claim-text>- means configured for evaluating the combined correlation values for different disparities for an extremum value of the combined correlation values,</claim-text>
<claim-text>- means configured for calculating a distance value of the sensor system to the physical object based on a disparity value at which the extremum occurs, and</claim-text>
<claim-text>- means configured for outputting the distance value.</claim-text></claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>The sensor system according to claim 13, wherein the sensor system is adapted to perform a method according to any one of the claims 1-12.</claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>A land, air, sea or space vehicle or a robotic device equipped with a sensor system according to claim 13 or 14.</claim-text></claim><claim id="c-en-0016" num="0016"><claim-text>The vehicle according to claim 15, wherein the vehicle is a robot, a motorcycle, a scooter, other 2-wheeled vehicle, a passenger car, a plane, a sea vehicle or a lawn mower.</claim-text></claim><claim id="c-en-0017" num="0017"><claim-text>A vehicle driver assistance system including the sensor system according to any one of the claims 13 or 14 and/or performing the method according to any one of the claims 1-12.</claim-text></claim><claim id="c-en-0018" num="0018"><claim-text>A computer program product performing, when executed on a computer, the method according to any one of the claims 1 to 12.</claim-text></claim></claims><drawings mxw-id="PDW20422556" load-source="patent-office"><!-- EPO <DP n="19"> --><figure id="f0001" num="1,2"><img id="if0001" file="imgf0001.tif" wi="165" he="205" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="20"> --><figure id="f0002" num="3,4"><img id="if0002" file="imgf0002.tif" wi="165" he="202" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="21"> --><figure id="f0003" num="5,6"><img id="if0003" file="imgf0003.tif" wi="165" he="226" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="159" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="159" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
