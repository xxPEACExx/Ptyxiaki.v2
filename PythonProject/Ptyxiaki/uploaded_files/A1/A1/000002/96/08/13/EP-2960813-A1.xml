<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960813-A1" country="EP" doc-number="2960813" kind="A1" date="20151230" family-id="53502395" file-reference-id="318616" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451464" ucid="EP-2960813-A1"><document-id><country>EP</country><doc-number>2960813</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15001836-A" is-representative="YES"><document-id mxw-id="PAPP193865896" load-source="patent-office" format="original"><country>EP</country><doc-number>15001836.4</doc-number><date>20150622</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865897" load-source="docdb" format="epo"><country>EP</country><doc-number>15001836</doc-number><kind>A</kind><date>20150622</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162031252" ucid="US-201414316460-A" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201414316460</doc-number><kind>A</kind><date>20140626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988524094" load-source="docdb">G06F  17/30        20060101AFI20151026BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1984696964" load-source="docdb" scheme="CPC">G06F  17/30463     20130101 LI20160106BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697175" load-source="docdb" scheme="CPC">G06F  17/30445     20130101 FI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984701584" load-source="docdb" scheme="CPC">G06F  17/30864     20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984701930" load-source="docdb" scheme="CPC">G06F  17/30474     20130101 LI20151231BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545858" lang="DE" load-source="patent-office">OPTIMIERUNG DER PARALLELISIERUNG VON BENUTZERDEFINIERTEN FUNKTIONEN MIT FLEXIBLER PARTITIONIERUNG</invention-title><invention-title mxw-id="PT165545859" lang="EN" load-source="patent-office">OPTIMIZATION OF PARALLELIZATION OF USER-DEFINED FUNCTIONS WITH FLEXIBLE PARTITIONING</invention-title><invention-title mxw-id="PT165545860" lang="FR" load-source="patent-office">OPTIMISATION DE LA PARALLÉLISATION DE FONCTIONS DÉFINIES PAR L'UTILISATEUR AVEC UN PARTITIONNEMENT FLEXIBLE</invention-title><citations><non-patent-citations><nplcit><text>CHAUDHURI S ED - ASSOCIATION FOR COMPUTING MACHINERY: "AN OVERVIEW OF QUERY OPTIMIZATION IN RELATIONAL SYSTEMS", PROCEEDINGS OF THE 1998 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA; [PROCEEDINGS OF THE ACM SIGACT-SIGMOD-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS],, 1998, pages 34 - 43, XP000782631, ISBN: 978-0-89791-996-8, DOI: 10.1145/275487.275492</text><sources><source mxw-id="PNPL69101527" load-source="docdb" name="SEA" category="I"/></sources></nplcit><nplcit><text>None</text><sources><source mxw-id="PNPL62638791" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103332130" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SAP SE</last-name><address><country>DE</country></address></addressbook></applicant><applicant mxw-id="PPAR1103310067" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SAP SE</last-name></addressbook></applicant><applicant mxw-id="PPAR1101652821" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SAP SE</last-name><iid>101471558</iid><address><street>Dietmar-Hopp-Allee 16</street><city>69190 Walldorf</city><country>DE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103309945" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>GROSSE PHILIPP</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103313994" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>GROSSE, PHILIPP</last-name></addressbook></inventor><inventor mxw-id="PPAR1101648315" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>GROSSE, PHILIPP</last-name><address><street>c/o SAP SE, Global IP Group, Dietmar-Hopp-Allee 16</street><city>69190 Walldorf</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103336699" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>LEHNER WOLFGANG</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103308112" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>LEHNER, WOLFGANG</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647134" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>LEHNER, WOLFGANG</last-name><address><street>c/o SAP SE, Global IP Group, Dietmar-Hopp-Allee 16</street><city>69190 Walldorf</city><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103311607" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>MAY NORMAN</last-name><address><country>DE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103326592" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>MAY, NORMAN</last-name></addressbook></inventor><inventor mxw-id="PPAR1101652727" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>MAY, NORMAN</last-name><address><street>c/o SAP SE, Global IP Group, Dietmar-Hopp-Allee 16</street><city>69190 Walldorf</city><country>DE</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101652340" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Müller-Boré &amp; Partner Patentanwälte</last-name><iid>100060440</iid><address><street>Friedenheimer Brücke 21</street><city>80639 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660610392" load-source="docdb">AL</country><country mxw-id="DS660685032" load-source="docdb">AT</country><country mxw-id="DS660610394" load-source="docdb">BE</country><country mxw-id="DS660783438" load-source="docdb">BG</country><country mxw-id="DS660606539" load-source="docdb">CH</country><country mxw-id="DS660690313" load-source="docdb">CY</country><country mxw-id="DS660685033" load-source="docdb">CZ</country><country mxw-id="DS660610403" load-source="docdb">DE</country><country mxw-id="DS660690314" load-source="docdb">DK</country><country mxw-id="DS660690331" load-source="docdb">EE</country><country mxw-id="DS660687704" load-source="docdb">ES</country><country mxw-id="DS660783439" load-source="docdb">FI</country><country mxw-id="DS660783440" load-source="docdb">FR</country><country mxw-id="DS660610404" load-source="docdb">GB</country><country mxw-id="DS660690332" load-source="docdb">GR</country><country mxw-id="DS660610405" load-source="docdb">HR</country><country mxw-id="DS660685034" load-source="docdb">HU</country><country mxw-id="DS660606540" load-source="docdb">IE</country><country mxw-id="DS660690333" load-source="docdb">IS</country><country mxw-id="DS660783441" load-source="docdb">IT</country><country mxw-id="DS660690334" load-source="docdb">LI</country><country mxw-id="DS660610538" load-source="docdb">LT</country><country mxw-id="DS660685039" load-source="docdb">LU</country><country mxw-id="DS660610547" load-source="docdb">LV</country><country mxw-id="DS660610548" load-source="docdb">MC</country><country mxw-id="DS660685427" load-source="docdb">MK</country><country mxw-id="DS660685428" load-source="docdb">MT</country><country mxw-id="DS660685040" load-source="docdb">NL</country><country mxw-id="DS660610406" load-source="docdb">NO</country><country mxw-id="DS660690339" load-source="docdb">PL</country><country mxw-id="DS660610550" load-source="docdb">PT</country><country mxw-id="DS660685041" load-source="docdb">RO</country><country mxw-id="DS660610563" load-source="docdb">RS</country><country mxw-id="DS660690340" load-source="docdb">SE</country><country mxw-id="DS660610564" load-source="docdb">SI</country><country mxw-id="DS660610415" load-source="docdb">SK</country><country mxw-id="DS660690341" load-source="docdb">SM</country><country mxw-id="DS660685429" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479834" lang="EN" load-source="patent-office"><p id="pa01" num="0001">Technologies are disclosed for generating query execution plans optimized for parallel execution for programs having both core database relational functions and user-defined functions. A variety of optimization strategies can be employed to improve performance in a parallel execution scenarios. A flexible range of permitted partition arrangements can be specified as acceptable to parallelized instances of the user-defined function. The optimizer can leverage such information when constructing an optimized query execution plan. Partitioning arrangements or other properties can be leveraged to avoid additional or unnecessary processing.
<img id="iaf01" file="imgaf001.tif" wi="78" he="100" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759646" lang="EN" source="EPO" load-source="docdb"><p>Technologies are disclosed for generating query execution plans optimized for parallel execution for programs having both core database relational functions and user-defined functions. A variety of optimization strategies can be employed to improve performance in a parallel execution scenarios. A flexible range of permitted partition arrangements can be specified as acceptable to parallelized instances of the user-defined function. The optimizer can leverage such information when constructing an optimized query execution plan. Partitioning arrangements or other properties can be leveraged to avoid additional or unnecessary processing.</p></abstract><description mxw-id="PDES98404535" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">BACKGROUND</heading><p id="p0001" num="0001">Today's computing environment is often characterized by large data sets. Although impressive technologies have evolved to efficiently handle such large data sets, software developers continue to push the envelope to new limits. For example, systems already have techniques for handling built-in database operations on large data sets, but often falter when presented with custom functionality.</p><p id="p0002" num="0002">Thus, there is a need for technologies to better address processing large data sets with custom functionality.</p><heading id="h0002">SUMMARY</heading><p id="p0003" num="0003">The summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. The summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><p id="p0004" num="0004">An embodiment can be implemented as a system comprising: one or more processors; memory coupled to the one or more processors; a stored representation of a software program, wherein the representation of the software program comprises both one or more user-defined functions and one or more other operations, and the representation of the software program indicates at least one property for a user-defined function out of the user-defined functions and at least one property for an operation out of the other operations, wherein the operation comprises another user-defined function or a core relational database operation; and an optimizer configured to transform the representation of the software program into an optimized query execution plan optimized for parallel execution according to the user-defined function property and the operation property.<!-- EPO <DP n="2"> --></p><p id="p0005" num="0005">An embodiment can be implemented as a method comprising receiving a representation of one or more user-defined functions and one or more core database relational operations, wherein the representation comprises at least one property for at least one of the user-defined functions and at least one property for at least one of the core database relational operations; and generating a query execution plan optimized for parallel execution of the one or more user-defined functions and the one or more core database relational operations according to the user-defined function property and the core database relational operation property.</p><p id="p0006" num="0006">An embodiment can be implemented as one or more computer-readable media comprising computer-executable instructions causing a computing system to perform a method comprising receiving a representation of a program comprising a core database relational operation and a user-defined function coupled to the at least one core database relational operation, wherein the representation comprises an input property specifying a range of permitted partition arrangements as an input pre-condition for parallelized instances of the user-defined function, and the representation comprises an output property specifying a partition arrangement post-condition for the core database relational operation; detecting a mismatch between the pre-condition and the post-condition in the representation of the program; responsive to detecting the mismatch between the pre-condition and the post-condition, considering a plurality of possible exchange operations remedying the mismatch that satisfy the input pre-condition for parallelized instances of the user-defined function; and incorporating the exchange operation into an optimized query execution plan implementing the program, whereby the optimized query execution plan avoids a full merge and initial partitioning operation to remedy the mismatch.</p><heading id="h0003">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0007" num="0007"><ul><li><figref idrefs="f0001">FIG. 1</figref> is a block diagram of an example system implementing optimized parallel execution of a program including one or more user-defined operations and one or more core database relational operations.<!-- EPO <DP n="3"> --></li><li><figref idrefs="f0002">FIG. 2</figref> is a flowchart of an example method of implementing optimized parallel execution of a program including one or more user-defined operations and one or more core database relational operations.</li><li><figref idrefs="f0003">FIG. 3</figref> is a block diagram of an example query execution plan optimization system integrating a variety of information to generate an optimized query execution plan.</li><li><figref idrefs="f0004">FIG. 4</figref> is a flowchart of an example method generating an optimized query execution plan.</li><li><figref idrefs="f0005">FIG. 5</figref> is a Venn diagram of an example indication of a range of permitted partition arrangements.</li><li><figref idrefs="f0006">FIG. 6</figref> is a block diagram of an example relationship between source code annotations and ultimate executing instances of a user-defined function resulting from the annotations.</li><li><figref idrefs="f0007">FIG. 7</figref> is a flowchart of an example method of constraining a query execution plan based on annotations.</li><li><figref idrefs="f0008">FIG. 8</figref> is a block diagram of an example query execution plan optimization system taking a representation of a program as input.</li><li><figref idrefs="f0009">FIG. 9</figref> is a flowchart of an example method of generating an optimized query execution plan based on represented properties of a user-defined function and a core database relational operation.</li><li><figref idrefs="f0010">FIG. 10</figref> is a block diagram of an exemplary output-input coupling for a representation of two operations in a program under optimization.</li><li><figref idrefs="f0011">FIG. 11</figref> is a flowchart of an example method of generating a query execution plan comprising an alternative to a full merge and initial partition.</li><li><figref idrefs="f0012">FIG. 12</figref> is a block diagram of example data processing with user-defined functions and JOINs.</li><li><figref idrefs="f0013">FIG. 13</figref> is pseudo code for a script that has been annotated according to the technologies described herein.</li><li><figref idrefs="f0014">FIG. 14</figref> is pseudo code for an algorithm that optimizes a program representation.</li><li><figref idrefs="f0015">FIG. 15</figref> is graph showing scale out of <i>Test Sample</i> (Join - UDF - Join) with different execution plans (for a single host).<!-- EPO <DP n="4"> --></li><li><figref idrefs="f0016">FIG. 16</figref> is graph showing scale out of <i>Test Sample</i> (Join - UDF - Join) with different execution plans (for a four-node cluster).</li><li><figref idrefs="f0017">FIG. 17</figref> is pseudo code for a script that has been annotated according to the technologies described herein.</li><li><figref idrefs="f0018">FIG. 18</figref> is pseudo code for another script that has been annotated according to the technologies described herein.</li><li><figref idrefs="f0019">FIG. 19</figref> depicts a generalized example of a suitable computing environment in which the described innovations may be implemented.</li><li><figref idrefs="f0020">FIG. 20</figref> is an example cloud-support environment that can be used in conjunction with the technologies described herein.</li></ul></p><heading id="h0004">DETAILED DESCRIPTION</heading><heading id="h0005"><b>Example 1-Overview</b></heading><p id="p0008" num="0008">The technologies described herein can be used for a variety of query execution plan optimization scenarios, and adoption of the technologies can provide improved techniques for parallel processing of large data sets with a mixture of user-defined functions and known relational operation.</p><p id="p0009" num="0009">To the degree that an execution environment treats a user-defined function as a black box, it can be difficult to optimize execution of the user-defined function because the details of the required pre-conditions, ensured post-conditions, and behavior of the user-defined function are not known. Annotations can be particularly helpful in this regard because the software developer can annotate the user-defined function to indicate required, ensured, or behavioral properties. Such properties can then be used during optimization of a software program incorporating the user-defined function, leading to improved parallel execution performance.</p><p id="p0010" num="0010">By whatever way such properties of user-defined functions are determined or acquired, they can be incorporated into a stored representation of the software program. The stored representation can then be processed for optimization, resulting in a query execution plan optimized for parallel execution.<!-- EPO <DP n="5"> --></p><p id="p0011" num="0011">A wide variety of properties can be supported, leading to intelligent decisions by the optimizer during the optimization process. In some cases, a flexible range of properties can be specified. Optimization can consider multiple arrangements within such a range, leading to superior performance.</p><p id="p0012" num="0012">Rather than simply applying full merge and initial partition for a user-defined function, an optimizer can break such an isolated view and take surrounding operations and their structural properties into account. For example, partial partitioning and partial merging can be supported.</p><p id="p0013" num="0013">Thus, the minimal and maximal partitioning supported by a user-defined function can be specified, allowing flexibility during optimization, which can result in higher performance during parallel execution of the user-defined function.</p><p id="p0014" num="0014">Other annotations can be supported, such as expected grouping and ordering, ensure key, preserving order, approximation of size, approximation of run time, and the like.</p><p id="p0015" num="0015">Alternatives to full merge and initial partitioning can also be supported during optimization. For example, tuples can be reshuffled, existing partitioning (e.g., from a relational operation) can be used (e.g., by a user-defined function). Existing partitioning can also be reused when entering a loop and across loop iterations.</p><p id="p0016" num="0016">As described herein, the technologies can be employed by software developers to improve the performance of programs that incorporate user-defined functions. End users also benefit because the programs exhibit improved performance.</p><p id="p0017" num="0017">Various other features can be implemented and combined as described herein.</p><heading id="h0006"><b>Example 2 - Example System Implementing Optimized Parallel Execution Technologies</b></heading><p id="p0018" num="0018"><figref idrefs="f0001">FIG. 1</figref> is an illustrative overview of an example system 100 implementing optimized parallel execution of a program including one or more user-defined operations and one or more core database relational operations. In <figref idrefs="f0001">FIG. 1</figref>, an execution environment 120 can accommodate execution of a program that comprises both one or more user-defined functions and one or more core database relational operations. As described herein, flexible partitioning arrangements can be considered during optimization.<!-- EPO <DP n="6"> --></p><p id="p0019" num="0019">The technologies can process one or more annotations 130 that are associated with source code defining a user-defined function that indicate one or more properties 135 of the user-defined function that can be processed to optimize parallelized execution as described herein. For example, an optimizer 150 can take the annotations 130 as input and generate a query execution plan 155 that is optimized for parallelized execution. The optimizer 150 can take advantage of properties of the user-defined function that are indicated by the annotations 130. Although shown as part of the execution environment 120, the optimizer 150 can be implemented outside of, or partially outside of the environment 120.</p><p id="p0020" num="0020">During execution, the input data 110 comprising one or more input tables 115 is divided into different partitions 140A-N and handed to parallelized instances 160A-N of a user-defined function, which produce results 170A-N that are then eventually merged into one or more output tables 185 that are part of the output data 180.</p><p id="p0021" num="0021">For ease of illustration, the partition operations, merge operations, and core database relational operations are not explicitly shown. In practice, there can be numerous such operations and numerous output-input couplings between user-defined functions and core database relational operations that involve partition and/or merge operations. The partition and merge arrangements can be implemented in a wide variety of ways and take any number of forms, and processing can be substantially different than those shown merely for illustrative purposes. For example, initial partitioning can be avoided, deferred, or partially leveraged as described herein. Similarly, merging output can also be avoided, deferred, or partially leveraged as described herein. Such processing can be based on properties of the user-defined functions (e.g., as indicated in the annotations 130).</p><p id="p0022" num="0022">Although input tables 115 and output tables 185 are shown as separate in the illustration, in practice, tables can be modified in place, partially modified in place, cached, stored in memory, or the like. For example, in-memory columnar database processing can be supported for superior performance.</p><p id="p0023" num="0023">In practice, the systems shown herein, such as system 100 can vary in complexity, with different functionality, components of differing complexity, and the like. For example, in practice, the execution environment 120 can comprise a variety of other functionality not shown<!-- EPO <DP n="7"> --> to address locality of information, synchronization, security, and the like. In practice, a large number of tables, some with large numbers of records can be supported.</p><p id="p0024" num="0024">Although various components of the systems herein are shown as a single component, in practice, the boundaries between components can be changed. For example, in practice, the execution environment 120 can be implemented across one or more machines, virtual or physical. Functionality can be distributed among such machines (e.g., to clients, server, or the like) as desired. Additional features relating to security, load balancing, and redundancy can also be included.</p><p id="p0025" num="0025">The system 100, any of the other systems described herein, and subsets of such systems can be implemented in conjunction with any of the hardware components described herein, such as the computing systems described below (e.g., processing units, memory, and the like). In any of the examples herein, the inputs, outputs, source code, annotations, databases, program representations, query execution plans, and the like can be stored in one or more computer-readable storage media or computer-readable storage devices. The technologies described herein can be generic to the specifics of operating systems or hardware and can be applied in any variety of environments to take advantage of the described features.</p><heading id="h0007"><b>Example 3 - Example Method Implementing Optimized Parallel Execution Technologies</b></heading><p id="p0026" num="0026"><figref idrefs="f0002">FIG. 2</figref> is a flowchart of an example method 200 of implementing optimized parallel execution of a program including one or more user-defined operations and one or more core database relational operations and can be implemented, for example, in the system shown in <figref idrefs="f0001">FIG. 1</figref>. As described herein, flexible partitioning arrangements can be considered during generation of an optimized query execution plan and leveraged during execution. As with the other methods described herein, the order of the acts can be changed while still implementing the described technologies.</p><p id="p0027" num="0027">At 210, a query execution plan optimized for parallel execution of the program is generated based on properties of a user-defined function according to the techniques described<!-- EPO <DP n="8"> --> herein. As shown herein, such properties can be derived from annotations associated with source code defining user-defined functions of the program.</p><p id="p0028" num="0028">At 220, the optimized query execution plan is implemented to achieve execution of the software program. At 230, during execution, input tables for the user-defined function are divided into partitions. Such a process is sometimes called "initial partitioning." At 240, the partitions are submitted to respective parallelized instances of the user-defined function as input. The user-defined function then performs processing on its respective partition.</p><p id="p0029" num="0029">At 250, the results from the parallelized instances of the user-defined functions are received.</p><p id="p0030" num="0030">At 260, the results are combined (e.g., via a merge operation).</p><p id="p0031" num="0031">The partitioning and merging operations can be performed according to the optimized query execution plan, which depends on the properties of the user-defined function, which themselves can be derived from source code annotations of the user-defined function.</p><p id="p0032" num="0032">Although a partition (e.g., before execution) and full merge (e.g., after execution) can be done in some cases for a set of parallelized instances of a user-defined function, the technologies described herein can take surrounding operations and their structural properties into account to avoid, leverage, or defer such processing. For example, initial partitioning may be omitted in some cases if the output of a previous user-defined function or core database relational operation result is already suitably partitioned.</p><p id="p0033" num="0033">Similarly, besides partition arrangements, other conditions of the data (e.g., sorting, grouping, ordering or the like) can similarly be indicated by properties to avoid, leverage, or defer processing such as sorting, grouping, ordering or the like.</p><p id="p0034" num="0034">The method 200 and any of the other methods described herein can be performed by computer-executable instructions (e.g., causing a computing system to perform the method) stored in one or more computer-readable media (e.g., storage or other tangible media) or stored in one or more computer-readable storage devices.<!-- EPO <DP n="9"> --></p><heading id="h0008"><b>Example 4 - Example Core Database Relational Operations</b></heading><p id="p0035" num="0035">In any of the examples herein, core database relational operations (or "operators") can take the form of any of a variety of relational operations performed on databases in a relational context. For example, JOIN, UNION, PROJECT, AGGREGATE, and the like and variations thereon (e.g., OUTER JOIN, UNION ALL, and the like) can be supported in an execution environment. In practice, optimization of such built-in functions can be incorporated into a compiler or interpreter based on intimate knowledge of how such functions work, characteristics of the system, characteristics of the data, and the like. An optimizer can thus parallelize such operations based on such knowledge.</p><p id="p0036" num="0036">For example, although properties of such operations are not shown explicitly in source code, such properties are known to the system and can be considered by an optimizer as described herein in conjunction with properties of user-defined functions that are determined via other techniques (e.g., via source code annotations as described herein).</p><p id="p0037" num="0037">In practice, such relational operations can be represented in source code using keywords, symbols, or the like that indicate the actual relational operation that is performed during execution.</p><heading id="h0009"><b>Example 5 - Example User-Defined Functions</b></heading><p id="p0038" num="0038">In any of the examples herein, the technologies can support any of a wide variety of user-defined functions to extend built-in functionality of a data management system. In practice, such functions can take the form of whatever processing is desired by a software developer that falls outside of core database relational operations. Because the system does not have knowledge about the internal workings of such functions, it essentially treats a user-defined function as a black box. However, via the annotations and/or properties described herein, pertinent knowledge about the pre-conditions, post-condition, and behavior of such user-defined functions can be communicated to the optimizer, which can take such information into account when optimizing for parallel execution.<!-- EPO <DP n="10"> --></p><p id="p0039" num="0039">Thus, the "user" in user-defined function can be a user in a role outside of the core database developers.</p><p id="p0040" num="0040">Any of a wide variety of functionality can be incorporated into user-defined functions. For example, even non-deterministic functions (e.g., incorporating statistical sampling), non-standard statistical processing, or the like can be accommodated.</p><heading id="h0010"><b>Example 6 - Example Optimization</b></heading><p id="p0041" num="0041">In any of the examples herein, although the terms "optimization," "optimizer," "optimal," or "superior" are used, the arrived at execution or query execution plan need not strictly take the form of the single most superior arrangement. Optimization can set a goal of improving performance in certain scenarios (e.g., parallel execution) given a set of constraints, properties, available hardware, and the like.</p><p id="p0042" num="0042">In practice, a plurality of alternatives are considered, and the optimizer chooses a preferred alternative (e.g., improved performance) in light of (e.g., based on) information available to the optimizer (e.g., properties, optimization factors, and the like).</p><p id="p0043" num="0043">Although improved performance is often measured in execution time, other metrics relating to computing resources can be improved, such as memory utilized, machines used, or the like.</p><heading id="h0011"><b>Example 7 - Example Software Program</b></heading><p id="p0044" num="0044">In any of the examples herein, the software program being optimized can take a wide variety of forms. Such a program is sometimes called a "workflow," "dataflow," or the like. In practice, such programs are typically written by developers as source code, and such source code can include the annotations described herein. However, the technologies can be applied to situations in which properties for user-defined functions are known or determined, whether or not they appear in source code annotations.</p><p id="p0045" num="0045">In some examples, the program comprises at least one core database relational operation and at least one user-defined function. As shown herein, actual implementations can have any<!-- EPO <DP n="11"> --> number of operations that are combined in a variety of ways. Any implementation in the context of a data management system can benefit.</p><p id="p0046" num="0046">Although examples are shown in particular programming languages, the technologies can operate independently of the source code language.</p><heading id="h0012"><b>Example 8 - Example Parallel Execution</b></heading><p id="p0047" num="0047">In any of the examples herein, the software program can be executed using parallel execution techniques. For example, plural cores in a single machine, plural machines, or the like can be used. Machines can be physical or virtual (e.g., that ultimately execute on physical hardware). The techniques described herein can be applied across a variety of parallel execution arrangements. Testing has shown that parallelization can make execution time much faster if execution is parallelized.</p><heading id="h0013"><b>Example 9 - Example Optimization Factors</b></heading><p id="p0048" num="0048">In any of the examples herein, parallel execution can be optimized based on a variety of factors in addition to processing time and memory consumption. For example, factors such as number of cores, number of nodes (e.g., machines), processing cost, and processing size can be included. Specifics such as table size can also be considered. As described herein, annotations can provide the optimizer with size and run time knowledge (e.g., relative to input size).</p><p id="p0049" num="0049">An optimizer can implement a cost-based decision making process to choose a degree of parallelism.</p><heading id="h0014"><b>Example 10 - Example Properties</b></heading><p id="p0050" num="0050">In any of the examples herein, one or more properties can be associated with a user-defined function. For example, a stored representation of a program comprising the user-defined function can include such properties and be used during optimization for parallel execution. Annotations described herein can be used in source code of the user-defined function to indicate such properties.<!-- EPO <DP n="12"> --></p><p id="p0051" num="0051">A wide variety of properties can be supported. Properties can be broadly divided into categories such as pre-condition properties, post-condition properties, and behavior properties (e.g., compiler or optimizer hints).</p><p id="p0052" num="0052">Pre-conditions can include a pre-condition for input to the user-defined function (e.g., parallelized instances of the user-defined function). For example, a pre-condition property can indicate a partition property expected by the user-defined function. As described herein, the technologies support specifying a flexible range of properties for a user-defined function (e.g., via an annotation); the range is then included in a stored representation of the associated software program and respected in the resulting optimized query execution plan.</p><p id="p0053" num="0053">Partition properties can include global structural properties that are expected to hold true across parallelized instances of the user-defined function (e.g., globally across the instances).</p><p id="p0054" num="0054">Other pre-conditions for input to the user-defined function (e.g., parallelized instances of the user-defined functions) can include local structural properties that define how data within each partition is to be organized. Examples include grouping and sorting properties. For example, a property can indicate that the data submitted to a particular (e.g., "local") instance is expected to be sorted.</p><p id="p0055" num="0055">Post-conditions can include a post-condition for output of a user-defined function (e.g., parallelized instances of the user defined function). For example, a post-condition property can indicate a grouping property, sorting property, or other condition that is guaranteed or ensured to be true by the user-defined function for its output (e.g., local sorting is guaranteed by the user-defined function).</p><p id="p0056" num="0056">Post-conditions can also indicate whether a condition of the data is modified by the user-defined function (e.g., a parallelized instance). For example, a preserve property for ordering can specify that the user-defined function guarantees that the data remains in the same order (e.g., if the data is sorted at input, it will continue to be sorted at output).</p><p id="p0057" num="0057">Post-conditions can also specify an expected merge function for combining results of parallelized instances. For example, it can be specified whether order should be preserved, duplicates should be removed, or the like. A custom (e.g., user-defined) merge function can be specified if desired standard alternatives such as UNION ALL are not desired.<!-- EPO <DP n="13"> --></p><p id="p0058" num="0058">Behavior properties can include whether the user-defined function is deterministic, an expected size of the output, an expected run time for the user-defined function, and the like.</p><p id="p0059" num="0059">Similar properties can also be associated with core database relational operations as described herein for use when optimizing a query execution plan. In contrast to properties of user-defined functions, properties for sorting and partitioning core database relational operations are typically known by and built in to the optimization system (e.g., annotations are not needed).</p><heading id="h0015"><b>Example 11 - Example Source Code Annotations</b></heading><p id="p0060" num="0060">In any of the examples herein, source code annotations can be used to specify properties of user-defined functions. Such an approach can be helpful during optimization because the software developer's knowledge of the user-defined function can be passed to the optimizer for consideration during the optimization process.</p><p id="p0061" num="0061">A wide variety of annotations indicating a wide variety of properties of user-defined functions can be supported. Like properties, annotations can be divided into categories such as pre-conditions (e.g., for input), post-conditions (e.g., for output), and behavior. An annotation can thus be used to specify any of the properties described herein.</p><p id="p0062" num="0062">For example, a source code annotation can specify a pre-condition (e.g., for input tables) for parallelized instances of a user-defined input. The format of the annotation can include an indication of the table involved (e.g., an input table, output table, or the like).</p><p id="p0063" num="0063">As described herein, an annotation (e.g., pre-condition) can specify a range of acceptable partition arrangements (e.g., for input tables) via an explicit upper bound and an explicit lower bound. Examples herein refer to such bounds as max or MAXPART and min or MINPART, where MAXPART explicitly specifies a maximum permitted partition arrangement, and MINPART explicitly specifies a minimum permitted partition arrangement. Keywords such as NONE, ANY, or a set of one or more column names can be used to specify maximum and minimum permissible partition arrangements.</p><p id="p0064" num="0064">The annotation can specify a range in terms of one or more table columns (e.g., by column name) as described herein. The table column can be interpreted as specifying GROUP<!-- EPO <DP n="14"> --> BY partitioning. Such a pre-condition specifies a global structural property that is expected to hold true across parallelized instances of the user-defined function.</p><p id="p0065" num="0065">As described herein, the annotations can be used to construct a representation of the software program incorporating the user-defined function, which is used to generate an optimized query plan that respects the pre-conditions and can leverage the post-conditions and behaviors specified.</p><p id="p0066" num="0066">Source code annotations can also specify an expected grouping or sorting condition for an input table of the user-defined function (e.g., parallelized instances thereof), resulting in association with the corresponding property.</p><p id="p0067" num="0067">Source code annotations can also specify a permitted merge arrangement for combining results of the parallelized instances of the user-defined function, resulting in association with the corresponding property.</p><p id="p0068" num="0068">As described for the properties, a source code annotation can specify a grouping or sorting condition for an output table of the user-defined function (e.g., a parallelized instance thereof).</p><p id="p0069" num="0069">Behavior annotations can include an expected size of an output (e.g., in terms of an input size). For example, a factor can be specified. Similarly, an annotation can specify an expected run time of a parallelized instance of the user-defined function in terms of an input size (e.g., via a factor). Factors can be scalar, multiplicative, exponential, or the like.</p><p id="p0070" num="0070">The specific form and syntax of annotations can be modified as desired. In one implementation, the annotations are used at the border of a procedure defining the user-defined function, but a variety of programming languages and formats can be supported.</p><heading id="h0016"><b>Example 12 - Example Query Execution Plan Optimization System</b></heading><p id="p0071" num="0071"><figref idrefs="f0003">FIG. 3</figref> is a block diagram of an example query execution plan optimization system integrating a variety of information to generate an optimized query execution plan. In the example, the optimizer 350 accepts a program 310 comprising one or more user-defined<!-- EPO <DP n="15"> --> functions 315 with annotations and one or more core database relational operations 317 and optimization factors 340 as input.</p><p id="p0072" num="0072">Based on the inputs, the optimizer 350 can generate an optimized query execution plan 380 that is optimized for parallel execution. The optimizer can be configured to take into account properties associated with user-defined functions to optimize parallel execution of the program 310 via the optimized query execution plan 380.</p><p id="p0073" num="0073">As described herein, the optimizer 350 can be configured to already have detailed knowledge of the relational operations 317 that is sufficient to optimize parallel execution of the operations 317 in a variety of scenarios involving tables of large size. The annotations for the user-defined functions 315 can also inform the optimizer how to generate the plan 380, even though the exact functionality of the user-defined functions 315 remains unknown by the optimizer 350. For example, properties of the user-defined functions can be determined via the annotations, and a wide variety of annotations and properties can be supported, including a flexible range of partition arrangements as described herein.</p><p id="p0074" num="0074">As described herein, the optimizer 350 can store an internal representation of the program 310 that includes a representation of the couplings and properties of the operations. The plan 380 can then be generated based on the internal representation.</p><p id="p0075" num="0075">In any of the examples herein, the plan 380 can then be executed in an execution environment to accomplish processing of the program 310.</p><heading id="h0017"><b>Example 13 - Example Query Execution Plan Optimization System</b></heading><p id="p0076" num="0076"><figref idrefs="f0004">FIG. 4</figref> is a flowchart of an example method 400 generating an optimized query execution plan and can be implemented, for example, in a system such as that shown in <figref idrefs="f0003">FIG. 3</figref>.</p><p id="p0077" num="0077">At 410, one or more annotations for a user-defined function are received. For example, for source code defining an executable program comprising both one or more relational operations and one or more user-defined functions, one or more source code annotations for a user-defined function appearing in the source code can be received. At least one of the source code annotations can specify a property for a parallelized instance of the user-defined function<!-- EPO <DP n="16"> --></p><p id="p0078" num="0078">(e.g., a global structural property or a local structure property). Further properties as described herein can be included.</p><p id="p0079" num="0079">At 420, at least one of the one or more relational operations (e.g., indications of such operations) of the executable program is received. After receipt, a representation of the relational operation can be stored for consideration by an optimizer. The relational operation can have an output-input coupling with the user-defined function as described herein.</p><p id="p0080" num="0080">At 430, based on the relational operation, the source code annotation, and the user-defined function, a query execution plan optimized for parallel execution according to the property can be generated as described herein. In practice, a plurality of properties, including known properties of the relational operation, can be considered. As described herein, such a plan can be generated via a stored representation of an input program. If desired, a canonical query execution plan can be generated and then optimized (e.g., based on the properties indicated by the annotations).</p><p id="p0081" num="0081">Although the example shows a scenario involving at least one user-defined function and at least one relational operation, the optimization technologies herein can be applied to various other combinations of user-defined functions and relational operations (e.g., scenarios comprising a relational operation to user-defined function coupling, a user-defined function to relational operation coupling, a user-defined function to user-defined function coupling, or the like).</p><p id="p0082" num="0082">The query execution plan can then be implemented. Implementation can include executing the parallelized instances of the user-defined function, other operations, or both. In some cases, the optimizer may find that parallelization is not appropriate.</p><heading id="h0018"><b>Example 14 - Example Range of Permitted Partition Arrangements</b></heading><p id="p0083" num="0083"><figref idrefs="f0005">FIG. 5</figref> is a Venn diagram of an example indication of a range of permitted partition arrangements. As shown by the diagram, there is a set of possible 500 partitioning arrangements, some of which are permitted 560 and some of which are impermissible 510, 570. A "permitted" arrangement is one that is expected (e.g., required) by a function, such as the user-defined<!-- EPO <DP n="17"> --> functions described herein. If the input to the function is not within the permitted range, correct operation of the function is not guaranteed (e.g., the input data may break the logic of the user-defined function).</p><p id="p0084" num="0084">The technologies herein can accommodate a range of permitted partition arrangements 560 defined by an upper bound 520, sometimes called "max" and a lower bound 530, sometimes called "min." Such bounds can be specified explicitly and define a flexible range of permitted partition arrangements. In annotations, explicit specification can be by keyword, one or more column names, or both (e.g., "maxPart(fileID1)" to specify an upper bound in terms of a column). A table name can also be included to indicate for which table the partition range is being specified.</p><p id="p0085" num="0085">Under one perspective, the upper bound 520 can be considered as defining to what degree records having the same values (e.g., for a column or permutations for a plurality of columns) must be kept together in a same partition. Thus, an impermissible partitioning arrangement 510 is one that does not keep the records sufficiently together (e.g., they are all in separate partitions). However, as long as like values (e.g., for a column) are kept together in like partitions, further keeping together is permitted (e.g., the records can all be in one partition).</p><p id="p0086" num="0086">Conversely, the lower bound 530 can be considered as defining to what degree records having the same values must be kept apart. Thus, an impermissible partitioning arrangement 570 is one that does not keep the records sufficiently apart (e.g., they are all in the same partition). However, as long as different values (e.g., for a column) are kept apart in different partitions, further separation (e.g., into single tuples) is permitted.</p><p id="p0087" num="0087">As described herein, if the partitioning arrangement presented to an input of a function is not within the permitted range 560 (e.g., there is a property mismatch), the partitioning arrangement can be remedied. As described herein, having a range of permitted arrangements allows the choice of a remedy that exhibits superior performance. For example, some alternatives may be cheaper, while others are more costly to generate. By way of a general example, if partitioning is course, the partitions can possibly be sub-partitioned. Similarly, partial merges of partitions can result in a desired maximum partitioning. Any number of other remedies can be applied as described herein.<!-- EPO <DP n="18"> --></p><p id="p0088" num="0088">Further details and example formal definitions are included below. The diagram is presented for illustration purposes. Some partition arrangement ranges may not be divided as shown. For example, partition arrangements for plural database columns can be supported as described herein.</p><p id="p0089" num="0089">Such a range can be specified by annotations in source code for a user-defined function and subsequently represented in a stored representation of a program incorporating the user-defined function.</p><p id="p0090" num="0090">The ability to specify a range allows an optimizer flexibility when generating a query execution plan. For example, partition or merge operations can be chosen or avoided based on what partition arrangements are permitted as described herein. Such a flexible range gives the optimizer some degree of freedom when deciding how to partition data.</p><heading id="h0019"><b>Example 15 - Example Relationships between Annotations and Execution</b></heading><p id="p0091" num="0091"><figref idrefs="f0006">FIG. 6</figref> is a block diagram of an example system 600 exhibiting a relationship between source code annotations and ultimate executing instances of a user-defined function resulting from the annotations. In the example, source code 610 comprises a core database relational operation 630 and the user-defined function 620 with an annotation 622 and the actual functionality 624. As shown during execution, the core database relational operation 630 can be in an output-input relationship with the user-defined function 620.</p><p id="p0092" num="0092">The source code serves as structured data causing a compiler 650 (e.g., comprising an optimizer) to generate a query execution plan 660 comprising instances 672A-N of the relational database option 630 and instances 676A-N of the user defined function 620.</p><p id="p0093" num="0093">As described herein, the annotation 622 can specify a range of permitted partition arrangements for parallelized executing instances 676A-N of the user-defined function 620. Such permitted arrangements constrain the query execution plan 660 so that the resulting execution in the environment 670 respected the range (e.g., the partitions 674A-N generated by the relational operation 672A-N are within the specified range).<!-- EPO <DP n="19"> --></p><p id="p0094" num="0094">As described herein, such an annotation can specify a range via a minimum permitted partition arrangement and a maximum permitted partition arrangement. The arrangements can be specified in terms of one or more database columns (e.g., for which a given permutation of values for the columns must be present in the same partition when executing the parallelized instances 676A-N of the user-defined function 620).</p><heading id="h0020"><b>Example 16 - Example Optimization Constraint by Annotations</b></heading><p id="p0095" num="0095"><figref idrefs="f0007">FIG. 7</figref> is a flowchart of an example method of constraining a query execution plan based on annotations.</p><p id="p0096" num="0096">At 710, source code comprising a core database relational operation and a user-defined function coupled to the relational operation are received. An annotation for the user-defined function indicates a range of permitted partition arrangement for inputs to parallelized instances of the user-defined function via an explicit upper boundary of permitted partition arrangements and an explicit lower boundary of permitted partition arrangements.</p><p id="p0097" num="0097">At 730, an optimized, parallelized query execution plan is generated for the source code based on (e.g., constrained by) the annotation as described herein. Such generation can comprise considering a plurality of partition possibilities within the range of permitted partition arrangements inclusively between the explicit upper boundary and the explicit lower boundary and choosing a preferred partition out of the plurality of partition possibilities and an optimal degree of parallelism. For some operations, no parallelism may be specified.</p><p id="p0098" num="0098">As described herein, the range of permitted partition arrangements can be specified in the annotation in terms of a column of an input database table to the user-defined function.</p><heading id="h0021"><b>Example 17 - Example Query Plan Execution Plan Optimization System</b></heading><p id="p0099" num="0099"><figref idrefs="f0008">FIG. 8</figref> is a block diagram of an example query execution plan optimization system 800 taking a representation 810 of a program as input. The system 800 and any of the systems herein can be implemented in hardware (e.g., one or more processors coupled to memory).<!-- EPO <DP n="20"> --></p><p id="p0100" num="0100">In the example, a stored representation 810 of a software program comprises both one or more user-defined functions 820 and one or more core database relational operations 830. The representation operation 810 can indicate at least one property 822 for a user-defined function 820 and at least one property 832 for a core database relational operation 830. The user-defined function property 822 can be determined from annotations described herein. The relational operation property 832 can be determined on known properties of relational operations. As described herein, a property can be specified in terms of a flexible range.</p><p id="p0101" num="0101">The optimizer 850 can be configured to transform the representation 810 into an optimized query execution plan 880 optimized for parallel execution according to the properties 822, 832. The plan 880 can represent loop and/or nested operations as described herein.</p><p id="p0102" num="0102">As described herein, a canonical query execution plan can be used as the program representation 810, which is then transformed via rewrites into the optimized plan 880. Parallelism can be introduced or optimized accordingly. Techniques, including heuristics, rules, substitutions, or other optimization techniques can be applied to arrive at the plan 880. When executed, the plan 880 can instantiate multiple instances of the user-defined function, the relational operation, or both to achieve high performance processing of the represented program.</p><p id="p0103" num="0103">For example, if the representation 810 represents a full merge and initial partitioning operation, the optimizer 850 can rewrite the full merge and initial partitioning operation into one or more rewritten operations as described herein. Such an operation can appear between a first operation with an output having an output property and a second operation with an input having an input property and coupled to the output of the first operation (e.g., the input property specifying a range of permitted partition arrangements). The rewritten operations can be selected based on the output property and the input property. Alternative re-writes can also be chosen based on the optimization factors described herein.</p><p id="p0104" num="0104">The optimizer 850 can support optimization over a range of pre-condition properties, post-condition properties, and behavioral properties as described herein.</p><p id="p0105" num="0105">As described herein, a user-defined function property 822 can include a permitted range of partition arrangements as an input pre-condition when the user-defined function 820 is executed in parallel. The property 822 is thus a global structural property. The optimizer 850 can<!-- EPO <DP n="21"> --> consider a plurality of partition arrangements in the range. The optimizer 850 can be configured to leverage partitioning already performed when within the permitted range of partition arrangements.</p><p id="p0106" num="0106">Other pre-conditions described herein can be in the representation 810. For example, an expected sorting condition for input to parallelized instances of the user-defined functions can appear, and the optimizer 850 can enforce such sorting conditions when generating the plan 880.</p><p id="p0107" num="0107">The property 832 can include a global structural property comprising a post-condition partition arrangement of an output of the core database relational operation 830. Post-conditions for user-defined functions described herein can also be represented. For example, an ensured sorting condition can be indicated by the property 822, and the optimizer 850 can leverage the condition when generating the plan 880.</p><p id="p0108" num="0108">The optimizer can introduce exchange operations at detected property mismatches at input-output couplings in the query execution plan as described herein.</p><p id="p0109" num="0109">The representation 810 can specify a merge technique for parallelized instances of the user-defined function, and the optimizer 850 can apply the specified merge technique when generating the optimized query execution plan 880 for parallel execution. As described herein, the "END PARALLEL" annotation can be used to achieve such an arrangement. The merge technique can comprise user-defined operations that are applied by the optimizer 850 when generating the plan 880.</p><p id="p0110" num="0110">Still further properties, such as an expected size of an output relation (e.g., in terms of an input relation) can be represented. The optimizer 850 can consider the expected size when generating the plan 880. Similarly, run time can also be represented.</p><heading id="h0022"><b>Example 18 - Example Query Plan Execution Plan Optimization Method</b></heading><p id="p0111" num="0111"><figref idrefs="f0009">FIG. 9</figref> is a flowchart of an example method of generating an optimized query execution plan based on represented properties of a user-defined function and a core database relational operation and can be implemented, for example, in a system such as that shown in <figref idrefs="f0008">FIG. 8</figref>.<!-- EPO <DP n="22"> --></p><p id="p0112" num="0112">At 910, a representation of one or more user-defined functions and one or more core database relational operations (e.g., that are part of a software program) can be received. The representation can comprise at least one property for one of the user-defined functions and at least one property for one of the core database relational operations as described herein.</p><p id="p0113" num="0113">As described herein, the representation can be generated based on annotations or other information indicating properties of the operations and functions.</p><p id="p0114" num="0114">At 940, a query execution plan optimized for parallel execution of the user-defined functions and the relational operations can be generated according to the properties. As described herein, an optimizer can select from a plurality of possible execution arrangements based on the properties, optimization factors, and the like. The degree of parallelization can also be controlled as part of optimization. As described herein, a cost-based approach can be used to arrive at an execution arrangement exhibiting superior performance. Generation can comprise rewriting a canonical query execution plan as described herein.</p><p id="p0115" num="0115">In practice, optimization can comprise traversing the representation 910 one or more times, applying recursion, and the like. Traversing can comprise comparing properties of a child plan (e.g., output) with permitted properties of a current operation (e.g., input). In the case of a mismatch, an exchange operation or rewritten exchange operation can be introduced. Alternatives can be considered, and one can be selected based on estimated costs and an explicitly specified range of permitted partition arrangements (e.g., specified in a representation of the program). A full merge and initial partitioning operation to remedy a mismatch can be avoided. An example implementation of optimization is shown at <figref idrefs="f0014">FIG. 14</figref> herein.</p><p id="p0116" num="0116">Subsequently, at 960, the query execution plan can be implemented (e.g., executed) to achieve the processing of the user-defined functions and the core database relational operators.</p><heading id="h0023"><b>Example 19 - Example Exchange Operations</b></heading><p id="p0117" num="0117">In any of the examples herein, an exchange operation can be introduced to remedy a mismatch in properties detected in a representation of the software program during optimization. A basic exchange operation for global structural properties (e.g., partition arrangements)<!-- EPO <DP n="23"> --> comprises a full merge followed by an initial partitioning operation. However, alternative exchange operations can be derived as described herein. Such alternatives include partitioning only, merging only, keeping current partitioning (e.g., which may result in no operation), partial partitioning, or partial merging as described herein. The applicability of such alternatives can depend on the particular situation involved as described herein.</p><p id="p0118" num="0118">Such alternatives are sometimes called "rewrites" because they can be applied to a canonical query execution plan by substituting the alternative for a full merge and initial partitioning operation. However, the technologies can be implemented by skipping the intermediate act of representing the full merge/initial partition.</p><p id="p0119" num="0119">The alternatives can improve performance of the resulting parallelized execution by leveraging, avoiding, or deferring processing, such as partitioning, merging, or the like.</p><heading id="h0024"><b>Example 20 - Example Program Representation for Optimization</b></heading><p id="p0120" num="0120">In any of the examples herein, a representation of a program, user-defined functions, and core database relational operations can be stored internally for manipulation by an optimizer to arrive at an optimized query execution plan. For example, nodes can represent operations, whether they be user-defined functions or relational operations. Edges can represent data flow between the operations. Output-input couplings can be represented as described herein to assist in determining whether there is a mismatch between properties.</p><p id="p0121" num="0121">In the case of annotations, the annotations can be translated into the internal stored representation (e.g., comprising properties for respective annotations) that is then processed to generate the optimized query execution plan.</p><heading id="h0025"><b>Example 21 - Example Output-Input Coupling</b></heading><p id="p0122" num="0122"><figref idrefs="f0010">FIG. 10</figref> is a block diagram of an exemplary output-input coupling 1000 for a representation of two operations in a program under optimization that can be applied to any of the examples herein. Such a coupling 1000 can represent parts of a software program where information is passed between two operations 1030, 1040. The operations can be user-defined<!-- EPO <DP n="24"> --> functions or core database relational operations. Thus, scenarios comprising a relational operation to user-defined function coupling, a user-defined function to relational operation coupling, a user-defined function to user-defined function coupling, or the like can be supported. As described herein, loops and nested couplings can also be supported.</p><p id="p0123" num="0123">Optimization can take advantage of the known properties of the output to satisfy required properties of the input. For example, in any of the examples herein, for a coupling between output of an operation in the software program and an input of another operation (e.g., a user-defined function or core relational database operation), partitioning already performed on the output can be leveraged, deferred, or avoided.</p><p id="p0124" num="0124">In the example, the output 1032 of the first operation 1030 is associated with one or more stored properties 1037 in the software program representation 1020, and the input 1042 of the second operation 1040 is associated with one or more stored properties 1047 in the software program representation 1020. For user-defined functions, the properties 1037, 1047 can be based on annotations or other information as described herein.</p><p id="p0125" num="0125">As described herein, a mismatch between the properties 1037 and 1047 indicates that the data for the output 1032 must be altered before passing to the input 1042. Properties can be specified as a flexible range. For example, if a flexible range of partitioning arrangements are permitted, a mismatch can be avoided by choosing an overlapping permitted arrangement. However, in some cases a mismatch cannot be avoided, or optimization otherwise indicates that additional processing is to be performed before passing data to the second operation 1040.</p><p id="p0126" num="0126">In the case of partitioning arrangements, an exchange operation (e.g., a full merge followed by an initial partition 1080) can be performed to cure mismatches. However, as described herein, other operations such as partial partitioning, partial merge, or the like can be performed instead. In such a case, the resulting query execution plan can avoid the full merge/initial partition sequence and save processing, resulting in improved performance when optimized for parallel execution.</p><p id="p0127" num="0127">Other properties, such as sorting, ordering, and the like can similarly be used when determining how to handle output-input coupling between operations.</p><p id="p0128" num="0128">The optimizer can choose between or among options based on the properties.<!-- EPO <DP n="25"> --></p><p id="p0129" num="0129">Because the properties can differ for different operations, partitioning can be performed differently for a single user-defined function appearing in two different places (e.g., if the user-defined operation is coupled to operations with different output properties).</p><heading id="h0026"><b>Example 22 - Example Query Execution Plan Method with Full Merge/Initial Partition Alternative</b></heading><p id="p0130" num="0130"><figref idrefs="f0011">FIG. 11</figref> is a flowchart of an example method of generating a query execution plan comprising an alternative to a full merge and initial partition and can be implemented, for example, via an arrangement such as that shown in <figref idrefs="f0010">FIG. 10</figref>.</p><p id="p0131" num="0131">At 1110 properties of the output of a first operation are received. Such properties can indicate the structural properties (e.g., guaranteed by the first operation) described herein of output of the first operation.</p><p id="p0132" num="0132">At 1130, the properties for input of a second operation are received. Such properties can indicate the structure properties (e.g., expected by the second operation) described herein of input of the second operation.</p><p id="p0133" num="0133">Assuming there is a mismatch between the properties, at 1140, based on the properties, an alternative to a full merge and initial partition is determined for the output-input coupling of the two operations. Examples of alternatives include doing nothing (e.g., to leverage work already done), performing partitioning only, performing merging only, performing a partial merge, performing a partial re-partition, or the like. The alternative can be chosen based on the properties, cost factors, and the like.</p><p id="p0134" num="0134">At 1150, the alternative is applied in an optimized query execution plan implementing execution of the operations. As described herein, applying the alternative can take the form of re-writing a query execution plan or constructing a query execution plan from a representation of the software program incorporating the operations.<!-- EPO <DP n="26"> --></p><heading id="h0027"><b>Example 23 - Example Large Datasets</b></heading><p id="p0135" num="0135">In any of the examples herein, tables with large data sets can be accommodated. For example, tables having hundreds of thousands, millions, tens or millions, hundreds of millions, or more rows can be accommodated, whether in base tables, relational results between tables, internal tables, or the like.</p><heading id="h0028"><b>Example 24 - Example Alternatives</b></heading><p id="p0136" num="0136">The technologies are not limited to any specific database architecture or storage layout. Any system supporting user-defined functions can implement the technologies described herein.</p><heading id="h0029"><b>Example 25 - Example Implementations</b></heading><p id="p0137" num="0137">Large-scale data analysis relies on custom code both for preparing the data for analysis as well as for the core analysis algorithms. The map-reduce framework offers a simple model to parallelize custom code, but it does not integrate well with relational databases. Likewise, the literature on optimizing queries in relational databases has largely ignored user-defined functions ("UDFs"). Herein are discussed annotations for user-defined functions that facilitate optimizations that both consider relational operators and UDFs. Such an approach can be superior to just linking map-reduce evaluation to a relational database because it enables a broader range of optimizations. Herein are described optimizations that enable the parallel execution of relational operators and UDFs for a number of typical patterns. A study on real-world data investigates the opportunities for parallelization of complex data flows containing both relational operators and UDFs.</p><heading id="h0030"><b>Example 26 -Introduction to Example Implementations</b></heading><p id="p0138" num="0138">Much valuable information is stored in relational databases today. However, analyzing these large data sets is often limited by the expressiveness of SQL. For example, data collected for radio astronomy may require data preprocessing including data cleansing and iterative<!-- EPO <DP n="27"> --> algorithms for deriving good parameters or for analyzing the data. Business applications may analyze customer data for segmentation and classification of customers to derive targeted product offers. In one example application, software predicts the relevant test cases given a code change which also requires the preprocessing steps of the test data, non-trivial data analysis and iterative computation of connected components. In such aforementioned cases there is much data with a rigid schema, and relational databases are a good candidate to store such data.</p><p id="p0139" num="0139">However, it may be desired to implement core parts of the analysis with user-defined functions (UDFs), e.g. the data preparation or iterative algorithms. As optimizers of databases today have limited capabilities to optimize complex queries using UDFs, they are often only used as storage containers, but not considered for the execution of such complex analytical tasks.</p><p id="p0140" num="0140">In recent years, the map-reduce (MR) framework has become popular for large-scale data analysis because it offers a simple model to implement custom code. The MR framework also promises good scalability because the MR runtime handles the parallel execution of independent tasks. However, MR does not integrate well with relational databases where a significant amount of relevant data is stored. Database vendors attempt to remedy this situation by implementing adapters to MR, but this limits the ability for optimizations across relational and custom logic.</p><p id="p0141" num="0141">Large-scale data analysis can incorporate the following. First, it is desirable to use a declarative language. The expected benefits of declarative languages are better productivity and more opportunities to optimize the resulting code. Second, the (family of) languages used to implement large-scale analysis tasks can be expressive. For example, iteration and state maintenance are typically required for analysis tasks. Third, the performance of the analysis tasks can be good. This means that it can be possible to optimize the code including the custom logic expressed in UDFs even if treating the UDF code itself as black box. Considering the size of the data, the optimizations can consider a scale out by parallelizing code and exploiting parallelization even across multiple nodes in the database.</p><p id="p0142" num="0142">Today, large scale data analysis is mainly approached from two directions. On the one side, SQL is used in data warehouse applications for the repeated analysis of well-structured data. In this domain, developers benefit from the high-level declarative query language, which is<!-- EPO <DP n="28"> --> easy to optimize and makes them more productive. However, ad-hoc modes for analysis using complex algorithms are limited by the expressiveness of SQL queries. On the other side, map-reduce offers freedom to implement specialized algorithms. This can come at the cost of manual implementation, testing and tuning. Also, the effectiveness of map-reduce has been questioned, e.g. because the basic map-reduce framework does not consider schema information and because intermediate results are stored on disk. As a middle ground, data-oriented workflow engines seem to evolve.</p><p id="p0143" num="0143">The workflow-oriented approach can be helpful because it can bring the power of parallelizable data-oriented workflows to database technology. The example implementations can contribute the following:
<ul><li>Introduction of a set of UDF annotations describing UDF behavior.</li><li>Demonstration of how parallel UDF execution can be combined with relational database operations.</li><li>Description of plan rewrites and a rewrite strategy to transform an initial sequential plan to a parallelized one.</li><li>Description of how optimization for parallel execution can also be combined with iterative loops.</li></ul></p><p id="p0144" num="0144">In the following section, the preprocessing of an application that predicts the relevant tests for code changes in a large development project based on past test failures. is introduced The example is used later to demonstrate the effectiveness of an optimization and execution strategy. After that the description surveys other work. Another section introduces the UDF annotations upon which the plan optimizations are based. Another section presents a translation process from workflow definition to basic execution plan. Another section presents the rewrites and the rewrite strategy and another section discusses the effectiveness of a strategy based on the example application.<!-- EPO <DP n="29"> --></p><heading id="h0031"><b>Example 27 -Example Application</b></heading><p id="p0145" num="0145">To illustrate the benefit of an integrated optimization and execution of relational operators and user-defined functions, a use case in the form of an application was analyzed. In the test environment, there were exceedingly long turn-around times for testing after changes were pushed to the Git repository via Gerrit. It was desired to only execute the tests that are affected by a code change, starting with the most relevant ones.</p><p id="p0146" num="0146">To tackle the problem a classification model was trained based on the history of test executions stored in the test database. The classification model assigns each regression test a probability of failure given the changed files identified by their file-ID. This allows one to define an ordering of the regression tests starting with the regression test with the highest probability of failure and also to exclude tests from the regression test run.</p><p id="p0147" num="0147"><figref idrefs="f0012">FIG. 12</figref> shows a simplified version of the data preprocessing done for the classifier training, and the schema of the test database is shown in Table 1. The model combines relational processing and user-defined functions in a single DAG-structured query execution plan. Such plans can be found in scientific workflows and large-scale data analysis. But as discussed, current systems typically either shine in processing relational operators or UDFs, but rarely in both. The process of creating the training data is illustrated in <figref idrefs="f0012">FIG. 12</figref> and can roughly be separated into two parts.
<tables id="tabl0001" num="0001"><table frame="all"><title><b><u>Table 1: Tables of Example Use Case</u></b></title><tgroup cols="3"><colspec colnum="1" colname="col1" colwidth="26mm"/><colspec colnum="2" colname="col2" colwidth="120mm"/><colspec colnum="3" colname="col3" colwidth="21mm"/><thead><row><entry valign="top"><u>Table</u></entry><entry valign="top"><u>Description</u></entry><entry align="center" valign="top"><u>Cardinality</u></entry></row></thead><tbody><row><entry>Test_Profiles</entry><entry>the test configurations, primary key Id_Test, Id_Make for the test execution</entry><entry align="center">4.8M</entry></row><row><entry>Test_Cases</entry><entry>actual regression tests, primary key ID, foreign key Id_TEST references Test_Profiles, test result in Status</entry><entry align="center">28.8M</entry></row><row><entry>Git_Commit</entry><entry>one submitted change, primary key Hash, the submission time Commit_Date, make id Id_Make</entry><entry align="center">256K</entry></row><row><entry>Changed_File</entry><entry>the modified source files Id_File and the corresponding Git hash Hash</entry><entry align="center">2M</entry></row></tbody></tgroup></table></tables><!-- EPO <DP n="30"> --></p><p id="p0148" num="0148">The first part collects information on the test case behavior joining the tables Test_Profiles and Test_Cases and the first UDF function sample. The sample UDF creates a sample of the regression tests at different rates depending on the outcome of the test. Because successful test executions are much more common than failures, one can handle the data skew on the outcome of tests by down-sampling successful test cases and keeping all failing tests.</p><p id="p0149" num="0149">The second part of the process is more complex. Instead of looking at the impact of a single source file modification from the table Changed_File for the classifier, one can group files that are commonly changed together. It may be that a strong relationship between these groups of files and sets of relevant tests exists. For example, file groups may relate to a common system component in the source code. To identify those file groups one can look for files that were often modified together in one Git commit. The second part of the process therefore starts with the self join of the base table Changed_File followed by the filter UDF. The UDF groups the join result by pairs of Id_File of common Git commits and aggregates the Hash values of the commits into a list of Hashes. After that, pairs of files are discarded that are below a given threshold of co-occurrence in a commit. It may seem that these two operations could be performed by regular relational operators, but the text-based "'folding"' of strings into a CLOB (or set-valued attribute) is not defined in the SQL standard. Inspired by the built-in aggregation function GROUP_CONCAT provided by MySQL, one can implement this grouping with a user-defined aggregate. The final UDF unfold in this part of the process unfolds the CLOB column again for further processing. After the second UDF, one has identified common file pairs. But to identify all groups of connected files, further UDFs are required. One can implement the computation of the connected components with an iterative algorithm using two UDFs: UDF1 and UDF2 with a data-driven break condition.</p><p id="p0150" num="0150">The details of involved UDFs are discussed below. For now one can note that the input of each UDF can be partitioned and the UDFs can therefore be executed in parallel-similar to relational operators. One can extend optimizers to exploit these opportunities for better performance. Unfortunately, it is difficult to analyze the UDF code and detect if it can and should be parallelized. To solve this problem, one can use the annotations described herein to<!-- EPO <DP n="31"> --> declare opportunities for parallelization but also to annotate how the output cardinality of the UDF relates to its input.</p><heading id="h0032"><b>Example 28 -Example Techniques</b></heading><heading id="h0033"><i>User-Defined Functions in Relational Databases.</i></heading><p id="p0151" num="0151">If complex algorithms are implemented inside a relational database, one usually needs to rely on user-defined functions (UDFs) to implement non-standard operations like data cleansing, specific sampling algorithms or machine-learning algorithms. This leads to the desire to efficiently support UDFs inside a database, i.e., considering UDFs during query optimization as well as efficient parallel execution. The placement of expensive predicates and scalar UDFs within an execution plan has been examined. Related work has been surveyed and proved that ordering predicates and map operators (i.e. functions) is an NP-hard problem and provided an optimal algorithm for that optimization problem.</p><p id="p0152" num="0152">The optimization and parallel execution of user-defined aggregate and table functions has been focused upon. While deterministic scalar UDFs are trivial to parallelize, user-defined aggregation functions are aggregated locally and later combined or reduced to the final result of the aggregate. Some techniques require a static partition strategy be defined for a user-defined table function so that the database can parallelize it. Opportunities for parallelization of user-defined table functions have been idenfied, which are derived dynamically from the context during query optimization.</p><p id="p0153" num="0153">One can address all classes of UDFs; the distinction between scalar, aggregate, and table UDFs can be derived from its annotation. For example, UDFs whose input can be partitioned arbitrarily are treated as scalar UDFs if they only attach additional data to a row. One can implement the parallelization of UDFs using the Worker Farm pattern. This allows one to combine worker farm patterns and to exploit opportunities to parallelize the execution of the UDFs.</p><p id="p0154" num="0154">By default, most commercial databases do not allow for parallelization of user-defined table functions. In some databases, user-defined table functions can be annotated so that their<!-- EPO <DP n="32"> --> input can be partitioned, and they can be executed in parallel. However, optimization strategy details are unavailable. Overall, the ability to parallelize user-defined table functions in relational databases is very limited today, which also limits the scalability of custom code. Some database vendors offer an interface to standard map-reduce frameworks to benefit from the parallel execution capabilities of map-reduce, but those interfaces do not allow for optimizations across relational and custom logic.</p><heading id="h0034"><i>Workflow Engines</i></heading><p id="p0155" num="0155">The performance deficiencies of map-reduce have been analyzed compared to parallel databases for some basic analysis tasks. These limitations led to the development of workflow engines targeted at large scale data analytics. These engines allow for more flexible compositions of user-defined code than map-reduce while keeping the ability to parallelize tasks. The work of SCOPE by Chaiken, Zhou, <i>et al.</i> describes parallelization. The technologies herein can support both relational operators and UDFs. In PACT by Alexandrov <i>et al.,</i> contracts are bound to tasks as pre- and post-conditions. Contracts can enable rewrites of the workflow. The Worker Farm skeleton described herein can have split() and merge() methods and can focus on the integrated optimization of existing database operators and UDFs.</p><p id="p0156" num="0156">Rewrites applied on workflows depend on the availability of cardinality information. A repository of statistics for workflows can be maintained, and re-occurring plan patterns can be detected and used for cardinality information. As described herein, annotations on the UDF can be used to derive cardinality information.</p><heading id="h0035"><i>Advanced Analytics Applications.</i></heading><p id="p0157" num="0157">Among the most prominent applications of map-reduce are machine learning algorithms because they are not supported well in databases today but also because they are performance-critical. Apache Mahout is a collection of basic machine learning algorithms implemented on Apache Hadoop, the map-reduce implementation of the Apache project. As many scientific optimization problems, machine learning algorithms and data preprocessing tasks rely on iteration. HaLoop proposed to integrate iteration natively into the map-reduce framework rather<!-- EPO <DP n="33"> --> than managing iterations on the application level. This includes the integration of certain optimizations into Hadoop, e.g. caching loop invariants instead of producing them multiple times. In the technologies described herein, iteration handling can be explicitly applied during the optimization phase, rather than implicitly hidden in the execution model (by caching).</p><p id="p0158" num="0158">Another important application area that is not well-supported by databases today is advanced statistical applications. While basic aggregates like average, variance, or standard deviation are usually provided by databases, more advanced statistical functions are not supported well. Many algorithms are available in statistical packages like R or SPSS. Furthermore, domain experts feel more comfortable implementing their algorithms using R. Finally, the basic data for these statistics are stored in relational databases. Consequently, it is desirable to support statistical algorithms implemented, e.g. in the R language, as a special kind of UDFs. The integration of external statistical packages has been mainly treated as a black box, and consequently, the execution of these workflows has missed the potential for better performance. The technologies described herein can remedy the situation.</p><heading id="h0036"><i>UDF ANNOTATIONS</i></heading><p id="p0159" num="0159">SQLScript procedures are a dialect for stored procedures in SAP HANA to define dataflows and also to include custom coding to be executed in SAP HANA. Several implementation languages are supported including R (RLANG). For the technologies described herein, one can abstract from the implementation language and use pseudo C (PSEUDOC) instead. In the following section is described a number of annotations for SQLScript procedures to provide metadata that help the query optimizer to derive better query execution plans, in particular to increase the degree of parallelism for dataflows with UDFs. These optimizations are available independent from the implementation language chosen. The annotations help to distinguish between scalar, aggregate, and table UDFs based on their partitioning and grouping requirements. The classical Map and Reduce operations are only two possible instances one can describe with the given set of annotations.</p><p id="p0160" num="0160">Table 2 gives a short overview of all possible annotations. The annotations can be classified into three groups. The first part describes the partitioning pre-conditions expected by<!-- EPO <DP n="34"> --> the UDF, the second group contains the post-conditions ensured by the UDF, and the third group contains optimizer hints. The keywords BEGIN and END enclose the UDF code, and BEGIN PARALLEL and END PARALLEL mark the beginning and end of the annotations. Those annotations are purely optional, although without partitioning information, the UDF will only be invoked once, and thus no data-level parallelism will be exploited. <figref idrefs="f0013">FIG. 13</figref> shows an example Script 1 that has been annotated according to the technologies herein. The example in Script 1 for the sample UDF ("sample") described above in <figref idrefs="f0012">FIG. 12</figref> shows a complete set of possible annotations, even those that can be implied by others. Since UDFs support multiple inputs and outputs, annotations may apply only to specific input or output tables. This is realized by stating the name of the parameter and enclosing the properties in parentheses.
<tables id="tabl0002" num="0002"><table frame="all"><title><u><b>Table 2</b>: <b>Overview of the Annotations</b></u></title><tgroup cols="3"><colspec colnum="1" colname="col1" colwidth="30mm"/><colspec colnum="2" colname="col2" colwidth="33mm"/><colspec colnum="3" colname="col3" colwidth="103mm"/><thead><row><entry align="center" valign="top">Class</entry><entry align="center" valign="top">Annotation</entry><entry valign="top">Description</entry></row></thead><tbody><row><entry>PARTITION (Pre-conditions)</entry><entry align="center"/><entry>Expected global partition properties: a set of input tables each attached to a MINPART and a MAXPART annotation.</entry></row><row><entry/><entry align="center" valign="middle">MINPART</entry><entry>Required logical partitioning: NONE (<i>Default</i>) as no partitioning required; ANY as expected logical partitioning down to single tuples; A set of column names &lt;<i>C</i><sub>1</sub>,...,<i>C<sub>n</sub></i>&gt; required as GROUP BY partitioning.</entry></row><row><entry/><entry align="center" valign="middle">MAXPART</entry><entry>Highest possible supported logical partitioning: NONE as no partitioning supported. Pass table as copy; ANY (<i>Default</i>) as support of arbitrary partitioning (e.g. round robin); A set of column names <i>&lt;C</i><sub>1</sub><i>,...,C<sub>n</sub></i> &gt; supported as GROUP BY partitioning.</entry></row><row><entry>EXPECTED (Pre-conditions)</entry><entry align="center"/><entry>Expected partition local properties: A set of input tables each attached to a list of actions <i>{Â<sub>1</sub>,Â<sub>2</sub>, ..., Â<sub>n</sub>}; Â</i> is either a SORTING or a GROUPING annotation</entry></row><row><entry/><entry align="center">GROUPING</entry><entry>NONE (<i>default</i>) as no partition local grouping required A set of grouping columns <i>{G</i><sub>1</sub><i>, G</i><sub>2</sub><i>,..., Gn}<sup>g</sup></i></entry></row><row><entry/><entry align="center">SORTING</entry><entry>NONE <i>(Default)</i> as no partition local sorting required A list of sorting columns <maths id="math0001" num=""><math display="inline"><mrow><mfenced open="{" close="}" separators=""><msubsup><mi>S</mi><mn>1</mn><mi>O</mi></msubsup><mo>,</mo><msubsup><mi>S</mi><mn>2</mn><mi>O</mi></msubsup><mo>,</mo><mo>…</mo><mo>,</mo><msubsup><mi>S</mi><mi>n</mi><mi>O</mi></msubsup></mfenced></mrow></math><img id="ib0001" file="imgb0001.tif" wi="23" he="8" img-content="math" img-format="tif" inline="yes"/></maths> each given with an order <i>o ∈{</i>ASC, DESC<i>}</i>.</entry></row><row><entry>ENSURE (Post-conditions)</entry><entry align="center"/><entry>Ensured partition local properties after UDF: A set of output relations each attached to a list of actions <i>{Â</i>,<i>Â</i><sub>2</sub>,...,<i>Â<sub>n</sub>}</i>; <i>Â</i> is either a SORTING or a GROUPING annotation</entry></row><row><entry/><entry align="center">GROUPING</entry><entry>NONE as no partition local grouping guaranteed by UDF A set of grouping columns <i>{G<sub>1</sub>, G<sub>2</sub>, ..., Gn}<sup>g</sup></i></entry></row><!-- EPO <DP n="35"> --><row><entry/><entry align="center">SORTING</entry><entry>NONE as no partition local sorting guaranteed by UDF A list of sorting columns <maths id="math0002" num=""><math display="inline"><mrow><mfenced open="{" close="}" separators=""><msubsup><mi>S</mi><mn>1</mn><mi>O</mi></msubsup><mo>,</mo><msubsup><mi>S</mi><mn>2</mn><mi>O</mi></msubsup><mo>,</mo><mo>…</mo><mo>,</mo><msubsup><mi>S</mi><mi>n</mi><mi>O</mi></msubsup></mfenced></mrow></math><img id="ib0002" file="imgb0002.tif" wi="23" he="8" img-content="math" img-format="tif" inline="yes"/></maths> each given with an order <i>o ∈{</i>ASC, DESC<i>}</i>.</entry></row><row><entry/><entry morerows="2" rowsep="1" align="center">KEY</entry><entry>UDF behavior regarding grouping columns <i>=</i> (<i>Default</i>) the grouping columns do only contain values which have been as part of the local partition</entry></row><row><entry/><entry>input != the grouping columns may contain values which have not been input as part of the local partition (existing partitioning cannot be reused)</entry></row><row><entry/><entry>-&gt; describes functional dependencies for new columns derived from previous grouping columns</entry></row><row><entry>PRESERVE (Post-conditions)</entry><entry align="center">ORDER (<i>Default</i>)</entry><entry>As alternative to ENSURE SORTING or GROUPING this annotation denotes FIFO logic preserving existing SORTING or GROUPING of the input table</entry></row><row><entry>- (Optimizer Hints)</entry><entry align="center">DETERM</entry><entry>1 (<i>Default</i>): UDF has deterministic behavior. 0: UDF has no deterministic behavior.</entry></row><row><entry>-(Optimizer Hints)</entry><entry align="center">SIZE</entry><entry><i>=</i> (<i>Default</i>): expected size of the output relation is equal to the size of the input relation factor: expected size of the output relation can be derived by the size of the input relation multiplied by a factor</entry></row><row><entry>-(Optimizer Hints)</entry><entry align="center">RUNTIMEAPPROX</entry><entry>= (<i>Default</i>): expected run time of the UDF is determined by the size of the input relation factor: expected run time of the UDF is determined by the size of the input relation and a factor</entry></row><row><entry>END PARALLEL (Post-conditions)</entry><entry align="center">UNION ALL (<i>Default</i>)</entry><entry>By default it assumes an order-preserving concat merge combining the partitions</entry></row><row><entry>END PARALLEL (Post-conditions)</entry><entry align="center">AGG</entry><entry>Instead of an order-preserving concat merge the UDF can also be followed by any kind of aggregation function known to the database system. In this case an additional repartitioning between UDF and aggregation may be required.</entry></row></tbody></tgroup></table></tables></p><heading id="h0037"><i>Partitioning Pre-conditions</i></heading><p id="p0161" num="0161">The first annotation PARTITION precedes the code block of the procedure. One can describe the granularity of partitioning supported by the UDF by defining MINPART and MAXPART for each input set. MINPART defines the lower bound of required partitioning, whereas MAXPART defines the upper bound of the highest possible partitioning. One can distinguish between those two so that the optimizer can choose the granularity depending on the surrounding operators and their required partitioning. By default MINPART is set to NONE, which means that the UDF does not need partitioning, whereas the default for MAXPART is ANY, which means that the UDF can handle any partitioning. This default setting-like in Script 1-refers to a UDF table function implementing a scalar operation, which can be executed on<!-- EPO <DP n="36"> --> each tuple of the relation independently. Since the UDF table function code can cope with multiple tuples at a time and could even consume the whole non-partitioned table, the optimizer can decide freely how to partition the input to the UDF and how to parallelize it.</p><p id="p0162" num="0162">Many UDFs (such as user-defined aggregates) operate on groups of tuples. A partitioning schema can be described by defining grouping columns for MINPART and MAXPART: The MINPART annotation over a set of columns with <i>n</i> distinct values enforces at least n distinct UDF instances for processing, but it does not ensure that the data is not further partitioned. The MAXPART annotation over a set of grouping columns with n distinct values ensures that the input relation is not partitioned into more than those <i>n</i> distinct parts. It effectively describes a group-by partitioning, but it does not guarantee that a UDF instance consumes only one distinct group. The annotation of MAXPART with the keyword NONE tells the optimizer that the UDF code does not allow partitioning and will only work if the UDF consumes the whole table in a non-partitioned way. Setting MINPART and MAXPART to the same set of columns ensures that each distinct instance of the grouping columns is associated to exactly one distinct UDF instance, which would be equivalent to a reduce function in map-reduce. However, setting MINPART to NONE or a subset of MAXPART can help the optimizer to make better decisions by picking the optimal degree of partitioning. Additionally to the global partitioning, the annotation EXPECTED() followed by a list of SORTING and GROUPING actions and their respective columns describes local grouping and sorting of tuples within each partitioned table part. In the example of Script 1 the information is redundant with the annotation MAXPART(ANY) and could be removed.</p><heading id="h0038"><i>UDF Behavior and Post-conditions</i></heading><p id="p0163" num="0163">Because one can treat user-defined code as a black box, the behavior and possible side effects of the code are-in contrast to the well-defined relational operations-not known to the database management system. Without further information, it is difficult for the optimizer to distinguish between user-defined aggregations, user-defined table functions or some other user-defined logic. It also cannot exploit any characteristics of the UDF that may allow optimizations to be applied to the dataflow. Hence, one can allow for adding a set of post-conditions after the<!-- EPO <DP n="37"> --> code block of the UDF. The annotation KEY makes a statement about the behavior of the UDF regarding columns used as partitioning columns in the MAXPART annotation. To support a wide range of possible UDFs, those columns are not hidden from the UDF and can be manipulated during processing just like every other column. In order to combine UDF parallelism with relational operators, it is often assumed that those grouping columns are not modified by the UDF. This behavior is annotated as KEY(=). Although the UDF may introduce new tuples or remove existing tuples, the annotation KEY(=) states that the grouping columns contain no new values compared to the input table visible for each respective UDF instance. In contrast, KEY(!=) annotates a modification of grouping columns, which effectively means that existing partitioning on those columns cannot be reused for further processing, since the values and thereby possibly the semantic have been changed. In a similar way, KEY(→) describes functional dependencies for new columns derived from previous grouping columns and indicates that an existing grouping is reusable even though original grouping columns are missing in the output schema.</p><p id="p0164" num="0164">If the UDF itself ensures sorting or grouping independent of the given input, it can be annotated by the keyword ENSURE followed by a list of SORTING and GROUPING actions and their respective columns, similar to the EXPECTED annotation from the pre-conditions. Alternatively the annotation PRESERVE ORDER-as in Script 1-states that the UDF implements a first-in-first-out logic preserving the order of tuples derived from the input. Analog to BEGIN PARALLEL PARTITION annotation describing the expected partitioning the END PARALLEL annotation describes the combining of the parallel processed UDF results. By default one can assume an order-preserving concat merge (UNION ALL) is to be used. Alternatively the UDF may also be followed by any kind of aggregation function known to the database system. In this case an additional repartitioning between UDF and aggregation may be required.</p><heading id="h0039"><i>Optimizer Hints</i></heading><p id="p0165" num="0165">In addition to the pre- and post-conditions describing data manipulation there are a number of annotations that describe runtime characteristics of a UDF. This may provide further<!-- EPO <DP n="38"> --> hints to the optimizer to derive better execution plans. The DETERM annotation tells the optimizer whether the UDF is deterministic or not. By default one can assume the UDF to behave deterministically. However, in the example (see Script 1), the UDF has non-deterministic behavior due to the random function used for sampling. The RUNTIMEAPPROX annotation tells the optimizer something about the expected runtime for the UDF relative to the input respectively output size. In the example, RUNTIMEAPPROX(1_data) states that the runtime of the UDF is linear to the input size. The SIZE annotation tells the optimizer the expected data size of the output relation compared to the input relation. In the example (see Script 1), one knows for sure that the size will not increase and even expect the size to decrease depending on the column STATUS of the input data.</p><p id="p0166" num="0166">Further annotations are possible to describe UDF properties, e.g. commutativity, associativity or decomposability of aggregate functions. They enable optimizations beyond those discussed here, e.g. to be able to reorder UDFs.</p><heading id="h0040"><i>PROCESSING WORKFLOWS</i></heading><p id="p0167" num="0167">Herein is described how to translate a workflow, which may include both UDFs and relational operators. The properties used during optimization are also described.</p><heading id="h0041"><i>Structural Properties</i></heading><p id="p0168" num="0168">To assist in implementation of the technologies, structural properties can be used for reasoning about partitioning, grouping and sorting in a uniform framework. Partitioning applies to the whole relation; it is a <i>global structural property.</i> Grouping and sorting properties define how the data within each partition is organized and are therefore <i>local structural properties.</i> As such properties (and the derived optimizations) do not only apply in the context of relational operations, but are also important in the context of parallel execution of UDFs, one can adapt such a formal description and apply it along with the UDFs annotations. Thus the formalism is described:</p><p id="p0169" num="0169"><b>Definition (Structural Properties)</b> The structural properties of a table <img id="ib0003" file="imgb0003.tif" wi="6" he="5" img-content="character" img-format="tif" inline="yes"/> can be represented by partitioning information and an ordered sequence of actions:<!-- EPO <DP n="39"> --> <maths id="math0003" num=""><math display="block"><mrow><mfenced open="{" close="}" separators=";"><mi mathvariant="normal">P</mi><mfenced open="{" close="}" separators=""><msub><mrow><mover><mi>A</mi><mrow><mo>^</mo></mrow></mover></mrow><mn>1</mn></msub><mo>,</mo><msub><mrow><mover><mi>A</mi><mrow><mo>^</mo></mrow></mover></mrow><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mrow><mover><mi>A</mi><mrow><mo>^</mo></mrow></mover></mrow><mi>n</mi></msub></mfenced></mfenced></mrow></math><img id="ib0004" file="imgb0004.tif" wi="67" he="8" img-content="math" img-format="tif"/></maths><br/>
where the first part defines its global structural property, and the second sequence defines its local structural property. Global structural properties imply the partitioning function used. The annotations described herein assume a <i>non-ordered partitioning</i> (e.g. hash partitioning) for a given column set or a non-deterministic partitioning (e.g. round-robin or random partitioning) in the case of an empty column set. Local structural properties are represented by an ordered sequence of actions <i>Â<sub>i</sub>.</i> Each action is either grouping on a set of columns {<i>C<sub>i</sub>, ...,C<sub>j</sub></i>}<i><sup>g</sup>,</i> or sorting on a single column <i>C</i>°.
<tables id="tabl0003" num="0003"><table frame="all"><title><b>Table 3 - Partitioned Relation with Grouping and Sorting</b></title><tgroup cols="3"><colspec colnum="1" colname="col1" colwidth="29mm"/><colspec colnum="2" colname="col2" colwidth="29mm"/><colspec colnum="3" colname="col3" colwidth="29mm"/><thead><row><entry valign="top">Partition 1</entry><entry valign="top">Partition 2</entry><entry valign="top">Partition 3</entry></row></thead><tbody><row><entry>{1, 4, 2},</entry><entry>{4, 1, 5},</entry><entry>{6,2,1},</entry></row><row><entry>{1, 4, 5},</entry><entry>{3, 7, 8},</entry><entry>{6,2,9}</entry></row><row><entry>{7, 1, 2}</entry><entry>{3, 7, 9}</entry><entry/></row></tbody></tgroup></table></tables></p><p id="p0170" num="0170"><b>Definition (Non-ordered Partitioning)</b> A table <img id="ib0005" file="imgb0005.tif" wi="6" he="5" img-content="character" img-format="tif" inline="yes"/> is <i>non-ordered partitioned</i> on a set of columns X, if it satisfies the condition <maths id="math0004" num=""><math display="block"><mrow><mo>∀</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><msub><mi>r</mi><mn>2</mn></msub><mspace width="1em"/><mo>∈</mo><mspace width="1em"/><mo>ℜ</mo><mo>:</mo><msub><mi>r</mi><mn>1</mn></msub><mrow><mfenced open="[" close="]"><mi>X</mi></mfenced><mo>=</mo><msub><mi>r</mi><mn>2</mn></msub><mrow><mfenced open="[" close="]"><mi>X</mi></mfenced><mo>⇒</mo><mi>P</mi><mrow><mfenced><msub><mi>r</mi><mn>2</mn></msub></mfenced><mo>=</mo><mi>P</mi><mfenced><msub><mi>r</mi><mn>2</mn></msub></mfenced></mrow></mrow></mrow></mrow></math><img id="ib0006" file="imgb0006.tif" wi="83" he="11" img-content="math" img-format="tif"/></maths><br/>
where <i>r</i><sub>1</sub><i>,r</i><sub>2</sub> denote tuples and P the partitioning function used.</p><p id="p0171" num="0171">Table 3 shows an instance of a table with three columns <i>C</i><sub>1</sub><i>, C</i><sub>2</sub><i>, C</i><sub>3</sub> and structural properties {{<i>C</i><sub>1</sub>}<i><sup>g</sup></i>; <maths id="math0005" num=""><math display="inline"><mrow><mfenced open="{" close="}" separators=","><msup><mfenced open="{" close="}" separators=""><msub><mi>C</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>C</mi><mn>2</mn></msub></mfenced><mi>g</mi></msup><mo>⁢</mo><msubsup><mi>C</mi><mn>3</mn><mi>o</mi></msubsup></mfenced><mrow><mo>}</mo></mrow><mn>.</mn></mrow></math><img id="ib0007" file="imgb0007.tif" wi="28" he="6" img-content="math" img-format="tif" inline="yes"/></maths> These properties mean that the table is partitioned on column <i>C</i><sub>1</sub> and, within each partition, data is first grouped on columns <i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>, and, within each such group, sorted by column <i>C</i><sub>3</sub>. The required structural properties of relational operations and the optimizations combining relational operations are available. The annotations herein describe the required structural properties of the input tables consumed by the UDF as well as the structural properties of the returned output tables. This allows combining the optimization of<!-- EPO <DP n="40"> --> UDFs and relational operations.</p><heading id="h0042"><i>Modeling Workflows in SAP HANA</i></heading><p id="p0172" num="0172">Although SAP HANA is described herein, the technologies can be applied to other database systems. The most convenient way to express complex dataflows in SAP HANA is using SQLScript. One can use syntax that is similar to SQLScript for the UDFs. Relational expressions in SQLScript can be directly translated into the internal representation of a cost-based optimizer. Nonrelational operations, including UDFs, can be mapped to custom operators, which can be compiled into machine code using a just-in-time compiler. Developers of custom logic need to annotate UDFs with the metadata including the ones described herein.</p><p id="p0173" num="0173">Query processing of workflows in SAP HANA can be managed by the calculation engine. Nodes represent data processing operations, and edges between these nodes represent the dataflow. Data processing operations can be conventional database operations, e.g. projection, aggregation, union or join, but they can also be custom operators processing a UDF. Intermediate results are commonly represented as in-memory columnar tables, and hence, they carry schema information and other metadata, which can be exploited during optimization.</p><p id="p0174" num="0174">At the modeling level, the dataflow can be parallelized if operators are not in a producer-consumer relationship. At execution time, operators are scheduled for processing, once all their inputs are computed.</p><heading id="h0043"><i>Introducing Parallelism</i></heading><p id="p0175" num="0175">Starting with the basic translation of the workflow into a canonical execution plan, one can attempt to increase the opportunities to parallelize the execution of the dataflow graph. To make sure that the requested partitioning described through the global structural properties is generated, one can support operators such as Initial Partitioning, Repartitioning, Full Merge, Partial Repartitioning, and Partial Merge. One can support some additional merge-operations including Union All and Union Distinct. Any kind of global aggregation function or custom merge can be used to combine partial aggregates or results of user-defined table functions. For the relational operators one can apply property derivation and matching.<!-- EPO <DP n="41"> --></p><p id="p0176" num="0176">Herein such an approach can also be applied to UDFs. Given a non-partitioned input, the generic model to parallelize UDFs is to apply the Worker-Farm pattern. This pattern first applies an initial partitioning operation so that the UDF can be executed in parallel in the so-called work-method. Finally, a merge operation integrates the results of the parallel computations. Further patterns can be parallelized using the Worker-Farm pattern.</p><p id="p0177" num="0177">The enforcement of partitioning and a full merge for each UDF operation independently is a very simple approach. It is therefore the goal of the optimizer to break this isolated view and to take surrounding operations and their structural properties into account.</p><p id="p0178" num="0178">Below is described how one can avoid a full partitioning and another full merge between relational operators or UDFs. As analyzed below, exploiting partitioning properties across these operations improves the robustness and scalability of the workflow execution.</p><heading id="h0044"><i>OPTIMIZATION STRATEGY</i></heading><p id="p0179" num="0179">As one treats UDFs as table functions, it is often possible to define the UDF code in a way that it can work both as single instance consuming a non-partitioned table as well as with multiple instances on a partitioned table. To reflect this one can describe the required structural properties of the UDFs with a partitioning P that is in the range of a minimal partitioning column set P<sub>min</sub> and a maximum partitioning column set P<sub>max</sub>: Ø ⊆ <i>P<sub>min</sub></i> ⊆ P ⊆ <i>P<sub>max</sub></i> ⊂ T where 0 indicates that the input table is not partitioned (⊥) and is passed as a copy to the UDF. On the other extreme and even more general than a partitioning on all available columns, T indicates an arbitrary partitioning.</p><p id="p0180" num="0180">If the UDF consumes multiple input tables (e.g. table A and B) it can assume all input tables to be partitioned in the same way (P<sub>A</sub> = P<sub>B</sub>). An exception is the case where the UDF allows partitioning on one input table (<i>P<sub>maxA</sub> ≠</i> Ø) but not for all other input tables (<i>P<sub>maxB</sub> =</i> Ø). In this case, the non-partitioned table B is broadcast as copy to each UDF instance derived from the partitioning of the table A.<!-- EPO <DP n="42"> --></p><heading id="h0045"><i>Rewrites</i></heading><p id="p0181" num="0181">Based on the canonical parallelization of UDFs and relational operators described in herein and the properties derived for them, one can describe the basic plan rewrites in the case of a mismatch of structural properties. Each rewrite considers two consecutive operations OP<sub>1</sub> and OP<sub>2</sub>. Where [P<sub>1</sub>;* } describes the structural properties of the output of OP<sub>1</sub> and {P<sub>2</sub>;*} the expected structural properties for the input of OP<sub>2</sub> and * denotes any local structural property. In case of a mismatch of local <i>structural properties,</i> the framework may further enforce explicit sorting or grouping operations. Table 4 summarizes a possible notation used herein. The most basic exchange operation to fix mismatching <i>global structural properties</i> is a full merge operation followed by a initial partitioning operation.<maths id="math0006" num="(1)"><math display="block"><mrow><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>&gt;</mo><mo>&lt;</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mspace width="4em"/><mo>=</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>&lt;</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mspace width="5em"/><mi mathvariant="normal">if P</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>=</mo><mi mathvariant="normal">Ø</mi><mo>∧</mo><mi mathvariant="normal">P</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>≠</mo><mi mathvariant="normal">Ø</mi></mrow></math><img id="ib0008" file="imgb0008.tif" wi="113" he="6" img-content="math" img-format="tif"/></maths><maths id="math0007" num="(2)"><math display="block"><mrow><mo>=</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>&gt;</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mspace width="5em"/><mi mathvariant="normal">if P</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>≠</mo><mi mathvariant="normal">Ø</mi><mo>∧</mo><mi mathvariant="normal">P</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mo>=</mo><mi mathvariant="normal">Ø</mi></mrow></math><img id="ib0009" file="imgb0009.tif" wi="78" he="6" img-content="math" img-format="tif"/></maths><maths id="math0008" num="(3)"><math display="block"><mrow><mo>=</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>×</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mspace width="4em"/><mi mathvariant="normal">if P</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>≠</mo><mi mathvariant="normal">P</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></math><img id="ib0010" file="imgb0010.tif" wi="78" he="6" img-content="math" img-format="tif"/></maths><maths id="math0009" num="(4)"><math display="block"><mrow><mo>=</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>-</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mspace width="4em"/><mi mathvariant="normal">if P</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>⊆</mo><mi mathvariant="normal">P</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></math><img id="ib0011" file="imgb0011.tif" wi="78" he="6" img-content="math" img-format="tif"/></maths><maths id="math0010" num="(5)"><math display="block"><mrow><mo>=</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>=</mo><mo>&lt;</mo><mi mathvariant="normal">OP</mi><mo>⁢</mo><mn mathvariant="normal">2</mn><mspace width="4em"/><mi mathvariant="normal">if P</mi><mo>⁢</mo><mn mathvariant="normal">1</mn><mo>⊆</mo><mi mathvariant="normal">P</mi><mo>⁢</mo><mn mathvariant="normal">2</mn></mrow></math><img id="ib0012" file="imgb0012.tif" wi="78" he="6" img-content="math" img-format="tif"/></maths>
<tables id="tabl0004" num="0004"><table frame="all"><title><b>Table 4: Notation for Operators and Properties</b></title><tgroup cols="2"><colspec colnum="1" colname="col1" colwidth="33mm"/><colspec colnum="2" colname="col2" colwidth="100mm"/><thead><row><entry align="center" valign="top">Notation</entry><entry valign="top">Description</entry></row></thead><tbody><row><entry align="center">OP</entry><entry>Any operation (or sequence of operations) - relational operator or UDF</entry></row><row><entry align="center">UDF</entry><entry>UDF operation</entry></row><row><entry align="center">ROP</entry><entry>Relational operator</entry></row><row><entry align="center">HJ</entry><entry>Hash join operation</entry></row><row><entry align="center">BJ</entry><entry>Broadcast join operation</entry></row><row><entry align="center">&lt;</entry><entry>Initial Partitioning operation</entry></row><row><entry align="center">&gt;</entry><entry>Full Merge operation</entry></row><row><entry align="center">×</entry><entry>Repartitioning operation</entry></row><row><entry align="center">&gt;=</entry><entry>Partial Merge operation</entry></row><row><entry align="center">=&lt;</entry><entry>Partial Repartitioning operation</entry></row><row><entry align="center">-</entry><entry>Keep partitioning; if not partitioned - no-op</entry></row><row><entry align="center"><i>P</i></entry><entry>Partitioning</entry></row><row><entry align="center">*</entry><entry>Any properties (including empty)</entry></row></tbody></tgroup></table></tables><!-- EPO <DP n="43"> --></p><p id="p0182" num="0182">The first two rewrites ((1) and (2)) are two special cases where only one of the two involved operations is executed in parallel and has a non-empty partitioning requirement. Consequently, the repartitioning turns into an initial partitioning respectively a full merge operation. The third rewrite (3) is the most general rewrite and can always be applied as it just replaces a full merge and subsequent initial partitioning with a repartitioning, which can be parallelized independent of the concrete partitioning of either operation. The two alternatives ((4) and (5)) however can only be applied, if the result of the first operation OP<sub>1</sub> is partitioned on a subset of the partitioning column set required for the input of second operation OP<sub>2</sub>. Those two rewrites thereby leverage the fact that a result partitioned on column <i>C</i><sub>1</sub> alone is also partitioned on columns <i>C</i><sub>1</sub> and <i>C</i><sub>2</sub> because two rows that agree on <i>C</i><sub>1</sub> and <i>C</i><sub>2</sub> also agree on <i>C</i><sub>1</sub> alone and consequently they are in the same partition ({<i>C</i><sub>1</sub>}<i><sup>g</sup></i> ⇒ {{<i>C</i><sub>1</sub>,<i>C</i><sub>2</sub>}<i><sup>g</sup></i>}). While rewrite (4) keeps the existing partitions from OP<sub>1</sub> intact, the alternative rewrite (5) allows to increase the number of partitions-and consequently the degree of parallelization-using the partial repartitioning operation =&lt;. Which of those two rewrites is chosen by the framework is therefore implied by the degree of parallelization defined for each operation and whether the full degree of parallelization is used during each logical operation. The partial repartitioning operation (=&lt;) also introduces another basic rewrite (6), which reverses the partial repartitioning operation by applying a partial merge operation (=&gt;): <maths id="math0011" num="(6)"><math display="block"><mrow><msub><mi mathvariant="normal">OP</mi><mn>0</mn></msub><mo>=</mo><mo>&lt;</mo><msub><mi mathvariant="normal">OP</mi><mn>1</mn></msub><mo>&gt;</mo><mo>&lt;</mo><msub><mi mathvariant="normal">OP</mi><mn>2</mn></msub><mo>=</mo><msub><mi mathvariant="normal">OP</mi><mn>0</mn></msub><mo>=</mo><mo>&lt;</mo><msub><mi mathvariant="normal">OP</mi><mn>1</mn></msub><mo>&gt;</mo><mo>=</mo><msub><mi mathvariant="normal">OP</mi><mn>2</mn></msub></mrow></math><img id="ib0013" file="imgb0013.tif" wi="78" he="6" img-content="math" img-format="tif"/></maths> <maths id="math0012" num=""><math display="block"><mrow><msub><mi mathvariant="normal">if P</mi><mn mathvariant="normal">0</mn></msub><mo>⊆</mo><msub><mi mathvariant="normal">P</mi><mn mathvariant="normal">2</mn></msub><mo>∧</mo><msub><mi mathvariant="normal">P</mi><mn mathvariant="normal">1</mn></msub><mspace width="1em"/><mi mathvariant="normal">determ</mi><mn mathvariant="normal">.</mn></mrow></math><img id="ib0014" file="imgb0014.tif" wi="39" he="6" img-content="math" img-format="tif"/></maths></p><p id="p0183" num="0183">The partial merge can only be applied together with a previous partial repartition, because a result partitioned on columns <i>C</i><sub>1</sub> and <i>C</i><sub>2</sub> is not partitioned on <i>C</i><sub>1</sub> alone, since two rows with the same value for <i>C</i><sub>1</sub> may be in different partitions ({{<i>C</i><sub>1</sub>,<i>C</i><sub>2</sub>}<i><sup>g</sup></i>} ⇒ {<i>C</i><sub>1</sub>}<sup>g</sup>). However with a previous partial repartition it take advantage of the fact that for <i>P</i><sub>1</sub> it can for instance apply <maths id="math0013" num=""><math display="inline"><mrow><mfenced open="{" close="}" separators=","><msubsup><mi>C</mi><mn>1</mn><mi>g</mi></msubsup><mo>⁢</mo><msubsup><mi>C</mi><mn>2</mn><mi>g</mi></msubsup></mfenced><mo>⇒</mo><mfenced open="{" close="}"><msup><mfenced open="{" close="}" separators=","><msub><mi mathvariant="normal">C</mi><mn mathvariant="normal">1</mn></msub><mo>⁢</mo><msub><mi mathvariant="normal">C</mi><mn mathvariant="normal">2</mn></msub></mfenced><mi>g</mi></msup></mfenced></mrow></math><img id="ib0015" file="imgb0015.tif" wi="41" he="7" img-content="math" img-format="tif" inline="yes"/></maths> whereas for P<sub>2</sub> it may apply <maths id="math0014" num=""><math display="inline"><mrow><mfenced open="{" close="}" separators=","><msubsup><mi>C</mi><mn>1</mn><mi>g</mi></msubsup><mo>⁢</mo><msubsup><mi>C</mi><mn>2</mn><mi>g</mi></msubsup></mfenced><mo>⇒</mo><msup><mfenced open="{" close="}"><msub><mi>C</mi><mn mathvariant="normal">1</mn></msub></mfenced><mi>g</mi></msup></mrow></math><img id="ib0016" file="imgb0016.tif" wi="30" he="7" img-content="math" img-format="tif" inline="yes"/></maths> when reversing the<!-- EPO <DP n="44"> --> partitioning on <i>C</i><sub>2</sub> during the partial merge operation.</p><p id="p0184" num="0184">The above rewrites also apply in the context of nested operations such as loops. In this case however, it needs to distinguish between inner rewrites affecting succeeding operations within a nested operation (e.g. loop(&lt;OP<sub>1</sub>&gt;-&lt;OP<sub>2</sub>&gt;) = loop(OP<sub>1</sub> × OP<sub>2</sub>×)) and outer rewrites (e.g.OP<sub>1</sub>&gt;&lt;loop(OP<sup>2</sup>) = OP<sub>1</sub>×loop(OP<sub>2</sub>)) affecting the boarders of the nested operation connecting inner operations with outer operations. It can therefore apply first inner rewrites and then follow up with outer rewrites, which does in the case of a loop also take loop invariants into account. If a nested loop operation requires a loop invariant input to be partitioned, this global structural property will be passed to the outside and considered during outer rewrites.</p><heading id="h0046"><i>Rewriting Strategy</i></heading><p id="p0185" num="0185"><figref idrefs="f0014">FIG. 14</figref> shows Algorithm 1, which is used to first traverse the dataflow plan downwards starting from the top nodes to the child nodes to propagate required properties. In the case of nested operations, the optimizer can first step down the embedded-plan before the optimizer continues to with the surrounding plan. For each logical operator, the optimizer considers different alternative physical operators, derives which properties their inputs must satisfy, and requests plans for each input. For example, a hash join operation would request required partitions on the join attributes from its two child expressions.</p><p id="p0186" num="0186">In the case of a UDF operation, the technologies can use the annotations to derive required and provided properties. The function <i>DetermineChildReqdProperties</i> in Algorithm 1 thereby derives <i>structural properties</i> required by the UDF given the annotations referring to the UDF pre-conditions.</p><p id="p0187" num="0187"><b>Example:</b> Assume during the first traversal from parent to child one reaches the sample UDF from <i>Test Sample</i> in <figref idrefs="f0012">FIG. 12</figref> with the requirement of its hash join parent that its result be partitioned on {Id_make}. Since the sample UDF has the annotations (<b>minPart</b>(NONE), <b>maxPart</b>(ANY) and <b>KEY(</b>=)) the optimizer considers at least the following three alternatives:<!-- EPO <DP n="45"> -->
<ol><li>1. Execute the UDF without parallelization by setting partitioning of the UDF to NONE (P = {NONE}) and propagate this requirement to its child expression.</li><li>2. Execute the UDF with partitioning ANY and request ANY partitioning (P = {ANY}) from its child expression.</li><li>3. Execute the UDF with partitioning on {Id_make} by setting ANY to the requested partitioning of the parent node (P = {Id_make}) and therefore tunnel through this requirement to its child expression.</li></ol></p><p id="p0188" num="0188">The algorithm passes all of those requirements while calling the OptimizeExpr function recursively for each child expression. This recursive call is repeated until data source nodes (leaf nodes) are reached returning the data source node as <i>CheapestQueryPlans.</i></p><p id="p0189" num="0189">From there on the optimizer can traverse the dataflow plan in the reverse direction from child to parent. For each returned (sub-)query plan the <i>DeriveDlvdProperties</i> function is called to determine the delivered properties. In the case of an UDF operation the function <i>DeriveDlvdProperties</i> derives <i>structural properties</i> delivered by the UDF given the annotations referring to the UDF post-conditions and the properties of the UDF inputs.</p><p id="p0190" num="0190">Subsequently, the <i>PropertyMatch</i> function compares the delivered properties of the child plan with the required properties of the current operation. In the case of property mismatch, the optimizer introduces exchange operations in order to build a valid plan combining the delivered (sub-)plan with the current operation. As discussed herein, the basic exchange operation regarding <i>global structural properties</i> is a full merge followed by an initial partitioning operation. Based on the rewrites discussed herein, alternative exchange operations can be derived. If multiple different rewrites are applicable for the introduced exchange operation, the optimizer examines the alternatives (e.g., all of them) and selects the best plan based on estimated costs. For each required property, at least one plan alternative is added to valid plans, unless the optimizer prunes the alternative due to cost-based heuristics. It is also a cost-based decision to find the optimal degree of parallelism for each sub-plan. Available partitions for base tables, number of cores and nodes in the distributed landscape, but also communication costs and<!-- EPO <DP n="46"> --> I/O are factors for these decisions. Furthermore, the optimizer can use the cost and size annotations for UDFs and known cost formulas for relational operators for this decision.</p><p id="p0191" num="0191"><b>Example:</b> Assume during the reverse traversal from child to parent the technologies reach the sample UDF from <i>Test Sample</i> in <figref idrefs="f0012">FIG. 12</figref> with the delivered property <i>dlvd</i> of its child plan (a hash join) partitioned on {Id_test}. Given the required properties <i>reqd</i> of the sample UDF (P = {NONE}, {ANY}, {Id_make}) at least following three plans are considered:
<ol><li>1. Execute the UDF without parallelization and add an basic exchange operation:<br/>
HJ<sub>P=</sub>{Id_test}&gt;&lt;UDF<sub>P</sub>={NONE}</li><li>2. Execute the UDF with partitioning on {Id_test} by setting ANY to the delivered partitioning of the child plan and add an exchange operation.<br/>
HJ<sub>P=</sub>{Id_test}&gt;&lt;UDF<sub>P=</sub>{Id_test}</li><li>3. Execute the UDF with partitioning on {Id_make} and add a basic exchange operation:<br/>
HJ<sub>P=</sub>{Id_test}&gt;&lt;UDF<sub>P=</sub>{Id_make}</li></ol></p><p id="p0192" num="0192">Those basic exchange operations can be rewritten as follows:
<ol><li>1. <maths id="math0015" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub><mo>&gt;</mo><mo>&lt;</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">NONE</mi></mfenced></mrow></msub></mrow></math><img id="ib0017" file="imgb0017.tif" wi="45" he="6" img-content="math" img-format="tif"/></maths>
<ol><li>(a) Based on rewrite rule (2): <maths id="math0016" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>&gt;</mo></mrow></msub><mo>⁢</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">NONE</mi></mfenced></mrow></msub></mrow></math><img id="ib0018" file="imgb0018.tif" wi="42" he="6" img-content="math" img-format="tif"/></maths></li></ol></li><li>2. <maths id="math0017" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub><mo>&gt;</mo><mo>&lt;</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub></mrow></math><img id="ib0019" file="imgb0019.tif" wi="45" he="6" img-content="math" img-format="tif"/></maths>
<ol><li>(a) Based on rewrite rule (4): <maths id="math0018" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub><mo>-</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub></mrow></math><img id="ib0020" file="imgb0020.tif" wi="42" he="6" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="47"> --></li><li>(b) Based on rewrite rule (5): <maths id="math0019" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub><mo>=</mo><mo>&lt;</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub></mrow></math><img id="ib0021" file="imgb0021.tif" wi="45" he="6" img-content="math" img-format="tif"/></maths></li></ol></li><li>3. <maths id="math0020" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub><mo>&gt;</mo><mo>&lt;</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_make</mi></mfenced></mrow></msub></mrow></math><img id="ib0022" file="imgb0022.tif" wi="47" he="6" img-content="math" img-format="tif"/></maths>
<ol><li>(a) Based on rewrite rule (3): <maths id="math0021" num=""><math display="block"><mrow><msub><mi mathvariant="normal">HJ</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced></mrow></msub><mo>×</mo><msub><mi mathvariant="normal">UDF</mi><mrow><mi mathvariant="normal">P</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_make</mi></mfenced></mrow></msub></mrow></math><img id="ib0023" file="imgb0023.tif" wi="47" he="6" img-content="math" img-format="tif"/></maths></li></ol></li></ol></p><p id="p0193" num="0193">The optimizer examines the alternatives (e.g., all of them) and selects the best plan for each property group based on estimated costs. Assuming the optimizer selects following three plans and returns them as <i>CheapestQueryPlans</i> to its consumer:
<ol><li>1. <maths id="math0022" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>&gt;</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi>NONE</mi></mfenced></mrow></math><img id="ib0024" file="imgb0024.tif" wi="55" he="6" img-content="math" img-format="tif"/></maths></li><li>2. <maths id="math0023" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>-</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_test</mi></mfenced></mrow></math><img id="ib0025" file="imgb0025.tif" wi="54" he="6" img-content="math" img-format="tif"/></maths></li><li>3. <maths id="math0024" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>×</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0026" file="imgb0026.tif" wi="60" he="6" img-content="math" img-format="tif"/></maths></li></ol></p><p id="p0194" num="0194">With the consumer being a hash join with required property of (P = {Id_make}) the following plans are considered during the next higher level of the recursion:
<ol><li>1. <maths id="math0025" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>&gt;</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi>NONE</mi></mfenced><mo>&gt;</mo><mo>&lt;</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0027" file="imgb0027.tif" wi="86" he="6" img-content="math" img-format="tif"/></maths>
<ol><li>(a) Based on rewrite rule (1): <maths id="math0026" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>&gt;</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi>NONE</mi></mfenced><mo>&lt;</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0028" file="imgb0028.tif" wi="84" he="6" img-content="math" img-format="tif"/></maths></li></ol></li><li>2. <maths id="math0027" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>-</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>&gt;</mo><mo>&lt;</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0029" file="imgb0029.tif" wi="85" he="6" img-content="math" img-format="tif"/></maths>
<ol><li>(a) Based on rewrite rule (3):</li></ol><!-- EPO <DP n="48"> -->
<maths id="math0028" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>-</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>×</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0030" file="imgb0030.tif" wi="84" he="6" img-content="math" img-format="tif"/></maths></li><li>3. <maths id="math0029" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>×</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_make</mi></mfenced><mo>&gt;</mo><mo>&lt;</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0031" file="imgb0031.tif" wi="91" he="6" img-content="math" img-format="tif"/></maths>
<ol><li>(a) Based on rewrite rule (4): <maths id="math0030" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>×</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_make</mi></mfenced><mo>-</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0032" file="imgb0032.tif" wi="88" he="6" img-content="math" img-format="tif"/></maths></li><li>(b) Based on rewrite rule (5):</li></ol>
<maths id="math0031" num=""><math display="block"><mrow><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_test</mi></mfenced><mo>×</mo><mi mathvariant="normal">UDFP</mi><mo>=</mo><mfenced open="{" close="}"><mi mathvariant="normal">Id_make</mi></mfenced><mo>=</mo><mo>&lt;</mo><mi mathvariant="normal">HJP</mi><mo>=</mo><mfenced open="{" close="}"><mi>Id_make</mi></mfenced></mrow></math><img id="ib0033" file="imgb0033.tif" wi="91" he="6" img-content="math" img-format="tif"/></maths></li></ol></p><p id="p0195" num="0195">Since all of those alternatives deliver the same property (P = {Id_make}) for the next higher level of the recursion, the optimizer might decide to select only one of those alternative based on estimated costs.</p><heading id="h0047"><b>Example 29 - Example Evaluation and Additional Information</b></heading><p id="p0196" num="0196">Herein the impact of the optimization strategy for workflows using both relational operators and UDFs can be described. The example introduced above can be used to show the effect of optimizing a plan for increased parallelization and better scalability. Each plan was executed at least 5 times and in case of strong variation up to 15 times. The numbers presented in this section are the median values of those measurements.</p><p id="p0197" num="0197">The experiments were conducted using the SAP HANA database on a four node cluster. Every node had two Intel Xeon X5670 CPUs with 2.93 GHz, 148 GB RAM and 12 MB L3 cache. As each CPU is equipped with 6 cores and hyper threading, this resulted in 24 hardware threads per node. The measurements on a single node are labeled 24 <i>local,</i> and the measurements in the distributed landscape are labeled with <i>4x6 dist.</i> if four nodes with up to 6 parallel operators were used and <i>4x24 dist.</i> if all resources of all 4 nodes were used. The tests also used a stronger single-node machine labeled 24 <i>LOC</i> because some experiments did not run with 248 GB RAM available. This machine was an Intel Xeon 7560 CPU with 2.27 GHz 24 MB L3 cache per CPU and 256 GB RAM and 16 execution threads per CPU including hyper<!-- EPO <DP n="49"> --> threading.</p><heading id="h0048"><i>Test Sample</i></heading><p id="p0198" num="0198">The description starts by examining the sub-plan called <i>Test Sample</i> in <figref idrefs="f0012">FIG. 12</figref> of the example use case. Techniques for parallelizing the hash-join in a distributed environment are well-known. For the measurements, the table Test_Cases is partitioned based on Id_Test, and in the distributed case it also stores the partitions for local access. As the join predicates of the two join operations are different, a repartitioning is required if it wants to partition the input for both join operations. Alternatively, a broadcast join can be performed in either case.</p><p id="p0199" num="0199">Herein the focus is on optimizing the UDF (called sample UDF, see Script 1) which performs a biased sampling of the test cases. Without knowledge about the ability to parallelize this UDF, it has to first merge the input of the UDF, execute the UDF in a single call, and then potentially split its output again for parallel processing. This gives a first baseline for the experiments: HJ&gt;UDF&lt;HJ. Two more baseline plans were analyzed that take parallelization of the UDF into account: with full merge repartitioning (HJ&gt;&lt;UDF&gt;&lt;HJ) and with parallel repartitioning (HJxUDFxHJ).</p><p id="p0200" num="0200">It is a goal to do better than that, and the annotations introduced herein will help to rewrite the plan such that it can be parallelized better, and consequently evaluates faster than the initial alternative. The sample UDF can be processed in parallel because it does not require any specific partitioning (<b>minPart</b>(NONE), <b>maxPart</b>(ANY)). With that knowledge, the optimizer can either align the UDF with the left join (HJ-UDF×HJ) or the right join (HJ×-UDF-HJ). Even without cost estimates available, the SIZE annotation of the UDF indicates that the first alternative is the better choice because sampling leaves only approximately 10% of its input. Avoiding repartitioning by using a broadcast join instead of a hash join implementation introduces three more plan alternatives: 1) HJ-UDF-BJ 2) BJ-UDF-HJ, and 3) BJ-UDF-BJ. But since the first join works on very large input tables, the last two options are easily excluded, and thus it need only investigate the alternative HJ-UDF-BJ.<!-- EPO <DP n="50"> --></p><p id="p0201" num="0201"><figref idrefs="f0015 f0016">FIGS. 15-16</figref> plot how the plans scale with an increasing number of parallel operators on the x-axis-either on a single node (<figref idrefs="f0015">FIG. 15</figref>) or on up to four nodes (<figref idrefs="f0016">FIG. 16). Figure 16</figref> uses one node for the case of no parallelism, and two nodes for up to two parallel operators. For the other measurement in this figure all four nodes were used. A figure of local measurements on a bigger machine (LOC) looks similar. For readability, the scale is restricted to 24 seconds. The precise value for HJ&gt;&lt;UDF&gt;&lt;HJ in <figref idrefs="f0016">FIG. 16</figref> is 27.89 seconds.</p><p id="p0202" num="0202">In <figref idrefs="f0015 f0016">FIGS. 15-16</figref>, the baseline plan (HJ&gt;UDF&lt;HJ) with only a single instance of the UDF has the worst scaling behavior. This clearly indicates the benefit of integrating the optimization of relational operators and UDFs. Once one parallelizes processing the UDF with a shuffle (plan alternatives HJxUDFxHJ and HJ×UDF-HJ) one observes a much better scaling behavior. The performance degradation from 1 to 2 parallel operators is caused by the additional overhead of shuffling while the benefit of parallelizing the UDF is still small. But as one increases the degree of parallelism, performance improves significantly. As expected, one observes an even better performance when one keeps the partitioning for the first join and the sample UDF (plans HJ-UDF×HJ and HJ-UDF-BJ) because this minimizes the communication effort while it enjoys the benefits of parallelization. For up to 24 parallel operators, it does not matter if it shuffles the output of the UDF and uses a hash join or if it performs a broadcast join because it only deals with relatively small data. After that, reshuffling the data for the hash join creates larger overhead than keeping the existing partitioning. The measurements are similar for the setup with single-host and the scale-out indicating that data transmission costs over the network have limited impact for this part of the plan.</p><p id="p0203" num="0203">Overall, one sees significant benefits by exploiting the annotations for the sample UDF as it allows it to parallelize its execution. Of course, the choice between the plan alternatives must be based on cardinality and cost estimates. Again, the annotations help here.</p><heading id="h0049"><i>Build Graph</i></heading><p id="p0204" num="0204">Next, plan alternatives are analyzed for the sub-plan called <i>Build Graph</i> in <figref idrefs="f0012">FIG. 12</figref>. To compute the connected components of related files it can perform a self join on the table<!-- EPO <DP n="51"> --> Change_File over the column Hash. As a result it gets pairs of files identified by their Id_File that were changed in the same commit, i.e. with the same Hash of the change. After that a filter UDF removes all pairs of files below a threshold of 80% meaning that it only keeps those pairs of files that co-occur in at least 80% of all submissions of both respective files. In addition to that, this UDF folds all Hash values associated with the selected pairs of files into a CLOB. The UDF has the following annotations: <b>minPart</b>(NONE), <b>maxPart</b>(fileID1, fileID2) and <b>EXPECTED GROUPING</b>(fileID1). The annotations imply that no partitioned input is required, that the most fine-grained partitioning is on (fileID1, fileID2) and that the input must be grouped by fileID1 within each partition.</p><p id="p0205" num="0205">The basic plan alternative to parallelize this plan is to perform a hash join exploiting the partitioning of the base table on column Hash, merge and split again for the UDF, i.e. HJ&gt;&lt;UDF. A slightly more advanced alternative is to shuffle the data, i.e. HJ×UDF. In both cases the UDF would receive its input partitioned by fileID1 and fileID2. In the experiments only the second alternative is considered because the result of the join is so large that it cannot be processed on a single node. Also, the discussion herein showed that merging and repartitioning scales worse than the shuffle operator. The alternative plan (BJ-UDF) implements the self join using a broadcast join, partitioning one input of the self join over Id_File and broadcasting the second one, avoiding a repartitioning between self join and filter UDF.
<tables id="tabl0005" num="0005"><table frame="all"><title><b>Table 5: Build Graph</b></title><tgroup cols="5"><colspec colnum="1" colname="col1" colwidth="18mm"/><colspec colnum="2" colname="col2" colwidth="22mm"/><colspec colnum="3" colname="col3" colwidth="15mm"/><colspec colnum="4" colname="col4" colwidth="15mm"/><colspec colnum="5" colname="col5" colwidth="15mm"/><thead><row><entry valign="top">Env.</entry><entry valign="top">Plan Version</entry><entry valign="top">Time</entry><entry align="center" valign="top"><i>σ</i></entry><entry valign="top">Factor</entry></row></thead><tbody><row><entry><i>24 LOC</i></entry><entry>HJ × UDF</entry><entry>748.66</entry><entry>143.90</entry><entry>-</entry></row><row><entry><i>24 LOC</i></entry><entry>BJ-UDF</entry><entry>545.10</entry><entry>19.90</entry><entry>1.37</entry></row><row><entry><i>4x6 dist.</i></entry><entry>HJ×UDF</entry><entry>433.00</entry><entry>14.30</entry><entry>-</entry></row><row><entry><i>4x6 dist.</i></entry><entry>BJ-UDF</entry><entry>298.6</entry><entry>5.20</entry><entry>1.45</entry></row><row><entry><i>4x24 dist.</i></entry><entry>HJ×UDF</entry><entry>448.86</entry><entry>4.50</entry><entry>-</entry></row><row><entry><i>4x24 dist.</i></entry><entry>BJ-UDF</entry><entry>272.89</entry><entry>3.00</entry><entry>1.64</entry></row></tbody></tgroup></table></tables><!-- EPO <DP n="52"> --></p><p id="p0206" num="0206">Table 5 shows the execution times of both plans for execution on a single node (and more powerful) with up to 24 parallel operators (LOC), in the distributed setup with four nodes and either up to 24 (4x6 dist.) and up to 96 parallel operators (4x24 dist.).</p><p id="p0207" num="0207">Avoiding the repartitioning of 1929 mio. records results in a speed-up of factor 1.37 for the local plan and 1.45 (resp. 1.64) for the distributed plans. One would expect an even larger improvement with a tuned implementation of the prototype of the broadcast join. Another finding is that-even though the machines are not directly comparable-as it moves from the local plan to the distributed plan, the execution time improves. Moreover, the standard deviation (σ) decreases as one increases parallelism, and thus increasing the parallelism makes the runtime more predictable. Such speed-up can only be achieved because the two partitioning annotations <b>minPart</b> and <b>maxPart</b> yield the flexibility for the optimizer to choose a partitioning over Id_File instead of using a partitioning over fileID1 and fileID2 to execute the UDF in parallel. This flexibility is especially valuable because the following UDF1 (respectively UDF3) requires a partitioning on fileID1 (respectively Id_File). While the plan HJ×UDF needs to repartition its output again (which takes additional 15 seconds), the alternative (BH-UDF) can directly use the existing partitioning.</p><heading id="h0050"><i>Connected Component Loop</i></heading><p id="p0208" num="0208">Attention is now turned to the sub-plan called <i>Connected Components</i> in <figref idrefs="f0012">FIG. 12</figref>. The input for this subplan is an edge relation (fileID1, fileID2) where each edge connects two nodes representing files that were often changed in one commit. The computation of the connected component uses two UDFs-UDF1 and UDF2-inside a loop. The loop iteratively assigns each node a component groupID as <i>min</i>(groupID, fileID1, fileID2) until no new groupID is derived (see <figref idrefs="f0017">FIG. 17</figref> showing Script 2). Since the UDF1 processes the data grouped by fileID1 and the UDF2 processes the data grouped by fileID2, both UDFs can be processed in parallel by partitioning the data accordingly. Consequently, a repartitioning is required between both UDFs.<!-- EPO <DP n="53"> --></p><p id="p0209" num="0209">In the initial implementation (shown in Script 2) UDF1 and UDF2 pass the groupID as part of the edge set, which means that the full edge set with 21.5 mio. records has to be repartitioned during the iteration between each UDF instance. This leads to the two basic plan alternatives: <maths id="math0032" num=""><math display="block"><mrow><mo>•</mo><mi>loop</mi><mfenced separators=""><mo>&lt;</mo><mi>UDF</mi><mo>⁢</mo><mn>1</mn><mo>×</mo><mi>UDF</mi><mo>⁢</mo><mn>2</mn><mo>&gt;</mo></mfenced></mrow></math><img id="ib0034" file="imgb0034.tif" wi="42" he="14" img-content="math" img-format="tif"/></maths> and <maths id="math0033" num=""><math display="block"><mrow><mo>•</mo><mi>loop</mi><mfenced separators=""><mi>UDF</mi><mo>⁢</mo><mn>1</mn><mo>×</mo><mi>UDF</mi><mo>⁢</mo><mn>2</mn><mo>×</mo></mfenced><mn>.</mn></mrow></math><img id="ib0035" file="imgb0035.tif" wi="42" he="10" img-content="math" img-format="tif"/></maths><br/>
The former partitions and merges the edge set before and after each loop iteration, while the latter uses a shuffle operator.</p><p id="p0210" num="0210">The alternative implementation (shown in <figref idrefs="f0018">FIG. 18</figref> with Script 3) with UDF3 and UDF4 introduces a node set to transfer the groupID information between the UDFs. This has the advantage that instead of having to transfer the full edge set of 21.5 mio. records only the much smaller node set with 44 K records has to be passed between the UDFs. In the case of UDF3 and UDF4 the partitioning and thereby the parallel execution is based on the edge set (<b>maxPart</b>(fileID1) and <b>maxPart</b>(fileID2)), whereas the node set used for transferring the groupID update is broadcast (<b>maxPart</b>(NONE)) to each parallel executed UDF. As the node set is updated in each iteration, a full merge and repartition is required to integrate the updates of each partition. Given this annotation, the optimizer can make sure that the edge set is passed as loop invariant partitioned by fileID1 for the UDF3 and also partitioned by fileID2 for the UDF4. This gives the plan loop(&lt;UDF3&gt;&lt;UDF4&gt;).
<tables id="tabl0006" num="0006"><table frame="all"><title><b>Table 6: Connected Component</b></title><tgroup cols="5"><colspec colnum="1" colname="col1" colwidth="18mm"/><colspec colnum="2" colname="col2" colwidth="30mm"/><colspec colnum="3" colname="col3" colwidth="17mm"/><colspec colnum="4" colname="col4" colwidth="14mm"/><colspec colnum="5" colname="col5" colwidth="15mm"/><thead><row><entry valign="top"><u>Env.</u></entry><entry valign="top"><u>Plan Version</u></entry><entry align="center" valign="top"><u>Time</u></entry><entry align="center" valign="top"><i><u>σ</u></i></entry><entry valign="top"><u>Factor</u></entry></row></thead><tbody><row><entry><i>24 local</i></entry><entry>&lt;UDF1 × UDF2&gt;</entry><entry>2155.88</entry><entry align="center">499.7</entry><entry align="center">-</entry></row><row><entry>24 <i>local</i></entry><entry>UDF1 ×UDF2 ×</entry><entry>2654.26</entry><entry align="center">604.1</entry><entry align="center">0.81</entry></row><row><entry>24 <i>local</i></entry><entry>&lt;UDF3&gt;&lt;UDF4&gt;</entry><entry>2501.55</entry><entry align="center">901.5</entry><entry align="center">0.86</entry></row><row><entry>24 <i>LOC</i></entry><entry>&lt;UDF1 × UDF2&gt;</entry><entry>652.72</entry><entry align="center">33.0</entry><entry align="center">-</entry></row><row><entry><i>24 LOC</i></entry><entry>UDF1 ×UDF2 ×</entry><entry>262.96</entry><entry align="center">11.7</entry><entry align="center">2.48</entry></row><!-- EPO <DP n="54"> --><row><entry>24 <i>LOC</i></entry><entry>&lt;UDF3&gt;&lt;UDF4&gt;</entry><entry>181.26</entry><entry align="center">12.0</entry><entry align="center">3.60</entry></row><row><entry><i>4x6 dist.</i></entry><entry>&lt;UDF1 ×UDF2&gt;</entry><entry>640.76</entry><entry align="center">5.1</entry><entry align="center">-</entry></row><row><entry><i>4x6 dist.</i></entry><entry>UDF1 × UDF2 ×</entry><entry>200.80</entry><entry align="center">4.5</entry><entry align="center">3.20</entry></row><row><entry><i>4x6 dist.</i></entry><entry>&lt;UDF3&gt;&lt;UDF4&gt;</entry><entry>87.80</entry><entry align="center">1.5</entry><entry align="center">7.30</entry></row><row><entry><i>4x24 dist.</i></entry><entry>&lt;UDF1 × UDF2&gt;</entry><entry>645.55</entry><entry align="center">30.9</entry><entry align="center">-</entry></row><row><entry><i>4x24 dist.</i></entry><entry>UDF1 ×UDF2 ×</entry><entry>181.27</entry><entry align="center">0.8</entry><entry align="center">3.50</entry></row><row><entry><i>4x24 dist.</i></entry><entry>&lt;UDF3&gt;&lt;UDF4&gt;</entry><entry>62.62</entry><entry align="center">2.8</entry><entry align="center">10.30</entry></row></tbody></tgroup></table></tables></p><p id="p0211" num="0211">Table 6 shows the execution time of this iterative process involving nine iterations until convergence is reached. The plans were executed on one local machine with 24 parallel tasks (<i>24 LOC</i>/<i>24 local</i> as described at the beginning of the section), in the distributed setup with 4 nodes with up to 6 parallel operators per node <i>(4x6 dist</i>.) and also with up to 24 parallel operators per node <i>(4x24 dist</i>.).</p><p id="p0212" num="0212">In the local case (<i>24 local</i>), the machine reached the memory limit when processing the initial edge set including the CLOBs for the Hashes and started swapping the input tables in and out of memory. As a consequence, the execution time of these sub-plans were quite slow, but it also observed a large standard deviation (<i>σ</i>) in the execution time. An analysis of this effect revealed that L3 cache misses were a significant contributor to the longer execution time and also the variance. In contrast to that, the results for the distributed setup show that optimization for the loop execution can speed up the execution considerably. Nevertheless the highest speed-up can still be achieved by providing an efficient UDF implementation, i.e. by moving from UDF1 and UDF2 to the optimized implementation using UDF3 and UDF4. Because of this, it is of particular importance that the language interface and the annotations are flexible enough to support multiple input and output data structures and can describe their behavior under parallelization independent from each other.</p><heading id="h0051"><i>Full Execution Plan</i></heading><p id="p0213" num="0213">The findings for optimizing the execution plan presented above are summarized below. To show the potential of the optimizations presented, one can compare the basic plan alternative<!-- EPO <DP n="55"> --> with the best alternative found for each sub-plan. For the sub-plan <i>Test Sample</i> it only used one fast alternative (HJ-UDFxHJ) because this part of the plan contributes only a very small fraction to the runtime of the complete workflow. The basic alternative combines the hash join filter UDF (HJ × UDF x) with the unoptimized connected component (&lt;UDF1×UDF2&gt;) involving an additional repartitioning step. The best plan combines a broadcast join filter UDF (BJ-UDF-) with an invariant-sensitive connected component (&lt;UDF3&gt;&lt;UDF4&gt;), which can directly reuse the partitioning done for the filter UDF also for the UDF3.
<tables id="tabl0007" num="0007"><table frame="all"><title><b>Table 7: Full Execution Plan</b></title><tgroup cols="5"><colspec colnum="1" colname="col1" colwidth="18mm"/><colspec colnum="2" colname="col2" colwidth="24mm"/><colspec colnum="3" colname="col3" colwidth="17mm"/><colspec colnum="4" colname="col4" colwidth="14mm"/><colspec colnum="5" colname="col5" colwidth="15mm"/><thead><row><entry valign="top"><u>Env.</u></entry><entry valign="top"><u>Plan Version</u></entry><entry align="center" valign="top"><u>Time</u></entry><entry align="center" valign="top"><i><u>σ</u></i></entry><entry valign="top"><u>Factor</u></entry></row></thead><tbody><row><entry><i>24 LOC</i></entry><entry>basic plan</entry><entry align="char" char=".">1793.72</entry><entry align="char" char=".">168.1</entry><entry align="center">-</entry></row><row><entry><i>24 LOC</i></entry><entry>best plan</entry><entry align="char" char=".">907.68</entry><entry align="char" char=".">73.5</entry><entry align="center">2.0</entry></row><row><entry><i>4x6 dist.</i></entry><entry>basic plan</entry><entry align="char" char=".">1179.25</entry><entry align="char" char=".">8.1</entry><entry align="center">-</entry></row><row><entry><i>4x6 dist.</i></entry><entry>best plan</entry><entry align="char" char=".">471.56</entry><entry align="char" char=".">9.0</entry><entry align="center">2.5</entry></row><row><entry><i>4x24 dist.</i></entry><entry>basic plan</entry><entry align="char" char=".">1196.27</entry><entry align="char" char=".">20.4</entry><entry align="center">-</entry></row><row><entry><i>4x24 dist.</i></entry><entry>best plan</entry><entry align="char" char=".">390.49</entry><entry align="char" char=".">9.1</entry><entry align="center">3.1</entry></row></tbody></tgroup></table></tables></p><p id="p0214" num="0214">Table 7 shows the execution time for the full plan for the local execution (LOC 24). As the sub-plan <i>Build Graph</i> could not be executed in reasonable time on one of the nodes, it executed the local plan only on the more powerful machine mentioned at the beginning of the section. The distributed execution plans were generated for four nodes with up to 6 operators in parallel per node (4x6 dist.) and also for up to 24 operators in parallel per node with the same hardware as the previous experiments. As a consequence, the measurements on the local nodes and for the distributed environment are not immediately comparable because the nodes in the distributed landscape are less powerful.</p><p id="p0215" num="0215">The overall execution time is dominated by building the graph and the connected components. The experiments show that by increasing the parallelism (and distribution) in the execution plans one gets lower and more robust execution times. Only by applying the best possible rewrites does one achieve a speed-up of two on a single node. When adding compute power by distributing the plan across four nodes, one achieves even better performance. Also the benefit of an increased degree of parallelism during query execution is pronounced in the<!-- EPO <DP n="56"> --> distributed case-the performance improvement goes up to a factor of 2.5 or even 3.1 compared to the basic plan alternative. In absolute numbers the slowest plan on the more powerful single node is more than four times slower than the fastest plan in the four node distributed landscape.</p><heading id="h0052"><i>Findings</i></heading><p id="p0216" num="0216">It can be important to combine the optimization of relational operators and user-defined functions because many large-scale data analysis tasks can benefit. To be able to optimize such complex workflows, a number of annotations are described that enable the optimizer to apply rewrites that increase the ability to parallelize plan execution. Annotations can be helpful because in general a query optimizer will not be able to derive the algebraic properties of a user defined function. Based on these annotations a set of rewrites were developed that allow for better parallelization of complex workflows. In the experiments it is shown that the rewrites are beneficial for real-world scenarios. One observes significant speed-up and also lower variance in the query execution times after applying the optimizations.</p><heading id="h0053"><b>Example 30 - Example Computing Environment</b></heading><p id="p0217" num="0217"><figref idrefs="f0019">FIG. 19</figref> depicts a generalized example of a suitable computing environment (e.g., computing system) 1900 in which the described innovations may be implemented. The computing environment 1900 is not intended to suggest any limitation as to scope of use or functionality, as the innovations may be implemented in diverse general-purpose or special-purpose computing systems. For example, the computing environment 1900 can be any of a variety of computing devices (e.g., desktop computer, laptop computer, server computer, tablet computer, etc.).</p><p id="p0218" num="0218">With reference to <figref idrefs="f0019">FIG. 19</figref>, the computing environment 1900 includes one or more processing units 1910, 1915 and memory 1920, 1925. In <figref idrefs="f0019">FIG. 19</figref>, this basic configuration 1930 is included within a dashed line. The processing units 1910, 1915 execute computer-executable instructions. A processing unit can be a general-purpose central processing unit (CPU), processor in an application-specific integrated circuit (ASIC) or any other type of processor. In a<!-- EPO <DP n="57"> --> multi-processing system, multiple processing units execute computer-executable instructions to increase processing power. For example, <figref idrefs="f0019">FIG. 19</figref> shows a central processing unit 1910 as well as a graphics processing unit or co-processing unit 1915. The tangible memory 1920, 1925 may be volatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory, etc.), or some combination of the two, accessible by the processing unit(s). The memory 1920, 1925 stores software 1980 implementing one or more innovations described herein, in the form of computer-executable instructions suitable for execution by the processing unit(s).</p><p id="p0219" num="0219">A computing system may have additional features. For example, the computing environment 1900 includes storage 1940, one or more input devices 1950, one or more output devices 1960, and one or more communication connections 1970. An interconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing environment 1900. Typically, operating system software (not shown) provides an operating environment for other software executing in the computing environment 1900, and coordinates activities of the components of the computing environment 1900.</p><p id="p0220" num="0220">The tangible storage 1940 may be removable or non-removable, and includes magnetic disks, magnetic tapes or cassettes, CD-ROMs, DVDs, or any other medium which can be used to store information in a non-transitory way and which can be accessed within the computing environment 1900. The storage 1940 stores instructions for the software 1980 implementing one or more innovations described herein. For example, the rules engine and others described herein can be the software 1980 executed from the memory 1920.</p><p id="p0221" num="0221">The input device(s) 1950 may be a touch input device such as a keyboard, mouse, pen, or trackball, a voice input device, a scanning device, or another device that provides input to the computing environment 1900. The output device(s) 1960 may be a display, printer, speaker, CD-writer, or another device that provides output from the computing environment 1900.</p><p id="p0222" num="0222">The communication connection(s) 1970 enable communication over a communication medium to another computing entity. The communication medium conveys information such as<!-- EPO <DP n="58"> --> computer-executable instructions, audio or video input or output, or other data in a modulated data signal. A modulated data signal is a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media can use an electrical, optical, RF, or other carrier.</p><p id="p0223" num="0223">Although direct connection between computer systems is shown in some examples, in practice, components can be arbitrarily coupled via a network that coordinates communication.</p><p id="p0224" num="0224">Although the operations of some of the disclosed methods are described in a particular, sequential order for convenient presentation, it should be understood that this manner of description encompasses rearrangement, unless a particular ordering is required by specific language set forth below. For example, operations described sequentially may in some cases be rearranged or performed concurrently. Moreover, for the sake of simplicity, the attached figures may not show the various ways in which the disclosed methods can be used in conjunction with other methods.</p><p id="p0225" num="0225">Any of the disclosed methods can be implemented as computer-executable instructions stored on one or more computer-readable storage media (e.g., one or more optical media discs, volatile memory components (such as DRAM or SRAM), or nonvolatile memory components (such as flash memory or hard drives)) and executed on a computer (e.g., any commercially available computer, including smart phones or other mobile devices that include computing hardware). The term computer-readable storage media does not include communication connections, such as signals and carrier waves. Any of the computer-executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer-readable storage media. The computer-executable instructions can be part of, for example, a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application (such as a remote computing application). Such software can be executed, for example, on a single local computer (e.g., any suitable commercially available computer) or in a network environment (e.g., via the Internet, a wide-area network, a local-area<!-- EPO <DP n="59"> --> network, a client-server network (such as a cloud computing network), or other such network) using one or more network computers.</p><p id="p0226" num="0226">For clarity, only certain selected aspects of the software-based implementations are described. Other details that are well known in the art are omitted. For example, it should be understood that the disclosed technology is not limited to any specific computer language or program. For instance, the disclosed technology can be implemented by software written in C++, Java, Perl, JavaScript, Adobe Flash, or any other suitable programming language. Likewise, the disclosed technology is not limited to any particular computer or type of hardware. Certain details of suitable computers and hardware are well known and need not be set forth in detail in this disclosure.</p><p id="p0227" num="0227">It should also be well understood that any functionality described herein can be performed, at least in part, by one or more hardware logic components, instead of software. For example, and without limitation, illustrative types of hardware logic components that can be used include Field-programmable Gate Arrays (FPGAs), Program-specific Integrated Circuits (ASICs), Program-specific Standard Products (ASSPs), System-on-a-chip systems (SOCs), Complex Programmable Logic Devices (CPLDs), etc.</p><p id="p0228" num="0228">Furthermore, any of the software-based embodiments (comprising, for example, computer-executable instructions for causing a computer to perform any of the disclosed methods) can be uploaded, downloaded, or remotely accessed through a suitable communication means. Such suitable communication means include, for example, the Internet, the World Wide Web, an intranet, software applications, cable (including fiber optic cable), magnetic communications, electromagnetic communications (including RF, microwave, and infrared communications), electronic communications, or other such communication means.</p><p id="p0229" num="0229">The disclosed methods, apparatus, and systems should not be construed as limiting in any way. Instead, the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments, alone and in various combinations and subcombinations with one another. The disclosed methods, apparatus, and systems are not<!-- EPO <DP n="60"> --> limited to any specific aspect or feature or combination thereof, nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved.</p><heading id="h0054"><b>Example 31 - Example Cloud-Supported Environment</b></heading><p id="p0230" num="0230">In example environment 2000, the cloud 2010 provides services for connected devices 2030, 2040, 2050 with a variety of screen capabilities. Connected device 2030 represents a device with a computer screen 2035 (e.g., a mid-size screen). For example, connected device 2030 could be a personal computer such as desktop computer, laptop, notebook, netbook, or the like. Connected device 2040 represents a device with a mobile device screen 2045 (e.g., a small size screen). For example, connected device 2040 could be a mobile phone, smart phone, personal digital assistant, tablet computer, and the like. Connected device 2050 represents a device with a large screen 2055. For example, connected device 2050 could be a television screen (e.g., a smart television) or another device connected to a television (e.g., a set-top box or gaming console) or the like. One or more of the connected devices 2030, 2040, 2050 can include touch screen capabilities. Touchscreens can accept input in different ways. For example, capacitive touchscreens detect touch input when an object (e.g., a fingertip or stylus) distorts or interrupts an electrical current running across the surface. As another example, touchscreens can use optical sensors to detect touch input when beams from the optical sensors are interrupted. Physical contact with the surface of the screen is not necessary for input to be detected by some touchscreens. Devices without screen capabilities also can be used in example environment 2000. For example, the cloud 2010 can provide services for one or more computers (e.g., server computers) without displays.</p><p id="p0231" num="0231">Services can be provided by the cloud 2010 through cloud service providers 2020, or through other providers of online services (not depicted). For example, cloud services can be customized to the screen size, display capability, and/or touch screen capability of a particular connected device (e.g., connected devices 2030, 2040, 2050).</p><p id="p0232" num="0232">In example environment 2000, the cloud 2010 provides the technologies and solutions described herein to the various connected devices 2030, 2040, 2050 using, at least in part, the service providers 2020. For example, the service providers 2020 can provide a centralized<!-- EPO <DP n="61"> --> solution for various cloud-based services. The service providers 2020 can manage service subscriptions for users and/or devices (e.g., for the connected devices 2030, 2040, 2050 and/or their respective users).</p><heading id="h0055"><b>Non-Transitory Computer-Readable Media</b></heading><p id="p0233" num="0233">Any of the computer-readable media herein can be non-transitory (e.g., memory, magnetic storage, optical storage, solid-state drives, or the like).</p><heading id="h0056"><b>Storing in Computer-Readable Media</b></heading><p id="p0234" num="0234">Any of the storing actions described herein can be implemented by storing in one or more computer-readable media (e.g., computer-readable storage media or other tangible media).</p><p id="p0235" num="0235">Any of the things described as stored can be stored in one or more computer-readable media (e.g., computer-readable storage media or other tangible media).</p><heading id="h0057"><b>Methods in Computer-Readable Media</b></heading><p id="p0236" num="0236">Any of the methods described herein can be implemented by computer-executable instructions in (e.g., encoded on) one or more computer-readable media (e.g., computer-readable storage media or other tangible media). Such instructions can cause a computer to perform the method. The technologies described herein can be implemented in a variety of programming languages.</p><heading id="h0058"><b>Methods in Computer-Readable Storage Devices</b></heading><p id="p0237" num="0237">Any of the methods described herein can be implemented by computer-executable instructions stored in one or more computer-readable storage devices (e.g., memory, magnetic storage, optical storage, solid-state drives, or the like). Such instructions can cause a computer to perform the method.<!-- EPO <DP n="62"> --></p><heading id="h0059"><b>Alternatives</b></heading><p id="p0238" num="0238">The technologies from any example can be combined with the technologies described in any one or more of the other examples. In view of the many possible embodiments to which the principles of the disclosed technology may be applied, it should be recognized that the illustrated embodiments are examples of the disclosed technology and should not be taken as a limitation on the scope of the disclosed technology. Rather, the scope of the disclosed technology includes what is covered by the following claims. We therefore claim as our invention all that comes within the scope and spirit of the claims.</p></description><claims mxw-id="PCLM90459471" lang="EN" load-source="patent-office"><!-- EPO <DP n="63"> --><claim id="c-en-0001" num="0001"><claim-text>A query execution plan optimization system comprising:
<claim-text>one or more processors;</claim-text>
<claim-text>memory coupled to the one or more processors;</claim-text>
<claim-text>a stored representation of a software program, wherein the representation of the software program comprises both one or more user-defined functions and one or more other operations, and the representation of the software program indicates at least one property for a user-defined function out of the user-defined functions and at least one property for an operation out of the other operations, wherein the operation comprises another user-defined function or a core relational database operation; and</claim-text>
<claim-text>an optimizer configured to transform the representation of the software program into an optimized query execution plan optimized for parallel execution according to the user-defined function property and the operation property.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The query execution plan optimization system of claim 1 wherein:
<claim-text>the user-defined function property comprises a permitted range of partition arrangements as an input pre-condition when the user-defined function is executed in parallel; and</claim-text>
<claim-text>the optimizer considers a plurality of partition arrangements in the range.</claim-text></claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The optimization system of claim 2 wherein:
<claim-text>the optimizer is configured to leverage partitioning already performed when within the permitted range of partition arrangements.</claim-text><!-- EPO <DP n="64"> --></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The optimization system of any one of claims 1 to 3 wherein:
<claim-text>the operation property comprises a global structural property comprising a post-condition partition arrangement of an output of the operation;</claim-text>
<claim-text>the user-defined function property comprises a global structural property comprising an expected pre-condition partition arrangement for an input of the user-defined function when the user-defined function is executed in parallel; and</claim-text>
<claim-text>the optimizer is configured to introduce an exchange operation at a detected property mismatch at an input-output coupling in the stored representation of the software program.</claim-text></claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The optimization system of any one of claims 1 to 4 wherein:
<claim-text>the optimizer supports optimization over of a range of pre-condition properties.</claim-text></claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The optimization system of any one of claims 1 to 5 wherein:
<claim-text>the stored representation of the software program represents a full merge and initial partitioning operation; and</claim-text>
<claim-text>the optimizer is configured to rewrite the full merge and initial partitioning operation into one or more rewritten operations.</claim-text></claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The optimization system of claim 6 wherein:
<claim-text>the full merge and initial partitioning operation of the stored representation of the query execution plan appears between a first operation with an output having an output property and a second operation with an input having an input property and coupled to the output of the first operation, wherein the input property specifies a range of permitted partition arrangements; and</claim-text>
<claim-text>the one or more rewritten operations are selected based on the output property and the input property.</claim-text><!-- EPO <DP n="65"> --></claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The optimization system of any one of claims 1 to 7, wherein the optimizer selects from a plurality of alternative re-writes based on number of cores, number of nodes, processing cost, and processing size; and/or<br/>
wherein the optimizer implements a cost-based decision making process to choose a degree of parallelism.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>The optimization system of any one of claims 1 to 8 wherein:
<claim-text>the stored representation specifies a merge technique for parallelized instances of the user-defined function; and</claim-text>
<claim-text>the optimizer is configured to apply the merge technique when generating the optimized query execution plan optimized for parallel execution; and wherein optionally:
<claim-text>the merge technique comprises user-defined merge operations; and</claim-text>
<claim-text>the optimizer applies the user-defined merge operations when generating the optimized query execution plan optimized for parallel execution.</claim-text></claim-text></claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>The optimization system of any one of claims 1 to 9, wherein the stored representation indicates an expected sorting condition for input to parallelized instances of the user-defined function, and<br/>
the optimizer is configured to enforce the expected sorting condition when generating the optimized query execution plan optimized for parallel execution; and/or<br/>
wherein the stored representation indicates an ensured sorting condition for output of parallelized instances of the user-defined function, and<br/>
the optimizer is configured to leverage the ensured sorting condition when generating the optimized query execution plan optimized for parallel execution.<!-- EPO <DP n="66"> --></claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The optimization system of any one of claims 1 to 10 wherein for the user-defined function, the stored representation indicates an expected size of an output relation in terms of size of an input relation, and<br/>
the optimizer is configured to consider the expected size when generating the optimized query execution plan optimized for parallel execution; and/or<br/>
wherein for the user-defined function, the stored representation indicates an expected run time in terms of size of an input relation, and<br/>
the optimizer is configured to consider the expected run time when generating the optimized query execution plan optimized for parallel execution.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>The optimization system of any one of claims 1 to 11 wherein:
<claim-text>the query execution plan represents a loop or nested operations.</claim-text></claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>A method implemented at least in part by a computing system, the method comprising:
<claim-text>receiving a representation of one or more user-defined functions and one or more core database relational operations, wherein the representation comprises at least one property for at least one of the user-defined functions and at least one property for at least one of the core database relational operations; and</claim-text>
<claim-text>generating a query execution plan optimized for parallel execution of the one or more user-defined functions and the one or more core database relational operations according to the user-defined function property and the core database relational operation property.</claim-text></claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>The method of claim 13 wherein generating the optimized query execution plan comprises:
<claim-text>traversing the representation wherein the traversing comprises comparing properties of a child plan with permitted properties of a current operation, and, in case of a mismatch, introducing an exchange operation, and</claim-text>
<claim-text>wherein introducing an exchange operation may comprise:
<claim-text>considering alternatives, and selecting an alternative out of the alternatives based on estimated costs and an explicitly specified range of permitted partition</claim-text></claim-text><!-- EPO <DP n="67"> -->
15. One or more computer-readable media comprising computer-executable instructions causing a computing system to perform a method compris-ing:
<claim-text>receiving a representation of a program comprising a core database relational operation and a user-defined function coupled to the at least one core database relational operation, wherein the representation comprises an input property specifying a range of permitted partition arrangements as an input pre-condition for parallelized instances of the user-defined function, and the representation comprises an output property specifying a partition arrangement post-condition for the core database relational operation;</claim-text>
<claim-text>detecting a mismatch between the pre-condition and the post-condition in the representation of the program;</claim-text>
<claim-text>responsive to detecting the mismatch between the pre-condition and the post-condition, considering a plurality of possible exchange operations remedying the mismatch that satisfy the input pre-condition for parallelized instances of the user-defined function; and</claim-text>
<claim-text>incorporating the exchange operation into an optimized query execution plan implementing the program, whereby the optimized query execution plan avoids a full merge and initial partitioning operation to remedy the mismatch.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW20422198" load-source="patent-office"><!-- EPO <DP n="68"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="207" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="69"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="150" he="221" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="70"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="150" he="209" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="71"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="150" he="209" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="72"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="165" he="199" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="73"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="160" he="224" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="74"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="135" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="75"> --><figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="135" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="76"> --><figure id="f0009" num="9"><img id="if0009" file="imgf0009.tif" wi="135" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="77"> --><figure id="f0010" num="10"><img id="if0010" file="imgf0010.tif" wi="165" he="202" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="78"> --><figure id="f0011" num="11"><img id="if0011" file="imgf0011.tif" wi="141" he="213" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="79"> --><figure id="f0012" num="12"><img id="if0012" file="imgf0012.tif" wi="165" he="148" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="80"> --><figure id="f0013" num="13"><img id="if0013" file="imgf0013.tif" wi="165" he="162" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="81"> --><figure id="f0014" num="14"><img id="if0014" file="imgf0014.tif" wi="88" he="115" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="82"> --><figure id="f0015" num="15"><img id="if0015" file="imgf0015.tif" wi="165" he="178" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="83"> --><figure id="f0016" num="16"><img id="if0016" file="imgf0016.tif" wi="165" he="178" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="84"> --><figure id="f0017" num="17"><img id="if0017" file="imgf0017.tif" wi="165" he="216" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="85"> --><figure id="f0018" num="18"><img id="if0018" file="imgf0018.tif" wi="162" he="218" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="86"> --><figure id="f0019" num="19"><img id="if0019" file="imgf0019.tif" wi="128" he="188" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="87"> --><figure id="f0020" num="20"><img id="if0020" file="imgf0020.tif" wi="151" he="189" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="158" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
