<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960828-A1" country="EP" doc-number="2960828" kind="A1" date="20151230" family-id="53488145" file-reference-id="301210" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451449" ucid="EP-2960828-A1"><document-id><country>EP</country><doc-number>2960828</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15171613-A" is-representative="YES"><document-id mxw-id="PAPP193865866" load-source="docdb" format="epo"><country>EP</country><doc-number>15171613</doc-number><kind>A</kind><date>20150611</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865867" load-source="patent-office" format="original"><country>EP</country><doc-number>15171613.1</doc-number><date>20150611</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162035244" ucid="JP-2014131594-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2014131594</doc-number><kind>A</kind><date>20140626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988524806" load-source="docdb">G06K   9/00        20060101AFI20151120BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1984696522" load-source="docdb" scheme="CPC">G06K   9/46        20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697017" load-source="docdb" scheme="CPC">G06K   9/00315     20130101 FI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697023" load-source="docdb" scheme="CPC">G06K   9/00624     20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699175" load-source="docdb" scheme="CPC">G06K   9/00308     20130101 LI20160106BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984702535" load-source="docdb" scheme="CPC">G06K2009/4666      20130101 LA20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984702994" load-source="docdb" scheme="CPC">G06K   9/00228     20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984703312" load-source="docdb" scheme="CPC">G06K   9/00268     20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984703424" load-source="docdb" scheme="CPC">G06K   9/00288     20130101 LI20160106BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987773922" load-source="docdb" scheme="CPC">G06K   9/00261     20130101 LI20151118BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987789235" load-source="docdb" scheme="CPC">G06K   9/00906     20130101 LI20151118BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545813" lang="DE" load-source="patent-office">GESICHTSAUTHENTIFIZIERUNGSVORRICHTUNG UND GESICHTSAUTHENTIFIZIERUNGSVERFAHREN</invention-title><invention-title mxw-id="PT165545814" lang="EN" load-source="patent-office">FACE AUTHENTICATION DEVICE AND FACE AUTHENTICATION METHOD</invention-title><invention-title mxw-id="PT165545815" lang="FR" load-source="patent-office">APPAREIL ET PROCÉDÉ D'AUTHENTIFICATION DU VISAGE</invention-title><citations><patent-citations><patcit mxw-id="PCIT335743841" load-source="docdb" ucid="EP-2560123-A1"><document-id format="epo"><country>EP</country><doc-number>2560123</doc-number><kind>A1</kind><date>20130220</date></document-id><sources><source name="SEA" category="Y" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335743700" load-source="docdb" ucid="JP-2006115406-A"><document-id format="epo"><country>JP</country><doc-number>2006115406</doc-number><kind>A</kind><date>20060427</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335739887" load-source="docdb" ucid="US-20130015946-A1"><document-id format="epo"><country>US</country><doc-number>20130015946</doc-number><kind>A1</kind><date>20130117</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335745437" load-source="docdb" ucid="US-8457367-B1"><document-id format="epo"><country>US</country><doc-number>8457367</doc-number><kind>B1</kind><date>20130604</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>BING-ZHONG JING ET AL: "Anti-spoofing system for RFID access control combining with face recogntion", MACHINE LEARNING AND CYBERNETICS (ICMLC), 2010 INTERNATIONAL CONFERENCE ON, IEEE, PISCATAWAY, NJ, USA, 11 July 2010 (2010-07-11), pages 698 - 703, XP031759655, ISBN: 978-1-4244-6526-2</text><sources><source mxw-id="PNPL57906878" load-source="docdb" name="SEA" category="IY"/></sources></nplcit><nplcit><text>KOLLREIDER K ET AL: "Verifying liveness by multiple experts in face biometrics", COMPUTER VISION AND PATTERN RECOGNITION WORKSHOPS, 2008. CVPR WORKSHOPS 2008. IEEE COMPUTER SOCIETY CONFERENCE ON, IEEE, PISCATAWAY, NJ, USA, 23 June 2008 (2008-06-23), pages 1 - 6, XP031285671, ISBN: 978-1-4244-2339-2</text><sources><source mxw-id="PNPL57906879" load-source="docdb" name="SEA" category="A"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103304280" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>OMRON TATEISI ELECTRONICS CO</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR1103318342" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>OMRON CORPORATION</last-name></addressbook></applicant><applicant mxw-id="PPAR1101642610" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>OMRON CORPORATION</last-name><iid>100192151</iid><address><street>801, Minamifudodo-cho, Horikawahigashiiru, Shiokoji-dori, Shimogyo-ku</street><city>Kyoto-shi, Kyoto 600-8530</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103309059" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>MATSUNAGA JUMPEI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103314076" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>MATSUNAGA, JUMPEI</last-name></addressbook></inventor><inventor mxw-id="PPAR1101648559" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>MATSUNAGA, JUMPEI</last-name><address><street>c/o Omron Corporation, 801 Minamifudodo-cho, Horikawahigashiiru, Shiokoji-dori, Shimogyo-ku</street><city>Kyoto-shi, Kyoto 600-8530</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103321031" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>IRIE ATSUSHI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103304948" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>IRIE, ATSUSHI</last-name></addressbook></inventor><inventor mxw-id="PPAR1101643974" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>IRIE, ATSUSHI</last-name><address><street>c/o Omron Corporation, 801 Minamifudodo-cho, Horikawahigashiiru, Shiokoji-dori, Shimogyo-ku</street><city>Kyoto-shi, Kyoto 600-8530</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101645511" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Kilian Kilian &amp; Partner</last-name><iid>101356544</iid><address><street>Aidenbachstrasse 54</street><city>81379 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660684857" load-source="docdb">AL</country><country mxw-id="DS660687374" load-source="docdb">AT</country><country mxw-id="DS660684858" load-source="docdb">BE</country><country mxw-id="DS660610141" load-source="docdb">BG</country><country mxw-id="DS660606465" load-source="docdb">CH</country><country mxw-id="DS660690071" load-source="docdb">CY</country><country mxw-id="DS660690072" load-source="docdb">CZ</country><country mxw-id="DS660610314" load-source="docdb">DE</country><country mxw-id="DS660684871" load-source="docdb">DK</country><country mxw-id="DS660684872" load-source="docdb">EE</country><country mxw-id="DS660783365" load-source="docdb">ES</country><country mxw-id="DS660610142" load-source="docdb">FI</country><country mxw-id="DS660610155" load-source="docdb">FR</country><country mxw-id="DS660610323" load-source="docdb">GB</country><country mxw-id="DS660684873" load-source="docdb">GR</country><country mxw-id="DS660610324" load-source="docdb">HR</country><country mxw-id="DS660690073" load-source="docdb">HU</country><country mxw-id="DS660606466" load-source="docdb">IE</country><country mxw-id="DS660684874" load-source="docdb">IS</country><country mxw-id="DS660610156" load-source="docdb">IT</country><country mxw-id="DS660684879" load-source="docdb">LI</country><country mxw-id="DS660610325" load-source="docdb">LT</country><country mxw-id="DS660687379" load-source="docdb">LU</country><country mxw-id="DS660610157" load-source="docdb">LV</country><country mxw-id="DS660610326" load-source="docdb">MC</country><country mxw-id="DS660687380" load-source="docdb">MK</country><country mxw-id="DS660687381" load-source="docdb">MT</country><country mxw-id="DS660687382" load-source="docdb">NL</country><country mxw-id="DS660606467" load-source="docdb">NO</country><country mxw-id="DS660687387" load-source="docdb">PL</country><country mxw-id="DS660783366" load-source="docdb">PT</country><country mxw-id="DS660685006" load-source="docdb">RO</country><country mxw-id="DS660783367" load-source="docdb">RS</country><country mxw-id="DS660687388" load-source="docdb">SE</country><country mxw-id="DS660610332" load-source="docdb">SI</country><country mxw-id="DS660606468" load-source="docdb">SK</country><country mxw-id="DS660606469" load-source="docdb">SM</country><country mxw-id="DS660684880" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479819" lang="EN" load-source="patent-office"><p id="pa01" num="0001">High-reliability face authentication is performed at low processing cost. A face authentication device includes: a face image acquisition unit configured to continuously acquire face images of a user; a user identification unit configured to identify the user based on a first face image acquired by the face image acquisition unit; a parameter acquisition unit configured to acquire a face parameter based on a second face image acquired by the face image acquisition unit, the face parameter being a parameter associated with a facial expression or a face direction of the user; a tracking unit configured to extract a region of a part of a face included in the first face image, and to track the region between the plurality of continuously-acquired face images; and an authentication unit configured to make a determination of successful authentication when the user identified by the user identification unit is a registered user, when the acquired face parameter satisfies a predetermined condition, and when the extracted region is continuously tracked between the plurality of face images.</p></abstract><abstract mxw-id="PA166759631" lang="EN" source="EPO" load-source="docdb"><p>High-reliability face authentication is performed at low processing cost. A face authentication device includes: a face image acquisition unit configured to continuously acquire face images of a user; a user identification unit configured to identify the user based on a first face image acquired by the face image acquisition unit; a parameter acquisition unit configured to acquire a face parameter based on a second face image acquired by the face image acquisition unit, the face parameter being a parameter associated with a facial expression or a face direction of the user; a tracking unit configured to extract a region of a part of a face included in the first face image, and to track the region between the plurality of continuously-acquired face images; and an authentication unit configured to make a determination of successful authentication when the user identified by the user identification unit is a registered user, when the acquired face parameter satisfies a predetermined condition, and when the extracted region is continuously tracked between the plurality of face images.</p></abstract><description mxw-id="PDES98404520" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p0001" num="0001">This application is based on Japanese Patent Application No. <patcit id="pcit0001" dnum="JP2014131594A"><text>2014-131594</text></patcit> filed with the Japan Patent Office on June 26, 2014, the entire contents of which are incorporated herein by reference.</p><heading id="h0002">FIELD</heading><p id="p0002" num="0002">The present invention relates to a device that authenticates a user based on a face image.</p><heading id="h0003">BACKGROUND</heading><p id="p0003" num="0003">Nowadays, a device that identifies a person based on an image (face image) in which a face of the person is photographed is becoming widespread. For example, a feature amount is previously extracted from a face image of an authorized person, and is compared with a feature amount extracted from the acquired face image, which allows the person to be identified based on the face image.</p><p id="p0004" num="0004">On the other hand, by reading the face image (such as a photograph) of the authorized person, an unauthorized person impersonating as the authorized person may pass the authentication. Therefore, in the field of face authentication, prevention of impersonation becomes a major problem.</p><p id="p0005" num="0005">For example, in a face authentication method described in <patcit id="pcit0002" dnum="US8457367B"><text>U. S. Patent No. 8457367</text></patcit>, a plurality of face images are acquired, the authentication is performed a plurality of times using the plurality of face images, and a<!-- EPO <DP n="2"> --> determination of an authorized user is made when all the face images are authenticated. Therefore, because the authentication can be performed a plurality of times using different facial expressions such as blankness for a first time and smile for a second time, resistance against attack can be enhanced in which a face photograph of the authorized user is previously prepared.</p><p id="p0006" num="0006">According to the description of <patcit id="pcit0003" dnum="US8457367B"><text>U. S. Patent No. 8457367</text></patcit>, security can be enhanced compared with the authentication in which only one face image is used. However, because the authentication of a person in question based on the face image involves complicated processing, there is a problem in that processing cost increases when the number of authentication times is increased to enhance the security.</p><p id="p0007" num="0007">In the face authentication, because the comparison is performed by quantifying a feature of the face, the feature of the face cannot correctly be read in the state where the user makes the facial expression, namely, the state where the user strengthens muscles of facial expression, and sometimes a determination of mismatch is made for even the authorized user. That is, there is a problem in that a rate of failed authentication increases compared with the usual face authentication.</p><heading id="h0004">SUMMARY</heading><p id="p0008" num="0008">The present invention has been made in view of the above problems, and an object of the present invention is to provide a technique of performing high-reliability face authentication at low cost.</p><p id="p0009" num="0009">To solve the above problems, in accordance with one aspect of the present invention, a face authentication device includes: a face image<!-- EPO <DP n="3"> --> acquisition unit configured to continuously acquire face images of a user; a user identification unit configured to identify the user based on a first face image acquired by the face image acquisition unit; a parameter acquisition unit configured to acquire a face parameter based on a second face image acquired by the face image acquisition unit, the face parameter being a parameter associated with a facial expression or a face direction of the user; a tracking unit configured to extract a region of a part of a face included in the first face image, and to track the region between the plurality of continuously-acquired face images; and an authentication unit configured to make a determination of successful authentication when the user identified by the user identification unit is a registered user, when the acquired face parameter satisfies a predetermined condition, and when the extracted region is continuously tracked between the plurality of face images.</p><p id="p0010" num="0010">The face authentication device of the present invention acquires the plurality of face images, and authenticates the user using the plurality of face images.</p><p id="p0011" num="0011">The face image acquisition unit acquires the face image of the user. For example, the face image acquisition unit acquires the face image from a camera connected to an outside. It is not always necessary to directly acquire the face image from the camera. For example, the face image may be acquired from another system such as an entry management system. The face image acquisition unit can acquire the plurality of face images as the continuous frames.</p><p id="p0012" num="0012">The first face image is used to identify whether the face included in the image is the face of the authorized user (registered user). For example, the<!-- EPO <DP n="4"> --> user identification unit identifies the face included in the first face image using a well-known technique.</p><p id="p0013" num="0013">The second face image is used to read the parameter (face parameter) associated with the facial expression or the face direction of the user. For example, the face parameter may be the parameter indicating a degree of a specific facial expression or the parameter indicating the face direction. The facial expression may not necessarily express emotion as long as the facial expression is associated with an appearance of the face. For example, the face parameter may be the parameter indicating a degree of shutting an eye or a degree of opening a mouth.</p><p id="p0014" num="0014">For example, the parameter acquisition unit acquires the face parameter corresponding to the face included in the second face image using a well-known technique. The second face image may be a plurality of images.</p><p id="p0015" num="0015">The tracking unit tracks the region of the part included in the acquired face image between the plurality of continuously-acquired face images. For example, a predetermined face part (such as an eye, a region around the eye, a mouth, and lips) is extracted from the acquired face image, a corresponding face part is searched in different face images, and a moving destination is tracked.</p><p id="p0016" num="0016">The region of the tracking target may be the region corresponding to the face part included in the face, or a closed region having any shape. For example, the whole face may be set as the tracking target. In this case, a rectangle representing the whole face is set, and the tracking may be performed between the plurality of images.</p><p id="p0017" num="0017">The tracking target may be a feature point, and the plurality of tracking targets may be used.<!-- EPO <DP n="5"> --></p><p id="p0018" num="0018">The authentication unit performs the final authentication based on the pieces of information acquired from the user identification unit, the parameter acquisition unit, and the tracking unit. Specifically, in addition to the conventional processing of identifying the user based on the face image, the determination of the successful authentication is made when "the acquired face parameter satisfies the condition", and when "the extracted region is continuously tracked between the plurality of face images".</p><p id="p0019" num="0019">The description "the extracted region is continuously tracked" means that the region of the tracking target exist in the image region for all the target frames. That is, tracking unit fails to perform the tracking when the frame-out of the region of the tracking target is generated in the intermediate frame, or when a coordinate of the moving destination cannot be determined in the intermediate frame.</p><p id="p0020" num="0020">In the face authentication device, the face image acquisition unit may further acquire at least one intermediate face image between a time at which the first face image is acquired and a time at which the second face image is acquired, and the tracking unit may track the extracted region using the intermediate face image.</p><p id="p0021" num="0021">Thus, the tracking unit may track the region using the frame between the first face image and the second face image. For example, in the case where a frame rate of the face image acquisition unit is 30 FPS, the tracking may be performed every 1/30 seconds.</p><p id="p0022" num="0022">For example, a face image acquisition interval is 1 second or less, preferably 0.5 seconds or less, more preferably 0.2 seconds or less. The region of the tracking target is monitored while the face image acquisition<!-- EPO <DP n="6"> --> interval is set shorter, the false authentication caused by such actions that the face photograph presented to the camera is replaced can be prevented.</p><p id="p0023" num="0023">In the face authentication device, the second face image may be formed by a plurality of face images acquired at different times, the parameter acquisition unit may further acquire a plurality of face parameters corresponding to the plurality of face images, and the authentication unit may make the determination of successful authentication when all the plurality of acquired face parameters satisfy the predetermined condition.</p><p id="p0024" num="0024">The plurality of second face images may be used, and the second face images may be associated with the plurality of face parameters, respectively. For example, the parameter indicating a degree of delight may be used for the first time, and the parameter indicating a degree of sadness may be used for the second time. Therefore, the security can be further enhanced in the authentication.</p><p id="p0025" num="0025">The face parameter may be a parameter in which the facial expression of the user is quantified, and the face parameter may be a parameter in which the face direction of the user is quantified.</p><p id="p0026" num="0026">Thus, the comparison can easily be performed by dealing with the face parameter as a numerical value. The face parameter may be a single value or a set (vector) of a plurality of values.</p><p id="p0027" num="0027">The authentication unit may further acquire an amount of change per unit time of the face parameter, and authentication processing may be interrupted when the amount of change is larger than a predetermined value.</p><p id="p0028" num="0028">The tracking unit may further acquire an amount of movement per unit time of a tracking target region, and a determination of failed authentication may<!-- EPO <DP n="7"> --> be made when the amount of movement is larger than a predetermined value.</p><p id="p0029" num="0029">In the case where the face parameter changes rapidly, or in the case where the amount of movement per unit time is excessively large in the region of the tracking target, a determination of an unnatural state may be made to interrupt the authentication processing. Therefore, an impersonation attack with a plurality of previously-prepared face images (such as photographs) can be dealt with.</p><p id="p0030" num="0030">The authentication unit may interrupt the authentication processing when the authentication processing is not completed within a predetermined time.</p><p id="p0031" num="0031">In the case where the authentication processing is not completed within the predetermined time, it can be assumed that a person other than the authorized user tries the authentication. In such cases, preferably, the determination of the unnatural state may be made from the viewpoint of security to interrupt the authentication processing.</p><p id="p0032" num="0032">The tracking unit may track only a predetermined range existing near the extracted region when tracking the extracted region.</p><p id="p0033" num="0033">In the case where the target is the face of the human, it is hardly considered that the face moves rapidly in the image region. Therefore, the processing cost necessary for the tracking can be kept low only by setting the vicinity of the tracking target as the tracking range.</p><p id="p0034" num="0034">The present invention can be identified as a face authentication device including at least a part of the above units. The present invention can be identified as a face authentication method performed by the face authentication device. The present invention can be identified as a program<!-- EPO <DP n="8"> --> that causes a computer to execute the face authentication method. As long as the technical inconsistency is not generated, the pieces of processing or the units can be implemented while freely combined.</p><p id="p0035" num="0035">In the present invention, the high-reliability face authentication can be performed at low processing cost.</p><heading id="h0005">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0036" num="0036"><ul><li><figref idrefs="f0001">Fig. 1</figref> is a system configuration diagram of a face authentication device according to a first embodiment;</li><li><figref idrefs="f0002">Fig. 2</figref> illustrates an example of a face image at a person identification phase;</li><li><figref idrefs="f0002">Fig. 3</figref> illustrates an example of the face image at a facial expression determination phase;</li><li><figref idrefs="f0003">Figs. 4A to 4C</figref> are views illustrating an example of tracking a tracking region;</li><li><figref idrefs="f0004">Fig. 5</figref> is a view illustrating a flowchart of face authentication processing in the first embodiment;</li><li><figref idrefs="f0005">Figs. 6A to 6C</figref> illustrate an example of the face image in a second embodiment; and</li><li><figref idrefs="f0006">Figs. 7A to 7C</figref> illustrate an example of the face image in a third embodiment.</li></ul></p><heading id="h0006">DETAILED DESCRIPTION</heading><heading id="h0007">FIRST EMBODIMENT</heading><heading id="h0008">(System configuration)</heading><!-- EPO <DP n="9"> --><p id="p0037" num="0037">Hereinafter, a first exemplary embodiment of the present invention will be described with reference to the drawings.</p><p id="p0038" num="0038">A face authentication device according to the first embodiment acquires an image (hereinafter, referred to as a face image) including a face of a person to be authenticated, and authenticates the person based on the face image. The face authentication device of the first embodiment is connected to a door device including an electromagnetic lock, and the face authentication device has a function of unlocking the door when authentication is successfully performed. <figref idrefs="f0001">Fig. 1</figref> is a system configuration diagram of a face authentication device 100 of the first embodiment.</p><p id="p0039" num="0039">The face authentication device 100 includes a face image acquisition unit 101, a user identification unit 102, a facial expression determination unit 103, a tracking unit 104, a communication unit 105, an image generator 106, and an authentication unit 107.</p><p id="p0040" num="0040">The face image acquisition unit 101 acquires the face image from a camera 300 that is mounted so as to face a front side of the face authentication device 100. The face image acquisition unit 101 can continuously acquire the face image at a predetermined frame rate. The face image acquired by the face image acquisition unit 101 is supplied in real time to the user identification unit 102, the facial expression determination unit 103, and the tracking unit 104.</p><p id="p0041" num="0041">In the description of the embodiments, although the term "frame" is used in the meaning of one of the continuously-acquired face images, the frame is equivalent to the face image.</p><p id="p0042" num="0042">The user identification unit 102 detects the face included in the face image acquired by face image acquisition unit 101, and identifies the person<!-- EPO <DP n="10"> --> based on the face. Specifically, a region corresponding to the face is extracted from the face image to acquire a feature amount of the face. The feature amount of the face is compared with a previously-stored feature amount, and the person is identified based on a degree of matching (degree of similarity) of the feature amount.</p><p id="p0043" num="0043">The facial expression determination unit 103 acquires a value (hereinafter, referred to as a face parameter) regarding a facial expression based on the face image acquired by the face image acquisition unit 101. The face parameter is a value indicating a degree of delight, anger, sadness, surprise, fear, dislike, blankness, or the like. Because a method for acquiring the degree of facial expression in terms of value is disclosed in, for example, <patcit id="pcit0004" dnum="JP2006115406A"><text>JP 2006-115406 A</text></patcit>, the detailed description is omitted.</p><p id="p0044" num="0044">The tracking unit 104 tracks a region (hereinafter, referred to as a tracking region) set in the face image among a plurality of face images acquired by the face image acquisition unit 101. The tracking region is a region corresponding to a face part (such as a nose and an eye) included in the face. In the first embodiment, it is assumed that the tracking region is the region including the right eye of the person to be authenticated.</p><p id="p0045" num="0045">The tracking unit 104 determines whether the tracking region can be tracked to the end or is lost in midstream among the plurality of input face images. The successful tracking throughout all the frames means that the face of the person to be authenticated exists continuously in a vision of the camera 300 while the plurality of face images are captured. The failed tracking means that the face of the person to be authenticated has moved out of the vision of the camera. Thus, in the first embodiment, the tracking unit 104 performs the<!-- EPO <DP n="11"> --> tracking to check whether the face of the person to be authenticated exists continuously in the vision of the device during an authentication phase.</p><p id="p0046" num="0046">The communication unit 105 is an interface that conducts communication with an outside of the device. In the first embodiment, the door is unlocked by transmitting an unlocking signal to a door device 200 connected through wire.</p><p id="p0047" num="0047">The image generator 106 generates an image that is output to a display 400 arranged in parallel with the camera 300. The image generator 106 generates the image acquired by the face image acquisition unit 101 and the image including an instruction (for example, an instruction to make a specific facial expression) issued to the person to be authenticated, and provides the images to the person to be authenticated through the display 400.</p><p id="p0048" num="0048">The authentication unit 107 controls operations of the face image acquisition unit 101, the user identification unit 102, the facial expression determination unit 103, the tracking unit 104, the communication unit 105, and the image generator 106 to finally authenticate the person to be authenticated. The detailed processing content is described later.</p><p id="p0049" num="0049">The face image acquisition unit 101, the user identification unit 102, the facial expression determination unit 103, the tracking unit 104, the image generator 106, and the authentication unit 107 may be specifically-designed hardware or a software module. In the case where the face image acquisition unit 101, the user identification unit 102, the facial expression determination unit 103, the tracking unit 104, the image generator 106, and the authentication unit 107 are executed in the form of software, a program stored in an auxiliary storage device (not illustrated) is loaded on a main storage device (not<!-- EPO <DP n="12"> --> illustrated), and executed by a CPU (not illustrated) to function as each unit.</p><heading id="h0009">(Outline of authentication processing)</heading><p id="p0050" num="0050">An outline of the authentication processing performed by the face authentication device 100 will be described below. An image 20 in <figref idrefs="f0002">Fig. 2</figref> is generated by the image generator 106, and displayed on the display 400 during the authentication. The image includes the face image of the person to be authenticated, the face image being acquired by the face image acquisition unit 101.</p><p id="p0051" num="0051">The authentication processing performed by the face authentication device 100 can be divided into a phase to identify the person based on the face image and a phase to determine whether the facial expression of the person satisfies a predetermined condition.</p><heading id="h0010">&lt;Person identification phase&gt;</heading><p id="p0052" num="0052">The phase to identify the person phase based on the face image will be described. The phase described herein is referred to as a person identification phase. The user identification unit 102 performs the person identification phase by the following steps.</p><heading id="h0011">(1) Detection of face region</heading><p id="p0053" num="0053">When acquiring the face image from the face image acquisition unit 101, the user identification unit 102 detects the region including the face of the person (face region) from the face image. A face region 21 is detected in <figref idrefs="f0002">Fig. 2</figref>. For example, the face region can be detected by pattern matching in which texture information is used or template matching in which a template corresponding to a contour of the whole face is used. Alternatively, the template matching based on a face part constituting the face may be performed, or a region or points<!-- EPO <DP n="13"> --> indicating a part of the face are extracted and the face region may be determined based on an extraction result. Alternatively, the face region may be determined by detecting a skin color region in the face image. Any well-known method may be adopted as processing of detecting the face region.</p><heading id="h0012">(2) Arrangement of feature points</heading><p id="p0054" num="0054">Then, the user identification unit 102 arranges a plurality of feature points on the detected face region. For example, the feature points are arranged based on positions of face parts (such as an eye, a nose, a nostril, a mouth, an eyebrow, a jaw, and a forehead) of the face. Therefore, the user identification unit 102 detects a target face part from the face region. The detection target face part may be previously set, or dynamically fixed.</p><p id="p0055" num="0055">The feature points are arranged based on the position of the detected face part. Preferably, the feature points are densely arranged as they get closer to the face part, and the feature points are coarsely arranged as they separate farther from the face part. In the feature point arranging method, different arranging method may be used according to age or sexuality of the person to be authenticated.</p><heading id="h0013">(3) Acquisition of feature amount</heading><p id="p0056" num="0056">Then, the feature amount is calculated based on the arranged feature points. The feature amount can be calculated based on a value obtained through filtering processing such as a Gabor filter.</p><heading id="h0014">(4) Identification of person</heading><p id="p0057" num="0057">Then, the person included in the face image is identified by comparing the acquired feature amount with the previously-stored feature amount. Because well-known methods may be adopted as the feature amount acquisition<!-- EPO <DP n="14"> --> method and the comparison method, the detailed description is omitted.</p><p id="p0058" num="0058">In the first embodiment, the feature points are arranged after the face is detected, and the feature amount is calculated using the feature points. Alternatively, the person may be identified by another method. For example, the feature amount is directly calculated from the face region acquired from the above (1), and the person may be identified.</p><heading id="h0015">&lt;Facial expression determination phase&gt;</heading><p id="p0059" num="0059">The phase to determine whether the person to be authenticated makes a predetermined facial expression based on the face image will be described below. The phase described herein is referred to as a facial expression determination phase. In the first embodiment, the facial expression determination unit 103 acquires a face parameter, and makes the determination based on the face parameter. The face parameter is a value indicating a degree of a specific facial expression. In the first embodiment, it is a value indicating a degree of delight in a range of 0 to 100.</p><p id="p0060" num="0060">Only by performing the person identification phase as described above, even though the face included in the face image is identified as the face of the authorized person, it is difficult to determine whether the person actually exists in the vision of the camera. For example, a possibility that a person who tries impersonation (identity thief) holds a face photograph cannot completely be eliminated. Therefore, in the first embodiment, the facial expression determination unit 103 performs the following processing to further acquire the face parameter, and checks whether the person to be authenticated has made the facial expression satisfying the condition.</p><p id="p0061" num="0061">Similarly to the processing performed by the user identification unit<!-- EPO <DP n="15"> --> 102, the facial expression determination unit 103 arranges the feature points in the face image to acquire the feature amount, thereby acquiring the face parameter. However, the acquired feature amount is not the feature amount identifying an individual person, but the feature amount identifying the facial expression.</p><p id="p0062" num="0062">The feature point arrangement method is also an arrangement method specific to the identification of the facial expression. For example, in the case where the individual person is authenticated, the feature points are arranged around the face part (such as eyes, a nose, and a mouth) where an individual difference is easily obtained. On the other hand, in the case where the face parameter is acquired, the feature points are arranged around the face part (such as eyes, an eyebrow, a cheek, a forehead, and lips) where a change in facial expression is easily obtained.</p><p id="p0063" num="0063">In the first embodiment, similarly to the identification of the person, the feature points are arranged, and the feature amount is calculated using the feature points. Alternatively, the face parameter may be acquired by another method. For example, the feature amount may be calculated directly from the acquired face region to acquire the face parameter.</p><p id="p0064" num="0064">The facial expression determination unit 103 calculates the face parameter by the comparison with the previously-stored feature amount in each facial expression. For example, in the case where the degree of matching (the degree of similarity) with the feature amount of the facial expression of previously-set "delight" is 85%, a face parameter of "delight: 85%" is calculated. Whether the calculated face parameter satisfies the previously-set condition is determined. For example, in the case where a condition of "delight ≥ 80%" is<!-- EPO <DP n="16"> --> previously set, the determination is made that the condition is satisfied.</p><p id="p0065" num="0065">In the first embodiment, the value indicating "delight" is used as the face parameter. Alternatively, a value associated with another facial expression may be used as the face parameter. For example, the value indicating "anger", "sadness", "surprise", "fear", "dislike", or "blankness" may be used as the face parameter. Alternatively, the degree of matching with each of the plurality of facial expressions described above may be calculated, and a vector having the plurality of values may be output as the face parameter.</p><p id="p0066" num="0066">The facial expression determination phase described above is performed in succession to the person identification phase. For example, after the face image in the blankness state of <figref idrefs="f0002">Fig. 2</figref> is acquired to identify the person, the person is encouraged to make a specific facial expression in <figref idrefs="f0002">Fig. 3</figref>, and the facial expression is determined.</p><heading id="h0016">&lt;Tracking region tracking processing&gt;</heading><p id="p0067" num="0067">The tracking processing performed by the tracking unit 104 will be described below.</p><p id="p0068" num="0068">Through the two phases described above, the individual person can be identified based on the face image, and whether the person to be authenticated makes the predetermined facial expression can be identified based on the acquired face parameter. That is, it can be assumed that the human, not the photograph, exists in the vision of the camera.</p><p id="p0069" num="0069">However, in the case where the identity thief holds a plurality of photographs having different facial expressions in front of the camera, the device may falsely recognize the identity thief as the authorized person. For this reason, in the face authentication device of the first embodiment, using the<!-- EPO <DP n="17"> --> frames acquired between the face image used in the person identification phase and the face image used in the facial expression determination phase, whether continuity exists between the both is determined.</p><heading id="h0017">This point will be described in detail.</heading><p id="p0070" num="0070">In the case where the person to be authenticated actually exists in the vision of the camera, the face of the person is photographed in all the frames acquired between the face image used in the person identification phase and the face image used in the facial expression determination phase. On the other hand, in the case where the photograph or the like is used, frame-out of the face is generated when replacing the photograph.</p><p id="p0071" num="0071">After the person identification phase is started and until the facial expression determination phase is ended, the tracking unit 104 monitors the plurality of face images supplied from the face image acquisition unit 101, and determines whether the face of the person to be authenticated remains continuously in the image.</p><p id="p0072" num="0072"><figref idrefs="f0003">Figs. 4A to 4C</figref> are views illustrating processing performed by the tracking unit 104. <figref idrefs="f0003">Fig. 4A</figref> illustrates a face image (hereinafter, referred to as a first face image) acquired at the start of the person identification phase, <figref idrefs="f0003">Fig. 4C</figref> illustrates a face image (hereinafter, referred to as a second face image) acquired at the start of the facial expression determination phase. <figref idrefs="f0003">Fig. 4B</figref> illustrates a face image (hereinafter, referred to as an intermediate face image) acquired between the person identification phase and the facial expression determination phase. Because the person to be authenticated similes gradually in response to the instruction, the value of "delight" becomes a value between 0% and 85% in the face image of <figref idrefs="f0003">Fig. 4B</figref>.<!-- EPO <DP n="18"> --></p><p id="p0073" num="0073">When acquiring a first face image, the tracking unit 104 sets the tracking region into the image. The tracking region is a region where the presence of the face is tracked in the image. In the first embodiment, the tracking region is the rectangular region including the right eye of the person to be authenticated. The tracking unit 104 sets the tracking region after extracting the target face part (right eye).</p><p id="p0074" num="0074">The target face part can be extracted by the template matching in which the corresponding template is used. Alternatively, the position of the target face part may be determined based on the detected face region. Any well-known method may be adopted as processing of detecting the target face part. In the first embodiment, a tracking region 41 is set in <figref idrefs="f0003">Figs. 4A to 4C</figref>.</p><p id="p0075" num="0075">In the first embodiment, the region corresponding to the face part included in the face of the person to be authenticated is set as the tracking region. However, the tracking region is not limited to the region corresponding to the face part. For example, the region corresponding to the whole face (such as the rectangular region including the whole face) may be set as the tracking region.</p><p id="p0076" num="0076">The tracking unit 104 tracks the position of the set tracking region every time the face image is supplied from the face image acquisition unit 101. For example, in setting the tracking region, the corresponding image is cut out, and the template matching is performed with the image as the template. A predetermined range (for example, a range 42) around the position of the tracking region in the preceding frame may be set as a search range in performing the template matching. Therefore, search cost can be suppressed because it is hardly considered that the face of the person to be authenticated<!-- EPO <DP n="19"> --> moves largely during the face authentication.</p><p id="p0077" num="0077">Finally, the tracking unit 104 continues the tracking until the facial expression determination phase is ended, and the tracking unit 104 makes the determination of "successful tracking" when the presence of the tracking region is checked in all the frames. The tracking unit 104 makes the determination of "failed tracking" when the tracking region is lost in a frame.</p><p id="p0078" num="0078">In the example of <figref idrefs="f0003">Figs. 4A to 4C</figref>, the determination of the successful tracking is made because the presence of the tracking region 41 can be checked in all the three face images. That is, in the case where all the three items are satisfied, namely,
<ol><li>(1) the face included in the face image is identified to be the face of the authorized person,</li><li>(2) the facial expression of the person satisfies the condition, and</li><li>(3) the tracking region is successfully tracked after the item (1) is started and until the item (2) is ended,<br/>
the face authentication device of the first embodiment makes the determination of successful authentication to unlock the door device 200. In the case where any one of the three items is not satisfied, the door device 200 is not unlocked.</li></ol></p><heading id="h0018">&lt;Processing flowchart&gt;</heading><p id="p0079" num="0079">A processing flowchart for performing the above processing will be described below.</p><p id="p0080" num="0080"><figref idrefs="f0004">Fig. 5</figref> is a flowchart illustrating the processing performed by the face authentication device 100 of the first embodiment. The flowchart in <figref idrefs="f0004">Fig. 5</figref> is started when the person to be authenticated standing in the vision of the camera<!-- EPO <DP n="20"> --> 300 is detected, and the authentication unit 107 controls the processing of the flowchart.</p><p id="p0081" num="0081">In Step S11, the user identification unit 102 acquires the face image (first face image) delivered from the face image acquisition unit 101.</p><p id="p0082" num="0082">In Step S12, the user identification unit 102 acquires the feature amount from the first face image through the above processing.</p><p id="p0083" num="0083">In Step S13, the acquired feature amount is compared with the feature amount, which is previously stored in the user identification unit 102 and corresponds to the faces of the plurality of persons, and the person is identified based on the degree of matching (degree of similarity). When the person is determined to be the registered person, the processing goes to Step S14. When the person is determined to be not the registered person, the person to be authenticated is informed that the person to be authenticated is not the registered person through the image generator 106, and the processing returns to Step S11.</p><p id="p0084" num="0084">In Step S14, the tracking unit 104 extracts the region corresponding to the right eye from the first face image delivered from the face image acquisition unit 101, and sets the region as the tracking region.</p><p id="p0085" num="0085">In Step S15, the facial expression determination unit 103 and the tracking unit 104 acquire a second face image delivered from the face image acquisition unit 101.</p><p id="p0086" num="0086">In Step S16, the tracking unit 104 tracks the tracking region by comparing the preceding frame with the frame acquired in Step S15. That is, whether the extracted right-eye region is included in both the frames is checked. When the tracking unit 104 successfully performs the tracking (YES in Step S17),<!-- EPO <DP n="21"> --> the processing goes to Step S18. When the tracking unit 104 fails to perform the tracking (NO in Step S17), namely, when the right eye of the person to be authenticated is lost, the person to be authenticated is informed that the tracking unit 104 has failed to perform the tracking through the image generator 106, and the processing returns to Step S11.</p><p id="p0087" num="0087">In Step S18, the facial expression determination unit 103 acquires the feature amount from the second face image, and acquires the face parameter through the above processing.</p><p id="p0088" num="0088">In Step S19, the facial expression determination unit 103 determines whether the acquired face parameter satisfies a prescribed condition. For example, for the condition that the value of "delight" is greater than or equal to 80%, the value included in the face parameter is referred to, the determination that the condition is satisfied is made when the value of "delight" is greater than or equal to 80%, and the processing goes to Step S20. When the condition is not satisfied, the processing goes to Step S15 to continuously acquire the second face image.</p><p id="p0089" num="0089">Finally, in Step S20, the authentication unit 107 transmits an unlock command to the door device 200 through the communication unit 105, and the door is unlocked.</p><p id="p0090" num="0090">In the conventional face authentication device, the face authentication processing is performed once or a plurality of times to confirm authorization of the person to be authenticated, which results in the problem of the trade-off between the cost and the reliability. That is, in the one-time authentication, there is a risk that an unauthorized person passes the authentication. Additionally, the processing cost increases when the authentication is performed<!-- EPO <DP n="22"> --> a plurality of times in order to prevent the unauthorized authentication.</p><p id="p0091" num="0091">On the other hand, the face authentication device of the first embodiment determines whether the facial expression satisfies the predetermined condition in addition to the usual face authentication processing. The local region is set into the face image to perform the tracking, thereby checking whether both the processing targets are the identical face. Therefore, unauthorized attempt using the photograph or the like can be eliminated. For example, even if the person other than a person in question tries to deceive the device using the face photograph, the face matched with the assigned face parameter cannot be provided by one face photograph. Even if at least two photographs are prepared, the device fails to perform the tracking because the frame-out of the currently-tracked region is generated in replacing the photograph.</p><p id="p0092" num="0092">Additionally, in the facial expression determination processing, high accuracy is not required as compared with the processing of identifying the individual person. That is, the high-reliability face authentication technique can be provided while the costly processing is suppressed to the minimum.</p><p id="p0093" num="0093">In the first embodiment, the determination of the failed tracking is made when the tracking region is completely lost. Alternatively, even if the tracking region can be tracked between the frames, the determination of the failed tracking may be made in the case where an amount of movement of the tracking region is larger than a predetermined threshold. This is because it can be assumed that the identity thief is attempting to replace the photograph when the tracking region moves unnaturally largely between the frames.</p><heading id="h0019">SECOND EMBODIMENT</heading><!-- EPO <DP n="23"> --><p id="p0094" num="0094">In the first embodiment, at the facial expression determination phase, the person to be authenticated is caused to make the assigned facial expression to determine the facial expression using the face parameter. On the other hand, in a second embodiment, the person to be authenticated is caused to make a plurality of facial expressions, and the determination of the successful authentication is made in the case where the plurality of facial expressions satisfy the conditions.</p><p id="p0095" num="0095">Because the configuration of the face authentication device of the second embodiment is similar to that of the first embodiment, the detailed description is omitted, and only a different point of the processing will be described below.</p><p id="p0096" num="0096">In the second embodiment, the pieces of processing in Steps S15 to S19 are repeated a plurality of times. For example, the person to be authenticated is caused to make the facial expression in each repetition such that the person to be authenticated smiles for the first time and such that the person to be authenticated make an angry face for the second time, and the facial expression determination unit 103 makes the determination.</p><p id="p0097" num="0097"><figref idrefs="f0005">Figs. 6A to 6C</figref> illustrate an example of a screen presented to the person to be authenticated in the second embodiment. <figref idrefs="f0005">Fig. 6A</figref> illustrates a screen in the state where the person identification phase is completed. At this point, the instruction to smile is issued to the person to be authenticated, and the acquisition of the predetermined face parameter corresponding to "delight" is checked in Step S19. In the second loop, as illustrated in <figref idrefs="f0005">Fig. 6B</figref>, the instruction to make the angry face is issued to the person to be authenticated, and the acquisition of the predetermined face parameter corresponding to<!-- EPO <DP n="24"> --> "anger" is checked in Step S19. Finally, the state of the successful authentication is obtained as illustrated in <figref idrefs="f0005">Fig. 6C</figref>.</p><p id="p0098" num="0098">The tracking operation performed by the tracking unit 104 is similar to that of the first embodiment. That is, the tracking region is continuously tracked between the frames until the facial expression determination phase is completed, and the authentication processing is interrupted in the case where the tracking region is lost.</p><p id="p0099" num="0099">In the second embodiment, the person to be authenticated is caused to make a plurality of assigned facial expressions, so that security can be further enhanced.</p><p id="p0100" num="0100">In the first and second embodiments, there is no time restriction in the authentication. However, the determination of the failed authentication may be made to interrupt the processing in the case where a predetermined time elapses after the authentication processing is started.</p><p id="p0101" num="0101">Alternatively, a degree of change of the face parameter is acquired in each frame, and the determination of the failed authentication may be made in the case where the amount of change of the face parameter is larger than a predetermined threshold. This is because it can be assumed that the identity thief is attempting to replace the photograph when the face parameter varies unnaturally largely between the frames.</p><p id="p0102" num="0102">In the second embodiment, by way of example, the person to be authenticated continuously makes the facial expression of "delight" and the facial expression of "anger". Alternatively, a combination of other facial expressions may be used. The facial expression to be used is randomly fixed from the plurality of facial expressions in each case, and the instruction may be issued to<!-- EPO <DP n="25"> --> the person to be authenticated.</p><heading id="h0020">THIRD EMBODIMENT</heading><p id="p0103" num="0103">In the first and second embodiments, the facial expression determination unit 103 determines the facial expression of the person to be authenticated. On the other hand, in a third embodiment, a face direction of the person to be authenticated is determined instead of determining the facial expression.</p><p id="p0104" num="0104">In the third embodiment, the facial expression determination unit 103 is replaced with a face direction determination unit 108 (not illustrated) to determine the face direction of the person to be authenticated. In the third embodiment, the face parameter is not the parameter regarding the facial expression of the person to be authenticated but the parameter indicating the face direction of the person to be authenticated. For example, the face parameter of the third embodiment indicates vertical and horizontal angles in the case where the front face is set to 0 degrees. Because other configurations of the third embodiment are similar to those of the first embodiment, the detailed description is omitted, and only a different point of the processing will be described below.</p><p id="p0105" num="0105">In the third embodiment, in Step S18, whether the face direction of the person to be authenticated is oriented toward a predetermined direction is determined instead of determining the facial expression of the person to be authenticated. For example, as illustrated in <figref idrefs="f0006">Figs. 7A to 7C</figref>, an instruction to turn the face toward the right by 45 degrees is issued to the person to be authenticated, and then, whether the face direction of the person to be authenticated falls within a predetermined range (for example, a range of 35 degrees to 55 degrees to the right) is determined.<!-- EPO <DP n="26"> --></p><p id="p0106" num="0106">The tracking operation performed by the tracking unit 104 is similar to that of the first embodiment. That is, the tracking region is continuously tracked between the frames until the facial expression determination phase is completed, and the authentication processing is interrupted in the case where the tracking region is lost. However, in the third embodiment, because the face direction of the person to be authenticated changes, preferably, the tracking region is set in consideration of the change of the face direction. For example, the tracking region may be set to a face part, such a nose, which is unlikely to get out of sight even if the face direction changes. The tracking region may be set to the region other than the face parts included in the face. For example, the tracking region may be set to the rectangular region including the whole face.</p><p id="p0107" num="0107">Generally, the face direction determination processing can be performed at lower cost than that of the processing of identifying the person or facial expression. Therefore, in the third embodiment, the overall processing cost can be reduced similarly to the first and second embodiments.</p><heading id="h0021">MODIFICATIONS</heading><p id="p0108" num="0108">The above embodiments are described only by way of example, and various changes can be made without departing from the scope of the present invention.</p><p id="p0109" num="0109">For example, in the embodiments, by way of example, the tracking region is tracked using the plurality of intermediate images. However, it is not always necessary to use the intermediate image.</p><p id="p0110" num="0110">Although the seven kinds of facial expressions (delight, anger, sadness, surprise, fear, dislike, and blankness) are illustrated in the embodiments, the face parameter associated with a facial expression other than<!-- EPO <DP n="27"> --> the illustrated parameters may be used. For example, a face parameter indicating a degree of shutting of an eye, a direction of a line of sight, or a degree of opening of a mouth may be used.</p><p id="p0111" num="0111">In the embodiments, by way of example, the quantified facial expression and the quantified face direction are cited as an example of the face parameter. Alternatively, another parameter associated with an appearance of a user's face may be used.</p><p id="p0112" num="0112">In the first and second embodiments, by way of example, the instruction to make the specific facial expression is issued to the person to be authenticated. Alternatively, the specific instruction may not be issued in the case where the person to be authenticated previously understands which facial expression is to be made. For example, the instruction to make the specific facial expression such as "smile" and "anger" may not be issued, and the person to be authenticated may be informed only that the predetermined facial expression is checked. In such cases, the image may not be provided to the person to be authenticated. The person to be authenticated may be informed of the progress of the authentication step by a lamp or sound.</p><p id="p0113" num="0113">The person to be authenticated may not be informed at all until the authentication is successfully performed. In this manner, a hint may not be provided to a person other than the authorized person.</p><p id="p0114" num="0114">In the embodiments, the face image is acquired with the camera. Alternatively, any device other than the camera may be used as long as the face image is acquired. For example, the face image may be acquired from a remote place through a network.</p><p id="p0115" num="0115">In the embodiments, the face part included in the face is set as the<!-- EPO <DP n="28"> --> tracking region. Alternatively, the tracking target may be other than the face part, or may be the feature point set in the face image. Any part may be set as the tracking target as long as the continuous presence of the face of the person to be authenticated can be checked.</p></description><claims mxw-id="PCLM90459456" lang="EN" load-source="patent-office"><!-- EPO <DP n="29"> --><claim id="c-en-0001" num="0001"><claim-text>A face authentication device comprising:
<claim-text>a face image acquisition unit configured to continuously acquire face images of a user;</claim-text>
<claim-text>a user identification unit configured to identify the user based on a first face image acquired by the face image acquisition unit;</claim-text>
<claim-text>a parameter acquisition unit configured to acquire a face parameter based on a second face image acquired by the face image acquisition unit, the face parameter being a parameter associated with a facial expression or a face direction of the user;</claim-text>
<claim-text>a tracking unit configured to extract a region of a part of a face included in the first face image, and to track the region between the plurality of continuously-acquired face images; and</claim-text>
<claim-text>an authentication unit configured to make a determination of successful authentication when the user identified by the user identification unit is a registered user, when the acquired face parameter satisfies a predetermined condition, and when the extracted region is continuously tracked between the plurality of face images.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The face authentication device according to claim 1, wherein the face image acquisition unit further acquires at least one intermediate face image between a time at which the first face image is acquired and a time at which the second face image is acquired, and<br/>
the tracking unit tracks the extracted region using the intermediate face<!-- EPO <DP n="30"> --> image.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The face authentication device according to claim 1 or 2, wherein the second face image is formed by a plurality of face images acquired at different times,<br/>
the parameter acquisition unit further acquires a plurality of face parameters corresponding to the plurality of face images, and<br/>
the authentication unit makes the determination of successful authentication when all the plurality of acquired face parameters satisfy the predetermined condition.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The face authentication device according to any one of claims 1 to 3, wherein the face parameter is a parameter in which the facial expression of the user is quantified.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The face authentication device according to any one of claims 1 to 3, wherein the face parameter is a parameter in which the face direction of the user is quantified.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The face authentication device according to any one of claims 1 to 5, wherein the authentication unit further acquires an amount of change per unit time of the face parameter, and authentication processing is interrupted when the amount of change is larger than a predetermined value.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The face authentication device according to any one of claims 1 to 6,<br/>
<!-- EPO <DP n="31"> -->wherein the tracking unit further acquires an amount of movement per unit time of a tracking target region, and a determination of failed authentication is made when the amount of movement is larger than a predetermined value.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The face authentication device according to any one of claims 1 to 7, wherein the authentication unit interrupts the authentication processing when the authentication processing is not completed within a predetermined time.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>The face authentication device according to any one of claims 1 to 8, wherein the tracking unit tracks only a predetermined range existing near the extracted region when tracking the extracted region.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>A face authentication method performed by a face authentication device, the face authentication method comprising:
<claim-text>a face image acquiring step of continuously acquiring face images of a user;</claim-text>
<claim-text>a face authentication step of identifying the user based on an acquired first face image;</claim-text>
<claim-text>a parameter acquiring step of acquiring a face parameter based on an acquired second face image, the face parameter being a parameter associated with a facial expression or a face direction of the user;</claim-text>
<claim-text>a tracking step of extracting a region of a part of a face included in the first face image, and tracking the region between the plurality of continuously-acquired face images; and</claim-text>
<claim-text>an authentication step of making a determination of successful<!-- EPO <DP n="32"> --> authentication when the user identified in the face authentication step is a registered user, when the acquired face parameter satisfies a predetermined condition, and when the extracted region is continuously tracked between the plurality of face images.</claim-text></claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>A program configured to cause a computer to execute each step of the face authentication method according to claim 10.</claim-text></claim></claims><drawings mxw-id="PDW20422183" load-source="patent-office"><!-- EPO <DP n="33"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="151" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> --><figure id="f0002" num="2,3"><img id="if0002" file="imgf0002.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> --><figure id="f0003" num="4A,4B,4C"><img id="if0003" file="imgf0003.tif" wi="161" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> --><figure id="f0004" num="5"><img id="if0004" file="imgf0004.tif" wi="157" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0005" num="6A,6B,6C"><img id="if0005" file="imgf0005.tif" wi="158" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> --><figure id="f0006" num="7A,7B,7C"><img id="if0006" file="imgf0006.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="159" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="159" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
