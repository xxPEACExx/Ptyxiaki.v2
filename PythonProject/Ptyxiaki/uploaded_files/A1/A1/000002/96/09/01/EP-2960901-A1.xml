<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960901-A1" country="EP" doc-number="2960901" kind="A1" date="20151230" family-id="53483732" file-reference-id="287255" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451430" ucid="EP-2960901-A1"><document-id><country>EP</country><doc-number>2960901</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15173407-A" is-representative="YES"><document-id mxw-id="PAPP193865828" load-source="docdb" format="epo"><country>EP</country><doc-number>15173407</doc-number><kind>A</kind><date>20150623</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865829" load-source="patent-office" format="original"><country>EP</country><doc-number>15173407.6</doc-number><date>20150623</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162034462" ucid="US-201462015879-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201462015879</doc-number><kind>P</kind><date>20140623</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988520199" load-source="docdb">G10L  15/07        20130101ALN20150814BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988522273" load-source="docdb">G10L  15/30        20130101ALI20150814BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988522553" load-source="docdb">G10L  15/06        20130101ALN20150814BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988523265" load-source="docdb">G10L  17/00        20130101ALN20150814BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988525707" load-source="docdb">G10L  15/32        20130101AFI20150814BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988526282" load-source="docdb">G10L  15/22        20060101ALI20150814BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1925884437" load-source="docdb" scheme="CPC">G10L2015/228       20130101 LA20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925885460" load-source="docdb" scheme="CPC">G10L  15/30        20130101 LI20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925885912" load-source="docdb" scheme="CPC">G10L  15/22        20130101 LI20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925885988" load-source="docdb" scheme="CPC">G10L  17/00        20130101 LA20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925886230" load-source="docdb" scheme="CPC">G10L  15/32        20130101 LI20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925886231" load-source="docdb" scheme="CPC">G10L  15/07        20130101 LA20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925887359" load-source="docdb" scheme="CPC">G10L2015/0631      20130101 LA20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1925890662" load-source="docdb" scheme="CPC">G10L2015/227       20130101 LA20160624BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699469" load-source="docdb" scheme="CPC">G10L2015/025       20130101 LA20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984702075" load-source="docdb" scheme="CPC">G10L  15/02        20130101 FI20151224BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545756" lang="DE" load-source="patent-office">BENUTZERANGEPASSTE SPRACHERKENNUNG</invention-title><invention-title mxw-id="PT165545757" lang="EN" load-source="patent-office">USER-ADAPTED SPEECH RECOGNITION</invention-title><invention-title mxw-id="PT165545758" lang="FR" load-source="patent-office">RECONNAISSANCE VOCALE ADAPTEE A L'UTILISATEUR</invention-title><citations><patent-citations><patcit mxw-id="PCIT335962037" load-source="docdb" ucid="EP-1385148-A1"><document-id format="epo"><country>EP</country><doc-number>1385148</doc-number><kind>A1</kind><date>20040128</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962038" load-source="docdb" ucid="US-20050102142-A1"><document-id format="epo"><country>US</country><doc-number>20050102142</doc-number><kind>A1</kind><date>20050512</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT375624846" load-source="docdb" ucid="US-20070276651-A1"><document-id format="epo"><country>US</country><doc-number>20070276651</doc-number><kind>A1</kind><date>20071129</date></document-id><sources><source name="SEA" category="XI" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962041" load-source="docdb" ucid="US-20120179457-A1"><document-id format="epo"><country>US</country><doc-number>20120179457</doc-number><kind>A1</kind><date>20120712</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962042" load-source="docdb" ucid="US-20140129222-A1"><document-id format="epo"><country>US</country><doc-number>20140129222</doc-number><kind>A1</kind><date>20140508</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962043" load-source="docdb" ucid="US-20140163977-A1"><document-id format="epo"><country>US</country><doc-number>20140163977</doc-number><kind>A1</kind><date>20140612</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962040" load-source="docdb" ucid="WO-2011149837-A1"><document-id format="epo"><country>WO</country><doc-number>2011149837</doc-number><kind>A1</kind><date>20111201</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>None</text><sources><source mxw-id="PNPL62251171" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103322503" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>HARMAN INT IND</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR1103306907" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>HARMAN INTERNATIONAL INDUSTRIES, INCORPORATED</last-name></addressbook></applicant><applicant mxw-id="PPAR1101651588" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Harman International Industries, Incorporated</last-name><iid>101475493</iid><address><street>400 Atlantic Street, 15th Floor</street><city>Stamford, CT 06901</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103320037" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>KREIFELDT RICHARD ALLEN</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR1103326801" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KREIFELDT, RICHARD ALLEN</last-name></addressbook></inventor><inventor mxw-id="PPAR1101649226" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>KREIFELDT, RICHARD ALLEN</last-name><address><street>11307 Brown Summit Circle</street><city>South Jordan, UT Utah 84095</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101645682" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Grünecker, Kinkeldey, Stockmair &amp; Schwanhäusser Anwaltssozietät</last-name><iid>100060488</iid><address><street>Leopoldstrasse 4</street><city>80802 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660609394" load-source="docdb">AL</country><country mxw-id="DS660783274" load-source="docdb">AT</country><country mxw-id="DS660609400" load-source="docdb">BE</country><country mxw-id="DS660684658" load-source="docdb">BG</country><country mxw-id="DS660689220" load-source="docdb">CH</country><country mxw-id="DS660606353" load-source="docdb">CY</country><country mxw-id="DS660783275" load-source="docdb">CZ</country><country mxw-id="DS660609401" load-source="docdb">DE</country><country mxw-id="DS660606354" load-source="docdb">DK</country><country mxw-id="DS660606355" load-source="docdb">EE</country><country mxw-id="DS660687155" load-source="docdb">ES</country><country mxw-id="DS660684663" load-source="docdb">FI</country><country mxw-id="DS660684664" load-source="docdb">FR</country><country mxw-id="DS660609402" load-source="docdb">GB</country><country mxw-id="DS660606356" load-source="docdb">GR</country><country mxw-id="DS660609411" load-source="docdb">HR</country><country mxw-id="DS660783276" load-source="docdb">HU</country><country mxw-id="DS660689221" load-source="docdb">IE</country><country mxw-id="DS660606357" load-source="docdb">IS</country><country mxw-id="DS660684665" load-source="docdb">IT</country><country mxw-id="DS660606358" load-source="docdb">LI</country><country mxw-id="DS660610019" load-source="docdb">LT</country><country mxw-id="DS660684740" load-source="docdb">LU</country><country mxw-id="DS660610020" load-source="docdb">LV</country><country mxw-id="DS660610021" load-source="docdb">MC</country><country mxw-id="DS660684741" load-source="docdb">MK</country><country mxw-id="DS660684742" load-source="docdb">MT</country><country mxw-id="DS660783277" load-source="docdb">NL</country><country mxw-id="DS660609412" load-source="docdb">NO</country><country mxw-id="DS660606359" load-source="docdb">PL</country><country mxw-id="DS660610031" load-source="docdb">PT</country><country mxw-id="DS660783278" load-source="docdb">RO</country><country mxw-id="DS660610032" load-source="docdb">RS</country><country mxw-id="DS660606360" load-source="docdb">SE</country><country mxw-id="DS660684666" load-source="docdb">SI</country><country mxw-id="DS660609413" load-source="docdb">SK</country><country mxw-id="DS660606361" load-source="docdb">SM</country><country mxw-id="DS660684755" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479800" lang="EN" load-source="patent-office"><p id="pa01" num="0001">One embodiment of the present disclosure sets forth an approach for performing speech recognition. A speech recognition system receives an electronic signal that represents human speech of a speaker. The speech recognition system converts the electronic signal into a plurality of phonemes. The speech recognition system, while converting the plurality of phonemes into a first group of words based on a first voice recognition model, encounters an error when attempting to convert one or more of the phonemes into words. The speech recognition system transmits a message associated with the error to a server machine. The speech recognition system causes the server machine to convert the one or more phonemes into a second group of words based on a second voice recognition model resident on the server machine. The speech recognition system receives the second group of words from the server machine.
<img id="iaf01" file="imgaf001.tif" wi="78" he="118" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759612" lang="EN" source="EPO" load-source="docdb"><p>One embodiment of the present disclosure sets forth an approach for performing speech recognition. A speech recognition system receives an electronic signal that represents human speech of a speaker. The speech recognition system converts the electronic signal into a plurality of phonemes. The speech recognition system, while converting the plurality of phonemes into a first group of words based on a first voice recognition model, encounters an error when attempting to convert one or more of the phonemes into words. The speech recognition system transmits a message associated with the error to a server machine. The speech recognition system causes the server machine to convert the one or more phonemes into a second group of words based on a second voice recognition model resident on the server machine. The speech recognition system receives the second group of words from the server machine.</p></abstract><description mxw-id="PDES98404501" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>CROSS-REFERENCE TO RELATED APPLICATIONS</b></heading><p id="p0001" num="0001">This application claims the benefit of United States provisional patent application, titled "USER ADAPTED SPEECH RECOGNITION," filed on <patcit id="pcit0001" dnum="US62015879B"><text>June 23, 2014 and having Serial No. 62/015,879</text></patcit>. The subject matter of this related application is hereby incorporated herein by reference.</p><heading id="h0002"><b>BACKGROUND</b></heading><heading id="h0003"><b>Field of the Embodiments of the Present Disclosure</b></heading><p id="p0002" num="0002">Embodiments of the present disclosure relate generally to speech recognition and, more specifically, to user-adapted speech recognition.</p><heading id="h0004"><b>Description of the Related Art</b></heading><p id="p0003" num="0003">Various computing devices include mechanisms to support speech recognition, thereby improving the functionality and safe use of such devices. Examples of such computing devices include, without limitation, smartphones, vehicle navigation systems, laptop computers, and desktop computers. Computing devices that include mechanisms to support speech recognition typically receive an electronic signal representing the voice of a speaker via a wireless connection, such as a Bluetooth connection, or via a wired connection, such as an analog audio cable or a digital data cable. The computing device then converts the electronic signal into phonemes, where phonemes are perceptually distinct units of sound that distinguish one word from another. These phonemes are then analyzed and compared to the phonemes that make up the words of a particular language in order to determine the spoken words represented in the received electronic signal. Typically, the computing device includes a memory for storing mappings of phoneme groups against the words and phrases in the particular language. After determining the words and phrases spoken by the user, the computing device then performs a particular response, such as performing a command specified via the electronic signal or creating human readable text corresponding to the electronic signal that can be transmitted, via a text message, for example, or stored in a document for later use.<!-- EPO <DP n="2"> --></p><p id="p0004" num="0004">One drawback of the approach described above is that the mechanisms to support speech recognition for a particular language consume a significant amount of memory within the computing device. The computing device allocates a significant amount of memory in order to store the entire phoneme to word and phrase mappings and language processing support for a particular language. Because computing devices usually have only a limited amount of local memory, most computing devices are generally limited to supporting only one or two languages simultaneously, such as English and Spanish. If a speaker wishes to use mechanisms to support speech recognition for a third language, such as German, the mechanisms to support either English or Spanish speech recognition have to first be removed from the computing device to free up the memory necessary to store the mechanisms to support German speech recognition. Removing the mechanisms to support one language and installing the mechanisms to support another language is often a cumbersome and time consuming process, and typically requires some skill with electronic devices. As a result, such computing devices are difficult to use, particularly when a user desires mechanisms to support more languages than the computing device can simultaneously store.</p><p id="p0005" num="0005">In addition, such computing devices often have difficulty recognizing speech spoken by non-native speakers with strong accents or with certain speech impediments. In such circumstances, the computing device may fail to correctly recognize the words of the speaker. As a result, these computing devices can be difficult or impossible to use reliably by non-native speakers with strong accents or speakers who have speech impediments.</p><p id="p0006" num="0006">One solution to the above problems is to place the mechanisms to support speech recognition on one or more servers, where the computing device simply captures the electronic signal of the voice of the speaker and transmits the electronic signal over a wireless network to the remote server for phoneme matching and speech processing. Because the remote servers typically have higher storage and computational capability relative to the above-described computing devices, the servers are capable of simultaneously supporting speech recognition for a much larger number of languages. In addition, such remote servers can typically support reliable speech recognition under challenging conditions, such as when the speaker has a strong accent or speech impediment.</p><p id="p0007" num="0007">One drawback to conventional server implementations, though, is that the server is contacted for each speech recognition task. If the computing device is in motion, as is typical for<!-- EPO <DP n="3"> --> vehicle navigation and control systems, the computing device may be able to contact the server in certain locations, but may be unable to contact the server in other locations. In addition, wireless network traffic may be sufficiently high such that the computing device cannot reliably establish and maintain communications with the server. As a result, once communications with the remote server is lost, the computing device may be unable to perform speech recognition tasks until the computing device reestablishes communications with the server. Another drawback is that processing speech via a remoter server over a network generally introduces higher latencies relative to processing speech locally on a computing device. As a result, additional delays can be introduced between receiving the electronic signal corresponding to the human speech and performing the desired action associated with the electronic signal.</p><p id="p0008" num="0008">As the foregoing illustrates, more effective techniques for performing speech recognition would be useful.</p><heading id="h0005"><b>SUMMARY</b></heading><p id="p0009" num="0009">One or more embodiments set forth a method for performing speech recognition. The method includes receiving an electronic signal that represents human speech of a speaker. The method further includes converting the electronic signal into a plurality of phonemes. The method further includes, while converting the plurality of phonemes into a first group of words based on a first voice recognition model, encountering an error when attempting to convert one or more of the phonemes into words. The method further includes transmitting a message associated with the error to a server machine. The method further includes causing the server machine to convert the one or more phonemes into a second group of words based on a second voice recognition model resident on the server machine. The method further includes receiving the second group of words from the server machine.</p><p id="p0010" num="0010">Other embodiments include, without limitation, a computer readable medium including instructions for performing one or more aspects of the disclosed techniques, as well as a computing device for performing one or more aspects of the disclosed techniques.</p><p id="p0011" num="0011">At least one advantage of the disclosed approach is that speech recognition can be performed for multilingual speakers or speakers with strong accents or speech impediments with lower latency and higher reliability relative to prior approaches.<!-- EPO <DP n="4"> --></p><heading id="h0006"><b>BRIEF DESCRIPTION OF THE DRAWINGS</b></heading><p id="p0012" num="0012">So that the manner in which the above recited features of embodiments of the invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.
<ul><li><figref idrefs="f0001">Figure 1</figref> illustrates a speech recognition system configured to implement one or more aspects of the various embodiments;</li><li><figref idrefs="f0002">Figure 2</figref> sets forth a flow diagram of method steps for performing user-adapted speech recognition, according to various embodiments; and</li><li><figref idrefs="f0003">Figure 3</figref> sets forth a flow diagram of method steps for analyzing speech data to select a new voice recognition model, according to various embodiments.</li></ul></p><heading id="h0007"><b>DETAILED DESCRIPTION</b></heading><p id="p0013" num="0013">In the following description, numerous specific details are set forth to provide a more thorough understanding of certain specific embodiments. However, it will be apparent to one of skill in the art that other embodiments may be practiced without one or more of these specific details or with additional specific details.</p><p id="p0014" num="0014">Embodiments disclosed herein provide a speech recognition system, also referred to herein as a voice recognition (VR) system, that is tuned to specific users. The speech recognition system includes an onboard, or local, client machine executing a VR application that employs locally stored VR models and one or more network-connected server machines executing a VR application that employs additional VR models stored on the server machines. The VR application executing on the client machine operates with a lower latency relative to the network-connected server machines, but is limited in terms of the quantity and type of VR models that can be stored locally to the client machine. The VR applications executing on the server machines operate with a higher latency relative to the client machine, because of the latency associated with the network. On the other hand, because the server machines typically have significantly more storage capacity relative to the client machine, the server machines have access to many more VR<!-- EPO <DP n="5"> --> models and more robust and sophisticated VR models than the client machine. Over time, the VR models located on the server machines are used to improve the local VR models stored on the client machine for each individual user. The server machines may analyze a speech of a user in order to identify the best data model to process the speech of that specific user. The server machine may inform the client machine of the best VR model, or modifications thereto, in order to process the speech of the user. Because the disclosed speech recognition system includes both local VR models and remote VR models, the speech recognition system is referred to herein as a hybrid speech recognition system. This hybrid speech recognition system is now described in greater detail.</p><p id="p0015" num="0015"><figref idrefs="f0001">Figure 1</figref> illustrates a speech recognition system 100 configured to implement one or more aspects of the various embodiments. As shown, the speech recognition system 100 includes, without limitation, a client machine 102 connected to one or more server machines 150-1, 150-2, and 150-3 via a network 130.</p><p id="p0016" num="0016">Client machine 102 includes, without limitation, a processor 102, memory 104, storage 108, a network interface 118, input devices 122, and output devices 124, all interconnected via a communications bus 120. In at least one embodiment, the client machine 102 may be in a vehicle, and may be configured to provide various services, including, without limitation, navigation, media content playback, hands-free calling, and Bluetooth® communications with other devices.</p><p id="p0017" num="0017">The processor 104 is generally under the control of an operating system (not shown). Examples of operating systems include the UNIX operating system, versions of the Microsoft Windows operating system, and distributions of the Linux operating system. (UNIX is a registered trademark of The Open Group in the United States and other countries. Microsoft and Windows are trademarks of Microsoft Corporation in the United States, other countries, or both. Linux is a registered trademark of Linus Torvalds in the United States, other countries, or both.) More generally, any operating system supporting the functions disclosed herein may be used. The processor 104 is included to be representative of, without limitation, a single CPU, multiple CPUs, and a single CPU having multiple processing cores.</p><p id="p0018" num="0018">As shown, the memory 106 contains the voice recognition (VR) application 112, which is an application generally configured to provide voice recognition that is tuned to each specific<!-- EPO <DP n="6"> --> user. The storage 108 may be a persistent storage device. As shown, storage 108 includes the user data 115 and the VR models 116. The user data 115 includes unique speech profiles and other data related to each of a plurality of unique users that may interact with the VR application 112. The VR models 116 include a set of voice recognition models utilized by the VR application 112 to process user speech. Although the storage 108 is shown as a single unit, the storage 108 may be a combination of fixed and/or removable storage devices, such as fixed disc drives, solid state drives, SAN storage, NAS storage, removable memory cards or optical storage. The memory 106 and the storage 108 may be part of one virtual address space spanning multiple primary and secondary storage devices.</p><p id="p0019" num="0019">As shown, the VR models 116 include, without limitation, acoustic models 130, language models 132, and statistical models 134. Acoustic models 130 include the data utilized by the VR application 112 to convert sampled human speech, where phonemes represent perceptually distinct units of sound which are combined with other phonemes to form meaningful units. Language models 132 include the data utilized by the VR application 112 to convert groups of phonemes from the acoustic models 130 into the words of a particular human language. In some embodiments, the language models may be based on a probability function, where a particular set of phonemes may correspond to a number of different words, with varying probability. As one example, and without limitation, a particular set of phonemes could correspond to wear, where, or ware, with different relative probabilities. Statistical models 134 include the data utilized by the VR application 112 to convert groups of words from the language models 130 into phrases and sentences. The statistical models 134 consider various aspects of word groups, including, without limitation, word order rules of a particular language, grammatical rules of the language, and the probability that a particular word appears near an associated word. For example, and without limitation, if a consecutive set of received words processed via the acoustic models 130 and the language models 132 results in the phrase, "wear/where/ware the black pants," the VR application 112, via the statistical models 134, could determine that the intended phrase is, "wear the black pants." In some embodiments, the techniques described herein may modify the language models 132 and the statistical models 134 stored in the memory 108 while leaving the acoustic models 130.</p><p id="p0020" num="0020">The network interface device 118 may be any type of network communications device allowing the client machine 102 to communicate with other computers, such as server machines<!-- EPO <DP n="7"> --> 150-1, 150-2, and 150-3, via the network 130. Input devices 122 may include any device for providing input to the computer 102. For example, a keyboard and/or a mouse may be used. In at least some embodiments, the input device 122 is a microphone configured to capture user speech. Output devices 124 may include any device for providing output to a user of the computer 102. For example, the output device 124 may include any conventional display screen or set of speakers. Although shown separately from the input devices 122, the output devices 124 and input devices 122 may be combined. For example, a display screen with an integrated touch-screen may be used.</p><p id="p0021" num="0021">Exemplary server machine 150-1 includes, includes, without limitation, an instance of the VR application 152 (or any application generally configured to provide the functionality described herein), user data 155, and VR models 156. As shown, the VR models 156 include, without limitation, language models 160, acoustic models 162, and statistical models 164. The user data 155 and VR models 156 on the server machine 150-1 typically include a greater number of user entries and VR models, respectively, than the user data 115 and the VR models 116 in the storage 108 of the client machine 102. In various embodiments, server machine 150-1 further includes, without limitation, a processor, memory, storage, a network interface, and one or more input devices and output devices, as described in conjunction with client machine 102.</p><p id="p0022" num="0022">Network 130 may be any telecommunications network or wide area network (WAN) suitable for facilitating communications between the client machine 102 and the server machines 150-1, 150-2, and 150-3. In a particular embodiment, the network 130 may be the Internet.</p><p id="p0023" num="0023">Generally, the VR application 112 provides speech recognition functionality by translating human speech into computer-usable formats, such as text or control signals. In addition, the VR application 112 provides accurate voice recognition for non-native speakers, speakers with strong accents, and greatly improve recognition rates for individual speakers. The VR application 112 utilizes the local instances of the user data 115 and the VR models 116 (in the storage 208) in combination with cloud-based versions of the user data 155 and VR models 156 on the server machines 150-1, 150-2, and 150-3. The client machine 102 converts spoken words to computer-readable formats, such as text. For example, a user may speak commands while in a vehicle. Client machine 102 in the vehicle captures the spoken commands through an in-vehicle microphone, a Bluetooth® headset, or other data connection, and compares the speech of a user to one or more VR models 116 in order to determine what the user said. Once the client<!-- EPO <DP n="8"> --> machine 102 analyzes the spoken commands, a corresponding predefined function is performed in response, such as changing a radio station or turning on the climate control system.</p><p id="p0024" num="0024">However, memory limitations constrain the number of VR models 116 that client machine 102 system can store. Consequently, speech recognition on an individual level may be quite poor, especially for non-native speakers and users with strong accents or speech impediments. Embodiments disclosed herein leverage local and remote resources in order to improve the overall accuracy of voice recognition for individual users. When speech of a user is received by the client machine 102 in the vehicle (the local speech recognition system), the client machine 102 analyzes the speech of a user to correctly identify unique users (or speakers) by comparing the speech of a user to stored speech data. The client machine 102 identifies N regular users of the system, where N is limited by the amount of onboard memory 106 of the client machine 102. The client machine 102 then processes the speech of a user according to a VR model 116 selected for the user.</p><p id="p0025" num="0025">If the client machine 102 determines that an error has occurred in translating (or otherwise processing) the speech of a user, then the client machine 102 transmits the speech received from the user to a remote, cloud-based machine, such as server machine 150-1. The error may occur in any manner, such as when client machine 102 cannot recognize the speech, or when the client machine 102 recognizes the speech incorrectly, or when a user is forced to repeat a command, or when the user does not get an expected result from a command.</p><p id="p0026" num="0026">In one example, and without limitation, the client machine 102 could fail to correctly recognize speech when spoken by a user who speaks with a strong accent, as with a non-native speaker of a particular language. In another example, and without limitation, the client machine 102 could fail to correctly recognize speech when spoken by a user who speaks with certain speech impediments. In yet another example, and without limitation, the client machine 102 could fail to correctly recognize speech when a user, speaking in one language, speaks one or more words in a different language, such as when an English speaker utters a word or phrase in Spanish or German. In yet another example, and without limitation, the client machine 102 could fail to correctly recognize speech when a user is speaking in a language that is only partially supported in the currently loaded VR models 116. That is, a particular language could have a total vocabulary of 20,000 words, where only 15,000 words are currently stored in the loaded VR models 116. If a user speaks using one or more of the 5,000 words not current stored in the VR<!-- EPO <DP n="9"> --> models 116, then the client machine 102 would fail to correctly recognize such words. If an error occurs during speech recognition under any of these examples, or if an error occurs for any other reason, then the client machine 102 transmits the speech received from the user, or a portion thereof, to a remote, cloud-based machine, such as server machine 150-1.</p><p id="p0027" num="0027">The server machine 150-1 analyzes the speech, or portion thereof, of a user in order to find a VR model 156 that is better suited to process the speech of a user. The server machine 150-1 transmits the VR model 156 to the client machine 102. Alternatively, server machine 150-1 transmits modification information regarding adjustments to perform on the VR model 116 stored in the client machine 102. In various embodiments, the modification information may include, without limitation, data to add to the VR model 116, data in the VR model 116 to modify or replace, and data to remove from the VR model 116. In response, the client machine 102 adds to, modifies, replaces, or removes corresponding data in the VR model 116. As a result, if the client machine 102 encounters the same speech pattern at a future time, the client machine 102 is able to resolve the speech pattern locally using the updated VR model 116 without the aid of the server machine 150-1.</p><p id="p0028" num="0028">Additionally, the server machine 150-1 returns the processed speech signal to the client machine 102. In some embodiments, the transmission of new VR models or VR model modifications from the server machine 150-1 to the client machine 102 may be asynchronous with the transmission of the processed speech signal. In other words, the server machine 150-1 may transmit new VR models or VR model modifications to the client machine 102 prior to, concurrently with, or subsequent to transmitting the processed speech signal for a particular transaction.</p><p id="p0029" num="0029">Wherever possible, the client machine 102, executing a local instance of the VR application 112, performs speech recognition via the local instances of the user data 115 and VR models 116 for reduced latency and improved performance relative to using remote instances of the user data 155 and VR models 156. In contrast, the remote instances of the user data 155 and VR models 156 on the server machine 150-1 generally provide improved mechanisms to support speech recognition relative to the local VR models 116 albeit at relatively higher latency. The client machine 102 receives user speech data (in audio format) from the user, such as a voice command spoken by a user in a vehicle. The client machine 102 then correctly identifies unique users based on an analysis of the received speech data against unique user speech profiles in the<!-- EPO <DP n="10"> --> local user data 115. The client machine 102 then selects the unique speech profile of the user in the local user data 115, and processes the speech data using the selected model. If the client machine 102 determines that errors in translating the speech of a user have occurred using the selected model, the client machine 102 transmits the received user speech input, or a portion thereof, to the server machine 150-1 for further processing by the remote instance of the VR application 152 (or some other suitable application). Although each error is catalogued on the remote server machine 150-1, the local instance of the VR application 112 may variably send the user speech input to the server machine 150-1 based on heuristics and network connectivity.</p><p id="p0030" num="0030">The server machine 150-1, executing the remote instance of the VR application 152, identifies a remote VR model 156 on the server machine 150-1 that is better suited to process the speech of a user. The remote VR model 156 may be identified as being better suited to process the speech of a user in any feasible manner. For example, an upper threshold number of errors could be implemented, such that if the number of errors encountered by the client machine 102 exceeds the threshold, then the server machine 150-1 could transmit a complete remote VR model 156 to the client machine 102 to completely replace the local VR model 116. Additionally or alternatively, if the client machine 102 encounters a smaller number of errors below the threshold, then the server machine 150-1 could transmit modification data to the client machine 102 to apply to the local VR model 116. The server machine 150-1 transmits the identified VR model, or the modifications thereto, to the client machine 102. The client machine 102, then replaces or modifies the local VR model 116 accordingly. The client machine 102 then re-processes the user speech data using the new VR model 116 stored in the storage 108. In some embodiments, the number of recognition errors reduces over time, and the number of requests to the server machine 150-1, and corresponding updates to the VR models 116, may be less frequent.</p><p id="p0031" num="0031"><figref idrefs="f0002">Figure 2</figref> sets forth a flow diagram of method steps for performing user-adapted speech recognition, according to various embodiments. Although the method steps are described in conjunction with the systems of <figref idrefs="f0001">Figure 1</figref>, persons skilled in the art will understand that any system configured to perform the method steps, in any order, is within the scope of the present disclosure.</p><p id="p0032" num="0032">As shown, a method 200 begins at step 210, where the client machine 102 executing the VR application 112 receives a portion of user speech. The speech may be, include, without<!-- EPO <DP n="11"> --> limitation, a command spoken in a vehicle, such as "tune the radio to 78.8 FM." The client machine 102 receives the speech through any feasible input source, such as a microphone or a Bluetooth data connection. At step 220, the client machine 102 encounters an error while translating the speech of a user using the local VR models 116 in the storage 108. The error may be any error, such as the client machine 102 incorrectly interpreting the speech of a user, the client machine 102 being unable to interpret the speech at all, or any other predefined event. At step 230, the client machine 102 transmits data representing the speech, or portion thereof, to the server machine 150-1. The data transmitted may include an indication of the error, the speech data, and the local VR model 116 with which the VR application 112 attempted to process the speech. In some embodiments, the VR application 112 may only transmit an indication of the error, which may include a description of the error, and not transmit the VR model 116 or the speech data.</p><p id="p0033" num="0033">At step 240, the server machine 150-1 executing the VR application 152 analyzes the received speech to select a new VR model 156 which is better suited to process the speech of a user. The server machine 150-1 identifies the new VR model 116 as being better suited to process the speech of a user in any feasible manner. At step 250, the server machine 150-1 transmits the selected VR model 156 to the client machine 102. In some embodiments, the VR application 112 may transmit modifications for the VR model 116 to the client machine 102 instead of transmitting the entire VR model 156 itself. At step 260, if the client machine 102 receives a new VR model 156 from the server machine 150-1, then the client machine replaces the existing VR model 116 with the newly received VR model 156. If the client machine 102 receives VR model modification information from the server machine 150-1, then the client machine 102 modifies the local VR model 116 in the storage 108 based on the received modification information. At step 270, the client machine 102 processes the speech of a user using the replaced or modified VR model 116. At step 280, the client machine 102 causes the desired command (or request) spoken by the user to be completed. The method 200 then terminates.</p><p id="p0034" num="0034">Thereafter, whenever the client machine 102 receives new speech input from the same user, the client machine 102 processes the speech of a user using the newly replaced or modified VR model 116 transmitted at step 250. The client machine 102 may also re-execute the steps of the method 200 in order to further refine the VR model 116 for unique users, such that over time,<!-- EPO <DP n="12"> --> further modifications to the VR models 116 are not likely needed in order to correctly interpret speech of a user using the local VR model 116.</p><p id="p0035" num="0035"><figref idrefs="f0003">Figure 3</figref> sets forth a flow diagram of method steps for analyzing speech data to select a new voice recognition model, according to various embodiments. Although the method steps are described in conjunction with the systems of <figref idrefs="f0001 f0002">Figures 1-2</figref>, persons skilled in the art will understand that any system configured to perform the method steps, in any order, is within the scope of the present disclosure.</p><p id="p0036" num="0036">As shown, a method 300 begins at step 310, where the server machine 150-1 executing the VR application 152 computes feature vectors for the speech data transmitted to the server machine 150-1 at step 230 of method 200. The computed feature vectors describe one or more features (or attributes) of each interval (or segment) of the speech data. At step 320, the server machine 150-1 analyzes the feature vectors of the speech to identify cohort groups having similar speech features. In at least one embodiment, the server machine 150-1 may perform a clustering analysis of stored speech data on the server machine 150-1 to identify a cohort group whose speech features most closely matches the received speech data. In this manner, the server machine 150-1 may identify what type of speaker the user is (such as non-native speaker, a person with a speech disability or impairment, or a native speaker having a regional dialect) and may allow the server machine 150-1 to identify a VR model better suited to process this class of speech. For example, the server machine 150-1 may determine that the received speech data clusters into a group of speech data associated with southern United States English speakers.</p><p id="p0037" num="0037">However, the storage 108 on the client machine 102 may not include a VR model in the VR models 116 that is suited to process speech for southern U.S. English speakers. Consequently, at step 330, the server machine 150-1 identifies one or more VR models for the cohort group identified at step 320. For example, and without limitation, the server machine 150-1 could identify one or more VR models stored in the VR models 156 stored on the server machine 150-1 that are associated with southern U.S. English speakers. Similarly, the server machine 150-1 could identify a VR model for people with a speech impediment, or a regional dialect. At step 340, the server machine 150-1 transmits to the client machine 102 the selected VR model (or updates to the local VR models) that are best suited to process the received speech. The method 300 then terminates.<!-- EPO <DP n="13"> --></p><p id="p0038" num="0038">In sum, a speech recognition system includes a local client machine and one or more remote server machines. The client machine receives a speech signal and converts the speech to text via locally stored VR models. If the client machine detects an error during local speech recognition, then the client machine transmits information regarding the error to one or more server machines. The server machine, which includes a larger number of VR models, as well as more robust VR models, resolves the error and transmits the processed speech signal back to the client machine. The server machine, based on received errors, also transmits new VR models or VR model modification information to the client machine. The client machine, in turn, replaces or modifies the locally stored VR models based on the information received from the server machine.</p><p id="p0039" num="0039">At least one advantage of the disclosed approach is that speech recognition can be performed for multilingual speakers or speakers with strong accents or speech impediments with lower latency and higher reliability relative to prior approaches. At least one additional advantage of the disclosed approach is that, over time, the ability of the client machine to correctly recognize speech of one or more users without relying on a server machine improves, resulting in additional latency reductions and performance improvements.</p><p id="p0040" num="0040">The descriptions of the various embodiments have been presented for purposes of illustration, but are not intended to be exhaustive or limited to the embodiments disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the described embodiments.</p><p id="p0041" num="0041">Aspects of the present embodiments may be embodied as a system, method or computer program product. Accordingly, aspects of the present disclosure may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a "circuit," "module" or "system." Furthermore, aspects of the present disclosure may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.</p><p id="p0042" num="0042">Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to,<!-- EPO <DP n="14"> --> an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.</p><p id="p0043" num="0043">Aspects of the present disclosure are described above with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems) and computer program products according to embodiments of the disclosure. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, enable the implementation of the functions/acts specified in the flowchart and/or block diagram block or blocks. Such processors may be, without limitation, general purpose processors, special-purpose processors, application-specific processors, or field-programmable</p><p id="p0044" num="0044">The flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present disclosure. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted<!-- EPO <DP n="15"> --> that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.</p><p id="p0045" num="0045">Embodiments of the disclosure may be provided to end users through a cloud computing infrastructure. Cloud computing generally refers to the provision of scalable computing resources as a service over a network. More formally, cloud computing may be defined as a computing capability that provides an abstraction between the computing resource and its underlying technical architecture (e.g., servers, storage, networks), enabling convenient, on-demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. Thus, cloud computing allows a user to access virtual computing resources (e.g., storage, data, applications, and even complete virtualized computing systems) in "the cloud," without regard for the underlying physical systems (or locations of those systems) used to provide the computing resources.</p><p id="p0046" num="0046">Typically, cloud computing resources are provided to a user on a pay-per-use basis, where users are charged only for the computing resources actually used (e.g. an amount of storage space consumed by a user or a number of virtualized systems instantiated by the user). A user can access any of the resources that reside in the cloud at any time, and from anywhere across the Internet. In context of the present disclosure, a user may access applications (e.g., video processing and/or speech analysis applications) or related data available in the cloud.</p><p id="p0047" num="0047">While the preceding is directed to embodiments of the present disclosure, other and further embodiments of the disclosure may be devised without departing from the basic scope thereof, and the scope thereof is determined by the claims that follow.</p></description><claims mxw-id="PCLM90459437" lang="EN" load-source="patent-office"><!-- EPO <DP n="16"> --><claim id="c-en-0001" num="0001"><claim-text>A method for performing speech recognition, the method comprising:
<claim-text>receiving an electronic signal that represents human speech of a speaker;</claim-text>
<claim-text>converting the electronic signal into a plurality of phonemes;</claim-text>
<claim-text>while converting the plurality of phonemes into a first group of words based on a first voice recognition model, encountering an error when attempting to convert one or more of the phonemes into words;</claim-text>
<claim-text>transmitting a message associated with the error to a server machine, wherein the server machine is configured to convert the one or more phonemes into a second group of words based on a second voice recognition model resident on the server machine; and</claim-text>
<claim-text>receiving the second group of words from the server machine.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The method of claim 1, further comprising:
<claim-text>receiving the second voice recognition model from the server machine; and</claim-text>
<claim-text>replacing the first voice recognition model with the second voice recognition model.</claim-text></claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The method of claim 1 or 2, further comprising:
<claim-text>receiving modification information associated with the second voice recognition model from the server machine; and</claim-text>
<claim-text>modifying the first voice recognition model based on the modification information.</claim-text></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The method of one of claims 1 to 3, wherein the error is associated with a speech impediment that is unrecognizable via the first voice recognition model but is recognizable via the second voice recognition model.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The method of one of claims 1 to 3, wherein the error is associated with a word uttered in a language that is unrecognizable via the first voice recognition model but is recognizable via the second voice recognition model.<!-- EPO <DP n="17"> --></claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The method of one of claims 1 to 3, wherein the error is associated with a word uttered with an accent that is unrecognizable via the first voice recognition model but is recognizable via the second voice recognition model.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The method of one of claims 1 to 6, further comprising converting, via the server machine, the one or more phonemes into a second group of words based on a second voice recognition model resident on the server machine.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The method of one of claims 1 to 7, wherein the first voice recognition model includes a subset of the words included in the second voice recognition model, and the error is associated with a word that is included the second voice recognition model but not included in the first voice recognition model.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>A computer-readable storage medium including instructions that, when executed by a processor, cause the processor to perform speech recognition, by performing the steps of one of claims 1 to 8.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>A speech recognition system, comprising:
<claim-text>a memory that includes a voice recognition application; and</claim-text>
<claim-text>a processor coupled to the memory, wherein, when executed by the processor, the voice recognition program configures the processor to:
<claim-text>convert an electronic signal that represents human speech of a speaker into a plurality of phonemes;</claim-text>
<claim-text>while converting the plurality of phonemes into a first group of words based on a first voice recognition model, encounter an error when attempting to convert one or more of the phonemes into words; and</claim-text>
<claim-text>transmit a message associated with the error to a server machine, wherein the server machine is configured to convert the one or more phonemes into a second group of words based on a second voice recognition model resident on the server machine.</claim-text></claim-text><!-- EPO <DP n="18"> --></claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The speech recognition system of claim 10, wherein, when executed by the processor, the voice recognition application is further configured to:
<claim-text>receive the second voice recognition model from the server machine; and</claim-text>
<claim-text>replace the first voice recognition model with the second voice recognition model.</claim-text></claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>The speech recognition system of claim 10 or 11, wherein, when executed by the processor, the voice recognition application is further configured to:
<claim-text>receive modification information associated with the second voice recognition model from the server machine; and</claim-text>
<claim-text>modify the first voice recognition model based on the modification information.</claim-text></claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>The speech recognition system of one of claims 10 to 12, wherein each of the first voice recognition model and the second voice recognition model comprises at least one of an acoustic model, a language model, and a statistical model.</claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>The speech recognition system of one of claims 10 to 13, wherein, when executed by the processor, the voice recognition application is further configured to combine the first group of words and the second group of words to form a third group of words.</claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>The speech recognition system of one of claims 10 to 14, wherein, when executed by the processor, the voice recognition application is further configured to perform an operation based on the third group of words.</claim-text></claim></claims><drawings mxw-id="PDW20422164" load-source="patent-office"><!-- EPO <DP n="19"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="165" he="209" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="20"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="141" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="21"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="141" he="172" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="161" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="161" he="233" type="tif"/><doc-page id="srep0003" file="srep0003.tif" wi="161" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
