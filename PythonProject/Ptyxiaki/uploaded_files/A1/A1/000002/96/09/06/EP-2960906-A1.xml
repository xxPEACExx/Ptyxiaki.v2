<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960906-A1" country="EP" doc-number="2960906" kind="A1" date="20151230" family-id="51178836" file-reference-id="311904" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451425" ucid="EP-2960906-A1"><document-id><country>EP</country><doc-number>2960906</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14306008-A" is-representative="YES"><document-id mxw-id="PAPP193865818" load-source="patent-office" format="original"><country>EP</country><doc-number>14306008.5</doc-number><date>20140626</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865819" load-source="docdb" format="epo"><country>EP</country><doc-number>14306008</doc-number><kind>A</kind><date>20140626</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162036543" ucid="EP-14306008-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>14306008</doc-number><kind>A</kind><date>20140626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988522144" load-source="docdb">H04N  21/81        20110101ALI20141002BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988524003" load-source="docdb">G06T  19/20        20110101ALI20141002BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988524586" load-source="docdb">G11B  27/036       20060101AFI20141002BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988526442" load-source="docdb">G06Q  30/02        20120101ALN20141002BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988527470" load-source="docdb">H04N   5/272       20060101ALN20141002BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988527782" load-source="docdb">G06K   9/00        20060101ALN20141002BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1627894644" load-source="docdb" scheme="CPC">H04N  21/81        20130101 LI20180609BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1627894645" load-source="docdb" scheme="CPC">G06Q  30/0241      20130101 LA20180609BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1627894646" load-source="docdb" scheme="CPC">G06K   9/6212      20130101 LI20180609BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1627894647" load-source="docdb" scheme="CPC">H04N  21/816       20130101 LI20180609BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1642409264" load-source="docdb" scheme="CPC">H04N  13/122       20180501 LI20180510BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1811116171" load-source="docdb" scheme="CPC">G11B  27/036       20130101 FI20170519BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1811116172" load-source="docdb" scheme="CPC">G06T2219/2004      20130101 LA20170519BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1811116173" load-source="docdb" scheme="CPC">G06K   9/00201     20130101 LI20170519BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1811116174" load-source="docdb" scheme="CPC">G06T  19/20        20130101 LI20170519BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545741" lang="DE" load-source="patent-office">Verfahren zur Verarbeitung einer Videoszene sowie entsprechende Vorrichtung</invention-title><invention-title mxw-id="PT165545742" lang="EN" load-source="patent-office">Method for processing a video scene and corresponding device</invention-title><invention-title mxw-id="PT165545743" lang="FR" load-source="patent-office">Procédé de traitement d'une scène vidéo et dispositif correspondant</invention-title><citations><patent-citations><patcit mxw-id="PCIT335962018" load-source="docdb" ucid="EP-2437220-A1"><document-id format="epo"><country>EP</country><doc-number>2437220</doc-number><kind>A1</kind><date>20120404</date></document-id><sources><source name="SEA" category="Y" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT373873015" load-source="docdb" ucid="EP-2439664-A1"><document-id format="epo"><country>EP</country><doc-number>2439664</doc-number><kind>A1</kind><date>20120411</date></document-id><sources><source name="SEA" category="YA" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962020" load-source="docdb" ucid="EP-2562688-A2"><document-id format="epo"><country>EP</country><doc-number>2562688</doc-number><kind>A2</kind><date>20130227</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT373873014" load-source="docdb" ucid="US-20040264777-A1"><document-id format="epo"><country>US</country><doc-number>20040264777</doc-number><kind>A1</kind><date>20041230</date></document-id><sources><source name="SEA" category="YA" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962016" load-source="docdb" ucid="US-20080033812-A1"><document-id format="epo"><country>US</country><doc-number>20080033812</doc-number><kind>A1</kind><date>20080207</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962017" load-source="docdb" ucid="US-20100122286-A1"><document-id format="epo"><country>US</country><doc-number>20100122286</doc-number><kind>A1</kind><date>20100513</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335962021" load-source="docdb" ucid="US-20130141530-A1"><document-id format="epo"><country>US</country><doc-number>20130141530</doc-number><kind>A1</kind><date>20130606</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>LUIS ALEXANDRE: "3D Descriptors for Object and Category Recognition: a Comparative Evaluation", 7 October 2012 (2012-10-07), XP055054904, Retrieved from the Internet &lt;URL:http://www.di.ubi.pt/~lfbaa/pubs/iros2012.pdf&gt; [retrieved on 20130228]</text><sources><source mxw-id="PNPL63646566" load-source="docdb" name="SEA" category="YDA"/></sources></nplcit><nplcit><text>None</text><sources><source mxw-id="PNPL57937260" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103305027" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>THOMSON LICENSING</last-name><address><country>FR</country></address></addressbook></applicant><applicant mxw-id="PPAR1103334062" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>THOMSON LICENSING</last-name></addressbook></applicant><applicant mxw-id="PPAR1101639923" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Thomson Licensing</last-name><iid>101157220</iid><address><street>1-5, rue Jeanne d'Arc</street><city>92130 Issy-Les-Moulineaux</city><country>FR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103315308" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ALLEAUME VINCENT</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103334969" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>ALLEAUME, VINCENT</last-name></addressbook></inventor><inventor mxw-id="PPAR1101639535" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>ALLEAUME, VINCENT</last-name><address><street>TECHNICOLOR R&amp;D FRANCE 975 AVENUE DES CHAMPS BLANCS</street><city>35576 Cesson Sevigne Cedex</city><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103325932" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>JOUET PIERRICK</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103333973" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>JOUET, PIERRICK</last-name></addressbook></inventor><inventor mxw-id="PPAR1101649864" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>JOUET, PIERRICK</last-name><address><street>TECHNICOLOR R&amp;D FRANCE 975 AVENUE DES CHAMPS BLANCS</street><city>35576 Cesson Sevigne Cedex</city><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103324748" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>BOURDON PASCAL</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103303142" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>BOURDON, PASCAL</last-name></addressbook></inventor><inventor mxw-id="PPAR1101646471" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>BOURDON, PASCAL</last-name><address><street>TECHNICOLOR R&amp;D FRANCE 975 AVENUE DES CHAMPS BLANCS</street><city>35576 Cesson Sevigne Cedex</city><country>FR</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101653599" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Huchet, Anne</last-name><iid>101463286</iid><address><street>Technicolor 1-5 rue Jeanne d'Arc</street><city>92130 Issy-Les-Moulineaux</city><country>FR</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660606317" load-source="docdb">AL</country><country mxw-id="DS660609959" load-source="docdb">AT</country><country mxw-id="DS660609303" load-source="docdb">BE</country><country mxw-id="DS660783248" load-source="docdb">BG</country><country mxw-id="DS660684670" load-source="docdb">CH</country><country mxw-id="DS660609304" load-source="docdb">CY</country><country mxw-id="DS660609960" load-source="docdb">CZ</country><country mxw-id="DS660606319" load-source="docdb">DE</country><country mxw-id="DS660609305" load-source="docdb">DK</country><country mxw-id="DS660609306" load-source="docdb">EE</country><country mxw-id="DS660687107" load-source="docdb">ES</country><country mxw-id="DS660783249" load-source="docdb">FI</country><country mxw-id="DS660684675" load-source="docdb">FR</country><country mxw-id="DS660606320" load-source="docdb">GB</country><country mxw-id="DS660609315" load-source="docdb">GR</country><country mxw-id="DS660606321" load-source="docdb">HR</country><country mxw-id="DS660609961" load-source="docdb">HU</country><country mxw-id="DS660687108" load-source="docdb">IE</country><country mxw-id="DS660609316" load-source="docdb">IS</country><country mxw-id="DS660783250" load-source="docdb">IT</country><country mxw-id="DS660609317" load-source="docdb">LI</country><country mxw-id="DS660783251" load-source="docdb">LT</country><country mxw-id="DS660684617" load-source="docdb">LU</country><country mxw-id="DS660783252" load-source="docdb">LV</country><country mxw-id="DS660783253" load-source="docdb">MC</country><country mxw-id="DS660684618" load-source="docdb">MK</country><country mxw-id="DS660684623" load-source="docdb">MT</country><country mxw-id="DS660684624" load-source="docdb">NL</country><country mxw-id="DS660684676" load-source="docdb">NO</country><country mxw-id="DS660684625" load-source="docdb">PL</country><country mxw-id="DS660687109" load-source="docdb">PT</country><country mxw-id="DS660689040" load-source="docdb">RO</country><country mxw-id="DS660687110" load-source="docdb">RS</country><country mxw-id="DS660684626" load-source="docdb">SE</country><country mxw-id="DS660606323" load-source="docdb">SI</country><country mxw-id="DS660684677" load-source="docdb">SK</country><country mxw-id="DS660684678" load-source="docdb">SM</country><country mxw-id="DS660609318" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479795" lang="EN" load-source="patent-office"><p id="pa01" num="0001">The present disclosure relates to a method for processing a video of a scene, the method comprising:<br/>
- receiving (S1) a 3D representation of the scene,<br/>
- determining (S2) into said 3D representation at least one candidate object, said at least one candidate object comprising a cloud of 3D points,<br/>
- selecting (S3), among said at least one candidate object, one object to be removed,<br/>
- computing (S4) a signature of the object to be removed, said signature being representative of 3D features of said selected object,<br/>
- determining (S5) a substitute object from a plurality of candidate substitute objects of a library, each candidate substitute object having a signature, said determination being based on comparison of signatures, and<br/>
- replacing (S6) the object to be removed by said substitute object into said video scene.
<img id="iaf01" file="imgaf001.tif" wi="57" he="130" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759607" lang="EN" source="EPO" load-source="docdb"><p>The present disclosure relates to a method for processing a video of a scene, the method comprising: 
- receiving (S1) a 3D representation of the scene, 
- determining (S2) into said 3D representation at least one candidate object, said at least one candidate object comprising a cloud of 3D points, 
- selecting (S3), among said at least one candidate object, one object to be removed, 
- computing (S4) a signature of the object to be removed, said signature being representative of 3D features of said selected object, 
- determining (S5) a substitute object from a plurality of candidate substitute objects of a library, each candidate substitute object having a signature, said determination being based on comparison of signatures, and 
- replacing (S6) the object to be removed by said substitute object into said video scene.</p></abstract><description mxw-id="PDES98404496" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>1. Technical field.</b></heading><p id="p0001" num="0001">The present disclosure relates to the domain of video processing and more specifically to the domain of the placement or replacement of objects or products in a film.</p><heading id="h0002"><b>2. Technical background</b></heading><p id="p0002" num="0002">The film industry is showing a growing trend that is the product placement in film, as it provides a new business and new source of revenues. This product placement can be for example the placement of a bottle of a major brand of soda in a film. This operation can also consist in replacing all the bottles in the scenes of a film by other bottles. For example, all the bottles of wine can be replaced by bottles of water or soda.</p><p id="p0003" num="0003">In film industry using mono camera capture or 2D shooting, the object placement is typically done once in the video scene before shooting occurs. Later replacing or adding of a wanted object is also possible by post processing methods but it is quite challenging regarding potential costs it can lead, notably for placing the object in numerous video scenes throughout the whole film.</p><heading id="h0003"><b>3. Summary</b></heading><p id="p0004" num="0004">The present disclosure proposes a new method for replacing an object from a video scene by an object to be placed,. This method proposes to make a 3D acquisition of the video scene and to replace an object of the video scene by a substitute object corresponding to the product having substantially common 3D features than the object to be replaced. It permits to minimize the appearance of hidden surfaces or occlusions.</p><p id="p0005" num="0005">More specifically, the present disclosure concerns a method for processing a video of a scene, the method comprising:
<ul><li>receiving a 3D representation of the scene,</li><li>determining into said 3D representation at least one candidate object, said at least one candidate object comprising a cloud of 3D points,</li><li>selecting, among said at least one candidate object, one object to be removed,<!-- EPO <DP n="2"> --></li><li>computing a signature of the object to be removed, said signature being representative of 3D features of said selected object,</li><li>determining a substitute object from a plurality of candidate substitute objects of a library, each candidate substitute object having a signature, said determination being based on comparison of signatures, and</li><li>replacing the object to be removed by said substitute object into said video scene.</li></ul></p><p id="p0006" num="0006">In a specific embodiment, the object to be removed is selected, among said at least one candidate object, by a user.</p><p id="p0007" num="0007">Advantageously, the object to be removed is selected among said at least one candidate object as a function of at least one the following criteria:
<ul><li>its geographical position in the video scene;</li><li>its localisation into the foreground of the video scene.</li></ul></p><p id="p0008" num="0008">The purpose of these criteria is to improve, after the replacement step, the visibility of the substitute object in the final scene.</p><p id="p0009" num="0009">In a specific embodiment, the library comprises a plurality of candidate substitute objects classified into object families and the substitute object is determined among candidate substitute objects belonging to at least one selected object families.</p><p id="p0010" num="0010">In a specific embodiment, the selected object families are chosen by a user.</p><p id="p0011" num="0011">In a specific embodiment, the signature of each object comprises 3D features representative of the geometry of the object, called geometrical features, and/or 3D features representative of the viewpoint of the object, called viewpoint features.</p><p id="p0012" num="0012">In a specific embodiment, the signature of an object is at least one histogram among the group of following histograms:
<ul><li>Viewpoint Feature Histogram or VFH;</li><li>Clustered Viewpoint Feature Histogram or CVFH;</li><li>Point Feature Histogram or PFH;</li><li>Fast Point Feature Histogram or FPFH.</li></ul></p><p id="p0013" num="0013">In a specific embodiment, the substitute object is determined by:
<ul><li>comparing the signature of the object to be removed and the signatures of the candidate substitute objects of the library, and</li><li>identifying the candidate substitute object of the library having a signature identical to or close to the signature of the object to be removed<!-- EPO <DP n="3"> --> according to a similarity criterion, the substitute object being the identified candidate substitute object of the library.</li></ul></p><p id="p0014" num="0014">In another embodiment, the substitute object is generated from a combination of a plurality of candidate substitute objects. More specifically, the substitute object is generated by:
<ul><li>comparing the signature of the object to be removed and the signatures of the candidate substitute objects of the library, and</li><li>identifying a plurality of candidate substitute objects of the library having a signature close to the signature of the object to be removed according to a similarity criterion, the substitute object being a combination of the plurality of the identified candidate substitute objects of the library.</li></ul></p><p id="p0015" num="0015">In a specific embodiment, the method further comprises a step of computing the signatures of the candidate substitute objects of the library before comparing them to the signature of the object to be removed.</p><p id="p0016" num="0016">In a variant, the signatures of the candidate substitute objects of the library are pre-computed and stored in the library.</p><p id="p0017" num="0017">In a specific embodiment, before computing the signature of the object to be removed and the signatures of the candidate substitute objects of the library, the method further comprises a step for filtering the object to be removed and/or the candidate substitute objects of the library in order that the object to be removed and the candidate substitute objects of the library have substantially the same point density.</p><p id="p0018" num="0018">The present disclosure concerns also a device for processing a video of a scene, the device comprising:
<ul><li>an interface configured for receiving a 3D representation of the scene, and</li><li>a processing unit for determining into said 3D representation at least one candidate object, said at least one candidate object comprising a cloud of 3D points, for selecting, among said at least one candidate object, one object to be removed, for computing a signature of the object to be removed, said signature being representative of 3D features of said selected object, for determining a substitute object from a plurality of candidate substitute objects of a library, each candidate substitute object having a signature, said determination being based on comparison of signatures, and<!-- EPO <DP n="4"> --> for replacing the object to be removed by said substitute object into said video scene.</li></ul></p><p id="p0019" num="0019">The present disclosure also relates to a computer program product comprising instructions of program code for execution by at least one processor to perform the above mentioned method, when the program is executed on a computer.</p><heading id="h0004">4. List of figures</heading><p id="p0020" num="0020">The present disclosure will be better understood, and other specific features and advantages will emerge upon reading the following description, the description making reference to the annexed drawings wherein:
<ul><li><figref idrefs="f0001">Fig.1</figref> shows a flow diagram of the steps of the method according to the present disclosure;</li><li><figref idrefs="f0002">Fig.2</figref> shows a first histogram as a signature for objects to be removed or to be inserted in a video scene according to a particular embodiment of the present disclosure;</li><li><figref idrefs="f0002">Fig.3</figref> shows a second histogram as a signature for objects to be removed or to be inserted in a video scene according to a particular embodiment of the present disclosure;</li><li><figref idrefs="f0003">Fig.4</figref> shows a video scene before and after implementation of the method according to present disclosure; and</li><li><figref idrefs="f0003">Fig.5</figref> shows a device for implementing the method of the present disclosure.</li></ul></p><heading id="h0005"><b>5. Detailed description of embodiments.</b></heading><p id="p0021" num="0021">The inventive method proposes to make a 3D acquisition of the video scene to be processed and to replace an object of this scene by a substitute object having substantially the same 3D features.</p><p id="p0022" num="0022"><figref idrefs="f0001"><b>Fig.1</b></figref> shows the different steps of a method according to the present disclosure. A first step, called S1, is to receive a 3D representation of the scene. The 3D representation may be performed through an acquisition of the scene. This acquisition can be made by a 3D or stereo camera or by an arrangement comprising a 2D camera and a laser scanner. According to a<!-- EPO <DP n="5"> --> variant, the 3D representation is received from a network, like internet, where 3D data are available</p><p id="p0023" num="0023">In a second step, called S2, the 3D representation of the scene is segmented in order to obtain a plurality of candidate objects to be replaced. Each candidate object is a cloud of 3D points.. A common approach of the segmentation operation consists in doing the following steps:
<ul><li>detecting planes in the 3D scene using for example a RANSAC (for Random Sample Consensus) algorithm;</li><li>removing all points belonging to at least of a part these planes, for example large planes corresponding to walls; and</li><li>Clustering the remaining points using for instance a criterion based on euclidian distance between set of points; Sets of points above a threshold distance are considered as distinct objects.</li></ul></p><p id="p0024" num="0024">At the end of the step S2, a plurality of candidate objects in the scene have been determined. For an indoor scene, the candidate objects are for example furniture like a table or chairs, and decorative items like a vase or a bottle.</p><p id="p0025" num="0025">In a third step, called S3, an object to be removed is selected among the candidate objects determined at step S2. This selection can be made by a user through an interface.</p><p id="p0026" num="0026">Advantageously, the object is selected as a function of its geographical position in the video scene and/or its localisation into the foreground or background of the video scene. The object to be replaced is preferably selected in the central area of the video scene and in the foreground in order to optimize the visibility of the object that will be placed in this area. This optimized placement is very interesting in an application of advertisement placement. In this embodiment, the user interface for selecting the object to be replaced will only propose objects that are present in the central area or the foreground of the scene.</p><p id="p0027" num="0027">In a fourth step, called S4, a 3D signature of the selected object is computed. The computed signature is representative of 3D features of the object. Advantageously, the signature of the object comprises 3D features representative of the geometry of the object, called geometrical features, and/or 3D features representative of the viewpoint of the object, called viewpoint features. The geometrical features are related to the shape of the object, for example the presence of edges or planes in the object. The<!-- EPO <DP n="6"> --> viewpoint features comprise for example the angle and/or distance at which the object has been shot by the capturing device.</p><p id="p0028" num="0028">This signature is preferably an histogram of 3D features, like for example a Viewpoint Feature Histogram (VFH), a Clustered Viewpoint Feature Histogram (CVFH), a Point Feature Histogram (PFH) or a Fast Point Feature Histogram (FPFH). These histograms are described in various documents and notably in the document "3D descriptors for Object and Category Recognition: a Comparative Evaluation" of Luis A. Alexandre.</p><p id="p0029" num="0029">An example of Viewpoint Feature Histogram (VFH) is given by <figref idrefs="f0002">Fig.2</figref>. In this histogram, a value (vertical axis) is computed for a plurality of 3D features (horizontal axis). In this histogram, the left part of the histogram values is related to geometrical features and the right part is related to viewpoint features.</p><p id="p0030" num="0030">The signature comprises all of these values or a part of them. The signature can comprise geometrical and viewpoint features values or only a part of them or only geometrical features values or only viewpoint features values.</p><p id="p0031" num="0031">In a specific embodiment, colour information of the object is also integrated into the signature.</p><p id="p0032" num="0032"><figref idrefs="f0002"><b>Fig.3</b></figref> shows a Fast Point Feature Histogram (FPFH), which is another example of histogram that can be used as a signature for the objects.</p><p id="p0033" num="0033">In a fifth step, called S5, a substitute object is determined from a plurality of candidate substitute objects of a library. The candidate substitute objects are preferably classified into object families within the library. Object families for indoor scenes are for example: bottles, furniture, books, boxes...</p><p id="p0034" num="0034">A signature is computed for each of these candidate substitute objects. This signature can be pre-computed and stored in the library. In a variant, the signature of the candidate substitute objects is computed later in the process in order to take into consideration the probable difference of point density between the point cloud of the selected object to be removed and the point cloud of the candidate substitute objects.</p><p id="p0035" num="0035">The determination step S5 is based on comparison of signatures. The signature of the object to be removed is compared to the signatures of the candidate substitute objects of the library. The candidate substitute object having a signature identical to or close to the signature of the object to be removed according to a similarity criterion is selected. A distance between<!-- EPO <DP n="7"> --> the signature of the object to be removed and the signature of each candidate substitute objects is computed and the candidate substitute object whose signature has the smallest distance with the signature of the object to be removed is selected as the substitute object of the object to be removed.</p><p id="p0036" num="0036">In a variant, the substitute object is a combination of the closest candidate substitute objects. For example, the substitute object is generated based on the two candidate substitute objects whose signature has the smallest distance with the signature of the object to be removed.</p><p id="p0037" num="0037">In a specific embodiment, before comparing the signatures, step S5 proposes through a user interface to select an object family of the library in order to reduce the number of comparisons to be carried out and reduce the processing time of the step S5. This selection can also be done for business reasons. The selected object families are those comprising objects of a client. The user may select through the user interface the object families corresponding to the object selected at step S3. For example, if the object selected at step S3 is a bottle, the user will select the object families corresponding to bottles. The user may also select an object family that does not correspond to the object selected at step S3. The user may want to replace bottles by books. In that case, the user will select the object family comprising books and the inventive method will select a book having the most similar 3D features with the bottle selected at step S3.</p><p id="p0038" num="0038">In an advantageous embodiment, the point density of the point cloud of the object to be removed and the point density of point cloud of the candidate substitute objects of the library are compared. If these point densities are different, a filtering step is applied to the point cloud of the object to be removed or to the point cloud of the candidate substitute objects in order to compare (in step S5) objects having the same or substantially the same point density. If the point density of the candidate substitute objects is higher than that of the object to be removed, the point density of the candidate substitute objects is reduced to be substantially equal to the point density of the object to be removed. This is often the case. In the same manner, if the point density of the object to be removed is higher than that of the candidate substitute objects, the point density of the object to be removed is reduced to be substantially equal to the point density of the candidate substitute objects.<!-- EPO <DP n="8"> --></p><p id="p0039" num="0039">This filtering step is done before the signature of the candidate substitute objects and the signature of the object to be removed are computed in order that it be computed with the same point densities.</p><p id="p0040" num="0040">In a final step, called S6, the object to be removed selected at step S3 is replaced into the video scene by the substitute object determined at step S5.</p><p id="p0041" num="0041">If occlusions appear after the replacement step S6, inpainting operations are made in order to fill these occlusions. The inpainting process can efficiently take benefit of 3D information of that scene issued from the preceding steps:</p><p id="p0042" num="0042">The principle of the inventive method is illustrated through an example of application illustrated by <figref idrefs="f0003"><b>Fig.4</b></figref><b>.</b> In this application, it is proposed to replace all the bottles in a scene by water bottles of a client. Different candidate substitute bottles of the client are stored in a library. These candidate substitute bottles correspond to different bottles of different shapes or different sizes of the client.</p><p id="p0043" num="0043">The scene to be processed is shown on left side of <figref idrefs="f0003">Fig.4</figref>. This scene comprises a watch W and a bottle B put on a table T. The replacement of the bottle B by a client bottle according to the present disclosure is now detailed.</p><p id="p0044" num="0044">In the step S1, a 3D representation of the scene to be processed is acquired. In the step S2, this 3D representation is segmented into objects. In the present case, 3 objects are found in the scene: the table T, the watch W and the bottle B. In step S3, the bottle B is selected as the object to be removed or to be replaced. In step S4, a histogram based signature is computed for the bottle B. In step S5, a substitute bottle is determined among the client's bottles B1, B2, B3 and B4 stored in the library. These four bottles are shown in the middle part of <figref idrefs="f0003">Fig.4</figref>. In this step, the signatures, pre-computed or not, of the bottles B1 to B4 are compared to the signature of the bottle B and the bottle B3 is considered as being the more appropriate in terms of 3D features to replace the bottle B. In step S6, the bottle B is replaced by bottle B3 in the scene. The final scene is shown on right side of <figref idrefs="f0003">Fig.4</figref></p><p id="p0045" num="0045">This example illustrates the processing of a video scene at a given time in a film or a video game. Of course, this replacement operation is preferably applied to the scene at different times if needed. The replacement of object points could also be done once, if the scene remains static with respect to the observing viewpoint.<!-- EPO <DP n="9"> --></p><p id="p0046" num="0046"><figref idrefs="f0003"><b>Fig.5</b></figref> schematically illustrates an example of a hardware embodiment of a device 11 arranged for implementing the method of the present disclosure.</p><p id="p0047" num="0047">This hardware embodiment comprise a 3D capturing device 10, such a 3D camera or 2D camera together with a depth device (for example , a laser scanner), for acquiring the 3D representation of the scene and a processing device 11 for implementing the steps S2 to S6 of the inventive method. This processing device 11 is equipped with Input/Output devices 12, for instance a display for displaying user interfaces and intermediary or final results of the inventive processing and a keyboard or a mouse for selection operations through user interfaces.</p><p id="p0048" num="0048">The processing device may comprise
<ul><li>a microprocessor or CPU</li><li>a graphics board including:
<ul><li>a plurality of graphics processors (or GPUs) ;</li><li>a Graphical Random Access Memory (or GRAM);</li></ul></li><li>a Read Only Memory (ROM);</li><li>an interface configured to receive and/or transmit data; and</li><li>a Random Access Memory (RAM).</li></ul></p><p id="p0049" num="0049">The devices 10, 12 and the different components within the device 11 may be interconnected by an address and data bus.</p><p id="p0050" num="0050">The process of the present disclosure is expected to be done after scene capture by the capturing device 10. But, according to a variant, the 3D data of the scene may be provided by / received from a network, like internet, where 3D data are available.</p><p id="p0051" num="0051">Naturally, the present disclosure is not limited to the embodiments previously described.</p><p id="p0052" num="0052">The implementations described herein may be implemented in, for example, a method or a process, an apparatus or a software program. Even if only discussed in the context of a single form of implementation (for example, discussed only as a method or a device), the implementation of features discussed may also be implemented in other forms (for example a program). An apparatus may be implemented in, for example, appropriate hardware, software, and firmware. The methods may be implemented in, for example, an apparatus such as, for example, a processor, which refers to processing devices in general, including, for example, a computer, a<!-- EPO <DP n="10"> --> microprocessor, an integrated circuit, or a programmable logic device. Processors also include communication devices, such as, for example, Smartphones, tablets, computers, mobile phones, portable/personal digital assistants ("PDAs"), and other devices that facilitate communication of information between end-users.</p><p id="p0053" num="0053">Implementations of the various processes and features described herein may be embodied in a variety of different equipment or applications, particularly, for example, equipment or applications associated with video editing.</p><p id="p0054" num="0054">Additionally, the methods may be implemented by instructions being performed by a processor, and such instructions (and/or data values produced by an implementation) may be stored on a processor-readable medium such as, for example, an integrated circuit, a software carrier or other storage device such as, for example, a hard disk, a compact diskette ("CD"), an optical disc (such as, for example, a DVD, often referred to as a digital versatile disc or a digital video disc), a random access memory ("RAM"), or a read-only memory ("ROM"). The instructions may form an application program tangibly embodied on a processor-readable medium. Instructions may be, for example, in hardware, firmware, software, or a combination. Instructions may be found in, for example, an operating system, a separate application, or a combination of the two. A processor may be characterized, therefore, as, for example, both a device configured to carry out a process and a device that includes a processor-readable medium (such as a storage device) having instructions for carrying out a process. Further, a processor-readable medium may store, in addition to or in lieu of instructions, data values produced by an implementation.</p><p id="p0055" num="0055">As will be evident to one of skill in the art, implementations may produce a variety of signals formatted to carry information that may be, for example, stored or transmitted. The information may include, for example, instructions for performing a method, or data produced by one of the described implementations.</p><p id="p0056" num="0056">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made. For example, elements of different implementations may be combined, supplemented, modified, or removed to produce other implementations. Additionally, one of ordinary skill will understand that other structures and processes may be substituted for those disclosed and the resulting implementations will perform<!-- EPO <DP n="11"> --> at least substantially the same function(s), in at least substantially the same way(s), to achieve at least substantially the same result(s) as the implementations disclosed. Accordingly, these and other implementations are contemplated by this application.</p></description><claims mxw-id="PCLM90459432" lang="EN" load-source="patent-office"><!-- EPO <DP n="12"> --><claim id="c-en-0001" num="0001"><claim-text>Method for processing a video of a scene, <b>characterized in that</b> the method comprises:
<claim-text>- receiving (S1) a 3D representation of the scene,</claim-text>
<claim-text>- determining (S2) into said 3D representation at least one candidate object, said at least one candidate object comprising a cloud of 3D points,</claim-text>
<claim-text>- selecting (S3), among said at least one candidate object, one object to be removed,</claim-text>
<claim-text>- computing (S4) a signature of the object to be removed, said signature being representative of 3D features of said selected object,</claim-text>
<claim-text>- determining (S5) a substitute object from a plurality of candidate substitute objects of a library, each candidate substitute object having a signature, said determination being based on comparison of signatures, and</claim-text>
<claim-text>- replacing (S6) the object to be removed by said substitute object into said video of the scene.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>Method according to claim 1, wherein the object to be removed is selected, among said at least one candidate object, by a user.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>Method according to claim 1 or 2, wherein the object to be removed is selected among said at least one candidate object as a function of at least one the following criteria:
<claim-text>- its geographical position in the video of the scene;</claim-text>
<claim-text>- its localisation into the foreground of the video of the scene.</claim-text></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>Method according to any one of claims 1 to 3, wherein, the library comprising a plurality of candidate substitute objects classified into object families, the substitute object is determined among candidate substitute objects belonging to at least one selected object families.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>Method according to claim 4 wherein the selected object families are chosen by a user.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>Method according to any one of claims 1 to 5, wherein the signature of each object comprises 3D features representative of the<!-- EPO <DP n="13"> --> geometry of the object, called geometrical features, and/or 3D features representative of the viewpoint of the object, called viewpoint features.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>Method according to claim 6, wherein the signature of an object is at least one histogram among the group of following histograms:
<claim-text>- Viewpoint Feature Histogram;</claim-text>
<claim-text>- Clustered Viewpoint Feature Histogram;</claim-text>
<claim-text>- Point Feature Histogram;</claim-text>
<claim-text>- Fast Point Feature Histogram.</claim-text></claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>Method according to any one of claims 1 to 7, wherein the substitute object is determined by:
<claim-text>- comparing the signature of the object to be removed and the signatures of the candidate substitute objects of the library, and</claim-text>
<claim-text>- identifying the candidate substitute object of the library having a signature identical to or close to the signature of the object to be removed according to a similarity criterion, the substitute object being the identified candidate substitute object of the library.</claim-text></claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>Method according to any one of claims 1 to 7, wherein the substitute object is determined by:
<claim-text>- comparing the signature of the object to be removed and the signatures of the candidate substitute objects of the library, and</claim-text>
<claim-text>- identifying a plurality of candidate substitute objects of the library having a signature identical to or close to the signature of the object to be removed according to a similarity criterion, the substitute object being a combination of the plurality of the identified candidate substitute objects of the library.</claim-text></claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>Method according to claim 8 or 9, wherein it further comprises a computing the signatures of the candidate substitute objects of the library.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>Method according to any one of claims 1 to 9, wherein the signatures of the candidate substitute objects of the library are pre-computed and stored in the library.<!-- EPO <DP n="14"> --></claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>Device configured for processing a video of a scene, <b>characterized in that</b> it comprises
<claim-text>- an interface configured for receiving a 3D representation of the scene, and</claim-text>
<claim-text>- a processing unit (11) for determining into said 3D representation at least one candidate object, said at least one candidate object comprising a cloud of 3D points, for selecting, among said at least one candidate object, one object to be removed, for computing a signature of the object to be removed, said signature being representative of 3D features of said selected object, for determining a substitute object from a plurality of candidate substitute objects of a library, each candidate substitute object having a signature, said determination being based on comparison of signatures, and for replacing the object to be removed by said substitute object into said video of the scene.</claim-text></claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>Computer program product comprising instructions of program code for execution by at least one processor to perform the method according to at least one of claims 1 to 11, when the program is executed on a computer.</claim-text></claim></claims><drawings mxw-id="PDW20422159" load-source="patent-office"><!-- EPO <DP n="15"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="102" he="227" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="16"> --><figure id="f0002" num="2,3"><img id="if0002" file="imgf0002.tif" wi="165" he="201" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="17"> --><figure id="f0003" num="4,5"><img id="if0003" file="imgf0003.tif" wi="165" he="209" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="159" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="159" he="233" type="tif"/><doc-page id="srep0003" file="srep0003.tif" wi="159" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
