<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2961168-A1" country="EP" doc-number="2961168" kind="A1" date="20151230" family-id="51228384" file-reference-id="313851" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451174" ucid="EP-2961168-A1"><document-id><country>EP</country><doc-number>2961168</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14306029-A" is-representative="YES"><document-id mxw-id="PAPP193865316" load-source="patent-office" format="original"><country>EP</country><doc-number>14306029.1</doc-number><date>20140627</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865317" load-source="docdb" format="epo"><country>EP</country><doc-number>14306029</doc-number><kind>A</kind><date>20140627</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162036553" ucid="EP-14306029-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>14306029</doc-number><kind>A</kind><date>20140627</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988520893" load-source="docdb">H04N  19/51        20140101ALI20141217BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988521392" load-source="docdb">H04N  19/593       20140101ALI20141217BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988521708" load-source="docdb">H04N  19/50        20140101AFI20141217BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988523919" load-source="docdb">H04N  19/186       20140101ALI20141217BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988524234" load-source="docdb">H04N  19/98        20140101ALI20141217BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988524936" load-source="docdb">H04N  19/46        20140101ALI20141217BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1799262171" load-source="docdb" scheme="CPC">H04N  19/98        20141101 FI20170617BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1799262172" load-source="docdb" scheme="CPC">H04N  19/593       20141101 LI20170617BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1799262173" load-source="docdb" scheme="CPC">H04N  19/52        20141101 LI20170615BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1799262174" load-source="docdb" scheme="CPC">H04N  19/50        20130101 LI20170617BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1799262175" load-source="docdb" scheme="CPC">H04N  19/186       20141101 LI20170617BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1799262176" load-source="docdb" scheme="CPC">H04N  19/46        20130101 LI20170617BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1799262177" load-source="docdb" scheme="CPC">H04N  19/176       20130101 LI20170617BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987780327" load-source="docdb" scheme="CPC">H04N  19/51        20130101 LI20141215BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165544988" lang="DE" load-source="patent-office">Verfahren und Vorrichtung zur Vorhersage von Bilddaten zur Codierung oder Decodierung</invention-title><invention-title mxw-id="PT165544989" lang="EN" load-source="patent-office">Method and apparatus for predicting image samples for encoding or decoding</invention-title><invention-title mxw-id="PT165544990" lang="FR" load-source="patent-office">Procédé et appareil permettant de prédire des données d'image pour le codage ou le décodage</invention-title><citations><non-patent-citations><nplcit><text>B. BROSS; W.J. HAN; G. J. SULLIVAN; J.R. OHM; T. WIEGAND: "High Efficiency Video Coding (HEVC) text specification draft 9", JCTVC-K1003, October 2012 (2012-10-01)</text><sources><source mxw-id="PNPL72805833" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>E. RICHARDSON: "H.264 and MPEG-4 video compression", September 2003, J. WILEY &amp; SONS</text><sources><source mxw-id="PNPL72805834" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>GARBAS J ET AL: "Inter-layer prediction for backwards compatible high dynamic range video coding with SVC", 2012 PICTURE CODING SYMPOSIUM (PCS 2012) : KRAKOW, POLAND, 7 - 9 MAY 2012 ; [PROCEEDINGS], IEEE, PISCATAWAY, NJ, 7 May 2012 (2012-05-07), pages 285 - 288, XP032449889, ISBN: 978-1-4577-2047-5, DOI: 10.1109/PCS.2012.6213348</text><sources><source mxw-id="PNPL72805835" load-source="docdb" name="SEA" category="XAI"/></sources></nplcit><nplcit><text>SEGALL A ET AL: "System for bit-depth scalable coding", 23. JVT MEETING; 80. MPEG MEETING; 21-04-2007 - 27-04-2007; SAN JOSÃ CR ,US; (JOINT VIDEO TEAM OF ISO/IEC JTC1/SC29/WG11 AND ITU-T SG.16 ),, no. JVT-W113, 25 April 2007 (2007-04-25), XP030007073, ISSN: 0000-0153</text><sources><source mxw-id="PNPL72805836" load-source="docdb" name="SEA" category="XAI"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103307794" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>THOMSON LICENSING</last-name><address><country>FR</country></address></addressbook></applicant><applicant mxw-id="PPAR1103308093" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>THOMSON LICENSING</last-name></addressbook></applicant><applicant mxw-id="PPAR1101641544" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Thomson Licensing</last-name><iid>101463287</iid><address><street>1-5 Rue Jeanne d'Arc</street><city>92130 Issy-Les-Moulineaux</city><country>FR</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103317627" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>LELEANNEC FABRICE</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103319197" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>LELEANNEC, FABRICE</last-name></addressbook></inventor><inventor mxw-id="PPAR1101642844" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>LELEANNEC, FABRICE</last-name><address><street>Technicolor R&amp;D France 975, avenue des Champs Blancs ZAC des Champs Blancs CS 176 16</street><city>35 576 Cesson Sévigné</city><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103323133" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>LASSERRE SEBASTIEN</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103322708" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>LASSERRE, SEBASTIEN</last-name></addressbook></inventor><inventor mxw-id="PPAR1101651315" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>LASSERRE, SEBASTIEN</last-name><address><street>Technicolor R&amp;D France 975, avenue des Champs Blancs ZAC des Champs Blancs CS 176 16</street><city>35 576 Cesson Sévigné</city><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103329398" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>THOREAU DOMINIQUE</last-name><address><country>FR</country></address></addressbook></inventor><inventor mxw-id="PPAR1103319511" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>THOREAU, DOMINIQUE</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647082" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>THOREAU, DOMINIQUE</last-name><address><street>Technicolor R&amp;D France 975, avenue des Champs Blancs ZAC des Champs Blancs CS 176 16</street><city>35 576 Cesson Sévigné</city><country>FR</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101652661" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Browaeys, Jean-Philippe</last-name><suffix>et al</suffix><iid>101187942</iid><address><street>Technicolor 1, rue Jeanne d'Arc</street><city>92443 Issy-Les-Moulineaux</city><country>FR</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660683244" load-source="docdb">AL</country><country mxw-id="DS660604972" load-source="docdb">AT</country><country mxw-id="DS660681022" load-source="docdb">BE</country><country mxw-id="DS660680956" load-source="docdb">BG</country><country mxw-id="DS660680850" load-source="docdb">CH</country><country mxw-id="DS660681027" load-source="docdb">CY</country><country mxw-id="DS660604973" load-source="docdb">CZ</country><country mxw-id="DS660683246" load-source="docdb">DE</country><country mxw-id="DS660681028" load-source="docdb">DK</country><country mxw-id="DS660681029" load-source="docdb">EE</country><country mxw-id="DS660782009" load-source="docdb">ES</country><country mxw-id="DS660680957" load-source="docdb">FI</country><country mxw-id="DS660680855" load-source="docdb">FR</country><country mxw-id="DS660683287" load-source="docdb">GB</country><country mxw-id="DS660681030" load-source="docdb">GR</country><country mxw-id="DS660683288" load-source="docdb">HR</country><country mxw-id="DS660604974" load-source="docdb">HU</country><country mxw-id="DS660680856" load-source="docdb">IE</country><country mxw-id="DS660681039" load-source="docdb">IS</country><country mxw-id="DS660680958" load-source="docdb">IT</country><country mxw-id="DS660681040" load-source="docdb">LI</country><country mxw-id="DS660680963" load-source="docdb">LT</country><country mxw-id="DS660605149" load-source="docdb">LU</country><country mxw-id="DS660680964" load-source="docdb">LV</country><country mxw-id="DS660680965" load-source="docdb">MC</country><country mxw-id="DS660605150" load-source="docdb">MK</country><country mxw-id="DS660605159" load-source="docdb">MT</country><country mxw-id="DS660680966" load-source="docdb">NL</country><country mxw-id="DS660680857" load-source="docdb">NO</country><country mxw-id="DS660680975" load-source="docdb">PL</country><country mxw-id="DS660782010" load-source="docdb">PT</country><country mxw-id="DS660683290" load-source="docdb">RO</country><country mxw-id="DS660782011" load-source="docdb">RS</country><country mxw-id="DS660680976" load-source="docdb">SE</country><country mxw-id="DS660603651" load-source="docdb">SI</country><country mxw-id="DS660680858" load-source="docdb">SK</country><country mxw-id="DS660680871" load-source="docdb">SM</country><country mxw-id="DS660681041" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479544" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A method of encoding or decoding at least part of a high dynamic range image, the image being defined in a perceptual space of high dynamic range having a luminance component and a color difference metric, the method comprising for a segment of the at least part of the image: converting reference samples for prediction of the segment into the LDR space of the segment to be predicted, the reference samples having been previously reconstructed in their associated LDR space, and predicting the segment using the converted reference samples, for encoding or decoding of the segment using an encoding or decoding technique applicable to an LDR image.
<img id="iaf01" file="imgaf001.tif" wi="165" he="75" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759356" lang="EN" source="EPO" load-source="docdb"><p>A method of encoding or decoding at least part of a high dynamic range image, the image being defined in a perceptual space of high dynamic range having a luminance component and a color difference metric, the method comprising for a segment of the at least part of the image: converting reference samples for prediction of the segment into the LDR space of the segment to be predicted, the reference samples having been previously reconstructed in their associated LDR space, and predicting the segment using the converted reference samples, for encoding or decoding of the segment using an encoding or decoding technique applicable to an LDR image.</p></abstract><description mxw-id="PDES98404245" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><u>TECHNICAL FIELD</u></heading><p id="p0001" num="0001">The present invention relates to a method and an apparatus for predicting image data for an encoding or decoding process. Particularly, but not exclusively, the invention relates to prediction of image data for encoding and decoding of video data for High Dynamic Range (HDR) applications.</p><heading id="h0002"><u>BACKGROUND</u></heading><p id="p0002" num="0002">The variation of light in a scene captured by an imaging device can vary greatly. For example, objects located in a shadow of the scene can appear very dark compared to an object illuminated by direct sunlight. The limited dynamic range and colour gamut provided by traditional low dynamic range (LDR) images do not provide a sufficient range for accurate reproduction of the changes in luminance and colour within such scenes. Typically the values of components of LDR images representing the luminance or colour of pixels of the image are represented by a limited number of bits (typically 8, 10 or 12 bits). The limited range of luminance provided by such representation does not enable small signal variations to be effectively reproduced, in particular in bright and dark ranges of luminance.</p><p id="p0003" num="0003">High dynamic range imaging (also referred to as HDR or HDRI) enables a greater dynamic range of luminance between light and dark areas of a scene compared to traditional LDR images. This is achieved in HDR imaging by extending the signal representation to a wider dynamic range in order to provide high signal accuracy across the entire range. In HDR images, component values of pixels are usually represented with a greater number of bits (for example from 16 bits to 64 bits) including in floating-point format (for example 32-bit or 16-bit for each component, namely float or half-float), the most popular format being openEXR half-float format (16-bit per RGB component, i.e. 48 bits per pixel) or in integers with a long representation, typically at least 16 bits. Such ranges correspond to the natural sensitivity of the human visual system. In this way HDR images more accurately<!-- EPO <DP n="2"> --> represent the wide range of luminance found in real scenes thereby providing more realistic representations of the scene.</p><p id="p0004" num="0004">Because of the greater range of values provided, however, HDR images consume large amounts of storage space and bandwidth, making storage and transmission of HDR images and videos problematic. Efficient coding techniques are therefore required in order to compress the data into smaller, more manageable data sizes. Finding suitable coding/decoding techniques to effectively compress HDR data while preserving the dynamic range of luminance for accurate rendering has proved challenging.</p><p id="p0005" num="0005">A typical approach for encoding an HDR image is to reduce the dynamic range of the image in order to encode the image by means of a traditional encoding scheme used to encode LDR images.</p><p id="p0006" num="0006">For example in one such technique, a tone-mapping operator is applied to the input HDR image and the tone-mapped image is then encoded by means of a conventional 8-10 bit depth encoding scheme such as JPEG/JPEG200 or MPEG-2, H.264/AVC for video (Karsten Suhring, H.264/AVC Reference Software, http://iphome.hhi.de/suehring/tml/download/, the book of<nplcit id="ncit0001" npl-type="b"><text> I. E. Richardson titled « H.264 and MPEG-4 video compression » published in J. Wiley &amp; Sons in September 2003</text></nplcit>). An inverse tone-mapping operator is then applied to the decoded image and a residual is calculated between the input image and the decoded and inverse-tone-mapped image. Finally, the residual is encoded by means of a second traditional 8-10 bit-depth encoder scheme.</p><p id="p0007" num="0007">The main drawbacks of this first approach are the use of two encoding schemes and the limitation of the dynamic range of the input image to twice the dynamic range of a traditional encoding scheme (16-20 bits). According to another approach, an input HDR image is converted in order to obtain a visually lossless representation of the image pixels in a colour space in which values belong to a dynamic range which is compatible with a traditional 8-10 or an extended 12, 14 or 16 bits depth encoding scheme such as HEVC for example (<nplcit id="ncit0002" npl-type="s"><text>B. Bross, W.J. Han, G. J. Sullivan, J.R. Ohm, T. Wiegand JCTVC-K1003, "High Efficiency Video Coding (HEVC) text specification draft 9," Oct 2012</text></nplcit>) and its high bit-depth extensions. Even if traditional codecs can operate high pixel (bit) depths it is generally difficult to encode<!-- EPO <DP n="3"> --> at such bit depths in a uniform manner throughout the image because the ratio of compression obtained is too low for transmission applications.</p><p id="p0008" num="0008">Other approaches using coding techniques applicable to LDR images result in artifacts in the decoded image. The present invention has been devised with the foregoing in mind.</p><heading id="h0003"><u>SUMMARY</u></heading><p id="p0009" num="0009">According to a first aspect of the invention there is provided a method of encoding or decoding at least part of a high dynamic range image, the image being defined in a color space of high dynamic range, the method comprising for a segment of the at least part of the image: converting reference samples for prediction of the segment into a local LDR space of the segment to be predicted, the reference samples having been previously reconstructed in their associated local LDR space, and predicting the segment using the converted reference samples, for encoding or decoding of the segment using an encoding or decoding technique applicable to an LDR image.</p><p id="p0010" num="0010">In an embodiment the conversion of the reference samples comprises:
<ul><li>scaling of the reference samples from their respective reconstructed LDR space to a local perceptual space of high dynamic range;</li><li>reconstruction of the scaled reference samples in the original HDR space of the image;</li><li>mapping of the reconstructed scaled reference samples to the local perceptual space of the segment to be predicted; and</li><li>reduction of the dynamic range of the converted reference samples to the LDR space used for encoding/decoding of the segment to be predicted.</li></ul></p><p id="p0011" num="0011">In an embodiment, the reconstruction of the scaled samples in the original HDR space is dependent upon common representative luminance components respectively associated with the reference samples.</p><p id="p0012" num="0012">In an embodiment, the mapping of the reconstructed scaled samples to the local HDR space of the segment to be predicted is dependent upon a common representative luminance component value associated with the segment to be predicted.<!-- EPO <DP n="4"> --></p><p id="p0013" num="0013">In an embodiment, a said common representative luminance component value for a reference sample is obtained based on the luminance values of the image samples of an image segment to which the reference sample belongs.</p><p id="p0014" num="0014">In an embodiment, the common representative luminance component value for the segment to be predicted is obtained based on the luminance values of the image samples of the said segment.</p><p id="p0015" num="0015">In an embodiment the method includes encoding the segment of the at least part of the image using an encoding process applicable to a low dynamic range (LDR) image by applying a coding parameter set including at least one coding parameter; reconstructing the encoded segment in the space of high dynamic range; evaluating a rate distortion cost for the encoded segment in the space of high dynamic range; and adjusting said coding parameter set for the encoding process of the segment based on the evaluated rate distortion cost.</p><p id="p0016" num="0016">In an embodiment, evaluating the rate distortion cost comprises evaluating the rate associated with encoding of the common representative component value of the segment to be encoded.</p><p id="p0017" num="0017">In an embodiment, the image segment to be predicted is represented in a local perceptual space of high dynamic range based on the corresponding common representative luminance component value prior to encoding of the segment.</p><p id="p0018" num="0018">In an embodiment, the method includes obtaining for the segment to be predicted at least one local residual luminance component in a local space of low dynamic range, said at least one local residual luminance component corresponding to the differential between the corresponding luminance component of the original image and the common representative luminance value of the segment.</p><p id="p0019" num="0019">In an embodiment, the method includes obtaining for the segment at least one image portion in the local perceptual space, said at least one image portion corresponding to the local residual luminance component or the color component of the image portion, normalized according to the at least one common representative luminance value of the segment.</p><p id="p0020" num="0020">In an embodiment, evaluating the rate distortion cost comprises evaluating the rate associated with encoding of the said at least one image portion.<!-- EPO <DP n="5"> --></p><p id="p0021" num="0021">In an embodiment, evaluating the rate distortion cost comprises evaluating the distortion associated with reconstruction of the encoded segment in the perceptual space of high dynamic range.</p><p id="p0022" num="0022">In an embodiment, the method includes performing virtual lossless refinement between samples of the residual image portion reconstructed in the local perceptual space and samples of the original texture and the corresponding samples of the said image.</p><p id="p0023" num="0023">A second aspect of the invention provides an encoding device for encoding at least part of an image of high dynamic range defined in a perceptual space having a luminance component and a color difference metric, the device comprising: a reference sample converter for converting reference samples for prediction of the segment into the LDR space of an image segment to be predicted, the reference samples having been previously reconstructed in their associated LDR space, and a prediction module for predicting the segment using the converted reference samples, an encoder for encoding the segment using an encoding technique applicable to an LDR image.</p><p id="p0024" num="0024">The encoding device according to the second aspect of the invention may be configured to perform the method of any one of claims 2 to 13.</p><p id="p0025" num="0025">A third aspect of the invention provides a decoding device for decoding at least part of an image of high dynamic range defined in a perceptual space having a luminance component and a color difference metric, the device comprising:
<ul><li>a decoder for decoding an image segment using an decoding technique applicable to an LDR image.</li><li>a reference sample converter for converting reference samples for prediction of the segment into the LDR space of the segment to be predicted, the reference samples having been previously reconstructed in their associated LDR space, and</li><li>a prediction module for predicting the segment using the converted reference samples.</li></ul></p><p id="p0026" num="0026">In the third or fourth aspect of the invention, the reference sample converter is configured to<br/>
scale the reference samples from their respective reconstructed LDR space to a local perceptual space of high dynamic range;<br/>
<!-- EPO <DP n="6"> -->reconstruct the scaled reference samples in the original HDR space of the image;<br/>
map the reconstructed scaled reference samples to the local perceptual space of the segment to be predicted; and<br/>
perform reduction of the dynamic range of the converted reference samples to the LDR space used for encoding/decoding of the segment to be predicted.</p><p id="p0027" num="0027">Embodiments of the invention provide encoding and decoding methods for high dynamic range image data for a wide range of applications providing improved visual experience.</p><p id="p0028" num="0028">At least parts of the methods according to the invention may be computer implemented. Accordingly, the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a "circuit", "module" or "system'. Furthermore, the present invention may take the form of a computer program product embodied in any tangible medium of expression having computer usable program code embodied in the medium.</p><p id="p0029" num="0029">Since the present invention can be implemented in software, the present invention can be embodied as computer readable code for provision to a programmable apparatus on any suitable carrier medium. A tangible carrier medium may comprise a storage medium such as a floppy disk, a CD-ROM, a hard disk drive, a magnetic tape device or a solid state memory device and the like. A transient carrier medium may include a signal such as an electrical signal, an electronic signal, an optical signal, an acoustic signal, a magnetic signal or an electromagnetic signal, e.g. a microwave or RE signal.</p><heading id="h0004"><u>BRIEF DESCRIPTION OF THE DRAWINGS</u></heading><p id="p0030" num="0030">Embodiments of the invention will now be described, by way of example only, and with reference to the following drawings in which:
<ul><li><figref idrefs="f0001">FIG. 1</figref> is a block diagram of an encoding process according to a first embodiment of the invention;<!-- EPO <DP n="7"> --></li><li><figref idrefs="f0002">FIG. 2</figref> is a schematic diagram illustrating an example of decomposition of a coding unit into prediction units and transform units according to the HEVC video compression standard;</li><li><figref idrefs="f0003">FIG. 3</figref> is a schematic diagram illustrating examples of intra prediction methods;</li><li><figref idrefs="f0004">FIG. 4</figref> is a flow chart of a reference sample conversion process according to an embodiment of the invention;</li><li><figref idrefs="f0005">FIG. 5</figref> is a block diagram illustrating an example of intra prediction in accordance with an embodiment of the invention;</li><li><figref idrefs="f0005">FIG. 6</figref> is a block diagram illustrating an example of inter prediction in accordance with an embodiment of the invention;</li><li><figref idrefs="f0006">FIG. 7</figref> is a block diagram of an encoding process according to a further embodiment of the invention;</li><li><figref idrefs="f0007">FIG. 8</figref> is a block diagram of an encoding process according to a further embodiment of the invention;</li><li><figref idrefs="f0008">FIG. 9</figref> is a block diagram of a decoding process in accordance with one or more embodiments of the invention;</li><li><figref idrefs="f0009">FIG. 10</figref> is a block diagram of a decoding process in accordance with one or more embodiments of the invention;</li><li><figref idrefs="f0010">FIG. 11</figref> is a block diagram of an encoding or decoding device in which one or more embodiments of the invention can be implemented;</li><li><figref idrefs="f0010">FIG. 12</figref> is a block diagram of an example of a data communication system in which one or more embodiments of the invention can be implemented;</li><li><figref idrefs="f0011">FIG. 13</figref> is a block diagram of an example of a data transmitter system in which one or more embodiments of the invention can be implemented; and</li><li><figref idrefs="f0011">FIG. 14</figref> is a block diagram of an example of a data receiver system in which one or more embodiments of the invention can be implemented.</li></ul></p><heading id="h0005"><u>DETAILED DESCRIPTION</u></heading><p id="p0031" num="0031"><figref idrefs="f0001">Figure 1</figref> is a schematic block diagram illustrating steps of a method for encoding at least part of an image I in accordance with a first embodiment of the invention. Encoding steps of the method of <figref idrefs="f0001">Figure 1</figref> are generally based on the HEVC compression standard applicable to LDR type images but it will be appreciated that<!-- EPO <DP n="8"> --> embodiments of the invention may be applied to other encoding standards applicable to LDR type images such as, for example H.264/AVC, MPEG2 or MPEG4.</p><p id="p0032" num="0032">The method begins with the acquisition of HDR image data. The HDR image data may be representative of a video sequence of images, an image or part of an image. For the purposes of simplifying the description which follows, the acquired image data corresponds to an HDR image. The HDR image data may be acquired directly from an imaging device such as a video camera, acquired from a memory device located locally or remotely on which it is stored, or received via a wireless or wired transmission line.</p><p id="p0033" num="0033">As used herein the term "HDR image" refers to any HDR image that comprises high dynamic range data in floating point (float or half float), fixed point or long representation integer format typically represented in by a number of bits greater than 16. The input HDR image may be defined in any colour or perceptual space. For example, in the present embodiment the input HDR image is defined in an RGB colour space. In another embodiment the input HDR image may be defined in another colour space such as YUV or any perceptual space. The input HDR image may also be defined in a linear or non-linear representation, for example the logarithmic representation issued from a gamma correction/transformation.</p><p id="p0034" num="0034">Generally, the encoding steps of the process are performed on an image including data representative of the luminance of pixels of the image. Such image data includes a luminance component L and potentially at least one colour or chroma component <i>C</i>(<i>i</i>) where i is an index identifying a colour or chroma component of the image. The components of the image define a colour space, usually a 3D space, for example the image may be defined in a colour perceptual space comprising a luminance component L and potentially two colour components C1 and C2.</p><p id="p0035" num="0035">It will be appreciated, however, that the invention is not restricted to a HDR image having colour components. For example, the HDR image may be a grey image in a perceptual space having a luminance component without any colour component.</p><p id="p0036" num="0036">A perceptual space is defined as a colour space defined by a plurality of components including a luminance component and has a colour difference metric <i>d((L, C</i>1, <i>C</i>2<i>),</i> (<i>L', C</i>1'<i>, C</i>2<i>'</i>)) whose values are representative of, preferably proportional<!-- EPO <DP n="9"> --> to, the respective differences between the visual perceptions of two points of said perceptual space.</p><p id="p0037" num="0037">Mathematically speaking, the colour difference metric <i>d((L, C</i>1<i>, C</i>2<i>),</i> (<i>L', C</i>1<i>', C</i>2<i>'</i>)) is defined such that a perceptual threshold Δ<i>E</i><sub>0</sub> (also referred to as JND, Just Noticeable Difference) exists, below which a human eye is unable to perceive a visual difference between two colours of the perceptual space, i.e. <maths id="math0001" num="(1)"><math display="block"><mi>d</mi><mfenced><mfenced separators=""><mi>L</mi><mo>,</mo><mi>C</mi><mo>⁢</mo><mn>1</mn><mo>,</mo><mi>C</mi><mo>⁢</mo><mn>2</mn></mfenced><mfenced separators=""><mi mathvariant="italic">Lʹ</mi><mo>,</mo><mi>C</mi><mo>⁢</mo><mn>1</mn><mo>⁢</mo><mi mathvariant="italic">ʹ</mi><mo>,</mo><mi>C</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><mi mathvariant="italic">ʹ</mi></mfenced></mfenced><mo>&lt;</mo><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mo>,</mo></math><img id="ib0001" file="imgb0001.tif" wi="90" he="13" img-content="math" img-format="tif"/></maths></p><p id="p0038" num="0038">The perceptual threshold Δ<i>E</i><sub>0</sub> is independent of the two points <i>(L, C</i>1<i>, C2)</i> and (<i>L', C</i>1<i>', C</i>2') of the perceptual space. Thus, encoding an image whose components belong to a perceptual space such that the metric of equation (1) remains below the bound Δ<i>E</i><sub>0</sub> ensures that displayed decoded version of the image is visually lossless.</p><p id="p0039" num="0039">When the acquired image I comprises components belonging to a non-perceptual space such as for example (R,G,B), a perceptual transform is applied to the image data I in step S101 by an image conversion module IC in order to obtain a HDR image I<sub>p</sub> having a luminance component L and potentially at least one colour component defining a perceptual space, for example a luminance component L and two colour components C1 and C2.</p><p id="p0040" num="0040">The perceptual transform performed depends on the lighting conditions of the display and on the initial colour space. For example, assuming the initial colour space is a (R,G,B) colour space, the image I is first transformed into the well-known linear space (X, Y, Z). This step includes performing linearization of the data, where appropriate, by applying an inverse gamma correction and then transforming the linear RGB space data into the XYZ space with a 3x3 transform matrix. For this step, data characterizing the visual environment of the image is used. For example a 3D vector of values (<i>X<sub>n</sub></i>,<i>Y<sub>n</sub></i>,<i>Z<sub>n</sub></i>) defining reference lighting conditions of the display in the (X,Y,Z) space may be used.</p><p id="p0041" num="0041">As an example, a perceptual transform is defined as follows in the case where the perceptual space LabCIE1976 is selected:<!-- EPO <DP n="10"> --> <maths id="math0002" num=""><math display="block"><msup><mi>L</mi><mo>*</mo></msup><mo>=</mo><mn>116</mn><mo>⁢</mo><mi>f</mi><mfenced separators=""><mi>Y</mi><mo>/</mo><msub><mi>Y</mi><mi>n</mi></msub></mfenced><mo>-</mo><mn>16</mn></math><img id="ib0002" file="imgb0002.tif" wi="59" he="10" img-content="math" img-format="tif"/></maths> <maths id="math0003" num=""><math display="block"><msup><mi>a</mi><mo>*</mo></msup><mo>=</mo><mn>500</mn><mo>⁢</mo><mfenced separators=""><mi>f</mi><mfenced separators=""><mi>X</mi><mo>/</mo><msub><mi>X</mi><mi>n</mi></msub></mfenced><mo>-</mo><mi>f</mi><mfenced separators=""><mi>Y</mi><mo>/</mo><msub><mi>Y</mi><mi>n</mi></msub></mfenced></mfenced></math><img id="ib0003" file="imgb0003.tif" wi="59" he="7" img-content="math" img-format="tif"/></maths> <maths id="math0004" num=""><math display="block"><msup><mi>b</mi><mo>*</mo></msup><mo>=</mo><mn>200</mn><mo>⁢</mo><mfenced separators=""><mi>f</mi><mfenced separators=""><mi>Y</mi><mo>/</mo><msub><mi>Y</mi><mi>n</mi></msub></mfenced><mo>-</mo><mi>f</mi><mfenced separators=""><mi>Z</mi><mo>/</mo><msub><mi>Z</mi><mi>n</mi></msub></mfenced></mfenced></math><img id="ib0004" file="imgb0004.tif" wi="59" he="10" img-content="math" img-format="tif"/></maths><br/>
where f is a gamma correction function for example given by: <maths id="math0005" num=""><math display="block"><mi>f</mi><mfenced><mi>r</mi></mfenced><mo>=</mo><msup><mi>r</mi><mrow><mn>1</mn><mo>/</mo><mn>3</mn></mrow></msup><mspace width="2em"/><mi mathvariant="italic">if r</mi><mo>&gt;</mo><msup><mfenced separators=""><mn>6</mn><mo>/</mo><mn>29</mn></mfenced><mn>3</mn></msup></math><img id="ib0005" file="imgb0005.tif" wi="55" he="9" img-content="math" img-format="tif"/></maths> <maths id="math0006" num=""><math display="block"><mi>f</mi><mfenced><mi>r</mi></mfenced><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>*</mo><msup><mfenced><mfrac><mn>29</mn><mn>6</mn></mfrac></mfenced><mn>2</mn></msup><mo>*</mo><mi>r</mi><mo>+</mo><mfrac><mn>4</mn><mn>29</mn></mfrac></math><img id="ib0006" file="imgb0006.tif" wi="52" he="16" img-content="math" img-format="tif"/></maths><br/>
otherwise</p><p id="p0042" num="0042">Two colours are humanly distinguishable from one another in the reference lighting conditions (<i>X<sub>n</sub></i>,<i>Y<sub>n</sub></i>,<i>Z<sub>n</sub></i>) when the following colour difference metric defined on the perceptual space LabCIE1976 is satisfied: <maths id="math0007" num=""><math display="block"><mi>d</mi><mo>⁢</mo><msup><mfenced><mfenced><msup><mi>L</mi><mo>*</mo></msup><msup><mi>a</mi><mo>*</mo></msup><msup><mi>b</mi><mo>*</mo></msup></mfenced><mfenced><msup><mi>L</mi><mrow><mo>*</mo><mi>ʹ</mi></mrow></msup><msup><mi>a</mi><mrow><mo>*</mo><mi>ʹ</mi></mrow></msup><msup><mi>b</mi><mrow><mo>*</mo><mi>ʹ</mi></mrow></msup></mfenced></mfenced><mn>2</mn></msup><mo>=</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msup><mi>L</mi><mo>*</mo></msup></mfenced><mn>2</mn></msup><mo>+</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msup><mi>a</mi><mo>*</mo></msup></mfenced><mn>2</mn></msup><mo>+</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msup><mi>b</mi><mo>*</mo></msup></mfenced><mn>2</mn></msup><mo>&lt;</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub></mfenced><mn>2</mn></msup></math><img id="ib0007" file="imgb0007.tif" wi="120" he="12" img-content="math" img-format="tif"/></maths><br/>
with Δ<i>L</i>* being the difference between the luminance components of the two colours (<i>L*, a*, b*</i>) and (<i>L*', a*', b*'</i>) and <i>Δa*</i> (respectively <i>Δb*</i>) being the difference between the colour components of the two colours. Typically Δ<i>E</i><sub>0</sub> has a value of between 1 and 2.</p><p id="p0043" num="0043">The image in the space (X,Y,Z) may, in some cases, be inverse transformed to obtain the estimate of the decoded image in the initial space such as, in the present example, (R,G,B) space. The corresponding inverse perceptual transform is given by: <maths id="math0008" num=""><math display="block"><mi>X</mi><mo>=</mo><msub><mi>X</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mfrac><mn>1</mn><mn>116</mn></mfrac><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced><mo>+</mo><mfrac><mn>1</mn><mn>500</mn></mfrac><mo>⁢</mo><msup><mi>a</mi><mo>*</mo></msup></mfenced></math><img id="ib0008" file="imgb0008.tif" wi="71" he="13" img-content="math" img-format="tif"/></maths> <maths id="math0009" num=""><math display="block"><mi>Y</mi><mo>=</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mn>1</mn><mo>/</mo><mn>116</mn><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced></mfenced></math><img id="ib0009" file="imgb0009.tif" wi="52" he="7" img-content="math" img-format="tif"/></maths> <maths id="math0010" num=""><math display="block"><mi>Z</mi><mo>=</mo><msub><mi>Z</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mfrac><mn>1</mn><mn>116</mn></mfrac><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced><mo>+</mo><mfrac><mn>1</mn><mn>200</mn></mfrac><mo>⁢</mo><msup><mi>b</mi><mo>*</mo></msup></mfenced></math><img id="ib0010" file="imgb0010.tif" wi="71" he="14" img-content="math" img-format="tif"/></maths></p><p id="p0044" num="0044">According to another example, when the perceptual space <i>Lu*v*</i> is selected, a perceptual transform may be defined as follows: <maths id="math0011" num=""><math display="block"><msup><mi>u</mi><mo>*</mo></msup><mo>=</mo><mn>13</mn><mo>⁢</mo><mi>L</mi><mo>⁢</mo><mfenced separators=""><msup><mi>u</mi><mi>ʹ</mi></msup><mo>-</mo><msub><msup><mi>u</mi><mi>ʹ</mi></msup><mi mathvariant="italic">white</mi></msub></mfenced><mspace width="1em"/><mi>and</mi><mspace width="1em"/><msup><mi>v</mi><mo>*</mo></msup><mo>=</mo><mn>13</mn><mo>⁢</mo><mi>L</mi><mo>⁢</mo><mfenced separators=""><msup><mi>v</mi><mi>ʹ</mi></msup><mo>-</mo><msub><msup><mi>v</mi><mi>ʹ</mi></msup><mi mathvariant="italic">white</mi></msub></mfenced></math><img id="ib0011" file="imgb0011.tif" wi="92" he="11" img-content="math" img-format="tif"/></maths><br/>
where the following are defined: <maths id="math0012" num=""><math display="block"><msup><mi>u</mi><mi>ʹ</mi></msup><mo>=</mo><mfrac><mrow><mn>4</mn><mo>⁢</mo><mi>X</mi></mrow><mrow><mi>X</mi><mo>+</mo><mn>15</mn><mo>⁢</mo><mi>Y</mi><mo>+</mo><mn>3</mn><mo>⁢</mo><mi>Z</mi></mrow></mfrac><mo>,</mo><msup><mi>v</mi><mi>ʹ</mi></msup><mo>=</mo><mfrac><mrow><mn>9</mn><mo>⁢</mo><mi>Y</mi></mrow><mrow><mi>X</mi><mo>+</mo><mn>15</mn><mo>⁢</mo><mi>Y</mi><mo>+</mo><mn>3</mn><mo>⁢</mo><mi>Z</mi></mrow></mfrac><mo>,</mo></math><img id="ib0012" file="imgb0012.tif" wi="59" he="15" img-content="math" img-format="tif"/></maths><br/>
<!-- EPO <DP n="11"> -->and <maths id="math0013" num=""><math display="block"><msub><msup><mi>u</mi><mi>ʹ</mi></msup><mi mathvariant="italic">white</mi></msub><mo>=</mo><mfrac><mrow><mn>4</mn><mo>⁢</mo><msub><mi>X</mi><mi>n</mi></msub></mrow><mrow><msub><mi>X</mi><mi>n</mi></msub><mo>+</mo><mn>15</mn><mo>⁢</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>+</mo><mn>3</mn><mo>⁢</mo><msub><mi>Z</mi><mi>n</mi></msub></mrow></mfrac><mo>,</mo><msub><msup><mi>v</mi><mi>ʹ</mi></msup><mi mathvariant="italic">white</mi></msub><mo>=</mo><mfrac><mrow><mn>9</mn><mo>⁢</mo><msub><mi>Y</mi><mi>n</mi></msub></mrow><mrow><msub><mi>X</mi><mi>n</mi></msub><mo>+</mo><mn>15</mn><mo>⁢</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>+</mo><mn>3</mn><mo>⁢</mo><msub><mi>Z</mi><mi>n</mi></msub></mrow></mfrac><mn>.</mn></math><img id="ib0013" file="imgb0013.tif" wi="81" he="12" img-content="math" img-format="tif"/></maths></p><p id="p0045" num="0045">The following Euclidean metric may be defined on the perceptual space <i>Lu*v*</i>: <maths id="math0014" num=""><math display="block"><mi>d</mi><mo>⁢</mo><msup><mfenced><mfenced><mi>L</mi><msup><mi>u</mi><mo>*</mo></msup><msup><mi>v</mi><mo>*</mo></msup></mfenced><mfenced><msup><mi>L</mi><mi>ʹ</mi></msup><msup><mi>u</mi><mrow><mo>*</mo><mi>ʹ</mi></mrow></msup><msup><mi>v</mi><mrow><mo>*</mo><mi>ʹ</mi></mrow></msup></mfenced></mfenced><mn>2</mn></msup><mo>=</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>L</mi></mfenced><mn>2</mn></msup><mo>+</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msup><mi>u</mi><mo>*</mo></msup></mfenced><mn>2</mn></msup><mo>+</mo><msup><mfenced separators=""><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msup><mi>v</mi><mo>*</mo></msup></mfenced><mn>2</mn></msup></math><img id="ib0014" file="imgb0014.tif" wi="101" he="15" img-content="math" img-format="tif"/></maths><br/>
with Δ<i>L*</i> being the difference between the luminance components of the two colours (<i>L , u*</i>, <i>v*</i>) and (<i>L'</i>,<i>u</i>*<i>'</i>,<i>v</i>*<i>'</i>)<i>,</i> and <i>Δu*</i> (respectively Δ<i>v</i>*) being the difference between the colour components of these two colours.</p><p id="p0046" num="0046">The corresponding inverse perceptual transform for the Luv space is given by: <maths id="math0015" num=""><math display="block"><mi>X</mi><mo>=</mo><mfrac><mrow><mn>9</mn><mo>⁢</mo><mi mathvariant="italic">Yuʹ</mi></mrow><mrow><mn>4</mn><mo>⁢</mo><mi mathvariant="italic">vʹ</mi></mrow></mfrac></math><img id="ib0015" file="imgb0015.tif" wi="22" he="13" img-content="math" img-format="tif"/></maths> <maths id="math0016" num=""><math display="block"><mi>Y</mi><mo>=</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mfrac><mn>1</mn><mn>116</mn></mfrac><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced></mfenced></math><img id="ib0016" file="imgb0016.tif" wi="54" he="12" img-content="math" img-format="tif"/></maths> <maths id="math0017" num=""><math display="block"><mi>Z</mi><mo>=</mo><mfrac><mrow><mn>3</mn><mo>⁢</mo><mi>Y</mi><mo>⁢</mo><mfenced separators=""><mn>4</mn><mo>-</mo><mi mathvariant="italic">uʹ</mi></mfenced></mrow><mrow><mn>4</mn><mo>⁢</mo><mi mathvariant="italic">vʹ</mi></mrow></mfrac><mo>-</mo><mn>5</mn><mo>⁢</mo><mi>Y</mi></math><img id="ib0017" file="imgb0017.tif" wi="41" he="15" img-content="math" img-format="tif"/></maths></p><p id="p0047" num="0047">It will be appreciated that the present invention is not limited to the perceptual space LabCIE1976 but may be extended to any type of perceptual space such as the LabCIE1994, LabCIE2000, which are the same Lab space but with a different metric to measure the perceptual distance, or to any other Euclidean perceptual space for instance.</p><p id="p0048" num="0048">Other examples are LMS spaces and IPT spaces. In such perceptual spaces the metric is defined such that it is preferably proportional to the perception difference; as a consequence, a homogeneous maximal perceptual threshold Δ<i>E</i><sub>0</sub> exists below which a human being is not able to perceive a visual difference between two colours of the perceptual space.</p><p id="p0049" num="0049">In step S102 the image is spatially decomposed into a series of spatial units, by a partitioning module PART1. An example of spatial coding structures in accordance with a HEVC video compression technique in encoding of images is illustrated in <figref idrefs="f0002">Figure 2</figref>. In the case of a HEVC type encoder the largest spatial unit is<!-- EPO <DP n="12"> --> referred to as a coding tree unit (CTU). Each spatial unit is decomposed into further elements according to a decomposition configuration, indicated by coding parameters, often referred to as a quad-tree. Each leaf of the quad-tree is called a coding unit (CU), and is further partitioned into one or more sub-elements referred to as prediction units (PU), the samples of which share common prediction parameters, and into transform units (TU) which define the processing segment size.</p><p id="p0050" num="0050">In step S102 of the example of <figref idrefs="f0001">Figure 1</figref> a coding unit is partitioned into one or more sub-elements or blocks BI which in the present example correspond to Prediction units (PU) for prediction based encoding in accordance with embodiments of the invention.</p><p id="p0051" num="0051"><figref idrefs="f0003">Figure 3</figref> illustrates examples of spatial prediction methods applied in the HEVC standard. In prediction based encoding or decoding methods a spatial block to be reconstructed is predicted from a reference sample typically referred to as a predictor. The predictor may be located in the same frame as the block to be predicted as in the case of Intra prediction or in a different frame of a sequence of frames, in the case of Inter prediction. In the case of intra prediction the predictors are indicated by a prediction mode, which can corresponds to a directional mode or to a non-directional mode. In the case of inter prediction the predictors can be indicated by prediction type (uni- or bi-prediction), frame indices and motion vectors. Prediction encoding or decoding produces better results if the reconstructed samples used in predicting the block correlate well with the samples of the block to be predicted.</p><p id="p0052" num="0052">While in the present example the output block BI of step S102 is a PU, it will be appreciated that in other embodiments of the invention in which a HEVC type technique is applied the output of step S102 may be a CU or a TU. In other embodiments the block BI will refer to a suitable spatial region of the image being encoded.</p><p id="p0053" num="0053">In the present example each Prediction Unit or block BI corresponds to a square or rectangular spatial region of the image associated with respective prediction (Intra or Inter) parameters:
<ul><li>Encoding parameters for encoding of the blocks may include one or more of the following coding parameters:<!-- EPO <DP n="13"> -->
<ul><li>the coding tree unit organization in terms of coding quad-tree, prediction units and transform units.</li><li>the coding mode (INTRA or INTER) assigned to coding units of the coding tree, where INTER indicates inter-picture (temporal) prediction and INTRA indicates intra-picture (spatial) prediction</li><li>the intra prediction mode (DC, planar or angular direction) for each Intra coding unit in the considered coding tree.</li><li>the inter prediction mode (uni-prediction, bi-prediction, skip) for each Inter coding unit in the considered coding tree.</li><li>the inter prediction parameters in case of inter coding units: motion vectors, reference picture indices, etc.</li></ul></li></ul></p><p id="p0054" num="0054">In step S103 each prediction unit or block is attributed a luminance component value, referred to as a low spatial frequency luminance component <i>L<sub>lf</sub></i> representative of the mean of the luminance values of the samples (a sample may comprise one or more pixels) making up that prediction unit or block. This is performed by a luminance processing module LF. Calculating a low spatial frequency luminance component for a block basically involves down-sampling the luminance components of the original image. It will be appreciated that the invention is not limited to any specific embodiment for obtaining a low-spatial-frequency version for each prediction unit or block and that any low-pass filtering or down-sampling of the luminance component of the image I<sub>p</sub> may be used. In step S104 the low-spatial frequency luminance component is quantized by a quantization unit Q to provide a quantized low-spatial frequency luminance component <i>L̂<sub>lf</sub> = Q</i>(<i>L<sub>lf</sub></i>)<i>.</i> Entropy coding is performed by an entropy encoder ENC1 in step S130 on the quantized low-spatial frequency luminance component <i>L̂<sub>lf</sub></i> for the output video bitstream. Encoding of the low spatial frequency luminance component may be referred to herein as a first layer of coding or luminance layer.</p><p id="p0055" num="0055">Based on the respective value of the quantized low-spatial frequency luminance component <i>L̂<sub>lf</sub></i>, the values of the luminance and colour components of the prediction unit or block are transformed in step S105 by a local perceptual transform unit LPT into a local perceptual space corresponding to the perceptual space transformation of step S101. This perceptual space in the present example is the perceptual space L*a*b*. The quantized low spatial frequency luminance component<!-- EPO <DP n="14"> --> <i>L̂<sub>lf</sub></i> associated with the block is used as the reference lighting conditions of the display for the transformation. The luminance and colour components of this local perceptual space L*a*b* of the block are noted <maths id="math0018" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>a</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>b</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup></mfenced><mn>.</mn></math><img id="ib0018" file="imgb0018.tif" wi="36" he="7" img-content="math" img-format="tif" inline="yes"/></maths> In practice, the transformation into the local perceptual space depends on the quantized low-spatial frequency luminance component <i>L̂<sub>lf</sub></i> and the maximum error threshold <i>ΔE</i> targeted in the encoding process in the local perceptual space.</p><p id="p0056" num="0056">The transformation into the local perceptual space <maths id="math0019" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>a</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>b</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup></mfenced><mn>.</mn></math><img id="ib0019" file="imgb0019.tif" wi="38" he="7" img-content="math" img-format="tif" inline="yes"/></maths> includes the following steps. The luminance signal is first transformed into a so-called local LDR representation, through the following luminance residual computation: <maths id="math0020" num=""><math display="block"><msub><mi>L</mi><mi>r</mi></msub><mo>=</mo><mi>L</mi><mo>-</mo><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></math><img id="ib0020" file="imgb0020.tif" wi="32" he="12" img-content="math" img-format="tif"/></maths></p><p id="p0057" num="0057">Where <i>L<sub>r</sub></i> represents the computed residual luminance component, L represents the corresponding luminance component in the original image, and <i>L̂<sub>lf</sub></i> represents the quantized low spatial frequency luminance component.</p><p id="p0058" num="0058">This step may be referred to herein as the LDR localization step.</p><p id="p0059" num="0059">Then the residual luminance component <i>L<sub>r</sub></i> is represented in a local perceptual space as follows. Assuming a nominal lighting luminance Y<sub>n</sub>, in the L*a*b* perceptual space mode, a change in lighting conditions by a factor Y<sub>E</sub> transforms the perceptual space components as follows: <maths id="math0021" num=""><math display="block"><mfenced><msub><mi mathvariant="normal">X</mi><mi mathvariant="normal">n</mi></msub><msub><mi mathvariant="normal">Y</mi><mi mathvariant="normal">n</mi></msub><msub><mi mathvariant="normal">Z</mi><mi mathvariant="normal">n</mi></msub></mfenced><mo>→</mo><mfenced separators=""><msub><mi mathvariant="normal">Y</mi><mi mathvariant="normal">E</mi></msub><mo>⁢</mo><msub><mi mathvariant="normal">X</mi><mi mathvariant="normal">n</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Y</mi><mi mathvariant="normal">E</mi></msub><mo>⁢</mo><msub><mi mathvariant="normal">Y</mi><mi mathvariant="normal">n</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Y</mi><mi mathvariant="normal">E</mi></msub><mo>⁢</mo><msub><mi mathvariant="normal">Z</mi><mi mathvariant="normal">n</mi></msub></mfenced></math><img id="ib0021" file="imgb0021.tif" wi="61" he="12" img-content="math" img-format="tif"/></maths></p><p id="p0060" num="0060">Accordingly, this results in a modified perceptual threshold corresponding to: <maths id="math0022" num=""><math display="block"><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi mathvariant="normal">Eʹ</mi><mn mathvariant="normal">0</mn></msub><mo>=</mo><msub><mi mathvariant="normal">ΔE</mi><mn mathvariant="normal">0</mn></msub><mn mathvariant="normal">.</mn><msup><msub><mi mathvariant="normal">Y</mi><mi mathvariant="normal">E</mi></msub><mfenced separators=""><mn mathvariant="normal">1</mn><mo>/</mo><mn mathvariant="normal">3</mn></mfenced></msup></math><img id="ib0022" file="imgb0022.tif" wi="35" he="12" img-content="math" img-format="tif"/></maths></p><p id="p0061" num="0061">Consequently, the perceptual threshold ΔE'<sub>0</sub> is adapted to the coding according to the maximum lighting change multiplicative factor in post-processing. The new threshold ΔE'<sub>0</sub> is derived from the Y<sub>E</sub> factor, where Y<sub>E</sub>=Y<sub>lf</sub>/Y<sub>n</sub>, the relationship between Y<sub>lf</sub> and <i>L̂<sub>lf</sub></i> being given by: <maths id="math0023" num=""><math display="block"><mover><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub><mo>^</mo></mover><mo>=</mo><mn>116</mn><mo>⁢</mo><msup><msub><mi mathvariant="normal">Y</mi><mi>lf</mi></msub><mfenced separators=""><mn>1</mn><mo>/</mo><mn>3</mn></mfenced></msup><mo>-</mo><mn>16.</mn></math><img id="ib0023" file="imgb0023.tif" wi="39" he="12" img-content="math" img-format="tif"/></maths><br/>
In this way the perceptual space is localized since it is based on the low-spatial frequency luminance component <i>L̂<sub>lf</sub></i> associated with the current prediction unit.<!-- EPO <DP n="15"> --></p><p id="p0062" num="0062">The localization of the perceptual space takes the following form in practice, in the embodiment that corresponds to the LabCIE76 perceptual space: <maths id="math0024" num=""><math display="block"><msubsup><mi>L</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><mo>=</mo><mfrac><msub><mi>L</mi><mi>r</mi></msub><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi></mrow></mfrac><mo>=</mo><mfrac><msub><mi>L</mi><mi>r</mi></msub><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mo>⁢</mo><msup><mfenced><msub><mi>Y</mi><mi>E</mi></msub></mfenced><mmultiscripts><msub><mo>/</mo><mn>3</mn></msub><mprescripts/><none/><mn>1</mn></mmultiscripts></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msub><mi>L</mi><mi>r</mi></msub><mn>.116</mn></mrow><mrow><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub><mn>.</mn><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub></mrow></mfrac></math><img id="ib0024" file="imgb0024.tif" wi="68" he="18" img-content="math" img-format="tif"/></maths><br/>
With respect to the color components a* and b*, no LDR localization is needed. The localization of the perceptual space involves the following transformation: <maths id="math0025" num=""><math display="block"><msubsup><mi>a</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><mo>=</mo><mfrac><msup><mi>a</mi><mo>*</mo></msup><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi></mrow></mfrac><mo>=</mo><mfrac><msup><mi>a</mi><mo>*</mo></msup><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mo>⁢</mo><msup><mfenced><msub><mi>Y</mi><mi>E</mi></msub></mfenced><mmultiscripts><msub><mo>/</mo><mn>3</mn></msub><mprescripts/><none/><mn>1</mn></mmultiscripts></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>a</mi><mo>*</mo></msup><mn>.116</mn></mrow><mrow><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub><mn>.</mn><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub></mrow></mfrac></math><img id="ib0025" file="imgb0025.tif" wi="68" he="18" img-content="math" img-format="tif"/></maths> <maths id="math0026" num=""><math display="block"><msubsup><mi>b</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><mo>=</mo><mfrac><msup><mi>b</mi><mo>*</mo></msup><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi></mrow></mfrac><mo>=</mo><mfrac><msup><mi>b</mi><mo>*</mo></msup><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mo>⁢</mo><msup><mfenced><msub><mi>Y</mi><mi>E</mi></msub></mfenced><mmultiscripts><msub><mo>/</mo><mn>3</mn></msub><mprescripts/><none/><mn>1</mn></mmultiscripts></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>b</mi><mo>*</mo></msup><mn>.116</mn></mrow><mrow><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub><mn>.</mn><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub></mrow></mfrac></math><img id="ib0026" file="imgb0026.tif" wi="68" he="18" img-content="math" img-format="tif"/></maths></p><p id="p0063" num="0063">The residual texture data to be coded in each prediction unit is thus represented in a local perceptual space <maths id="math0027" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>a</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>b</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup></mfenced><mn>.</mn></math><img id="ib0027" file="imgb0027.tif" wi="36" he="7" img-content="math" img-format="tif" inline="yes"/></maths></p><p id="p0064" num="0064">In step S106 an encoding process applicable to LDR image data is then applied. The encoding process includes defining for each prediction unit or coding unit one or more transform units (TU) or Prediction Units (PU) to which prediction processes will be applied to predict the transform unit from reference samples, typically referred to as predictors. In the case of an intra coding unit, for example, each transform unit of the coding unit is spatially predicted from samples of neighbouring TUs which have been previously coded and reconstructed. Processes in accordance with embodiments of the invention for providing reference samples for the prediction processes are applied in step S107. In step S107 reference samples for prediction of the current TU or PU to be encoded are transformed into local LDR space of the current TU or PU being processed. The prediction of the current TU or PU using the locally-LDR-space transformed samples is applied in the LDR encoding process of step S106. In the LDR encoding process, the residual texture associated with a current TU or PU is determined, transformed and quantized for entropy coding by entropy encoder ENC2 in step S140. Encoding of the texture residual may be referred to herein as a second layer of coding. While this particular embodiment of the<!-- EPO <DP n="16"> --> invention is described for a current TU, it will be appreciated that it may also apply to a current PU, and can be extended for the inter mode coding case.</p><p id="p0065" num="0065">The mechanism of providing reference samples in accordance with embodiments of the invention for spatial and temporal prediction applied in the encoding process comprises four main steps as set out in <figref idrefs="f0004">Figure 4</figref>. In the process reference data of the predictors are converted for the prediction process into the local LDR space of the block to be predicted. This process applies in the same way at the encoder and at the decoder sides.</p><p id="p0066" num="0066">In step S401 luminance and chroma components <maths id="math0028" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi>a</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi>b</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup></mfenced></math><img id="ib0028" file="imgb0028.tif" wi="30" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of samples of TUs (or PUs) previously reconstructed in their local LDR space, and indicated by the prediction parameters for the spatial prediction of the current TU or identified by virtue of the inter coding parameters from the previously coded pictures for the temporal prediction of the current PU, are scaled into corresponding components <maths id="math0029" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi>a</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi>b</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup></mfenced></math><img id="ib0029" file="imgb0029.tif" wi="29" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in their local perceptual LDR space: <maths id="math0030" num=""><math display="block"><mfenced><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi mathvariant="italic">a</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi mathvariant="italic">b</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup></mfenced><mo>→</mo><mfenced><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi mathvariant="italic">a</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi mathvariant="italic">b</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup></mfenced></math><img id="ib0030" file="imgb0030.tif" wi="68" he="11" img-content="math" img-format="tif"/></maths></p><p id="p0067" num="0067">In spatial intra prediction the reference samples for prediction of the block typically neighbor the block to be predicted.</p><p id="p0068" num="0068">The conversion or normalization step S401 involves a scaling process and is mathematically represented as: <maths id="math0031" num=""><math display="block"><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><mfenced><mi mathvariant="italic">Float</mi></mfenced><mspace width="1em"/><mfenced><mfrac><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><mi mathvariant="italic">LDRSCALING</mi></mfrac></mfenced></math><img id="ib0031" file="imgb0031.tif" wi="54" he="13" img-content="math" img-format="tif"/></maths><br/>
where:
<ul><li><maths id="math0032" num=""><math display="inline"><msubsup><mi>L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup></math><img id="ib0032" file="imgb0032.tif" wi="10" he="7" img-content="math" img-format="tif" inline="yes"/></maths> represents the luminance component of the reconstructed LDR reference sample in its local LDR space</li><li><i>LDRSCALING</i> represents a constant integer for fixing the dynamic range of the samples at the input of the LDR coding layer;<br/>
LDRSCALING is chosen so that, when multiple by the bound representing the maximum LDR signal value, the maximum value support by the in-place LDR codec used is reached. For instance, if the LDR codec used works over 10 bits,<!-- EPO <DP n="17"> --> then this product must be equal to 2 <sup>9</sup>=512 (the most significant bit being used for the sign).</li><li><maths id="math0033" num=""><math display="inline"><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup></math><img id="ib0033" file="imgb0033.tif" wi="9" he="6" img-content="math" img-format="tif" inline="yes"/></maths> represents a luminance sample reconstructed in the local perceptual Lab space associated with the reference TU or PU used for the prediction of the current TU or PU (for example a sample of a neighbouring TU in the case of spatial prediction, or a sample of a PU identified by a motion vector in a reference picture in the case temporal prediction).</li></ul></p><p id="p0069" num="0069">In step S402 the rescaled components <maths id="math0034" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi>a</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi>b</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup></mfenced></math><img id="ib0034" file="imgb0034.tif" wi="28" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of the previously reconstructed TU or PU samples are then further converted into corresponding components <maths id="math0035" num=""><math display="inline"><mfenced><msup><mi>L</mi><mi mathvariant="italic">rec</mi></msup><msup><mi>a</mi><mi mathvariant="italic">rec</mi></msup><msup><mi>b</mi><mi mathvariant="italic">rec</mi></msup></mfenced></math><img id="ib0035" file="imgb0035.tif" wi="29" he="7" img-content="math" img-format="tif" inline="yes"/></maths> in the original HDR space of the image using the quantized low spatial frequency luminance component <maths id="math0036" num=""><math display="inline"><msubsup><mover><mi mathvariant="italic">L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi><mi mathvariant="italic">ref</mi></msubsup></math><img id="ib0036" file="imgb0036.tif" wi="9" he="8" img-content="math" img-format="tif" inline="yes"/></maths> associated with the respective reference TU: <maths id="math0037" num=""><math display="block"><mfenced><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi mathvariant="italic">a</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><msubsup><mi mathvariant="italic">b</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup></mfenced><mo>→</mo><mfenced><msup><mi mathvariant="italic">L</mi><mi mathvariant="italic">rec</mi></msup><msup><mi mathvariant="italic">a</mi><mi mathvariant="italic">rec</mi></msup><msup><mi mathvariant="italic">b</mi><mi mathvariant="italic">rec</mi></msup></mfenced></math><img id="ib0037" file="imgb0037.tif" wi="66" he="12" img-content="math" img-format="tif"/></maths><br/>
In the reconstruction step S402 each reference sample for prediction of a current block is subjected to a reconstruction in the HDR space as a function of the low frequency luminance value <i>L̂<sub>lf</sub></i> of the TU in which it is contained. The step is mathematically represented as follows: <maths id="math0038" num=""><math display="block"><msup><mi mathvariant="italic">L</mi><mi mathvariant="italic">rec</mi></msup><mo>=</mo><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msubsup><mover><mi mathvariant="italic">L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi><mi mathvariant="italic">ref</mi></msubsup></mrow><mn>116</mn></mfrac><mo>+</mo><msubsup><mover><mi mathvariant="italic">L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi><mi mathvariant="italic">ref</mi></msubsup></math><img id="ib0038" file="imgb0038.tif" wi="53" he="15" img-content="math" img-format="tif"/></maths> <maths id="math0039" num=""><math display="block"><msup><mi>a</mi><mi mathvariant="italic">rec</mi></msup><mo>=</mo><msubsup><mi>a</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msubsup><mover><mi mathvariant="italic">L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi><mi mathvariant="italic">ref</mi></msubsup></mrow><mn>116</mn></mfrac></math><img id="ib0039" file="imgb0039.tif" wi="53" he="19" img-content="math" img-format="tif"/></maths> <maths id="math0040" num=""><math display="block"><msup><mi>b</mi><mi mathvariant="italic">rec</mi></msup><mo>=</mo><msubsup><mi>b</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>⋅</mo><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msubsup><mover><mi mathvariant="italic">L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi><mi mathvariant="italic">ref</mi></msubsup></mrow><mn>116</mn></mfrac></math><img id="ib0040" file="imgb0040.tif" wi="53" he="17" img-content="math" img-format="tif"/></maths><br/>
where:
<ul><li><i>L</i><sup><i>r</i>ec</sup> represents the reference sample reconstructed in the original HDR space of the images I<sub>p</sub> to be compressed;</li><li><maths id="math0041" num=""><math display="inline"><msubsup><mover><mi mathvariant="italic">L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi><mi mathvariant="italic">ref</mi></msubsup></math><img id="ib0041" file="imgb0041.tif" wi="10" he="9" img-content="math" img-format="tif" inline="yes"/></maths> represents the reconstructed low spatial frequency luminance component associated with the reference TU, after inverse quantization.</li></ul><!-- EPO <DP n="18"> --></p><p id="p0070" num="0070">The set of reference samples reconstructed in this way in the HDR space are then mapped in the local perceptual space associated with the current TU or PU block of the current image being encoded. To achieve this in step S403 the components of the previously reconstructed Tus or PUs converted into the original HDR space, are then converted from the HDR space of the original image to the local perceptual space of the TU or PU to be predicted. The quantized low spatial frequency luminance component <i>L̂<sub>lf</sub></i> for the current TU or PU to be predicted is applied in the conversion: <maths id="math0042" num=""><math display="block"><mfenced><msup><mi mathvariant="italic">L</mi><mi mathvariant="italic">rec</mi></msup><msup><mi mathvariant="italic">a</mi><mi mathvariant="italic">rec</mi></msup><msup><mi mathvariant="italic">b</mi><mi mathvariant="italic">rec</mi></msup></mfenced><mo>→</mo><mfenced><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup><msubsup><mi mathvariant="italic">a</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup><msubsup><mi mathvariant="italic">b</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup></mfenced></math><img id="ib0042" file="imgb0042.tif" wi="70" he="12" img-content="math" img-format="tif"/></maths></p><p id="p0071" num="0071">This process is mathematically represented as follows: <maths id="math0043" num=""><math display="block"><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">pred</mi></msubsup><mo>=</mo><mfenced separators=""><msup><mi>L</mi><mi mathvariant="italic">rec</mi></msup><mo>-</mo><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mfenced><mo>⁢</mo><mfrac><mn>116</mn><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mrow></mfrac></math><img id="ib0043" file="imgb0043.tif" wi="58" he="16" img-content="math" img-format="tif"/></maths> <maths id="math0044" num=""><math display="block"><msubsup><mi>a</mi><mi>l</mi><mi mathvariant="italic">pred</mi></msubsup><mo>=</mo><mfenced><msup><mi>a</mi><mi mathvariant="italic">rec</mi></msup></mfenced><mo>⁢</mo><mfrac><mn>116</mn><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mrow></mfrac></math><img id="ib0044" file="imgb0044.tif" wi="48" he="14" img-content="math" img-format="tif"/></maths> <maths id="math0045" num=""><math display="block"><msubsup><mi>b</mi><mi>l</mi><mi mathvariant="italic">pred</mi></msubsup><mo>=</mo><mfenced><msup><mi>b</mi><mi mathvariant="italic">rec</mi></msup></mfenced><mo>⁢</mo><mfrac><mn>116</mn><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mrow></mfrac></math><img id="ib0045" file="imgb0045.tif" wi="48" he="17" img-content="math" img-format="tif"/></maths><br/>
where:
<ul><li><maths id="math0046" num=""><math display="inline"><msubsup><mi>L</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup></math><img id="ib0046" file="imgb0046.tif" wi="12" he="8" img-content="math" img-format="tif" inline="yes"/></maths> represents the reference sample transformed in the local perceptual space of the current TU or PU to be predicted;</li></ul></p><p id="p0072" num="0072">In step S404 the data in the local perceptual space is then scaled back into the local LDR space used for the encoding of the current TU or PU: <maths id="math0047" num=""><math display="block"><mfenced><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup><msubsup><mi mathvariant="italic">a</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup><msubsup><mi mathvariant="italic">b</mi><mi mathvariant="italic">l</mi><mi mathvariant="italic">pred</mi></msubsup></mfenced><mo>→</mo><mfenced><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">pred</mi></msubsup><msubsup><mi>a</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">pred</mi></msubsup><msubsup><mi>b</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">pred</mi></msubsup></mfenced></math><img id="ib0047" file="imgb0047.tif" wi="81" he="13" img-content="math" img-format="tif"/></maths></p><p id="p0073" num="0073">This process is mathematically represented as follows: <maths id="math0048" num=""><math display="block"><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">pred</mi></msubsup><mo>=</mo><mo>⌊</mo><mrow><mo>(</mo><mi>max</mi><mo>⁢</mo><mfenced separators=""><mo>-</mo><msub><mi mathvariant="italic">Bound</mi><mi mathvariant="italic">LDR</mi></msub><mo>,</mo><mi>min</mi><mfenced><msub><mi mathvariant="italic">Bound</mi><mi mathvariant="italic">LDR</mi></msub><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">pred</mi></msubsup></mfenced></mfenced><mo>×</mo><mi mathvariant="italic">LDRSCALING</mi><mo>+</mo><mn>0.5.</mn><mo>⁢</mo><mi mathvariant="italic">sign</mi><mfenced><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">pred</mi></msubsup></mfenced></mrow><mo>⌋</mo></math><img id="ib0048" file="imgb0048.tif" wi="153" he="13" img-content="math" img-format="tif"/></maths><br/>
where:<!-- EPO <DP n="19"> -->
<ul><li><i>Bound<sub>LDR</sub></i> x <i>LDRSCALING</i> represents the absolute value of the dynamic of the image samples processed by the codec of the LDR layer</li><li>the operator └<i>x</i>┘ represent the integer value closest to x, with an absolute value equal or lower than that of x.</li></ul></p><p id="p0074" num="0074">The advantage of the proposed mechanism is that the sample components transformed into the local LDR space of the TU or PU to be predicted are better correlated with the content of the TU or PU to be predicted thus improving the efficiency and accuracy of the prediction leading to better compression of the data.</p><p id="p0075" num="0075"><figref idrefs="f0005">Figure 5</figref> schematically illustrates an example of the implementation of the conversion of samples between local LDR and HDR spaces in the case of Intra spatial prediction. The reference samples to be used for prediction of the current TU are determined from the intra prediction mode (DC, planar or angular direction). In the illustrated example the TU to be predicted 500 is predicted from previously reconstructed boundary samples of TUs neighbouring the TU to be predicted. In the illustrated example, samples of reconstructed TUs: TUa to TUe located at boundaries generally above and to the left of the TU 500 to be predicted are used to predict the texture of that TU. Each of the reconstructed TUs has an associated low frequency luminance value <i>L̂<sub>lf<sup2>a</sup2></sub>, L̂<sub>lf<sup2>b</sup2></sub></i>, <i>L̂<sub>lf<sup2>c</sup2></sub>, L̂<sub>lf<sup2>d</sup2></sub> L̂<sub>lf<sup2>e</sup2></sub>.</i> Each of the reference samples used for prediction of the current TU is subjected to the transformation process of <figref idrefs="f0004">Figure 4</figref>.</p><p id="p0076" num="0076"><figref idrefs="f0005">Figure 6</figref> schematically illustrates an example of the implementation of the conversion of samples between local LDR and HDR spaces in the case of temporal prediction. In the case of temporal prediction a current TU or PU block of the current image is linked by a motion vector <o><i>MV</i></o> to a reference block in a reference image. In the illustrated example, the reference block is intersected by one or more blocks of the reference image. Each intersecting reference block has its associated low frequency luminance <i>value L̂<sub>lf</sub>.</i></p><p id="p0077" num="0077">Each reference sample for prediction of a current block is subjected to a reconstruction in the HDR space as a function of the low frequency luminance value <i>L̂<sub>lf</sub></i> of the TU in which it is contained. The set of reference samples reconstructed in this way in the HDR space are then mapped in the local perceptual space associated with the TU block of the current image being encoded.<!-- EPO <DP n="20"> --></p><p id="p0078" num="0078"><figref idrefs="f0006">Figure 7</figref> is a schematic block diagram illustrating steps of a method of encoding at least part of an image according to a further embodiment of the invention. With reference to <figref idrefs="f0006">Figure 7</figref>, steps S701 to S707 are similar to corresponding steps S101 to S107 of <figref idrefs="f0001">Figure 1</figref>. The process of the embodiment of <figref idrefs="f0006">Figure 7</figref> differs to that of <figref idrefs="f0001">Figure 1</figref> in that it further includes reconstruction of the coding unit to be encoded in the original HDR space and the adjustment of encoding parameters of the encoding process according to a rate distortion cost calculated on the reconstructed coding unit.</p><p id="p0079" num="0079">After prediction processing in step S706 of the TU or PU to be encoded, the residual texture data to be coded in each prediction unit is thus represented in a local perceptual space <maths id="math0049" num=""><math display="inline"><mfenced><msubsup><mi>L</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>a</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup><msubsup><mi>b</mi><mi mathvariant="italic">local</mi><mo>*</mo></msubsup></mfenced><mn>.</mn></math><img id="ib0049" file="imgb0049.tif" wi="36" he="7" img-content="math" img-format="tif" inline="yes"/></maths> If a rate-distortion cost was calculated on the basis of the local perceptual space, for the choice of quad tree representation of the CTUs of the HDR image to be encoded, an inconsistency would be likely to arise. For example, supposing that for a given CU at a given quad tree level the partitioning unit of the encoder has to choose between two types of prediction units 2Nx2N and NxN the comparison between the corresponding rate-distortion costs would be as follows: <maths id="math0050" num=""><math display="block"><mi>D</mi><mfenced separators=""><msub><mi mathvariant="italic">CU</mi><mi mathvariant="italic">level</mi></msub><mo>,</mo><mn>2</mn><mo>⁢</mo><mi mathvariant="italic">Nx</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><mi>N</mi></mfenced><mo>+</mo><mi mathvariant="italic">λR</mi><mfenced separators=""><msub><mi mathvariant="italic">CU</mi><mi mathvariant="italic">level</mi></msub><mo>,</mo><mn>2</mn><mo>⁢</mo><mi mathvariant="italic">Nx</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><mi>N</mi></mfenced><mo>⁢</mo><mover><mo>&gt;</mo><mo>&lt;</mo></mover><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover></mstyle><mi>D</mi><mfenced><msubsup><mi mathvariant="italic">PU</mi><mi mathvariant="italic">level</mi><mi mathvariant="italic">i</mi></msubsup><mi mathvariant="italic">NxN</mi></mfenced><mo>+</mo><mi>λ</mi><mo>⁢</mo><mi>R</mi><mfenced><msubsup><mi mathvariant="italic">PU</mi><mi mathvariant="italic">level</mi><mi mathvariant="italic">i</mi></msubsup><mi mathvariant="italic">NxN</mi></mfenced></math><img id="ib0050" file="imgb0050.tif" wi="147" he="18" img-content="math" img-format="tif"/></maths> i.e: <maths id="math0051" num=""><math display="block"><mi>D</mi><mfenced separators=""><msub><mi mathvariant="italic">CU</mi><mi mathvariant="italic">level</mi></msub><mo>,</mo><mn>2</mn><mo>⁢</mo><mi mathvariant="italic">Nx</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><mi>N</mi></mfenced><mo>+</mo><mi mathvariant="italic">λR</mi><mfenced separators=""><msub><mi mathvariant="italic">CU</mi><mi mathvariant="italic">level</mi></msub><mo>,</mo><mn>2</mn><mo>⁢</mo><mi mathvariant="italic">Nx</mi><mo>⁢</mo><mn>2</mn><mo>⁢</mo><mi>N</mi></mfenced><mo>⁢</mo><mover><mo>&gt;</mo><mo>&lt;</mo></mover><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover></mstyle><mi>D</mi><mfenced><msubsup><mi mathvariant="italic">PU</mi><mi mathvariant="italic">level</mi><mi mathvariant="italic">i</mi></msubsup><mi mathvariant="italic">NxN</mi></mfenced><mo>+</mo><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover></mstyle><mi>R</mi><mfenced><msubsup><mi mathvariant="italic">PU</mi><mi mathvariant="italic">level</mi><mi mathvariant="italic">i</mi></msubsup><mi mathvariant="italic">NxN</mi></mfenced></math><img id="ib0051" file="imgb0051.tif" wi="156" he="19" img-content="math" img-format="tif"/></maths><br/>
In the term on the right it can be seen that an addition is performed on the calculated distortions for PUs represented in different colour spaces. This can lead to inconsistencies.</p><p id="p0080" num="0080">In order to address such a problem, in the embodiment of <figref idrefs="f0006">Figure 7</figref> the rate-distortion cost associated with a spatial entity of the image is considered in the original HDR space rather than in the local LDR perceptual space. In this way rate-distortion costs corresponding to different image blocks of the image are comparable since they have been calculated in the same perceptual space. A step of reconstructing the coding unit in the HDR space is thus included in the encoding process of the<!-- EPO <DP n="21"> --> embodiment of <figref idrefs="f0006">Figure 7</figref>. Reconstruction of a coding unit in the HDR space is carried out as follows.</p><p id="p0081" num="0081">Each TU or PU of the coding unit is reconstructed by performing inverse quantization in step S712 inverse transformation in step S714 and prediction addition in step S716. The reconstructed TU is then obtained in the original HDR space in step S718.</p><p id="p0082" num="0082">For the step S718 of reconstructing the residual TU or PU in the HDR space for which the local colour space in a particular embodiment of the invention is Lab 76, the following equations may be applied. The equations correspond respectively to the reconstruction of the decoded pixels of the TU in the HDR space for the luminance component L and the chrominance components a, b: <maths id="math0052" num="1."><math display="block"><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><mfenced><mi mathvariant="italic">float</mi></mfenced><mspace width="1em"/><mfenced><mfrac><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><mi mathvariant="italic">LDRSCALING</mi></mfrac></mfenced></math><img id="ib0052" file="imgb0052.tif" wi="64" he="14" img-content="math" img-format="tif"/></maths> <maths id="math0053" num="2."><math display="block"><msubsup><mi mathvariant="italic">L</mi><mi mathvariant="italic">HDR</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mn>.</mn><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mrow><mn>116</mn></mfrac><mo>+</mo><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></math><img id="ib0053" file="imgb0053.tif" wi="64" he="15" img-content="math" img-format="tif"/></maths> <maths id="math0054" num="3."><math display="block"><msubsup><mi>a</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><mfenced><mi mathvariant="italic">float</mi></mfenced><mspace width="1em"/><mfenced><mfrac><msubsup><mi mathvariant="italic">a</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><mi mathvariant="italic">LDRSCALING</mi></mfrac></mfenced></math><img id="ib0054" file="imgb0054.tif" wi="64" he="13" img-content="math" img-format="tif"/></maths> <maths id="math0055" num="4."><math display="block"><msubsup><mi>a</mi><mi mathvariant="italic">HDR</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><msubsup><mi>a</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mn>.</mn><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mrow><mn>116</mn></mfrac></math><img id="ib0055" file="imgb0055.tif" wi="64" he="14" img-content="math" img-format="tif"/></maths> <maths id="math0056" num="5."><math display="block"><msubsup><mi>b</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><mfenced><mi mathvariant="italic">Float</mi></mfenced><mspace width="1em"/><mfenced><mfrac><msubsup><mi>b</mi><mi mathvariant="italic">LDR</mi><mi mathvariant="italic">rec</mi></msubsup><mi mathvariant="italic">LDRSCALING</mi></mfrac></mfenced></math><img id="ib0056" file="imgb0056.tif" wi="64" he="15" img-content="math" img-format="tif"/></maths> <maths id="math0057" num="6."><math display="block"><msubsup><mi>b</mi><mi mathvariant="italic">HDR</mi><mi mathvariant="italic">rec</mi></msubsup><mo>=</mo><msubsup><mi>b</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mn>.</mn><mfrac><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><msub><mover><mi>L</mi><mo>^</mo></mover><mi mathvariant="italic">lf</mi></msub></mrow><mn>116</mn></mfrac></math><img id="ib0057" file="imgb0057.tif" wi="64" he="15" img-content="math" img-format="tif"/></maths><br/>
where :
<ul><li><i>LDRSCALING</i> represents a constant integer for fixing the dynamic range of the given pixels at the input of the LDR coding layer;</li><li><maths id="math0058" num=""><math display="inline"><msubsup><mi>L</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup><mo>,</mo><msubsup><mi>b</mi><mi>l</mi><mi mathvariant="italic">rec</mi></msubsup></math><img id="ib0058" file="imgb0058.tif" wi="24" he="7" img-content="math" img-format="tif" inline="yes"/></maths> represent the luminance and chrominance samples reconstructed in the local Lab space associated with the PU containing the sample;</li><li><maths id="math0059" num=""><math display="inline"><msubsup><mi>L</mi><mi mathvariant="italic">HDR</mi><mi mathvariant="italic">rec</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mi mathvariant="italic">HDR</mi><mi mathvariant="italic">rec</mi></msubsup><mo>,</mo><msubsup><mi>b</mi><mi mathvariant="italic">HDR</mi><mi mathvariant="italic">rec</mi></msubsup></math><img id="ib0059" file="imgb0059.tif" wi="30" he="7" img-content="math" img-format="tif" inline="yes"/></maths> represent the samples reconstructed in the HDR space of the original images I<sub>p</sub> to be compressed;</li><li><i>L̂<sub>lf</sub></i> represents the low spatial frequency luminance component associated with the PU, in the reconstructed version after inverse quantization.</li></ul><!-- EPO <DP n="22"> --></p><p id="p0083" num="0083">The ENCODER CONTROL module manages the strategy used to encode a given coding unit or sub-elements of a coding unit in a current image. To do so, it assigns candidate coding parameters to the current coding unit or coding unit sub-elements. Encoding parameters for encoding of the blocks may include one or more of the following coding parameters:
<ul><li>the coding tree unit organization in terms of coding quad-tree, prediction units and transform units.</li><li>the coding mode (INTRA or INTER) assigned to coding units of the coding tree, where INTER indicates inter-picture (temporal) prediction and INTRA indicates intra-picture (spatial) prediction</li><li>the intra prediction mode (DC, planar or angular direction) for each Intra coding unit in the considered coding tree.</li><li>the INTER prediction parameters in case of INTER coding units: motion vectors, reference picture indices, etc.</li></ul></p><p id="p0084" num="0084">The choice of coding parameters for a coding unit is performed by minimizing a rate-distortion cost as follows: <maths id="math0060" num=""><math display="block"><msub><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">opt</mi></msub><mo>=</mo><munder><mi mathvariant="bold">Argmin</mi><mrow><mi mathvariant="bold-italic">p</mi><mo>∈</mo><mi mathvariant="bold-italic">P</mi></mrow></munder><mfenced open="{" close="}" separators=""><mi mathvariant="bold-italic">D</mi><mfenced><mi mathvariant="bold-italic">p</mi></mfenced><mo>+</mo><mi mathvariant="bold-italic">λ</mi><mn mathvariant="bold-italic">.</mn><mi mathvariant="bold-italic">R</mi><mfenced><mi mathvariant="bold-italic">p</mi></mfenced></mfenced></math><img id="ib0060" file="imgb0060.tif" wi="64" he="13" img-content="math" img-format="tif"/></maths><br/>
where p represents the set of candidate coding parameters for a given coding unit and <i>λ</i> represents the Lagrange parameter, and <i>D(p)</i> and <i>R</i>(<i>p</i>) respectively represent the distortion and the rate associated with the coding of the current coding unit with the candidate set of coding parameters p.</p><p id="p0085" num="0085">In embodiments of the invention, the distortion term <i>D(p)</i> represents the coding error obtained in the initial HDR space of the image to be encoded. In general this involves reconstructing a CU or CU sub-elements being processed into the original (<i>L*, a*, b*</i>) space, as will be described in what follows, before calculating the distortion <i>D(p)</i> associated with coding parameter p. Such an approach helps to reduce the appearance of artefacts in the decoded image since the coding unit or sub-element in its original HDR space is considered.</p><p id="p0086" num="0086">A process for calculating the rate-distortion cost for encoding a coding unit with a set of encoding parameters p, according to one or more embodiments of the<!-- EPO <DP n="23"> --> invention is set out as follows. In the embodiment of <figref idrefs="f0006">Figure 7</figref> the rate distortion cost process is performed in step S720 by rate distortion module RATE-DIST.</p><p id="p0087" num="0087">The process is initialized by resetting the rate distortion cost J to 0 : <i>J ←</i> 0</p><p id="p0088" num="0088">After the low spatial frequency component L<i><sub>lf</sub></i>(<i>PU</i>) has been entropy encoded in step S730 an associated rate <i>R</i>(<i>L<sub>lf</sub></i>) is determined in step S720 for the entropy encoded low spatial frequency component L<i><sub>lf</sub></i>(<i>PU</i>). The rate-distortion cost J is then updated in accordance with: <maths id="math0061" num=""><math display="block"><mi>J</mi><mo>←</mo><mi>J</mi><mo>+</mo><mi>λ</mi><mn>.</mn><mi>R</mi><mfenced><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub></mfenced></math><img id="ib0061" file="imgb0061.tif" wi="32" he="12" img-content="math" img-format="tif"/></maths> where <i>λ</i> represents the Lagrange parameter.</p><p id="p0089" num="0089">An associated rate <i>R</i>(<i>TU,p</i>) is determined in step S720 for the entropy encoded residual texture of step S740.</p><p id="p0090" num="0090">A distortion for the reconstructed TU in the original HDR space is then calculated as follows: <maths id="math0062" num=""><math display="block"><msup><mi>D</mi><mi mathvariant="italic">HDR</mi></msup><mfenced><mi mathvariant="italic">TU</mi><mi>p</mi></mfenced><mo>=</mo><msubsup><mi mathvariant="normal">Σ</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msubsup><mo>⁢</mo><msup><mfenced separators=""><msubsup><mi mathvariant="italic">TU</mi><mi mathvariant="italic">rec</mi><mi mathvariant="italic">HDR</mi></msubsup><mfenced><mi>i</mi></mfenced><mo>-</mo><msubsup><mi mathvariant="italic">TU</mi><mi mathvariant="italic">orig</mi><mi mathvariant="italic">HDR</mi></msubsup><mfenced><mi>i</mi></mfenced></mfenced><mn>2</mn></msup><mo>,</mo></math><img id="ib0062" file="imgb0062.tif" wi="85" he="12" img-content="math" img-format="tif"/></maths> where <maths id="math0063" num=""><math display="inline"><msubsup><mi mathvariant="italic">TU</mi><mi mathvariant="italic">orig</mi><mi mathvariant="italic">HDR</mi></msubsup><mfenced><mi>i</mi></mfenced></math><img id="ib0063" file="imgb0063.tif" wi="18" he="7" img-content="math" img-format="tif" inline="yes"/></maths> corresponds to the sample of the TU in the original HDR image and <maths id="math0064" num=""><math display="inline"><msubsup><mi mathvariant="italic">TU</mi><mi mathvariant="italic">rec</mi><mi mathvariant="italic">HDR</mi></msubsup><mfenced><mi>i</mi></mfenced></math><img id="ib0064" file="imgb0064.tif" wi="18" he="7" img-content="math" img-format="tif" inline="yes"/></maths> corresponds to the sample of the reconstructed TU in the HDR space. The rate distortion cost J of the CU is then updated as follows: <maths id="math0065" num=""><math display="block"><mi>J</mi><mo>←</mo><mi>J</mi><mo>+</mo><msup><mi>D</mi><mi mathvariant="italic">HDR</mi></msup><mfenced><mi mathvariant="italic">TU</mi><mi>p</mi></mfenced><mo>+</mo><mi>λ</mi><mn>.</mn><mi>R</mi><mfenced><mi mathvariant="italic">TU</mi><mi>p</mi></mfenced></math><img id="ib0065" file="imgb0065.tif" wi="65" he="10" img-content="math" img-format="tif"/></maths></p><p id="p0091" num="0091">The rate-distortion cost associated with the encoding of a CU with a coding parameter p can be formulated as follows: <maths id="math0066" num=""><math display="block"><msup><mi>D</mi><mi mathvariant="italic">HDR</mi></msup><mfenced><mi mathvariant="italic">CU</mi><mi>p</mi></mfenced><mo>+</mo><mi>λ</mi><mo>⁢</mo><mfenced separators=""><msub><mi>R</mi><mi mathvariant="italic">LDR</mi></msub><mfenced><mi mathvariant="italic">CU</mi><mi>p</mi></mfenced><mo>+</mo><mi>R</mi><mfenced><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub><mi>p</mi></mfenced></mfenced></math><img id="ib0066" file="imgb0066.tif" wi="77" he="12" img-content="math" img-format="tif"/></maths> where :
<ul><li><i>R<sub>LDR</sub></i>(<i>Cu</i>,<i>p</i>) is the coding cost of the considered CU in the LDR layer <i>R</i>(<i>L<sub>lf</sub>,p</i>) is the coding cost of the low frequency luminance components associated with the PUs belonging to the CU considered.</li></ul></p><p id="p0092" num="0092">In step S722 the encoder control module ENCODER CONTROL adapts the coding parameters of the LDR encoding process based on the rate distortion cost calculated in step S720 for the encoded TU in the HDR space.<!-- EPO <DP n="24"> --></p><p id="p0093" num="0093"><figref idrefs="f0007">Figure 8</figref> is a schematic block diagram illustrating steps of a method of encoding at least part of an image according to a further embodiment of the invention. With reference to <figref idrefs="f0007">Figure 8</figref>, steps S801 to S807 are similar to corresponding steps S101 to S107 of <figref idrefs="f0001">Figure 1</figref>. In particular step S807 is implemented to perform conversion on the predictor samples and prediction according to embodiments of the invention. The process of the embodiment of <figref idrefs="f0007">Figure 8</figref> differs to that of <figref idrefs="f0001">Figure 1</figref> and <figref idrefs="f0006">Figure 7</figref> in that it includes a refinement step, typically referred to as quasi-lossless, in which refinement is performed on the texture data reconstructed in the local perceptual space of the PU being processed. The encoding may be referred to as tri-layer encoding since it involves entropy encoding of the low spatial frequency component L<i><sub>lf</sub></i>, the entropy encoding of the residual textual data and <i>L</i><sub>∞</sub> norm entropy encoding. The additional refinement step in the encoding process ensures a distortion based on the <i>L</i><sub>∞</sub> norm between the original texture data and the texture data reconstructed in the considered local perceptual space (steps S816 to S824). Encoding module ENC3 performs encoding for this encoding layer in step S821.</p><p id="p0094" num="0094">In each of the described embodiments an encoded bitstream representative of the original HDR image is transmitted to a destination receiving device equipped with a decoding device. Information on the coding parameters used to encode the image data may be transmitted to the decoding device to enable the bitstream representing the HDR image to be decoded and the original HDR image reconstructed. The information representative of the coding parameters may be encoded prior to transmission. For example, in the embodiments of <figref idrefs="f0006">Figure 7</figref> data representative of the coding parameters is provided by the encoder control module and encoded in the bitstream by encoder ENC2. In these examples the parameters are thus encoded in the bitstream corresponding to the second layer of coding (LDR layer).</p><p id="p0095" num="0095"><figref idrefs="f0008">Figure 9</figref> is a schematic block diagram illustrating an example of a decoding process implemented by a decoding device, in accordance with an embodiment of the invention for decoding a bitstream representing an image I. In the decoding process decoders DEC1, DEC2 and DEC3, are configured to decode data which have been encoded by the encoders ENC1, ENC2 and ENC3 respectively.</p><p id="p0096" num="0096">In the example the bitstream F which represents a HDR image I which comprising a luminance component and potentially at least one colour component.<!-- EPO <DP n="25"> --> Indeed the component(s) of the image I belong to a perceptual colour space as described above.</p><p id="p0097" num="0097">In step 901, a decoded version <maths id="math0067" num=""><math display="inline"><mover><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub><mo>^</mo></mover></math><img id="ib0067" file="imgb0067.tif" wi="7" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of the low-spatial-frequency version of the luminance component of the image I is obtained by decoding at least partially the bitstream F, by means of a decoder DEC1.</p><p id="p0098" num="0098">In step 902, a decoded version of the encoded residual textual data is obtained by at least a partial decoding of the bitstream F by means of the decoder DEC2.</p><p id="p0099" num="0099">In step 907 reference samples for prediction of TUs or of PUs undergo a conversion process in accordance with embodiments of the invention for prediction of the current TU or PU. Each reference sample for prediction of a current block is subjected to a reconstruction in the HDR space as a function of the low frequency luminance value <i>L̂<sub>lf</sub></i> of the TU or PU in which it is contained. The set of reference samples reconstructed in this way in the HDR space are then mapped in the local perceptual space associated with the TU or PU block of the current image being decoded before having its dynamic range reduced to the LDR space of the TU or PU block.</p><p id="p0100" num="0100">The conversion process is identical to the conversion process of S107 of <figref idrefs="f0001">Figure 1</figref>. The steps as detailed in <figref idrefs="f0004">Figure 4</figref> are performed to provide converted reference samples for prediction of the current TU or PU. In step 906 prediction of TUs or PUs is performed in accordance with embodiments of the invention. Step 906 is identical to step S106 of the corresponding encoding process. The conversion process of S907 applies in the same way at the encoder and at the decoder sides.</p><p id="p0101" num="0101">In step 909, the decoded version of residual textual data and the decoded version <maths id="math0068" num=""><math display="inline"><mover><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub><mo>^</mo></mover></math><img id="ib0068" file="imgb0068.tif" wi="7" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of the low-spatial-frequency version of the luminance component of the image are associated with each other to obtain a decoded image <i>Î.</i></p><p id="p0102" num="0102">In some embodiments of the invention, in which the image data has been encoded in accordance with a tri-layer encoding process such as the process of <figref idrefs="f0007">Figure 8</figref> a third layer of decoding is provided in which decoding is performed by decoder unit DEC3.<!-- EPO <DP n="26"> --></p><p id="p0103" num="0103"><figref idrefs="f0009">Figure 10</figref> illustrates a decoder according to an embodiment of the invention in which the encoding parameters have been adapted on the basis of a rate-distortion criteria, such as in the encoding example of <figref idrefs="f0006">Figure 7</figref> or <figref idrefs="f0007">8</figref>. Data P representative of the adapted encoding parameters may be received by the decoding device and decoded by a parameter decoder module DEC-PAR in additional step 930. The encoding parameter data P is transmitted in the bitstream with the image data I. The information on the encoding parameters employed is then provided to decoders DEC 1, DEC 2 and DEC 3 so that the encoded image data may be decoded with decoding parameters in accordance with the encoding parameters determined by encoder control module ENCODER CONTROL of the encoder. Steps 901 to 909 are performed in a similar manner to steps 901 to 909 of <figref idrefs="f0008">Figure 9</figref>.</p><p id="p0104" num="0104">The decoding precision of decoder DEC2 depends on a perceptual threshold <i>ΔE</i> that defines an upper bound of the metric, defined in the perceptual space, which insures a control of the visual losses in a displayed decoded version of the image. The precision of the decoding is thus a function of the perceptual threshold which changes locally.</p><p id="p0105" num="0105">As previously described, the perceptual threshold <i>ΔE</i> is determined, according to an embodiment, according to reference lighting conditions of the displaying (the same as those used for encoding) and the decoded version <maths id="math0069" num=""><math display="inline"><mover><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub><mo>^</mo></mover></math><img id="ib0069" file="imgb0069.tif" wi="7" he="7" img-content="math" img-format="tif" inline="yes"/></maths> of the low-spatial-frequency version of the luminance component of the image I.</p><p id="p0106" num="0106">According to an embodiment each component of a residual image has been normalized by means of the perceptual threshold Δ<i>E</i>, the residual image is decoded at a constant precision and each component of the decoded version of the differential image is re-normalized by the help the perceptual threshold <i>ΔE</i> where <maths id="math0070" num=""><math display="block"><mi mathvariant="normal">Δ</mi><mo>⁢</mo><mi>E</mi><mo>=</mo><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msub><mi>E</mi><mn>0</mn></msub><mn>.</mn><mfrac><mover><msub><mi>L</mi><mi mathvariant="italic">lf</mi></msub><mo>^</mo></mover><mn>116</mn></mfrac></math><img id="ib0070" file="imgb0070.tif" wi="30" he="16" img-content="math" img-format="tif"/></maths></p><p id="p0107" num="0107">According to an embodiment the re-normalization is the division by a value which is a function of the perceptual threshold Δ<i>E</i>.<!-- EPO <DP n="27"> --></p><p id="p0108" num="0108">The encoders ENC1, ENC2 and/or ENC3 (and decoders DEC1, DEC2 and/or DEC3) are not limited to a specific encoder (decoder) but when an entropy encoder (decoder) is required, an entropy encoder such as a Huffmann coder, an arithmetic coder or a context adaptive coder like Cabac used in h264/AVC or HEVC is advantageous.</p><p id="p0109" num="0109">The encoder ENC2 (and decoder DEC2) is not limited to a specific encoder which may be, for example, a lossy image/video coder like JPEG, JPEG2000, MPEG2, h264/AVC or HEVC.</p><p id="p0110" num="0110">The encoder ENC3 (and decoder DEC3) is not limited to a specific lossless or quasi lossless encoder which may be, for example, an image coder like JPEG lossless, h264/AVC lossless, a trellis based encoder, or an adaptive DPCM like encoder.</p><p id="p0111" num="0111">According to a variant, in step 910, a module IIC is configured to apply an inverse perceptual transform to the decoded image <i>Î</i>, output of the step 909. For example, the estimate of the decoded image <i>Î</i> is transformed to the well-known space (X, Y, Z).</p><p id="p0112" num="0112">When the perceptual space LabCIE1976 is selected, the inverse perceptual transform is given by: <maths id="math0071" num=""><math display="block"><mi>X</mi><mo>=</mo><msub><mi>X</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mfrac><mn>1</mn><mn>116</mn></mfrac><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced><mo>+</mo><mfrac><mn>1</mn><mn>500</mn></mfrac><mo>⁢</mo><msup><mi>a</mi><mo>*</mo></msup></mfenced></math><img id="ib0071" file="imgb0071.tif" wi="70" he="13" img-content="math" img-format="tif"/></maths> <maths id="math0072" num=""><math display="block"><mi>Y</mi><mo>=</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mn>1</mn><mo>/</mo><mn>116</mn><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced></mfenced></math><img id="ib0072" file="imgb0072.tif" wi="53" he="7" img-content="math" img-format="tif"/></maths> <maths id="math0073" num=""><math display="block"><mi>Z</mi><mo>=</mo><msub><mi>Z</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mfrac><mn>1</mn><mn>116</mn></mfrac><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced><mo>+</mo><mfrac><mn>1</mn><mn>200</mn></mfrac><mo>⁢</mo><msup><mi>b</mi><mo>*</mo></msup></mfenced></math><img id="ib0073" file="imgb0073.tif" wi="68" he="14" img-content="math" img-format="tif"/></maths></p><p id="p0113" num="0113">When the perceptual space Luv is selected, the inverse perceptual transform is given by: <maths id="math0074" num=""><math display="block"><mi>X</mi><mo>=</mo><mfrac><mrow><mn>9</mn><mo>⁢</mo><mi mathvariant="italic">Yuʹ</mi></mrow><mrow><mn>4</mn><mo>⁢</mo><mi mathvariant="italic">vʹ</mi></mrow></mfrac></math><img id="ib0074" file="imgb0074.tif" wi="24" he="12" img-content="math" img-format="tif"/></maths> <maths id="math0075" num=""><math display="block"><mi>Y</mi><mo>=</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>⁢</mo><msup><mi>f</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>⁢</mo><mfenced separators=""><mfrac><mn>1</mn><mn>116</mn></mfrac><mo>⁢</mo><mfenced separators=""><msup><mi>L</mi><mo>*</mo></msup><mo>+</mo><mn>16</mn></mfenced></mfenced></math><img id="ib0075" file="imgb0075.tif" wi="53" he="12" img-content="math" img-format="tif"/></maths> <maths id="math0076" num=""><math display="block"><mi>Z</mi><mo>=</mo><mfrac><mrow><mn>3</mn><mo>⁢</mo><mi>Y</mi><mo>⁢</mo><mfenced separators=""><mn>4</mn><mo>-</mo><msup><mi>u</mi><mi>ʹ</mi></msup></mfenced></mrow><mrow><mn>4</mn><mo>⁢</mo><msup><mi>v</mi><mi>ʹ</mi></msup></mrow></mfrac><mo>-</mo><mn>5</mn><mo>⁢</mo><mi>Y</mi></math><img id="ib0076" file="imgb0076.tif" wi="42" he="15" img-content="math" img-format="tif"/></maths><!-- EPO <DP n="28"> --></p><p id="p0114" num="0114">Potentially, the image in the space (X,Y,Z) is inverse transformed to get the estimate of the decoded image in the initial space such as (R,G,B) space.</p><p id="p0115" num="0115">In <figref idrefs="f0001">Figures 1</figref>, <figref idrefs="f0004">4</figref> and <figref idrefs="f0006 f0009">7 to 10</figref>, the modules are functional units, which may or may not correspond to distinguishable physical units. For example, a plurality of such modules may be associated in a unique component or circuit, or correspond to software functionalities. Moreover, a module may potentially be composed of separate physical entities or software functionalities.</p><p id="p0116" num="0116">Devices compatible with embodiments of the invention may be implemented either solely by hardware, solely by software or by a combination of hardware and software. In terms of hardware for example dedicated hardware, may be used, such ASIC or FPGA or VLSI, respectively «Application Specific Integrated Circuit », « Field-Programmable Gate Array », « Very Large Scale Integration », or by using several integrated electronic components embedded in a device or from a blend of hardware and software components.</p><p id="p0117" num="0117"><figref idrefs="f0010">Figure 11</figref> is a schematic block diagram representing functional components of an encoding device or a decoding device 1100 in which one or more embodiments of the invention may be implemented.</p><p id="p0118" num="0118">The device 1100 includes memory 1110, one or more processing units (CPUs) 1120, an input/output interface 1130 for transfer of data from and to an application. The components communicate over the one or more communication buses 1150.</p><p id="p0119" num="0119">Memory may include high speed random access memory (RAM) 1111 and read only memory (ROM) 1112. A register of memory may correspond to portion of small capacity (some bits) or to very large capacity portion (e.g at least entire computer program code or large amounts of compressed or uncompressed data) of any of the memories of the device. ROM 1112 stores at least program code and parameters. Algorithms of the methods according to embodiments of the invention may be stored in ROM 1112. When switched on, the CPU 1120 uploads the program in the RAM 1111 and executes the corresponding instructions.</p><p id="p0120" num="0120">RAM 111 comprises, in a register, the program executed by the CPU 1120 and uploaded after switch on of the device 1100, input data in a register, intermediate data<!-- EPO <DP n="29"> --> at different states of the algorithm in a register, and other variables used for the execution of the algorithm in a register.</p><p id="p0121" num="0121">Memory 1100 may include non-volatile memory, such as one or more magnetic disk storage devices, flash memory devices or other non-volatile solid state memory devices. In some embodiments, the memory may further include storage remotely located from the one or more CPUs 1120. For example storage accessible via an interface and/or a communication network.</p><p id="p0122" num="0122">In some embodiments the device is provided with a power source such as a battery 1140. According to alternative embodiments, the power source may be external to the device.</p><p id="p0123" num="0123">It will be appreciated that elements of <figref idrefs="f0010">Figure 11</figref> are well-known by those skilled in the art and consequently need not be described in further detail for an understanding of the invention.</p><p id="p0124" num="0124">According to an embodiment, the device of <figref idrefs="f0010">Figure 11</figref> further comprises means for obtaining reference lighting conditions of the displaying such as a maximal environmental brightness value Yn of the displaying lighting. For example a photosensitive diode or the like.</p><p id="p0125" num="0125">According to an embodiment, for example in the case of a decoding device, the device of <figref idrefs="f0010">Figure 11</figref> comprises a display 1160 and the means for obtaining reference lighting conditions of the display are configured to determine such reference lighting conditions of the display from some characteristics of the display 1160 or from lighting conditions around the display 1160 which are captured by the apparatus. For instance, the means for obtaining a maximal environmental brightness value Yn of the displaying lighting are a sensor attached to the display and which measures the environmental conditions. A photodiode or the like may be used to this purpose.</p><p id="p0126" num="0126">The display may in some embodiments of the invention by a touch sensitive display or any other type of display for displaying video data.</p><p id="p0127" num="0127"><figref idrefs="f0010">FIG. 12</figref> schematically illustrates an example of a data communication system in which one or more embodiments of the invention may be implemented. The data communication system 1200 comprises a transmission device, in this case a server 1210, which is operable to transmit data packets of a data stream to a receiving device, in this case a client terminal 1210, via a data communication network 1230.<!-- EPO <DP n="30"> --> The data communication network 1230 may be a wireless network or a wired network or a combination of wireless and wired communication links. For example, the data communication link may be Wide Area Network (WAN) or a Local Area Network (LAN). Such a network may be for example a wireless network (Wifi I 802.IIa or b or g), an Ethernet network, an Internet network or a mixed network composed of several different networks. In a particular embodiment of the invention the data communication system may be a broadcast system, such as for example a digital television broadcast system or any other audio visual data supply system in which the server 1210 sends the same data content to multiple clients 1220.</p><p id="p0128" num="0128">The data stream 1240 provided by the server 1210 comprises encoded data including video data encoded into a bitstream in accordance with embodiments of the invention.</p><p id="p0129" num="0129">The client 1220 receives the encoded bitstream and decodes the bitstream in accordance with embodiments of the invention to render the multimedia data on the client terminal. The client terminal may be fixed device such as a television or computer or a portable electronic device including, but not limited to a portable computer, a handheld computer, a tablet computer, a mobile telephone, a media player, a personal digital assistance or the like, including a combination of two or more of these items.</p><p id="p0130" num="0130"><figref idrefs="f0011">FIG. 13</figref> illustrates an exemplary transmitting system 1300. The input media data, for example, audio and video data including HDR image data, are encoded at media encoder 1310 in accordance with embodiments of the invention. The encoded data is multiplexed at multiplexer 1320, and transmitted at transmitter 1330. The transmitting system may be used in a typical TV broadcast environment, or may be used in any service providing audiovisual data for streaming or downloading.</p><p id="p0131" num="0131"><figref idrefs="f0011">FIG. 14</figref> illustrates an exemplary receiving system 1400. The input data of system 1400 may be multimedia data encoded in a bitstream, for example, the output of system 1300. The data is received at receiver 1410, de-multiplexed at de-multiplexer 1420, and then decoded at media decoder 1430 by applying decoding methods in accordance with embodiments of the invention. Decoded packets can be placed in a buffer of de-multiplexer 1420. Rendering Engine 1440 renders the media content, for example to display HDR images<!-- EPO <DP n="31"> --></p><p id="p0132" num="0132">The devices 1300 and 1400 may be configured to have access to information on the reference lighting conditions of the display such as a maximal environmental brightness value Yn of the display lighting for encoding and decoding of HDR image data in accordance with the embodiments of the invention.</p><p id="p0133" num="0133">For example, the devices 1300 and 1400 store the same reference lighting conditions of the display such as a maximal environmental brightness value Yn of the display lighting.</p><p id="p0134" num="0134">Alternatively, the device 1400 is configured to obtain the reference lighting conditions of the display such as a maximal environmental brightness value Yn of the display lighting and to send it to the device 1300. The device 1300 is then configured to receive transmitted reference lighting conditions of the display such as a maximal brightness value Yn of the display lighting.</p><p id="p0135" num="0135">Inversely, the device 1300 is configured to obtain the reference lighting conditions of the display such as maximal environmental brightness value Yn of the displaying lighting, for example from a storage memory, and to send it to the device 1400. The device 1400 is then configured to receive such a transmitted reference lighting conditions of the display such a maximal environmental brightness environmental value Yn of the display lighting.</p><p id="p0136" num="0136">Embodiments of the invention described herein may be implemented in, for example, a method or process, an apparatus, a software program, a data stream, or a signal. Even if only discussed in the context of a single form of implementation (for example, discussed only as a method), the implementation of features discussed may also be implemented in other forms (for example, an apparatus or program). An apparatus may be implemented in, for example, appropriate hardware, software, and firmware. The methods may be implemented in an apparatus such as, for example, a processor. The term processor refers to processing devices in general, including, for example, a computer, a microprocessor, an integrated circuit, or a programmable logic device. Processors may also include communication devices, such as, for example, computers, tablets, cell phones, portable/personal digital assistants ("PDAs"), and other devices that facilitate communication of information between end-users.</p><p id="p0137" num="0137">Reference to "one embodiment" or "an embodiment" or "one implementation" or "an implementation" of the present principles, as well as other variations thereof,<!-- EPO <DP n="32"> --> mean that a particular feature, structure, characteristic, and so forth described in connection with the embodiment is included in at least one embodiment of the present principles. Thus, the appearances of the phrase "in one embodiment" or "in an embodiment" or "in one implementation" or "in an implementation", as well any other variations, appearing in various places throughout the specification are not necessarily all referring to the same embodiment.</p><p id="p0138" num="0138">Additionally, the present description or claims may refer to "determining" various pieces of information. Determining the information may include one or more of, for example, estimating the information, calculating the information, predicting the information, or retrieving the information from memory.</p><p id="p0139" num="0139">Additionally, the present description or claims may refer to "receiving" various pieces of information. Receiving is, as with "accessing", intended to be a broad term. Receiving the information may include one or more of, for example, accessing the information, or retrieving the information (for example, from memory). Further, "receiving" is typically involved, in one way or another, during operations such as, for example, storing the information, processing the information, transmitting the information, moving the information, copying the information, erasing the information, calculating the information, determining the information, predicting the information, or estimating the information.</p><p id="p0140" num="0140">Although the present invention has been described hereinabove with reference to specific embodiments, it will be appreciated that the present invention is not limited to the specific embodiments, and modifications will be apparent to a skilled person in the art which lie within the scope of the present invention.</p><p id="p0141" num="0141">For instance, while in the foregoing examples an encoding or decoding process based on a HEVC coding process has been described it will be appreciated that the invention is not limited to any specific encoding or decoding process. Other encoding or decoding processes applicable to the encoding of LDR images may be applied in the context of the invention. For example the encoding process and complementary decoding process may be based on other encoding/decoding methods involving some encoding strategy optimization step such as MPEG2, MPEG4, AVC, H.263 and the like.</p><p id="p0142" num="0142">Many further modifications and variations will suggest themselves to those versed in the art upon making reference to the foregoing illustrative embodiments,<!-- EPO <DP n="33"> --> which are given by way of example only and which are not intended to limit the scope of the invention, that being determined solely by the appended claims. In particular the different features from different embodiments may be interchanged, where appropriate.</p></description><claims mxw-id="PCLM90459175" lang="EN" load-source="patent-office"><!-- EPO <DP n="34"> --><claim id="c-en-0001" num="0001"><claim-text>A method of encoding or decoding at least part of a high dynamic range image, the image being defined in a color space of high dynamic range, the method comprising for a segment of the at least part of the image:
<claim-text>converting reference samples for prediction of the segment into a local LDR space of the segment to be predicted, the reference samples having been previously reconstructed in their associated local LDR space, and</claim-text>
<claim-text>predicting the segment using the converted reference samples, for encoding or decoding of the segment using an encoding or decoding technique applicable to an LDR image.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>A method according to claim 1 wherein the conversion of the reference samples comprises:
<claim-text>scaling of the reference samples from their respective reconstructed LDR space to a local perceptual space of high dynamic range;</claim-text>
<claim-text>reconstruction of the scaled reference samples in the original HDR space of the image;</claim-text>
<claim-text>mapping of the reconstructed scaled reference samples to the local perceptual space of the segment to be predicted; and</claim-text>
<claim-text>reduction of the dynamic range of the converted reference samples to the LDR space used for encoding/decoding of the segment to be predicted.</claim-text></claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>A method according to claim 2 wherein the reconstruction of the scaled samples in the original HDR space is dependent upon common representative luminance components respectively associated with the reference samples.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>A method according to claim 2 or 3 wherein the mapping of the reconstructed scaled samples to the local HDR space of the segment to be predicted is dependent upon a common representative luminance component value associated with the segment to be predicted.<!-- EPO <DP n="35"> --></claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>A method according to claim 3 or 4 wherein a said common representative luminance component value for a reference sample is obtained based on the luminance values of the image samples of an image segment to which the reference sample belongs</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>A method according to claim 4 or 5 wherein the common representative luminance component value for the segment to be predicted is obtained based on the luminance values of the image samples of the said segment.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>A method according to any preceding claim further comprising encoding the segment of the at least part of the image using an encoding process applicable to a low dynamic range (LDR) image by applying a coding parameter set including at least one coding parameter; reconstructing the encoded segment in the space of high dynamic range; evaluating a rate distortion cost for the encoded segment in the space of high dynamic range; and adjusting said coding parameter set for the encoding process of the segment based on the evaluated rate distortion cost.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>A method according to claim 7 when dependent on claim 6 wherein evaluating the rate distortion cost comprises evaluating the rate associated with encoding of the common representative component value of the segment to be encoded.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>A method according to claim 8 comprising obtaining for the segment to be predicted at least one local residual luminance component in a local space of low dynamic range, said at least one local residual luminance component corresponding to the differential between the corresponding luminance component of the original image and the common representative luminance value of the segment.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>A method according to claim 9 further comprising obtaining for the segment at least one image portion in the local perceptual space, said at least one image portion corresponding to the local residual luminance component or the color component of the image portion, normalized according to the at least one common representative luminance value of the segment.<!-- EPO <DP n="36"> --></claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>A method according to claim 10 wherein evaluating the rate distortion cost comprises evaluating the rate associated with encoding of the said at least one image portion.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>A method according to any one of claims 7 to 11 wherein evaluating the rate distortion cost comprises evaluating the distortion associated with reconstruction of the encoded segment in the perceptual space of high dynamic range.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>A method according to any preceding claim further comprising performing virtual lossless refinement between samples of the residual image portion reconstructed in the local perceptual space and samples of the original texture and the corresponding samples of the said image.</claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>An encoding device for encoding at least part of an image of high dynamic range defined in a perceptual space having a luminance component and a color difference metric, the device comprising:
<claim-text>a reference sample converter for converting reference samples for prediction of the segment into the LDR space of an image segment to be predicted, the reference samples having been previously reconstructed in their associated LDR space, and</claim-text>
<claim-text>a prediction module for predicting the segment using the converted reference samples,</claim-text>
<claim-text>an encoder for encoding the segment using an encoding technique applicable to an LDR image.</claim-text></claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>A decoding device for decoding at least part of an image of high dynamic range defined in a perceptual space having a luminance component and a color difference metric, the device comprising:
<claim-text>a decoder for decoding an image segment using an decoding technique applicable to an LDR image.<!-- EPO <DP n="37"> --></claim-text>
<claim-text>a reference sample converter for converting reference samples for prediction of the segment into the LDR space of the segment to be predicted, the reference samples having been previously reconstructed in their associated LDR space, and</claim-text>
<claim-text>a prediction module for predicting the segment using the converted reference samples.</claim-text></claim-text></claim><claim id="c-en-0016" num="0016"><claim-text>An encoding device according to claim 14 or a decoding device according to claim 15 wherein the reference sample converter is configured to<br/>
scale the reference samples from their respective reconstructed LDR space to a local perceptual space of high dynamic range;<br/>
reconstruct the scaled reference samples in the original HDR space of the image;<br/>
map the reconstructed scaled reference samples to the local perceptual space of the segment to be predicted; and<br/>
reduce the dynamic range of the converted reference samples to the LDR space used for encoding/decoding of the segment to be predicted.</claim-text></claim><claim id="c-en-0017" num="0017"><claim-text>A computer program product for a programmable apparatus, the computer program product comprising a sequence of instructions for implementing a method according to any one of claims 1 to 13 when loaded into and executed by the programmable apparatus.</claim-text></claim></claims><drawings mxw-id="PDW20421911" load-source="patent-office"><!-- EPO <DP n="38"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="104" he="232" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="134" he="156" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="158" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="155" he="168" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="42"> --><figure id="f0005" num="5,6"><img id="if0005" file="imgf0005.tif" wi="150" he="188" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="43"> --><figure id="f0006" num="7"><img id="if0006" file="imgf0006.tif" wi="160" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="44"> --><figure id="f0007" num="8"><img id="if0007" file="imgf0007.tif" wi="155" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="45"> --><figure id="f0008" num="9"><img id="if0008" file="imgf0008.tif" wi="113" he="195" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="46"> --><figure id="f0009" num="10"><img id="if0009" file="imgf0009.tif" wi="120" he="194" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="47"> --><figure id="f0010" num="11,12"><img id="if0010" file="imgf0010.tif" wi="148" he="188" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="48"> --><figure id="f0011" num="13,14"><img id="if0011" file="imgf0011.tif" wi="157" he="187" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
