<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2961135-A1" country="EP" doc-number="2961135" kind="A1" date="20151230" family-id="53513913" file-reference-id="312929" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451207" ucid="EP-2961135-A1"><document-id><country>EP</country><doc-number>2961135</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15001795-A" is-representative="YES"><document-id mxw-id="PAPP193865382" load-source="docdb" format="epo"><country>EP</country><doc-number>15001795</doc-number><kind>A</kind><date>20150617</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865383" load-source="patent-office" format="original"><country>EP</country><doc-number>15001795.2</doc-number><date>20150617</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162034298" ucid="IL-23327514-A" load-source="docdb"><document-id format="epo"><country>IL</country><doc-number>23327514</doc-number><kind>A</kind><date>20140619</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988522513" load-source="docdb">H04L  29/08        20060101AFI20151120BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988527105" load-source="docdb">H04W   4/02        20090101ALI20151120BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1740202065" load-source="docdb" scheme="CPC">H04W   4/026       20130101 LI20171031BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1740202066" load-source="docdb" scheme="CPC">H04W   4/023       20130101 LI20171031BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1740202067" load-source="docdb" scheme="CPC">H04L  67/26        20130101 LI20171031BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1740202068" load-source="docdb" scheme="CPC">H04L  67/306       20130101 LI20171031BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984696445" load-source="docdb" scheme="CPC">H04M   3/42        20130101 LI20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700662" load-source="docdb" scheme="CPC">H04W  68/005       20130101 LI20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984705063" load-source="docdb" scheme="CPC">H04W   4/027       20130101 FI20151224BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545087" lang="DE" load-source="patent-office">VERFAHREN UND SYSTEM ZUM ERHALTEN VON DISTANZIERTEM AUDIO DURCH EINE TRAGBARE VORRICHTUNG</invention-title><invention-title mxw-id="PT165545088" lang="EN" load-source="patent-office">METHOD AND SYSTEM FOR OBTAINING DISTANCED AUDIO BY A PORTABLE DEVICE</invention-title><invention-title mxw-id="PT165545089" lang="FR" load-source="patent-office">PROCÉDÉ ET SYSTÈME PERMETTANT D'OBTENIR UN SIGNAL AUDIO À DISTANCE AU MOYEN D'UN DISPOSITIF PORTABLE</invention-title><citations><patent-citations><patcit mxw-id="PCIT335742780" load-source="docdb" ucid="IL-231527-D0"><document-id format="epo"><country>IL</country><doc-number>231527</doc-number><kind>D0</kind><date>20140831</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335743839" load-source="docdb" ucid="US-20090319348-A1"><document-id format="epo"><country>US</country><doc-number>20090319348</doc-number><kind>A1</kind><date>20091224</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335745053" load-source="docdb" ucid="US-20120295639-A1"><document-id format="epo"><country>US</country><doc-number>20120295639</doc-number><kind>A1</kind><date>20121122</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335742234" load-source="docdb" ucid="US-20130159463-A1"><document-id format="epo"><country>US</country><doc-number>20130159463</doc-number><kind>A1</kind><date>20130620</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit></patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103328199" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>DEUTSCHE TELEKOM AG</last-name><address><country>DE</country></address></addressbook></applicant><applicant mxw-id="PPAR1103305663" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>DEUTSCHE TELEKOM AG</last-name></addressbook></applicant><applicant mxw-id="PPAR1101639631" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Deutsche Telekom AG</last-name><iid>100109692</iid><address><street>Friedrich-Ebert-Allee 140</street><city>53113 Bonn</city><country>DE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103303366" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CHIZI BARAK</last-name><address><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103315602" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CHIZI, BARAK</last-name></addressbook></inventor><inventor mxw-id="PPAR1101652520" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>CHIZI, BARAK</last-name><address><street>6 Savion St.</street><city>Ashkelon</city><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103305034" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>SHAPIRA BRACHA</last-name><address><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103314544" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>SHAPIRA, BRACHA</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647355" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>SHAPIRA, BRACHA</last-name><address><street>35 Stroma St.</street><city>Beer Sheva</city><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103341823" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>MIMRAN DAVID DUDU</last-name><address><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103303919" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>MIMRAN, DAVID (DUDU)</last-name></addressbook></inventor><inventor mxw-id="PPAR1101650228" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>MIMRAN, DAVID (DUDU)</last-name><address><street>33 Shpinoza St.</street><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101646308" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Graf von Stosch, Andreas</last-name><suffix>et al</suffix><iid>101056440</iid><address><street>Graf von Stosch Patentanwaltsgesellschaft mbH Prinzregentenstrasse 22</street><city>80538 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660603815" load-source="docdb">AL</country><country mxw-id="DS660681469" load-source="docdb">AT</country><country mxw-id="DS660681405" load-source="docdb">BE</country><country mxw-id="DS660782173" load-source="docdb">BG</country><country mxw-id="DS660606000" load-source="docdb">CH</country><country mxw-id="DS660681406" load-source="docdb">CY</country><country mxw-id="DS660681407" load-source="docdb">CZ</country><country mxw-id="DS660603817" load-source="docdb">DE</country><country mxw-id="DS660681423" load-source="docdb">DK</country><country mxw-id="DS660681424" load-source="docdb">EE</country><country mxw-id="DS660606188" load-source="docdb">ES</country><country mxw-id="DS660782174" load-source="docdb">FI</country><country mxw-id="DS660782175" load-source="docdb">FR</country><country mxw-id="DS660603818" load-source="docdb">GB</country><country mxw-id="DS660681425" load-source="docdb">GR</country><country mxw-id="DS660603819" load-source="docdb">HR</country><country mxw-id="DS660681408" load-source="docdb">HU</country><country mxw-id="DS660606001" load-source="docdb">IE</country><country mxw-id="DS660681426" load-source="docdb">IS</country><country mxw-id="DS660782176" load-source="docdb">IT</country><country mxw-id="DS660681431" load-source="docdb">LI</country><country mxw-id="DS660603820" load-source="docdb">LT</country><country mxw-id="DS660681470" load-source="docdb">LU</country><country mxw-id="DS660782177" load-source="docdb">LV</country><country mxw-id="DS660782178" load-source="docdb">MC</country><country mxw-id="DS660681475" load-source="docdb">MK</country><country mxw-id="DS660681476" load-source="docdb">MT</country><country mxw-id="DS660681477" load-source="docdb">NL</country><country mxw-id="DS660606002" load-source="docdb">NO</country><country mxw-id="DS660681478" load-source="docdb">PL</country><country mxw-id="DS660606189" load-source="docdb">PT</country><country mxw-id="DS660683934" load-source="docdb">RO</country><country mxw-id="DS660606190" load-source="docdb">RS</country><country mxw-id="DS660681483" load-source="docdb">SE</country><country mxw-id="DS660603822" load-source="docdb">SI</country><country mxw-id="DS660606007" load-source="docdb">SK</country><country mxw-id="DS660606008" load-source="docdb">SM</country><country mxw-id="DS660681432" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479577" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A system and method for providing distanced audio from a remote location using mobile devices, according to which a change in the orientation of a destination mobile device to a pointing orientation, aimed to an object of interest, is detected using an exploration application that is installed in advance on the mobile device. The azimuth to which the mobile device is currently pointing is calculated and all point and/or of objects of interest which are within a predetermined range from the pointing device, are detected. Then the exploration application automatically connects to a remote server which continuously collects the current location information of any user of a mobile device and maps its location with respect to stationary object or points of interests, or to moving objects. The server provides via the application, one or more audio alerts to the user of the destination device that one or more object or points of interest are in range. The user may interact with one or more object or points of interest. The user may also scan the surrounding space by obtaining the overall audio from all objects at once.</p></abstract><abstract mxw-id="PA166759389" lang="EN" source="EPO" load-source="docdb"><p>A system and method for providing distanced audio from a remote location using mobile devices, according to which a change in the orientation of a destination mobile device to a pointing orientation, aimed to an object of interest, is detected using an exploration application that is installed in advance on the mobile device. The azimuth to which the mobile device is currently pointing is calculated and all point and/or of objects of interest which are within a predetermined range from the pointing device, are detected. Then the exploration application automatically connects to a remote server which continuously collects the current location information of any user of a mobile device and maps its location with respect to stationary object or points of interests, or to moving objects. The server provides via the application, one or more audio alerts to the user of the destination device that one or more object or points of interest are in range. The user may interact with one or more object or points of interest. The user may also scan the surrounding space by obtaining the overall audio from all objects at once.</p></abstract><description mxw-id="PDES98404278" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b><u>Field of the Invention</u></b></heading><p id="p0001" num="0001">The present invention relates to the field of monitoring systems. More particularly, the invention relates to a system and method for obtaining distanced audio from a remote location using mobile devices.</p><heading id="h0002"><b><u>Background of the Invention</u></b></heading><p id="p0002" num="0002">Using a mobile device (such as a cellular phone, a tablet a laptop computer etc.) for obtaining audio information from a remote location is a known method, according to which a cellular mobile device at the remote location is used as an "active microphone", which transmits audio data from the remote location (the source device) to another mobile device (the destination device), which collects the desired audio information. Audio data transmission from the source to the destination may be in the form of sending an audio file (recordation) which is performed by a user of the source device via a data network (such as the internet, WiFi, cellular or any other accessible wireless network). Alternatively, audio data transmission from the source to the destination may be in the form of audio streaming, where the microphone of the source is activated and transmits real time audio from the remote location. However, these existing solutions require dialing to the user of the source device and placing a call for obtaining the audio, and therefore, depend on the user's availability and cooperation.<!-- EPO <DP n="2"> --></p><p id="p0003" num="0003">It is therefore desired independently obtain distanced audio from a remote location using mobile devices.</p><p id="p0004" num="0004">It is an object of the present invention to provide a system and method for obtaining distanced audio from a remote location using mobile devices, which do not require dialing to the user of a mobile device and placing a call for obtaining the audio.</p><p id="p0005" num="0005">It is another additional object of the present invention to provide a system and method for obtaining distanced audio from a remote location using mobile devices, which independent of their user's availability and cooperation.</p><p id="p0006" num="0006">Other objects and advantages of the invention will become apparent as the description proceeds.</p><heading id="h0003"><b><u>Summary of the Invention</u></b></heading><p id="p0007" num="0007">The present invention is directed to a method for providing distanced audio from a remote location using mobile devices, according to which a change in the orientation of a destination mobile device to a pointing orientation, aimed to an object of interest, is detected using an exploration application that is installed in advance on the mobile device. The azimuth to which the mobile device is currently pointing is calculated and all point and/or of objects of interest which<!-- EPO <DP n="3"> --> are within a predetermined range from the pointing device, are detected. Then the exploration application automatically connects to a remote server which continuously collects the current location information of any user of a mobile device and maps its location with respect to stationary object or points of interests, or to moving objects. The server provides via the application, one or more audio alerts to the user of the destination device that one or more object or points of interest are in range. The user may interact with one or more object or points of interest. The user may also scan the surrounding space by obtaining the overall audio from all objects at once.</p><p id="p0008" num="0008">The pointing orientation may be automatically detected by using readings from the inherent location and orientation sensors of the mobile device. The point and/or of object of interest which that will be closer may be displayed first to the user.</p><p id="p0009" num="0009">In one aspect, a location map may be displayed with the detection range and the relative location between the destination device and changes in the location of the object of interest, along with a navigation path to the object of interest.</p><heading id="h0004"><b><u>Brief Description of the Drawings</u></b></heading><p id="p0010" num="0010">In the drawings:<!-- EPO <DP n="4"> -->
<ul><li>- <figref idrefs="f0001">Fig. 1</figref> is a schematic illustration of types of object and other users in an explored space; and</li><li><figref idrefs="f0002 f0003 f0004 f0005">Figs. 2-5</figref> schematically illustrate exemplary interfaces of the exploration application, when a friend is in range of the destination device.</li></ul></p><heading id="h0005"><b><u>Detailed Description of Preferred Embodiments</u></b></heading><p id="p0011" num="0011">The system and method of the present invention are capable of providing distanced audio from a remote location using mobile devices, which with no need for dialing to the user of a mobile device and placing a call for obtaining the audio, and are independent of their user's availability and cooperation. This way, the user of the mobile device at the destination can obtain more information and knowledge regarding point of interests, people or any physical object in a remote location, while avoiding any interaction with other users located in his vicinity.</p><p id="p0012" num="0012">According to the proposed method, the distanced audio can be obtained by using the destination mobile device as a pointing device, which may be aimed to an object of interest. For example, the user may walk in a street and point with his mobile device towards a bar nearby. A exploration application, which will be installed in advance on the mobile device, will automatically detect that the device is currently in its pointing orientation by using readings from its inherent sensors, such as acceleration sensors and orientation sensors (e.g., a gyro), as well as location data from the GPS. In turn, the application will calculate the azimuth to which the mobile device is currently pointing and detect that specific bar, as<!-- EPO <DP n="5"> --> well as all point and/or of objects of interest which are within a predetermined range from the pointing device. The point and/or of object of interest which will be closer will be displayed first.</p><p id="p0013" num="0013">The exploration application will automatically connect to a remote server (via any data network that has access to the internet), which continuously collects the current location information of any user of a mobile device (source or destination devices which has the exploration application) and maps its location with respect to stationary object or points of interests (the location of which is stored in a database), or to moving objects, such as other users. The database will also have the association between moving objects, such as friends or relatives of each user, so whenever one of them will be detected to be in range, the user of the destination device will get an alert that one of more fiends of him are in range.</p><p id="p0014" num="0014">This process will allow the user to obtain the music inside the bar, without entering the bar. Furthermore, the user will be able to discover any object or a point of interest that he encounters, just by pointing toward the object and gaining its sound signature. The sound signature he receives in turn will be correlated with the object or with its meaning.</p><p id="p0015" num="0015">According to an embodiment of the invention, a user can use his device for scanning the surrounding space by obtaining the overall audio from all objects at<!-- EPO <DP n="6"> --> once. This will enable the user to discover objects in the entire space by receiving the surrounding sounds, as illustrated in <figref idrefs="f0001">Fig. 1</figref>.</p><p id="p0016" num="0016">According to another embodiment, a user can use his mobile device to obtain knowledge regarding other user in his vicinity.</p><heading id="h0006"><u>Example 1: Another user (holding the source device) is standing nearby</u></heading><p id="p0017" num="0017">The user holding the destination device can obtain indications regarding the emotions (sad, happy etc.) of the other user by gaining an audio signal that is correlated with a specific emotion. Another option is to obtain the status of a user (available, busy etc.) for any kind of communication. This will be done also by getting an audio alert that reflects this status.</p><heading id="h0007"><u>Example 2: A Distanced User</u></heading><p id="p0018" num="0018">A user can locate other users by using his (destination) device to sense their audio. When the user will point toward a specific direction an audio signal (alert) will appear if another user is located in that direction. This audio signal will be the sound signature of the user from a database of sound signatures and their association with users. A technique of generating such a database is described for example in Israeli patent application <patcit id="pcit0001" dnum="IL231527"><text>IL 231527</text></patcit>. According to this technique, a database of sound signatures which are uniquely associated with entities that are linked to a communication system is created, such that each entity records and uploads its profile and characteristic sound alerts to the database and then<!-- EPO <DP n="7"> --> submits to the database an application for registering a sound signature which includes association to the entity. An official authority accepts or denies the registration of sound signatures as records in the database by examining the application and verifying that it complies with a set of predetermined criteria. if it complies, a license is granted to the sound signature database which is stored with association to the entity and its context. Each characteristic sound alert is analyzed according to predetermined criteria, to verify that it is appropriate and granting a permit to each examined characteristic sound alert that met the criteria. Finally, characteristic sound alerts that met the criteria are stored in the database, along with an association to the entity and his status, to be used as an alert for increasing awareness of users of the communication system to the entities.</p><p id="p0019" num="0019"><figref idrefs="f0002">Fig. 2</figref> illustrates an exemplary interface of the exploration application, when a friend is in range of the destination device. Here, the exploration application on the device detected that the user of the destination device pointed it to a direction (e.g., detected a change in its orientation in the user's pocket) and that the user named Diana Polasek (who is the user of the source device and also a friend of the user of the destination device) is in range and displays an icon 201 with her face, along with an indication 202 of the distance to her.</p><p id="p0020" num="0020">The exploration of the space and the detection of Diana were performed by the following process:<!-- EPO <DP n="8"> -->
<ul><li>First, the application contacts the server and transmits the location of the destination device. In turn, the server queries a database and extracts the list of fiends of the user of the destination device. Then the server checks the location of each one, extracts the sound signature of each friend who is in range from the database and transmits the location data of all friends in range to the application, which displayed them, along with the sound signatures.</li></ul></p><p id="p0021" num="0021">The interface offers the option to communicate with her (using the "locate" button) or to exit, if the user of the destination device does not wish to communicate. If he wishes to communicate, he can activate the "talk" button to speak (as shown in <figref idrefs="f0003">Fig. 3</figref>) or to record a voice message that will be sent to her as an audio file by the application, as shown in <figref idrefs="f0003">Fig. 3</figref>.</p><p id="p0022" num="0022">The application may also display a location map with the detection range (indicated by the circle) and the relative location between the destination device and the current location of her device, as shown in <figref idrefs="f0004">Fig. 4</figref>.</p><p id="p0023" num="0023">The application may also display changes in the location of her device, as shown in <figref idrefs="f0005">Fig. 5</figref>.</p><p id="p0024" num="0024">It is also possible to display a navigation path to her. In addition, the exploration application may also display other friends who are in range and add then to the session, to allow communication or interaction between them, as well.<!-- EPO <DP n="9"> --></p><p id="p0025" num="0025">Each user (of a source device) may use the exploration application to update his current status (such as his mood), in order to provide his status in the form of an audio alert to other users, regarding whether or not he is currently available for interaction.</p><p id="p0026" num="0026">The method proposed by the present invention allows unilateral exploration of the space nearby by a user of a destination mobile device, without the need to dial and establish a connection with other source devices. The user of a destination mobile device (the "exploring" device) only has to point toward a desired direction and in response, the exploration application will automatically detect the object and points/other users of interest on that direction.</p><p id="p0027" num="0027">While some embodiments of the invention have been described by way of illustration, it will be apparent that the invention can be carried out with many modifications, variations and adaptations, and with the use of numerous equivalents or alternative solutions that are within the scope of persons skilled in the art, without exceeding the scope of the claims.</p></description><claims mxw-id="PCLM90459208" lang="EN" load-source="patent-office"><!-- EPO <DP n="10"> --><claim id="c-en-0001" num="0001"><claim-text>A method for providing distanced audio from a remote location using mobile devices, comprising:
<claim-text>a) Detecting a change in the orientation of a destination mobile device to a pointing orientation, aimed to an object of interest, using an exploration application, installed in advance on said mobile device;</claim-text>
<claim-text>b) calculating the azimuth to which the mobile device is currently pointing;</claim-text>
<claim-text>c) detecting all point and/or of objects of interest being within a predetermined range from the pointing device;</claim-text>
<claim-text>d) allowing the exploration application to automatically connect to a remote server which continuously collects the current location information of any user of a mobile device and maps its location with respect to stationary object or points of interests, or to moving objects; and</claim-text>
<claim-text>e) providing by said server and via said application, one or more audio alerts to the user of the destination device that one or more object or points of interest are in range.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>A method according to claim 1, further comprising allowing the user to interact with one or more object or points of interest.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>A method according to claim 1, wherein the pointing orientation is automatically detected by using readings from the inherent location and orientation sensors of the mobile device.<!-- EPO <DP n="11"> --></claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>A method according to claim 1, wherein the point and/or of object of interest which that will be closer will be displayed first.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>A method according to claim 1, further comprising allowing the user to scan the surrounding space by obtaining the overall audio from all objects at once.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>A method according to claim 1, further comprising displaying a location map with the detection range and the relative location between the destination device and changes in the location of the object of interest.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>A method according to claim 1, further comprising displaying a navigation path to the object of interest.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>A system for providing distanced audio from a remote location using mobile devices, comprising:
<claim-text>a) an exploration application, installed in advance on said mobile device, for detecting a change in the orientation of a destination mobile device to a pointing orientation, aimed to an object of interest;</claim-text>
<claim-text>b) processing means for:<!-- EPO <DP n="12"> -->
<claim-text>b.1) calculating the azimuth to which the mobile device is currently pointing;</claim-text>
<claim-text>b.2) detecting all point and/or of objects of interest being within a predetermined range from the pointing device;</claim-text></claim-text>
<claim-text>c) a remote server, to which the exploration application automatically connects, for:
<claim-text>c.1) continuously collecting the current location information of any user of a mobile device and mapping its location with respect to stationary object or points of interests, or to moving objects; and</claim-text>
<claim-text>c.2) providing via said application, one or more audio alerts to the user of the destination device that one or more object or points of interest are in range.</claim-text></claim-text></claim-text></claim></claims><drawings mxw-id="PDW20421944" load-source="patent-office"><!-- EPO <DP n="13"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="121" he="183" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="14"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="121" he="197" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="15"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="121" he="222" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="16"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="121" he="216" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="17"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="121" he="216" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="158" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="158" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
