<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2961152-A1" country="EP" doc-number="2961152" kind="A1" date="20151230" family-id="51390934" file-reference-id="265839" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451190" ucid="EP-2961152-A1"><document-id><country>EP</country><doc-number>2961152</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14754488-A" is-representative="YES"><document-id mxw-id="PAPP193865348" load-source="docdb" format="epo"><country>EP</country><doc-number>14754488</doc-number><kind>A</kind><date>20140204</date><lang>JA</lang></document-id><document-id mxw-id="PAPP193865349" load-source="patent-office" format="original"><country>EP</country><doc-number>14754488.6</doc-number><date>20140204</date><lang>JA</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162034568" ucid="JP-2013031805-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2013031805</doc-number><kind>A</kind><date>20130221</date></document-id></priority-claim><priority-claim mxw-id="PPC162026540" ucid="JP-2014000561-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2014000561</doc-number><kind>W</kind><date>20140204</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1906584551" load-source="docdb">G06F   3/0482      20130101ALI20160812BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1906587570" load-source="docdb">H04N   5/232       20060101AFI20160812BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1906587887" load-source="docdb">G06F   3/0484      20130101ALI20160812BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1984697012" load-source="docdb" scheme="CPC">H04N   5/23216     20130101 FI20160106BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697859" load-source="docdb" scheme="CPC">H04N   5/23212     20130101 LI20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984702262" load-source="docdb" scheme="CPC">G06F   3/0482      20130101 LI20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984703105" load-source="docdb" scheme="CPC">G06F   3/04845     20130101 LI20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984705052" load-source="docdb" scheme="CPC">H04N   5/23293     20130101 LI20151224BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984705224" load-source="docdb" scheme="CPC">H04N   5/23296     20130101 LI20151224BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545036" lang="DE" load-source="patent-office">BILDVERARBEITUNGSVORRICHTUNG, BILDVERARBEITUNGSVERFAHREN UND PERMANENTES COMPUTERLESBARES MEDIUM</invention-title><invention-title mxw-id="PT165545037" lang="EN" load-source="patent-office">IMAGE PROCESSING DEVICE, IMAGE PROCESSING METHOD AND PERMANENT COMPUTER-READABLE MEDIUM</invention-title><invention-title mxw-id="PT165545038" lang="FR" load-source="patent-office">DISPOSITIF DE TRAITEMENT D'IMAGE, PROCÉDÉ DE TRAITEMENT D'IMAGE ET SUPPORT LISIBLE PAR ORDINATEUR PERMANENT</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103320117" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>NEC CORP</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR1103314957" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>NEC CORPORATION</last-name></addressbook></applicant><applicant mxw-id="PPAR1101639711" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Nec Corporation</last-name><iid>101193625</iid><address><street>7-1, Shiba 5-chome Minato-ku</street><city>Tokyo 108-8001</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103322960" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SHIRAKAWA HIROTSUGU</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103312298" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SHIRAKAWA, HIROTSUGU</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647726" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SHIRAKAWA, HIROTSUGU</last-name><address><street>c/o NEC Corporation 7-1, Shiba 5-chome Minato-ku</street><city>Tokyo 108-8001</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101652691" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Vossius &amp; Partner</last-name><iid>100751388</iid><address><street>Siebertstrasse 3</street><city>81675 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="JP-2014000561-W"><document-id><country>JP</country><doc-number>2014000561</doc-number><kind>W</kind><date>20140204</date><lang>JA</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014129127-A1"><document-id><country>WO</country><doc-number>2014129127</doc-number><kind>A1</kind><date>20140828</date><lang>JA</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660782087" load-source="docdb">AL</country><country mxw-id="DS660681222" load-source="docdb">AT</country><country mxw-id="DS660782088" load-source="docdb">BE</country><country mxw-id="DS660605390" load-source="docdb">BG</country><country mxw-id="DS660605175" load-source="docdb">CH</country><country mxw-id="DS660681127" load-source="docdb">CY</country><country mxw-id="DS660681128" load-source="docdb">CZ</country><country mxw-id="DS660603731" load-source="docdb">DE</country><country mxw-id="DS660782089" load-source="docdb">DK</country><country mxw-id="DS660782090" load-source="docdb">EE</country><country mxw-id="DS660681234" load-source="docdb">ES</country><country mxw-id="DS660605391" load-source="docdb">FI</country><country mxw-id="DS660605392" load-source="docdb">FR</country><country mxw-id="DS660603732" load-source="docdb">GB</country><country mxw-id="DS660782091" load-source="docdb">GR</country><country mxw-id="DS660603733" load-source="docdb">HR</country><country mxw-id="DS660681129" load-source="docdb">HU</country><country mxw-id="DS660605176" load-source="docdb">IE</country><country mxw-id="DS660782092" load-source="docdb">IS</country><country mxw-id="DS660605393" load-source="docdb">IT</country><country mxw-id="DS660782093" load-source="docdb">LI</country><country mxw-id="DS660603734" load-source="docdb">LT</country><country mxw-id="DS660681227" load-source="docdb">LU</country><country mxw-id="DS660605394" load-source="docdb">LV</country><country mxw-id="DS660603735" load-source="docdb">MC</country><country mxw-id="DS660681228" load-source="docdb">MK</country><country mxw-id="DS660681229" load-source="docdb">MT</country><country mxw-id="DS660605631" load-source="docdb">NL</country><country mxw-id="DS660605177" load-source="docdb">NO</country><country mxw-id="DS660605632" load-source="docdb">PL</country><country mxw-id="DS660681247" load-source="docdb">PT</country><country mxw-id="DS660603737" load-source="docdb">RO</country><country mxw-id="DS660681248" load-source="docdb">RS</country><country mxw-id="DS660605633" load-source="docdb">SE</country><country mxw-id="DS660683661" load-source="docdb">SI</country><country mxw-id="DS660605178" load-source="docdb">SK</country><country mxw-id="DS660605183" load-source="docdb">SM</country><country mxw-id="DS660782094" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479560" lang="EN" load-source="patent-office"><p id="pa01" num="0001">A storage portion stores (220) an image data and a distance data including a respective subject and a lens focus. A display (180) displays a display image based on the image data, and an operation regarding a depth of field and a focus position being performed. An image adjusting potion (320) detects a first operation and a second operation performed referring to the display (180), and adjusts a focus distance according to the first operation and the distance data as well as adjusts a depth of field according to the second operation and the distance data for the image data to generate the display image.<img id="iaf01" file="imgaf001.tif" wi="165" he="95" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759372" lang="EN" source="EPO" load-source="docdb"><p>A storage portion stores (220) an image data and a distance data including a respective subject and a lens focus. A display (180) displays a display image based on the image data, and an operation regarding a depth of field and a focus position being performed. An image adjusting potion (320) detects a first operation and a second operation performed referring to the display (180), and adjusts a focus distance according to the first operation and the distance data as well as adjusts a depth of field according to the second operation and the distance data for the image data to generate the display image.</p></abstract><description mxw-id="PDES98404261" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>Technical Field</b></heading><p id="p0001" num="0001">The present invention relates to an image processing device, an image processing method, and a non-transitory computer readable media.</p><heading id="h0002"><b>Background Art</b></heading><p id="p0002" num="0002">It is known that in a general single-lens reflex camera a depth of field can be varied by changing a stop of lens. However, in a small image pickup device such as a cellular phone and the like there is a case that the stop cannot be equipped due to a problem of cost or size.</p><p id="p0003" num="0003">Then, in recent years a technique which is called as a light field (light irradiating field) camera is gained attention. In the case of using the light field camera, a depth of field of image data can be changed after imaging. For example, the technique concerning the light field camera is described in patent literature 1 to patent literature 3 and the like. The light field camera is the technique of reconstructing the image data which is acquired by using a plenoptic camera to generate a refocused image data</p><heading id="h0003"><b>Citation List</b></heading><heading id="h0004"><b>Patent Literature</b></heading><p id="p0004" num="0004"><ul><li>patent literature 1: <patcit id="pcit0001" dnum="JP2012191376A"><text>Japanese Unexamined Patent Application Publication No. 2012-191376</text></patcit></li><li>patent literature 2: <patcit id="pcit0002" dnum="JP2012147167A"><text>Japanese Unexamined Patent Application Publication No. 2012-147167</text></patcit></li><li>patent literature 3: <patcit id="pcit0003" dnum="JP2012142918A"><text>Japanese Unexamined Patent Application Publication No. 2012-142918</text></patcit></li></ul><!-- EPO <DP n="2"> --></p><heading id="h0005"><b>Non Patent Literature</b></heading><p id="p0005" num="0005">non patent literature 1: <nplcit id="ncit0001" npl-type="s"><text>Nikkei-electronics 20 August 2012,"arayuru tokoroni kasikoi Camera</text></nplcit>"</p><heading id="h0006"><b>Summary of Invention</b></heading><heading id="h0007"><b>Technical Problem</b></heading><p id="p0006" num="0006">In the field of the afore-mentioned light field camera, the technique to change the depth of field after imaging is generally used. However, it is not sufficiently considered by which kind of interface is used to set the depth of field or focus in changing the depth of field on the user side. Accordingly, the problem occurred that the image data in which the depth of field or focus is adequately set can not be rapidly obtained.</p><p id="p0007" num="0007">The present invention has been made to solve the above-mentioned problems, and has a main object to provide an image processing device, an image processing method, and a non-transitory computer readable media, which have an operation interface to adjust and operate a depth of field in addition to adjust a focus.</p><heading id="h0008"><b>Solution to Problem</b></heading><p id="p0008" num="0008">An aspect of an image processing device in accordance with the present invention is an image processing device comprising:
<ul><li>storage means for storing an image data including a distance data between a respective subject and a lens focus;</li><li>display means for displaying a display image based on the image data, and an operation regarding a depth of field and a focus position being performed; and</li><li>image adjusting means for detecting a first operation and a second operation performed referring to the display means, and adjusting a focus distance according to the first operation and the distance data as well as adjusting a depth of field according to the second operation and the distance data for the image data to generate</li></ul><!-- EPO <DP n="3"> -->
the display image.</p><p id="p0009" num="0009">An aspect of an image processing method in accordance with the present invention is an image processing method comprising:
<ul><li>a display step of displaying a display image based on an image data including a distance data between a respective subject and a lens, and an operation regarding a depth of field and a focus position being performed; and</li><li>an image control step of detecting a first operation and a second operation performed referring to the display image, and adjusting a focus distance according to the first operation and the distance data as well as adjusting a depth of field according to the second operation and the distance data for the image data to generate the display image.</li></ul></p><p id="p0010" num="0010">An aspect of an image processing method in accordance with the present invention is a non-transitory computer readable media for causing a computer to execute:
<ul><li>a step of reading out an image data including a distance data between a respective subject and a lens from a storage device; and<br/>
an image control step of detecting a first operation and a second operation performed referring to the display of the display image based on the image data , and adjusting a focus distance according to the first operation and the distance data as well as adjusting a depth of field according to the second operation and the distance data for the image data to generate the display image.</li></ul></p><heading id="h0009"><b>Advantageous Effects of Invention</b></heading><p id="p0011" num="0011">According to the present invention, it is possible to provide an image processing device, an image processing method, and a non-transitory computer readable media, which have an operation interface to adjust and operate a depth of field in addition to adjust a focus.</p><heading id="h0010"><b>Brief Description of Drawings</b></heading><p id="p0012" num="0012"><!-- EPO <DP n="4"> --><ul><li><figref idrefs="f0001">Fig. 1</figref> is a block diagram showing a configuration of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0002">Fig. 2</figref> is a block diagram schematically showing a control structure of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0003">Fig. 3</figref> is a concept diagram showing an operation example of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0004">Fig. 4</figref> is a diagram showing slide bar S1 displayed on display 180 according to an exemplary embodiment 1;</li><li><figref idrefs="f0005">Fig. 5</figref> is a diagram showing slide bar S1 displayed on display 180 according to an exemplary embodiment 1;</li><li><figref idrefs="f0006">Fig. 6</figref> is a concept diagram showing an operation example of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0007">Fig. 7</figref> is a concept diagram showing an operation example of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0008">Fig. 8</figref> is a concept diagram showing an operation example of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0009">Fig. 9A</figref> is a concept diagram showing a focus position set according to an exemplary embodiment 1;</li><li><figref idrefs="f0009">Fig. 9B</figref> is a concept diagram showing a focus position set according to an exemplary embodiment 1;</li><li><figref idrefs="f0010">Fig. 10</figref> is a concept diagram showing an operation example of an image processing device 1 according to an exemplary embodiment 1;</li><li><figref idrefs="f0011">Fig. 11</figref> is a diagram showing slide bar S1 displayed on display 180 according to an exemplary embodiment 1; and</li><li><figref idrefs="f0012">Fig. 12</figref> is a block diagram showing a configuration of an image processing device 1 according the present invention.</li></ul></p><heading id="h0011"><b>Description of Embodiments</b></heading><heading id="h0012"><b>&lt;Example 1&gt;</b></heading><p id="p0013" num="0013">Exemplary embodiments of the present invention will be described below with reference to the drawings. <figref idrefs="f0001">Fig. 1</figref> is a block diagram showing a configuration of an image processing device according to an exemplary embodiment 1. Note that hereinafter image<!-- EPO <DP n="5"> --> processing device is explained as a smart phone, however it is not necessarily limited to this. Image processing device 1 may be an optional terminal having an operation interface (operation portion) such as a touch panel and the like.</p><p id="p0014" num="0014">The image processing device 1 includes an image portion 100, RAM (Random Access memory) 110, an image processing circuit 120, a YC converting circuit 130, a compressing and extending circuit 140, a median controller 150, a memory card 160, a LCD driver 170, a display 180, a flash memory 190, CPU (Central Processing Unit) 200, and data bus 210.</p><p id="p0015" num="0015">The image portion 100 includes a lens 101, a micro array lens 102, a CCD (Charged Coupled Device) 103, and a data processing portion 104. The lens 101 is a main lens for imaging a subject. The micro array lens 102 is for example a plurality of micro lens which are two-dimensionally in a matrix disposed, and is disposed on an image formation face of the lens 101. Each of the micro lens whose plane shape, is a circle shape or a hexagon shape is constituted of a solid lens, a liquid crystal lens, a liquid lens, a diffraction lens and the like.</p><p id="p0016" num="0016">In the CCD 103 a plurality of light receiving elements for accumulating an electric charge according to light receiving amount entered as a light from the micro array lens 102 via its light receiving face is disposed. The CCD 103 outputs an electric charge which is accumulated in the respective light receiving element by imaging as an image signal. The micro array lens 102 is constituted of a plurality of micro lens. Accordingly, a physical distance between respective subject and lens focus ( for example in case of imaging a child the physical distance between the position of the lens focus of the image portion 101 and the position of the child (10m)) can be obtained. Note that a CMOS (Complementary Metal-Oxide Semiconductor) may be used instead of the CCD 103. The data processing portion 104 is constituted of for example a CDS (Correlated Double Sampling), an<!-- EPO <DP n="6"> --> amplifier, A/D (Analog/ Digital) converter and the like, performs a general process in a general image device, and generates an image data including a distance between the respective subject within image range and the lens focus to store in RAM 110.</p><p id="p0017" num="0017">Note that the afore-mentioned constitution of the image portion 100 is an only exemplification. The constitution may be optional if image data which includes a distance between the respective subject within image range and the lens focus can be obtained. The constitution may be known lens module for realizing the structure of the light field camera. For example, the image portion 100 may include a lens for a right eye and a lens for a left eye, and may generate a parallax image data by the respective lens. Regarding the process of obtaining the distance data of the subject by the light field camera and the like, the structure is same as the structure described in non patent literature 1 and the like, accordingly, properly refer to it.</p><p id="p0018" num="0018">The image processing circuit 120 executes the process such as a gray scale conversion, a white balance correction, a gamma correction and the like. YC converting circuit 130 converts a data after imaging to a brightness signal Y and a color difference signal Cr, Cb. The compressing and extending circuit 140 compresses image data at an optional timing according to a predetermined compression format to write in an optional storage portion ( for example memory card 160, flash memory 190).</p><p id="p0019" num="0019">The median controller 150 writes and read all sorts of data for the memory card 160. The memory card 160 is a storage portion in which data is read and written via the median controller 150. The memory card 160 is constituted to be capable of being attached and detached to the image processing device 1. The display 180 is driven by LCD driver 170. The LCD driver 170 converts image data which is read out from all sorts of storage portion (for example the memory card 160, the flash memory 190 and the like) and is performed digital<!-- EPO <DP n="7"> --> processing by the CPU 200 to for example an analog signal in NSC (National Television System Committee) format, and outputs the converted an analog signal to the display 180.</p><p id="p0020" num="0020">The display 180 is provided on the front face of the image processing device 1 (smart phone). The display 180 is realized by a crystal display device, an organic EL (Electro Luminescence) display and the like, however it is not necessarily limited to this. The display 180 corresponds to a multi-touch such as a capacitance type, a resistance film type, an electromagnetic type and the like. The display 180 functions in an image mode as an electric view finder to display the image in real time. Further, the display 180 functions in a playback mode as a monitor to play back and display the image stored in all sorts storage (for example the memory card 160, the flash memory 190 and the like).</p><p id="p0021" num="0021">The display includes a touch sensor 181. The touch sensor 181 detects a contact position when a user's hand or finger contacts a display screen of the display 180. Then, the touch sensor 181 detects operations such as the so called pinch-in (an operation narrowing distance between two fingers), pinch-out (an operation widening distance between two fingers) and the like.</p><p id="p0022" num="0022">The flash memory 190 is a storage portion that stores all sorts of data (including image data). Note that the image processing device 1 properly includes a HDD (a hard disk drive) not shown as a storage portion.</p><p id="p0023" num="0023">CPU (Central Processing Unit) 200 is electrically connected to the respective processing portion in the image processing device 1 via the data bus 210. Data is inputted into the CPU 200 from all sorts of switches (for example a source switch) on the not-shown image processing device 1 (a smart phone in this example) and the afore-mentioned touch sensor 181. The CPU 200 reads out a program from<!-- EPO <DP n="8"> --> all sorts of storage devices (the not-shown HDD, memory card 160, flash memory 190 and the like) and controls the respective processing portion as well as performs the process of generating a display image displayed on the display 180 from the image data and distance data generated by the image portion. The generating process of the display image will be in detail explained referring to <figref idrefs="f0002">Fig. 2</figref>.</p><p id="p0024" num="0024">The control structure shown in <figref idrefs="f0002">Fig. 2</figref> (an image data read-out portion 300, an sensor information input portion 310, an image adjusting portion 320, an adjusting interface control portion 330) is realized by executing a program by CPU 200.</p><p id="p0025" num="0025">The program can be stored and provided to a computer using any type of non-transitory computer readable media. Non-transitory computer readable media include any type of tangible storage media. Examples of non-transitory computer readable media include magnetic storage media (such as floppy disks, magnetic tapes, hard disk drives, etc.), optical magnetic storage media (e.g. magneto-optical disks), CD-ROM (compact disc read only memory), CD-R (compact disc recordable), CD-R/W (compact disc rewritable), and semiconductor memories (such as mask ROM, PROM (programmable ROM), EPROM (erasable PROM), flash ROM, RAM (random access memory), etc.). The program may be provided to a computer using any type of transitory computer readable media. Examples of transitory computer readable media include electric signals, optical signals, and electromagnetic waves. Transitory computer readable media can provide the program to a computer via a wired communication line (e.g. electric wires, and optical fibers) or a wireless communication line.</p><p id="p0026" num="0026">The image data read-out portion 300 successively reads out the image data taken by the image portion 100 and stored in the respective storage portion (the memory card 160, flash memory 190, the not-shown hard disk and the like) and the distance data related with the<!-- EPO <DP n="9"> --> image data. Then, the image data read-out portion 300 successively reads out the image data and the distance data in case that the image portion 100 takes a moving picture. The image data read-out portion 300 provides the read-out image data and distance data for the image adjusting portion 320 and the adjusting interface control portion 330.</p><p id="p0027" num="0027">The movement of the finger on the display 180 detected by the touch sensor 181 is inputted into the sensor information input portion 310. In detail, a direction of the display 180 (a direction of the display 180 acquired by the not-shown acceleration sensor), a position of a finger or a contact state of fingers (how many fingers contacts on the display 180), a movement distance of a finger, a movement direction are inputted into the sensor information input portion 310. The sensor information input portion 310 provides the acquired information of the movement of finger on the display 180 ( the position of finger, the contact number of fingers, the movement distance of finger, the direction of the display 180) for the image adjusting portion 320 and the adjusting interface control portion 330.</p><p id="p0028" num="0028">The image adjusting portion 320 detects whether a first operation and a second operation is performed based on the movement distance of finger, the position of finger, and the movement direction of finger. The first operation is an operation to adjust a focus, and an example of the operation is later described referring to <figref idrefs="f0003">Fig. 3</figref> and the like. The second operation is an operation to adjust a depth of field and an example of the operation is later described referring to <figref idrefs="f0003">Fig. 3</figref> and the like.</p><p id="p0029" num="0029">The image adjusting portion 320 adjusts the focus of the image data read out by the image data read-out portion 300 according to the detection of the first operation to adjust the focus. In the adjustment the distance data read out by the image data read-out portion 300 is used. Then, the image adjusting portion 320 may adjust a focal distance (adjust a focal position) performed in a technique field of a<!-- EPO <DP n="10"> --> general light field camera.</p><p id="p0030" num="0030">Furthermore, the image adjusting portion 320 adjusts the depth of field of the image data read out by the image data read-out portion 300 according to the detection of the second operation to adjust the depth of field. In the adjustment the distance data read out by the image data read-out portion 300 is used. Then, the image adjusting portion 320 may adjust a focal distance performed in a technique field of a general light field camera.</p><p id="p0031" num="0031">The adjusting interface control portion 330 detects whether a first operation and a second operation is performed based on the movement distance of finger, the position of finger, and the movement direction of finger. The adjusting interface control portion 330 controls the display of an adjusting interface (preferably a slide bar shown in <figref idrefs="f0003">Fig. 3</figref>) displayed on the display 180 according to the detection. As after-mentioned, the slide bar is displayed with overlapping on the display image.</p><p id="p0032" num="0032">Next, the respective operation on the display 180 according to the exemplary embodiment. <figref idrefs="f0003">Fig. 3</figref> is a concept diagram showing a operation example of the display 180. Nora that in the below explanation it is assumed that the display is operated by using a thumb and a forefinger of user's right hand, however any of ten fingers of both hands may be used.</p><p id="p0033" num="0033">The slide bar S1 is displayed on the display 180. The height length of the slide bar S1 indicates all focus range of the taken image data. An upper direction of the slide bar S1 indicates a distant side (Far) and a lower direction indicates this side (Near). In a knob S2 of the slide bar S1 an upper side indicates a focusing end of the distant side and a lower side indicates a focusing end of the near side.</p><p id="p0034" num="0034">In case user selects a reference mode (a mode for reading a<!-- EPO <DP n="11"> --> taken image) of the image processing device, a live view scene appears on the display 180. The image processing device 1 have the afore-mentioned function of the light field camera and acquires the image data and the distance data of the image object and stores them in the storage device.</p><p id="p0035" num="0035">The adjusting interface control portion 330 displays the slide bar S1 on the display 180 with overlapping on the display image when the touch sensor 181 detects the contact of user's finger. As shown in the figure the slide bar S1 has a vertically long shape. Accordingly, in case the touch sensor 181 detects that user touches the right half of the screen, the adjusting interface control portion 330 displays the slide bar S1 on the right end of the display 180. On the other hand, in case the touch sensor 181 detects that user touches the left half of the screen, the adjusting interface control portion 330 displays the slide bar S1 on the left end of the display 180. In case the touch sensor 181 detects that user touches the approximate center portion of the screen in a horizontal direction, the adjusting interface control portion 330 displays the slide bar S1 on the right end of the display 180 considering there are many user with a right hand dominance. Note that the display position of the slide bar S1 may be displayed in the fixed position according to the settings set by user.</p><p id="p0036" num="0036">In case user widens a space between finger F1 and F2 in y axis direction (the so-called pinch-out), the image adjusting portion 320 and the adjusting interface control 330 detects the extension degree (the movement distance of the two fingers in y axis direction) according to input from the touch sensor 181. In the example of <figref idrefs="f0003">fig. 3</figref> the pinch-out (or the pinch-in) in y axis direction becomes the second operation to adjust the depth of field. The adjusting interface control portion 330 widens the width of the knob S2 according to the detected extension degree. The image adjusting portion 320 widens the focusing end of the distant side and the focusing end of the near side and changes the depth of field.<!-- EPO <DP n="12"> --></p><p id="p0037" num="0037">On the other hand, in case user narrows the space between finger F1 and F2 in y axis direction (the so-called pinch-in), the touch sensor 181 detects the reduction degree. The adjusting interface control portion 330 narrows the width of the knob S2 according to the detected reduction degree. The image adjusting portion 320 narrows the focusing end of the distant side and the focusing end of the near side and changes the depth of field. This is effective in case of wanting to display a gradating background with an only subject being focused.</p><p id="p0038" num="0038">In case the space of the user's fingers F1 and F2 is kept and fingers F1 and F2 slides in y axis direction, the touch sensor 181 detects the movement direction and the movement distance. In the example of <figref idrefs="f0003">fig.3</figref>, an operation to keep the space between two fingers F1 and F2 in y axis direction and move them is detected. The touch sensor 181 detects the movement degree. The adjusting interface control portion 330 moves the position of the knob S2 according to the detected movement degree. At this time, the width of the knob S2 (the length from the upper end to the lower end) is not changed. Further, the image adjusting portion adjusts the focus position by simultaneously adjusting the focus end of the distant side and the focus end of the near side according to the detected movement degree. That is, a manual focus operation is realized.</p><p id="p0039" num="0039">Note that the pinch-in (or the pinch-out) in x axis direction may be assigned to a general zoom process. Accordingly, a general zoom process can be realized and the depth of field and the focus position can be adjusted. Then the axis direction is an example. The pinch-in (the pinch-out) in x axis direction may be assigned to the adjustment of the depth of field (the second operation) (that is, the slide bar S1 may be extended and provided in x axis direction).</p><p id="p0040" num="0040">Note that the image adjusting portion 320 may adjust the so-called blur (Bokeh) according to the setting width of the depth of field.<!-- EPO <DP n="13"> --> The specific example is explained referring to <figref idrefs="f0004">Fig. 4</figref> and <figref idrefs="f0005">Fig. 5</figref>. <figref idrefs="f0004">Fig. 4</figref> and <figref idrefs="f0005">Fig. 5</figref> are a diagram showing slide bar S1 displayed on display 180 in displaying the same display image. The depth of field in the slide bar S1 shown in <figref idrefs="f0004">Fig.4</figref> is set to be narrower compared with the depth of field in the slide bar S1 shown in <figref idrefs="f0005">Fig.5</figref> That is, the width of area to be focused becomes small. With the setting width of the depth of field being smaller, the image adjusting portion 320 performs the image process such that the blur of the subject imaged outside the depth of field is stronger. For example, the blur of the subject imaged within A range of <figref idrefs="f0004">Fig.4</figref> is given more strongly than the blur of the subject imaged within A range of <figref idrefs="f0005">Fig.5</figref>. Note that in the image process to give the blur a manner in a general digital image process may be used.</p><p id="p0041" num="0041">In a general single-lens reflex camera in the area outside the depth of field with being farther from the depth of field, the blur is stronger. Here, the strength of the blur in the single-lens reflex camera depends on a lens feature of the camera. For example, in the single-lens reflex camera Z the strength of the blur far from the area indicating the depth of field by 10m is x. However, in the single-lens reflex camera W the strength of the blur far from the area indicating the depth of field by 10m is 2x. Thus, they are different.</p><p id="p0042" num="0042">Therefore, the image adjusting portion 320 may have a plurality of tables regarding a manner to give the blur, and may adjust the strength of the blur by substituting the distance from the area of the depth of field for the table selected by user. The table is a table related with the distance from the area of the depth of field and the strength of the blur. The plurality of tables included in the image adjusting portion 320 may be a table corresponding to the lens of the existing respective single-lens reflex camera. Accordingly, even if an image data taken by the image portion 100 without a stop is displayed on the display 180, a display effect can be given in the same manner as a single-lens reflex camera which user usually uses.<!-- EPO <DP n="14"> --></p><p id="p0043" num="0043">Next, referring to <figref idrefs="f0006">Fig. 6</figref>, it is a diagram showing a second operation example of the image processing device 1 according to the exemplary embodiment. In the second operation example, the rotation operation of fingers in a multi-touching state is assigned to the second operation (the adjustment operation of the depth of field).</p><p id="p0044" num="0044">In case user rotates fingers F1 and F2 (moving fingers F1 and F2 in arrow direction of <figref idrefs="f0006">Fig. 6</figref>), the touch sensor 181 detects the movement direction and movement distance. In case the fingers rotates clockwisely, the image adjusting portion 320 performs the image adjustment so that the depth of field becomes wider according to the movement distance. Meanwhile, in case the fingers rotates counterclockwisely, the image adjusting portion 320 performs the image adjustment so that the depth of field becomes narrower according to the movement distance. Further, the adjusting interface control portion 330 changes the size of the knob S2 according to the rotation width of fingers F1 and F2 by user. The rotation direction is a example. In case of rotating fingers counterclockwisely, it may be controlled so that the depth of field becomes wider.</p><p id="p0045" num="0045">As a third operation example, the example to set the depth of field and the focus position by using only one finger will be explained. A user longly pushes the subject to be focused in the image display with one finger F1. In the example of <figref idrefs="f0007">Fig. 7</figref>, the user longly pushes a face of a girl which is the subject. Then, the physical distance between the face of the girl and the lens focus is acquired by the afore-mentioned image portion 100 and stored in all sorts of storage portions. The image adjusting portion 320 and the adjusting interface control portion 330 detects this long push as the afore-mentioned first operation. Then, the image adjusting device 320 performs the image adjustment so that this position of the face of the girl is the focus position. Similarly, the adjusting interface control portion 330 adjusts the position of the knob S2 on the slide bar S1 according to the<!-- EPO <DP n="15"> --> position of the face of this girl (the distance from lens).</p><p id="p0046" num="0046">Further, the image adjusting portion 320 and the adjusting interface control portion 330 detects the second operation according to the operation of one finger. For example, as shown in <figref idrefs="f0007">Fig.7</figref>, the image adjusting portion 320 and the adjusting interface control portion 330 detects the operation drawing a circle as the second operation. In this case, the image adjusting portion 320 and the adjusting interface control portion 330 may adjust by widening the depth of field in detecting the clockwise operation and by narrowing the depth of field in detecting the counterclockwise operation.</p><p id="p0047" num="0047">Similarly, the depth of field and the focus position can also be set by using a pen device. In case of using the pen device, the image processing device 1 may detect the first operation (the operation of longly pushing on one point) and the second operation (the operation of drawing a circle) explained as the third operation example. The image processing device 1 can also detect the first operation and the second operation considering the push state of the button and the like which is provided with a pen-tablet.</p><p id="p0048" num="0048">Note that in case of using the pen-device, the image adjusting portion 320 may set the depth of field considering a thickness and the like of the pen point.</p><p id="p0049" num="0049">Next, referring to <figref idrefs="f0008">Fig.8</figref>, a fourth setting example according to the depth of field and the focus position will be explained. To set the focus position, similarly to <figref idrefs="f0007">Fig.7</figref> (the third operation example), the long push is performed. A user successively pushes the subject of object (F1-1, F1-2, and F1-3 of <figref idrefs="f0007">Fig.7</figref>) within a predetermined time interval. The physical distance between the respective subject position and the lens focus is acquired by the image portion 100 and stored in all sorts of storage portions as afore-mentioned. The image adjusting portion 320 and the adjusting interface control portion 330 recognizes<!-- EPO <DP n="16"> --> among a plurality of the subjects (the subjects selected by F1-1 to Fl-3) one between the shortest distance and the longest distance from lens as the depth of field. Then, the image adjusting portion 320 performs image adjustment according to the setting of the recognized depth of field. Simultaneously, the adjusting interface control portion 330 adjusts the width of the knob S2 on the slide bar S1 according to the setting of the recognized depth of field.</p><p id="p0050" num="0050">Next, referring to <figref idrefs="f0009">Fig.9</figref>, the variation of setting methods of the focus position by the image adjusting portion 320 will be explained. <figref idrefs="f0009">Fig. 9</figref> is a concept diagram showing the physical position and the focusing degree of the respective position regarding the focus position specified in the lens position of the image processing device 1 (a white circle portion in the figure) and the display 180.</p><p id="p0051" num="0051"><figref idrefs="f0009">Fig. 9A</figref> is a concept diagram showing a first variation of the setting of the focus position. The image adjusting portion320 calculates a distance between the physical position indicated by the touching position of the display 180 (the black circle portion in the figure) and the physical position of the lens of the image processing device 1 (the white circle position in the figure) from the distance data. Then, the image adjusting portion 320 performs image adjustment so that all of the points (the respective point of a2 in the figure) which is same distance as the distance are focused. That is, the image adjusting portion 320 performs image adjustment so that the focus position is not point but the respective point where the distance from the lens position is a predetermined distance is focused.</p><p id="p0052" num="0052"><figref idrefs="f0009">Fig. 9A</figref> is a concept diagram showing a first variation of the setting of the focus position. The image adjusting portion320 calculates a distance between the physical position indicated by the touching position of the display 180 (the black circle portion in the figure) and the physical position of the lens of the image processing device 1 (the white circle position in the figure) from the distance data.<!-- EPO <DP n="17"> --> Then, the image adjusting portion 320 performs image adjustment by defining one point of the physical position (the black circle in the figure) indicated by the touching position as a center of the focus position. That is, the image adjusting portion 320 performs image adjustment assuming the center of the focus position is one point.</p><p id="p0053" num="0053">A user may select and use either the focus adjustment shown in <figref idrefs="f0009">Fig.9(A)</figref> or the focus adjustment shown in <figref idrefs="f0009">Fig.9(B)</figref> from a not-shown setting scene.</p><p id="p0054" num="0054">Note that in the above description the various operation example (the first operation to the fourth operation) is explained. However, the setting interface relating the operation with the setting contents may be displayed on the display 180, and a user may freely relate the respective setting contents (a zoom, a depth of field, a focus position and the like) with operation contents (e.g. the pinch-in (pinch-out) in x axis direction).</p><p id="p0055" num="0055">Further, the afore-mentioned operation example is an example to the end. The different operation may also be assigned as the first operation and the second operation. For example, the process of successively touching one point (a plurality of taps) may be assigned. That is, the different operation from a zoom may be assigned to the first operation or the second operation.</p><p id="p0056" num="0056">Next, the effect of the image processing device 1 according to the exemplary embodiment will be explained. The image processing device 1 according to the exemplary embodiment detects the first operation to adjust the focus position and the second operation to adjust the depth of field as afore-mentioned with the display image displayed on the display 180, and performs image adjustment with the constitution of the light field camera according to the operations. Accordingly, a user can adjust the focus distance and the depth of field by the intuitional operation while referring to the display image.<!-- EPO <DP n="18"> --></p><p id="p0057" num="0057">As shown in <figref idrefs="f0003">Fig.3</figref>, the operation of performing a pinch-in or a pinch-out in a definite direction (y axis direction in <figref idrefs="f0003">Fig.3</figref>) different from direction of the zoom is assigned to the second operation. Thereby, a user can set the depth of field by the operation different from the zoom (e.g. the pinch-in or the pinch-out in x axis direction). Further, the first operation and the second operation regarding the adjustment of the depth of field or the focus position are assigned to general pinch-in, pinch-out, movement of fingers and the like. Thereby, the setting of the focus position and the depth of field is facilitated for many users.</p><p id="p0058" num="0058">Further, as shown in <figref idrefs="f0004">Fig.4</figref>, the operation of rotating fingers (moving in approximate circle direction) is assigned to the second operation. Thereby, a user can set the depth of field by the sensibly same operation as the stop adjustment.</p><p id="p0059" num="0059">Further, the image processing device 1 displays the slide bar S1 which is an adjustment interface on the display 180. Accordingly, the focus position and depth of field of the display image displayed at present can be rapidly grasped. Furthermore, the adjusting interface control portion 330 adjusts the display position of the slide bar S1 (a right end, a left end and the like) according to the touching position of a user. Accordingly, the slide bar S1 can be displayed at the position which is not an obstacle for a user.</p><p id="p0060" num="0060">In addition, the image processing device 1 changes the blur effect according to the width of the area of the depth of field as described referring to <figref idrefs="f0004">Fig.4</figref> and <figref idrefs="f0005">Fig.5</figref>. Thereby, the same display effect as a general single-lens reflex camera can be realized.</p><p id="p0061" num="0061">The afore-mentioned exemplary embodiment is only an example regarding applications of a technique idea obtained by the inventor. That is, the technique idea is not limited to the only afore-mentioned<!-- EPO <DP n="19"> --> exemplary embodiment, various changes may be performed.</p><p id="p0062" num="0062">Note that in the afore-mentioned explanation it is described that the image processing device 1 is a device including the image portion 100 (e.g. a smart phone, a digital camera and the like), however it is not limited to this. For example, the image processing device 1 may be a general personal computer. In this case, the image processing device 1 may be constituted to be able to download the image data (including the distance data) via an internet or read out the image data (including the distance data) from the attachable and detachable storage device (e.g. a USB (Universal Serial Bus) memory). A user performs the first operation and the second operation with a mouse referring to the display device (e.g. a liquid crystal display device) connected to a computer.</p><p id="p0063" num="0063">Furthermore, the image processing device 1 may be constituted to combine an auto focus function to focus a plurality of points with the afore-mentioned operations. Further, the display of the adjusting interface (S1) is necessarily not essential, it may not be displayed according to the user setting.</p><p id="p0064" num="0064">Further, in the setting of the depth of field, the movement distance of the focusing end of the near side may be different from the movement distance of the focusing end of the distant side. Referring to <figref idrefs="f0010">Fig.10</figref>, it will be explained. <figref idrefs="f0010">Fig.10</figref> is a setting screen of the depth of field. In the example it is explained assuming that the depth of field is widened by the rotational operation of fingers.</p><p id="p0065" num="0065">In case of setting so that the depth of field becomes wider (e.g. in case of clockwisely rotating fingers), the image adjusting portion 320 and the adjusting interface control portion 330 detects the adjusting degree (the rotation degree of the fingers). Then, the image adjusting portion 320 and the adjusting interface control portion 330 perform adjustment so that the adjustment width (namely movement<!-- EPO <DP n="20"> --> distance) of the focusing end of the near side is different from the adjustment width of the focusing end of the distant side. Referring to the slide bar S1 of <figref idrefs="f0001">Fig.1</figref>, it is explained in detail.</p><p id="p0066" num="0066">In case of setting so that the depth of field becomes wider, the image adjusting portion 320 and the adjusting interface control portion 330 set the movement distance of the focusing end of the distant side to be wider than the movement distance of the focusing end of the near side. Thereby, as shown in <figref idrefs="f0010">Fig.10</figref> in case that a girl to be focused as a subject has a flower, the image in which the flower in addition to the girl to be the subject is clearly displayed can be obtained. Further, the blur of the distant side (namely a background) is stronger, the image having the novel blur sense and indicating the subject of the display object to be more three-dimensional (clearly) can be obtained.</p><p id="p0067" num="0067">Note that the image adjusting portion 320 and the adjusting interface control portion 330 perform image adjustment without changing the center of the depth of field (the position corresponding to the white circle of the slide bar S2) (For example, the girl is positioned at the center of the depth of field).</p><p id="p0068" num="0068">This method to give the blur sense can not be realized by a general single-lens reflex camera, thereby the novel image can be obtained. Note that the image adjusting portion 320 and the adjusting interface control portion 330 set the movement distance of the focusing end of the distant side to be narrower than the movement distance of the focusing end of the near side. The movement distance of the focusing end may be properly changed by the user setting.</p><p id="p0069" num="0069">Finally, referring to <figref idrefs="f0012">Fig.12</figref>, the essential portion of the image processing device 1 according to the present invention will be explained again. <figref idrefs="f0012">Fig. 12</figref> is a block diagram showing a summary of an image processing device 1 according the present invention. The storage portion 220 is a general term such as the flash memory 190,<!-- EPO <DP n="21"> --> memory card 160, not-shown harddisk and the like according to <figref idrefs="f0001">Fig.1</figref>, and stores the image data acquired by the image portion 100 (or an image device outside the image processing device 1) having the constitution of the light field camera. Further, the storage portion 220 stores the image data as well as the distance data between the respective subject and the lens focus.</p><p id="p0070" num="0070">The display 180 functions as a display portion to display the generated display image based on the image data. The display 180 has for example the touch sensor 181 and the first operation and the second operation regarding the depth of field and the focus position are performed.</p><p id="p0071" num="0071">The CPU 200 is an arithmetic device to control the image processing device 1, and reads out a program from the storage portion 200 and executes it. The CPU 200 operates as the image adjusting portion 320 by executing the program according to the control of the light field camera.</p><p id="p0072" num="0072">The image adjusting portion 320 detects the first operation and the second operation performed on the display 180. Then the image adjusting portion 320 reads out the image data and the distance data from the storage portion 220, and adjusts the focus position of the image data according to the first operation (e.g. the parallel movement of the fingers in a multi-touching state). Further, the image adjusting portion 320 adjusts the depth of field of the image data according to the second operation (e.g. the pinch-in or pinch-out in the multi-touching state). Accordingly, the image adjusting portion 320 generates the display image displayed on the display 180.</p><p id="p0073" num="0073">This constitution enables obtaining the display image in which the focus position or the depth of field is simply adjusted according to the operation on the display portion (the display 180).</p><p id="p0074" num="0074"><!-- EPO <DP n="22"> --> While the invention has been particularly shown and described with reference to exemplary embodiments thereof, the invention is not limited to the afore-mention. It will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present invention as defined by the claims.</p><p id="p0075" num="0075">This application is based upon and claims the benefit of priority from <patcit id="pcit0004" dnum="JP2013031805A"><text>Japanese patent application No. 2013-031805, filed on February 21, 2013</text></patcit>, the disclosure of which is incorporated herein in its entirety by reference.</p><heading id="h0013"><b>Reference Signs List</b></heading><p id="p0076" num="0076"><dl id="dl0001" compact="compact"><dt>1</dt><dd>IMAGE PROCESSING DEVICE</dd><dt>100</dt><dd>IMAGE PORTION</dd><dt>110</dt><dd>RAM</dd><dt>120</dt><dd>IMAGE PROCESSING CIRCUIT</dd><dt>130</dt><dd>YC CONVERTING CIRCUIT</dd><dt>140</dt><dd>COMPRESSING AND EXTENDING CIRCUIT</dd><dt>150</dt><dd>MEDIAN CONTROLLER</dd><dt>160</dt><dd>MEMORY CARD</dd><dt>170</dt><dd>LCD DRIVER</dd><dt>180</dt><dd>DISPLAY</dd><dt>181</dt><dd>TOUCH SENSOR</dd><dt>190</dt><dd>FLASH MEMORY</dd><dt>200</dt><dd>CPU</dd><dt>210</dt><dd>DATA BUS</dd><dt>220</dt><dd>STRAGE PORTION</dd><dt>300</dt><dd>IMAGE DATA READ-OUT PORTION</dd><dt>310</dt><dd>SENSOR INFORMATION INPUT PORTION</dd><dt>320</dt><dd>IMAGE ADJUSTING PORTION</dd><dt>330</dt><dd>ADJUSTING INTERFACE CONTROL PORTION</dd></dl></p></description><claims mxw-id="PCLM90459191" lang="EN" load-source="patent-office"><!-- EPO <DP n="23"> --><claim id="c-en-0001" num="0001"><claim-text>An image processing device comprising:
<claim-text>storage means for storing an image data including a distance data between a respective subject and a lens focus;</claim-text>
<claim-text>display means for displaying a display image based on the image data, and an operation regarding a depth of field and a focus position being performed; and</claim-text>
<claim-text>image adjusting means for detecting a first operation and a second operation performed referring to the display means, and adjusting a focus distance according to the first operation and the distance data as well as adjusting a depth of field according to the second operation and the distance data for the image data to generate the display image.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The image processing device according to Claim 1, wherein the display means includes a touch sensor that detects a contact of a user's finger or all sorts of devices, and is a touch display that displays the display image.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The image processing device according to Claim 2, wherein the first operation is an operation of moving user's fingers in a predetermined direction without keeping apart from the touch display while the space between user's two fingers are kept to be approximately same.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The image processing device according to Claim 2, wherein the second operation is an operation of widening or narrowing the space between user's two fingers in a predetermined direction while user's fingers are contact with the touch display.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The image processing device according to Claim 2, wherein the second operation is an operation of moving in an approximate<!-- EPO <DP n="24"> --> circle direction while user's one finger or two fingers is contact with the touch display.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The image processing device according to Claim 2, wherein the first operation is an operation of pushing the subject for not less than a predetermined time in displaying the display image.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The image processing device according to Claim 2, wherein the second operation is an operation of touching the touch display several times within a predetermined time interval in displaying the display image.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The image processing device according to any one of Claims 2 to 7, further comprising adjusting display control means for displaying an adjusting interface indicating a setting degree of the focus position and the depth of field of the display image on the touch display based on the first operation and the second operation performed for the touch display.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>The image processing device according to Claim 8, wherein the adjusting display control means changes the display position of the adjusting interface on the touch display according to the contact position of user's fingers on the touch display detected by the touch sensor.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>The image processing device according to Claim 9, wherein the adjusting display control means displays the display position of the adjusting interface on the touch display from the center to the right area of the touch display in case that the contact position of user's fingers on the touch display detected by the touch sensor is a predetermined center area.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The image processing device according to any one of Claims 1 to 10, wherein the image adjusting means adjusts for the image data<!-- EPO <DP n="25"> --> the depth of field according to the second operation and the distance data as well as gives a blur effect based on a size of an area of the depth of field and a physical distance from the area, to the subject outside the area of the depth of field.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>The image processing device according to any one of Claims 1 to 11, wherein on adjusting for the image data the depth of field according to the second operation and the distance data, the image adjusting means adjusts it so that the movement distance of the focusing end of the near side is different from the movement distance of the focusing end of the distant side.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>The image processing device according to Claim 12, wherein on adjusting for the image data the depth of field according to the second operation and the distance data, the image adjusting means adjusts it so that the movement distance of the focusing end of the near side becomes narrower than the movement distance of the focusing end of the distant side.</claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>An image processing method comprising:
<claim-text>a display step of displaying a display image based on an image data including a distance data between a respective subject and a lens, and an operation regarding a depth of field and a focus position being performed; and</claim-text>
<claim-text>an image control step of detecting a first operation and a second operation performed referring to the display image, and adjusting a focus distance according to the first operation and the distance data as well as adjusting a depth of field according to the second operation and the distance data for the image data to generate the display image.</claim-text></claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>A non-transitory computer readable media for causing a computer to execute:
<claim-text>a step of reading out an image data including a distance data<!-- EPO <DP n="26"> --> between a respective subject and a lens from a storage device; and</claim-text>
<claim-text>an image control step of detecting a first operation and a second operation performed referring to the display of the display image based on the image data, and adjusting a focus distance according to the first operation and the distance data as well as adjusting a depth of field according to the second operation and the distance data for the image data to generate the display image.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW20421927" load-source="patent-office"><!-- EPO <DP n="27"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="151" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="28"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="112" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="29"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="161" he="227" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="30"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="111" he="183" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="31"> --><figure id="f0005" num="5"><img id="if0005" file="imgf0005.tif" wi="114" he="177" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="32"> --><figure id="f0006" num="6"><img id="if0006" file="imgf0006.tif" wi="157" he="229" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="33"> --><figure id="f0007" num="7"><img id="if0007" file="imgf0007.tif" wi="156" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="34"> --><figure id="f0008" num="8"><img id="if0008" file="imgf0008.tif" wi="152" he="230" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="35"> --><figure id="f0009" num="9A,9B"><img id="if0009" file="imgf0009.tif" wi="115" he="216" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="36"> --><figure id="f0010" num="10"><img id="if0010" file="imgf0010.tif" wi="157" he="231" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0011" num="11"><img id="if0011" file="imgf0011.tif" wi="151" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> --><figure id="f0012" num="12"><img id="if0012" file="imgf0012.tif" wi="129" he="223" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="165" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
