<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960758-A1" country="EP" doc-number="2960758" kind="A1" date="20151230" family-id="53502459" file-reference-id="318335" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451519" ucid="EP-2960758-A1"><document-id><country>EP</country><doc-number>2960758</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15173727-A" is-representative="YES"><document-id mxw-id="PAPP193866006" load-source="patent-office" format="original"><country>EP</country><doc-number>15173727.7</doc-number><date>20150625</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193866007" load-source="docdb" format="epo"><country>EP</country><doc-number>15173727</doc-number><kind>A</kind><date>20150625</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162036208" ucid="JP-2014130608-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2014130608</doc-number><kind>A</kind><date>20140625</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988521848" load-source="docdb">G06F   3/042       20060101AFI20151014BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1727831576" load-source="docdb" scheme="CPC">G06F   3/0423      20130101 LI20171127BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1727831577" load-source="docdb" scheme="CPC">G06F   3/0421      20130101 LI20171127BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984705191" load-source="docdb" scheme="CPC">G06F   3/017       20130101 FI20151231BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165546023" lang="DE" load-source="patent-office">EINGABEVORRICHTUNG</invention-title><invention-title mxw-id="PT165546024" lang="EN" load-source="patent-office">INPUT APPARATUS</invention-title><invention-title mxw-id="PT165546025" lang="FR" load-source="patent-office">APPAREIL D'ENTRÃ‰E</invention-title><citations><patent-citations><patcit mxw-id="PCIT335740934" load-source="docdb" ucid="EP-2711809-A2"><document-id format="epo"><country>EP</country><doc-number>2711809</doc-number><kind>A2</kind><date>20140326</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335742665" load-source="docdb" ucid="US-20110267262-A1"><document-id format="epo"><country>US</country><doc-number>20110267262</doc-number><kind>A1</kind><date>20111103</date></document-id><sources><source name="SEA" category="A" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335744449" load-source="docdb" ucid="WO-2008123500-A1"><document-id format="epo"><country>WO</country><doc-number>2008123500</doc-number><kind>A1</kind><date>20081016</date></document-id><sources><source name="APP" created-by-npl="N"/></sources></patcit></patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103324201" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>FUNAI ELECTRIC CO</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR1103321044" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>FUNAI ELECTRIC COMPANY LTD</last-name></addressbook></applicant><applicant mxw-id="PPAR1101653059" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Funai Electric Company Ltd</last-name><iid>101412921</iid><address><street>7-1, Nakagaito 7-chome</street><city>Daito-shi Osaka 574-0013</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103323125" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>NISHIOKA KEN</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103307865" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>NISHIOKA, KEN</last-name></addressbook></inventor><inventor mxw-id="PPAR1101641108" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>NISHIOKA, KEN</last-name><address><street>c/o FUNAI ELECTRIC CO., LTD. 7-1, Nakagaito 7-chome</street><city>DAITO-SHI, OSAKA 5740013</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101641005" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Vigand, Philippe</last-name><suffix>et al</suffix><iid>101349517</iid><address><street>Novagraaf International SA 3 chemin de l'Echo</street><city>1213 Onex Geneva</city><country>CH</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660611551" load-source="docdb">AL</country><country mxw-id="DS660606877" load-source="docdb">AT</country><country mxw-id="DS660611553" load-source="docdb">BE</country><country mxw-id="DS660692676" load-source="docdb">BG</country><country mxw-id="DS660685738" load-source="docdb">CH</country><country mxw-id="DS660783716" load-source="docdb">CY</country><country mxw-id="DS660606878" load-source="docdb">CZ</country><country mxw-id="DS660611077" load-source="docdb">DE</country><country mxw-id="DS660783717" load-source="docdb">DK</country><country mxw-id="DS660783718" load-source="docdb">EE</country><country mxw-id="DS660686314" load-source="docdb">ES</country><country mxw-id="DS660692677" load-source="docdb">FI</country><country mxw-id="DS660692678" load-source="docdb">FR</country><country mxw-id="DS660611554" load-source="docdb">GB</country><country mxw-id="DS660783719" load-source="docdb">GR</country><country mxw-id="DS660611559" load-source="docdb">HR</country><country mxw-id="DS660606883" load-source="docdb">HU</country><country mxw-id="DS660685743" load-source="docdb">IE</country><country mxw-id="DS660611560" load-source="docdb">IS</country><country mxw-id="DS660692683" load-source="docdb">IT</country><country mxw-id="DS660783720" load-source="docdb">LI</country><country mxw-id="DS660611078" load-source="docdb">LT</country><country mxw-id="DS660606884" load-source="docdb">LU</country><country mxw-id="DS660611087" load-source="docdb">LV</country><country mxw-id="DS660611088" load-source="docdb">MC</country><country mxw-id="DS660688550" load-source="docdb">MK</country><country mxw-id="DS660688555" load-source="docdb">MT</country><country mxw-id="DS660686319" load-source="docdb">NL</country><country mxw-id="DS660692684" load-source="docdb">NO</country><country mxw-id="DS660686320" load-source="docdb">PL</country><country mxw-id="DS660685744" load-source="docdb">PT</country><country mxw-id="DS660686321" load-source="docdb">RO</country><country mxw-id="DS660685745" load-source="docdb">RS</country><country mxw-id="DS660686322" load-source="docdb">SE</country><country mxw-id="DS660685746" load-source="docdb">SI</country><country mxw-id="DS660688556" load-source="docdb">SK</country><country mxw-id="DS660688557" load-source="docdb">SM</country><country mxw-id="DS660783721" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479889" lang="EN" load-source="patent-office"><p id="pa01" num="0001">An input apparatus 100 includes: a light source 10 that emits detection light 17 toward a first direction, and raster scans a detection interface 400 defined in space, with the detection light 17; a first light sensor 20 that is disposed closer to the light source 10 than the detection interface 400, and detects first reflected light 18 which is the detection light 17 that has been reflected; a second light sensor 30 that detects second reflected light 19 which is the detection light 17 that has been reflected off an instructing body 70 that has entered a detection region R extending from the detection interface 400 toward the light source 10; and a control unit 40 that detects a coordinate value of the instructing body 70 using received-light data obtained by the first light sensor 20 and the second light sensor 30 receiving light at the same timing.
<img id="iaf01" file="imgaf001.tif" wi="121" he="100" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759701" lang="EN" source="EPO" load-source="docdb"><p>An input apparatus 100 includes: a light source 10 that emits detection light 17 toward a first direction, and raster scans a detection interface 400 defined in space, with the detection light 17; a first light sensor 20 that is disposed closer to the light source 10 than the detection interface 400, and detects first reflected light 18 which is the detection light 17 that has been reflected; a second light sensor 30 that detects second reflected light 19 which is the detection light 17 that has been reflected off an instructing body 70 that has entered a detection region R extending from the detection interface 400 toward the light source 10; and a control unit 40 that detects a coordinate value of the instructing body 70 using received-light data obtained by the first light sensor 20 and the second light sensor 30 receiving light at the same timing.</p></abstract><description mxw-id="PDES98404590" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>Field</b></heading><p id="p0001" num="0001">The present invention relates to an input apparatus that identifies a position of an instructing body in three-dimensional space to enable input corresponding to the position.</p><heading id="h0002"><b>Background</b></heading><p id="p0002" num="0002">There are known conventional input apparatuses that identify a position of an instructing body in three-dimensional space to enable input corresponding to the position (for example, see Patent Literature (PTL) 1). According to the technique disclosed in PTL 1, an image captured by a camera is analyzed so that a position of an instructing body is identified.</p><heading id="h0003"><b>Citation List</b></heading><heading id="h0004"><b>Patent Literature</b></heading><p id="p0003" num="0003"><ul><li>[PTL 1] <patcit id="pcit0001" dnum="WO2008123500A"><text>WO 2008/123500</text></patcit></li></ul></p><heading id="h0005"><b>Summary</b></heading><heading id="h0006"><b>Technical Problem</b></heading><p id="p0004" num="0004">However, the technique disclosed in PTL 1 uses a camera, which increases the size of the apparatus itself. Another problem with the technique is that advanced software is required to analyze an image captured by the camera, which increases the image processing load.</p><p id="p0005" num="0005">As such, in view of the above situation, the present invention aims to provide an input apparatus capable of accurately identifying a position of an instructing body in three-dimensional space with a simple structure and reduced processing load.</p><heading id="h0007"><b>Solution to Problem</b></heading><!-- EPO <DP n="2"> --><p id="p0006" num="0006">In order to solve the above-described problems, an input apparatus according to an aspect of the present invention includes: a light source that emits light toward a predetermined region; a first light-receiving unit that receives light reflected off a subject toward a first region and outputs a first detection signal; a second light-receiving unit that receives light reflected off the subject toward a second region in a direction substantially perpendicular to a direction from the predetermined region toward the first region, and outputs a second detection signal; and a control unit that determines a position of the subject using the first detection signal during a period for which the second light-receiving unit detects the reflected light.</p><p id="p0007" num="0007">With this, a position of the subject is determined using not the whole received-light data obtained by the first light-receiving unit receiving light, but the received-light data obtained by the first light-receiving unit receiving light at the same timing as the second light-receiving unit receives light, and therefore a position of the subject can be accurately identified with reduced processing load.</p><p id="p0008" num="0008">For example, the control unit may determine the position of the subject using received-light data obtained by the first light-receiving unit receiving light at the same timing as the second light-receiving unit receives reflected light of an amount exceeding a predetermined threshold.</p><p id="p0009" num="0009">For example, the input apparatus may further include: a display unit that displays an image; and an image-forming unit that projects a three-dimensional image to the predetermined region by forming in the predetermined region the image displayed by the display unit.</p><p id="p0010" num="0010">With this, it is possible to project a three-dimensional image to the predetermined region.</p><p id="p0011" num="0011">For example, it may be that the display unit includes display units, and the display units are aligned in a light emitted direction in which the light source emits light, and each display a display image crossing the light emitted direction.<!-- EPO <DP n="3"> --></p><p id="p0012" num="0012">With this, a three-dimensional image is displayed using the display units arranged along the direction in which the light source emits light, and therefore it is possible to show a user a three-dimensional image that has a depth in the direction in which the light is emitted.</p><p id="p0013" num="0013">For example, the predetermined region may include an image screen on which the image is formed by the image-forming unit.</p><p id="p0014" num="0014">For example, it may be that the second light-receiving unit includes second light-receiving units, and each of the second light-receiving units is provided at a position corresponding to a different one of the display units.</p><p id="p0015" num="0015">With this, it is sufficient that the processing of detecting a position of the subject is performed directly using the results of detection by the second light-receiving units, and therefore it is possible to omit the processing for matching the position of the three-dimensional image shown to a user with the position of the subject.</p><p id="p0016" num="0016">For example, the second light-receiving units may be disposed at positions that allow each of the second light-receiving units to detect light reflected, in a display region for a corresponding one of the display units, in a direction different from a direction of reflected light for another one of the display units.</p><p id="p0017" num="0017">With this, even the second light-receiving unit that is in the shadow to receive light from one side can receive reflected light from the other side in more cases, and thus the impact due to shadow can be reduced.</p><p id="p0018" num="0018">For example, it may be that the first light-receiving unit receives light reflected off the subject toward one side in the direction substantially perpendicular to the direction from the predetermined region toward the first region, and the second light-receiving unit receives light reflected off the subject toward the direction substantially perpendicular to the direction from the predetermined region toward the first region.</p><p id="p0019" num="0019">With this, the first light-receiving unit and the second light-receiving unit are capable of receiving the light emitted from<!-- EPO <DP n="4"> --> the light source and reflected off the subject, and therefore it is possible to locate the subject with accuracy.</p><p id="p0020" num="0020">For example, the input apparatus may further include a projector that projects an image to the predetermined region.</p><p id="p0021" num="0021">With this, it is possible to project an image to the predetermined region.</p><p id="p0022" num="0022">An input apparatus according to another aspect of the present invention includes: a light source that emits detection light toward a first direction, and raster scans a detection interface with the detection light, the detection interface being defined in space; a first light sensor that is disposed closer to the light source than the detection interface, and detects first reflected light which is the detection light that has been reflected; a second light sensor that detects second reflected light which is the detection light that has been reflected off an instructing body that has entered a detection region extending from the detection interface toward the light source; and a control unit that detects a coordinate value of the instructing body using received-light data obtained by the first light sensor and the second light sensor receiving light at the same timing.</p><p id="p0023" num="0023">With this, the detection light is emitted to the detection interface, the first light sensor is disposed on the light source side, and the second light sensor is disposed at a position that allows the second light sensor to detect the second reflected light which is light that has been reflected off the instructing body that has entered the detection region. In other words, the first light sensor detects the first reflected light which is light reflected toward the light source among light beams reflected off the instructing body, and the second light sensor detects the second reflected light which is light reflected in the detection region. The control unit then detects a coordinate value of the instructing body using a sensor signal that is based on the first reflected light detected by the first light sensor at a timing when the second light sensor detects the second reflected light.</p><p id="p0024" num="0024">Thus, a coordinate value of the instructing body is detected using not the whole detection result for the first reflected light detected by the first light sensor, but the detection result for the first<!-- EPO <DP n="5"> --> reflected light detected at a timing when the second light sensor detects the second reflected light (that is, a part of the detection result for the first reflected light), and therefore a coordinate value of the instructing body can be accurately identified with reduced processing load.</p><p id="p0025" num="0025">For example, it may be that the second light sensor includes second light sensors, the second light sensors are disposed at different positions in the first direction, and the control unit detects the coordinate value of the instructing body preferentially using, among second reflected light beams detected by the second light sensors, a second reflected light beam detected by, among the second light sensors, a second light sensor disposed close to the light source.</p><p id="p0026" num="0026">With this, a coordinate value of the instructing body is detected preferentially using the second reflected light detected by the second light sensor disposed close to the light source among the second reflected light beams. With the input apparatus, a user performs input from a position facing the light source. This means that in the input apparatus such as that described above, since the coordinate position of the tip of a finger of a user is necessary, for example, the first sensor signal obtained in a front area close to the user is unnecessary among the results obtained from the second light sensors. Thus, a priority is given to a coordinate value of a user's instruction point in a back area in the Z-axis direction (close to the light source) so that no processing is performed on unnecessary data detected in the front area close to the user, and therefore it is possible to minimize data of the first sensor signal that is subject to the process of detecting coordinates. Accordingly, the processing load related to the detecting process can be reduced.</p><p id="p0027" num="0027">For example, it may be that the input apparatus further include a projector that projects a mid-air image to the detection region, and the control unit changes the mid-air image projected by the projector, according to the detected coordinate value of the instructing body.<!-- EPO <DP n="6"> --></p><p id="p0028" num="0028">With this, a mid-air image is projected, and the projected mid-air image is changed according to a coordinate value of the instructing body, and therefore it is possible to give a user a sense of directly manipulating the mid-air image, for example.</p><p id="p0029" num="0029">For example, the projector may include: display units aligned in the first direction and each having a display surface crossing the first direction; and an image-forming unit that projects a three-dimensional image as the mid-air image to the detection region by forming in the detection region images displayed on the display units.</p><p id="p0030" num="0030">With this, the display units aligned in the first direction are used to display a three-dimensional image, and therefore it is possible to show a user the three-dimensional image that has a depth in the first direction.</p><p id="p0031" num="0031">For example, the second light sensors may be provided at respective positions corresponding to the display units.</p><p id="p0032" num="0032">With this, it is sufficient that the processing of detecting a coordinate value is performed directly using the results of detection by the second light sensors, and thus it is possible to omit the coordinate transformation process for matching the coordinates of the three-dimensional image shown to a user with the coordinates of the instructing body.</p><p id="p0033" num="0033">For example, the second light sensors may be aligned in the first direction.</p><p id="p0034" num="0034">For example, it may be that a part of the second light sensors is disposed to detect light reflected off the instructing body in a second direction extending along the detection interface, and a remaining part of the second light sensors is disposed to detect light reflected off the instructing body in a third direction different from the second direction extending along the detection interface.</p><p id="p0035" num="0035">With this, not only the second reflected light travelling in the second direction, but also the second reflected light travelling in the third direction can be detected, and therefore it is possible to detect the instructing body that has entered the detection region if there is reflected light travelling in at least one of the second direction and<!-- EPO <DP n="7"> --> the third direction. Accordingly, even the second light-receiving unit that is in the shadow to receive light from one side can receive reflected light from the other side, and thus the impact due to shadow can be reduced.</p><p id="p0036" num="0036">For example, the light source may start the raster scan from an area of the detection interface that is on a side on which the second light sensor is provided.</p><p id="p0037" num="0037">With this, the light source starts the raster scan from the second light sensor side of the detection interfaces so that the raster scan on positions that will be in the shadows of the second light sensors is performed after positions that is less likely to be in the shadows. Accordingly, the result of the detection by the second light sensors can be reliably obtained.</p><p id="p0038" num="0038">Note that the present invention can not only be implemented as an input apparatus including characterizing processing units such as those described here, but as an input method in which the processes performed by the characterizing processing units included in the input apparatus are implemented as steps. Moreover, the present invention can be implemented as a computer program for making a computer perform as the characterizing processing units included in the input apparatus, or as a computer program that causes a computer to execute the characterizing steps included in the input method. It goes without saying that the computer program can be circulated on a non-transitory computer-readable recording medium such as a compact disc-read only memory (CD-ROM) or over a communications network such as the Internet.</p><heading id="h0008"><b>Advantageous Effects</b></heading><p id="p0039" num="0039">According the present invention, a position of an instructing body in three-dimensional space can be accurately identified with a simple structure and reduced processing load.</p><heading id="h0009"><b>Brief Description of Drawings</b></heading><p id="p0040" num="0040"><ul><li>[<figref idrefs="f0001">FIG. 1] FIG. 1</figref> schematically illustrates one example of a structure of an input apparatus according to Embodiment 1.<!-- EPO <DP n="8"> --></li><li>[<figref idrefs="f0002">FIG. 2] FIG. 2</figref> is an illustration for use in describing a detection region of a second light sensor of an input apparatus.</li><li>[<figref idrefs="f0003">FIG. 3] FIG. 3</figref> is a block diagram illustrating one example of a functional structure of an input apparatus according to Embodiment 1.</li><li>[<figref idrefs="f0004">FIG. 4] FIG. 4</figref> is a block diagram illustrating one example of a functional structure of a control unit according to Embodiment 1.</li><li>[<figref idrefs="f0005">FIG. 5A] FIG. 5A</figref> is an image obtained by converting a first sensor signal A in one raster scan.</li><li>[<figref idrefs="f0005">FIG. 5B] FIG. 5B</figref> is an image obtained by converting a second sensor signal B that exceeds a predetermined threshold among second sensor signals B in one raster scan.</li><li>[<figref idrefs="f0006">FIG. 5C] FIG. 5C</figref> is an image obtained by converting a first sensor signal A detected at a detection timing.</li><li>[<figref idrefs="f0007">FIG. 6] FIG. 6</figref> is a flowchart illustrating a detecting process in which a coordinate value of an instructing body is detected.</li><li>[<figref idrefs="f0008">FIG. 7] FIG. 7</figref> schematically illustrates one example of a functional structure of an input apparatus according to Embodiment 2.</li><li>[<figref idrefs="f0009">FIG. 8] FIG. 8</figref> is a block diagram illustrating one example of a functional structure of a control unit according to Embodiment 2.</li><li>[<figref idrefs="f0010">FIG. 9] FIG. 9</figref> is an illustration for use in describing a detection region of second light sensors in an input apparatus according to Embodiment 2.</li><li>[<figref idrefs="f0011">FIG. 10A] FIG. 10A</figref> is an illustration for use in describing mid-air images that are formed based on images displayed on respective display units.</li><li>[<figref idrefs="f0011">FIG. 10B] FIG. 10B</figref> is an illustration for use in describing a three-dimensional image that becomes visible to a user as a result of combination of mid-air images.</li><li>[<figref idrefs="f0012">FIG. 11] FIG. 11</figref> is a flowchart for use in describing a specific example of a detecting process that is performed in an input apparatus according to Embodiment 2.<!-- EPO <DP n="9"> --></li><li>[<figref idrefs="f0013">FIG. 12] FIG. 12</figref> is an illustration related to a first sensor signal, for use in describing a process of detecting a tip of an instructing body.</li></ul></p><heading id="h0010"><b>Description of Embodiments</b></heading><p id="p0041" num="0041">Hereinafter, exemplary embodiments of the present invention are described in greater detail with reference to the accompanying Drawings. Note that each of the embodiments described below shows a specific example of the present invention. The numerical values, shapes, materials, structural elements, the arrangement and connection of the structural elements, steps, the processing order of the steps etc. shown in the following exemplary embodiments are mere examples, and therefore do not limit the present invention, the scope of which is defined in the appended Claims. As such, among the structural elements in the following embodiments, structural elements not recited in any one of the independent claims are described as structural elements of a preferable embodiment, and are not absolutely necessary in the present invention to solve the problems.</p><heading id="h0011"><b>Embodiment 1</b></heading><p id="p0042" num="0042">Embodiment 1 describes an input apparatus that detects user's virtual touch operation on a mid-air image projected in space.</p><p id="p0043" num="0043"><figref idrefs="f0001">FIG. 1</figref> schematically illustrates one example of a structure of an input apparatus according to Embodiment 1. <figref idrefs="f0002">FIG. 2</figref> is an illustration for use in describing a detection region of a second light sensor of the input apparatus.</p><p id="p0044" num="0044">As illustrated in <figref idrefs="f0001">FIG. 1</figref>, an input apparatus 100 includes a light source 10, a first light sensor 20, a second light sensor 30, a control unit 40, and a projector 150.</p><p id="p0045" num="0045">The light source 10 emits detection light 17 toward a first direction (Z-axis direction) and performs a detection light raster scan on a detection interface 400 defined in space. The light source 10 is a light source that emits light toward a detection region R which is the predetermined region. Note that, using the detection light 17, the light source 10 performs the raster scan on the detection<!-- EPO <DP n="10"> --> interface 400 at a predetermined timing. The detection light 17 may be, for example, collimated infrared laser light.</p><p id="p0046" num="0046">The first light sensor 20 is disposed on the light source 10 side relative to the detection interface 400 and detects first reflected light 18 which is the detection light 17 that has been reflected. The first light sensor 20 is the first light-receiving unit that receives light reflected off an instructing body 70, which is the subject, toward the first region. The first light sensor 20 detects the first reflected light 18 at predetermined different timings, and outputs a first sensor signal A as the result of the detection.</p><p id="p0047" num="0047">The second light sensor 30 detects second reflected light 19 which is the detection light 17 that has been reflected off the instructing body 70 that has entered the detection region R extending from the detection interface 400 toward the light source 10. The second light sensor 30 is the second light-receiving unit that receives light reflected off the instructing body 70 toward the second region in a direction (the Y-axis direction) substantially perpendicular to a direction (the Z-axis direction) from the detection region R toward the first region. The second light sensor 30 detects the second reflected light 19 at predetermined different timings, and outputs a second sensor signal B as the result of the detection. The second light sensor 30 detects the second reflected light 19 which is light reflected in a detection region extending along the Y-axis direction that crosses a detection region for the first light sensor 20 that extends along the Z-axis direction. Furthermore, the second light sensor 30 detects light reflected along the Y-axis direction.</p><p id="p0048" num="0048">The instructing body 70 is, for example, a finger of a user or a pen in a user's hand that is moved. A virtual touch screen (not illustrated in the drawings) which the instructing body 70 virtually touches may be defined in the detection region R.</p><p id="p0049" num="0049">There is synchronization between the predetermined different timings at which the first light sensor 20 detects the first reflected light 18 and the predetermined different timings at which the second light sensor 30 detects the second reflected light 19. Furthermore, x-y coordinate values in the raster scan by the light source 10 are<!-- EPO <DP n="11"> --> associated with the predetermined different timings at which the first light sensor 20 and the second light sensor 30 detect light. This means that when one of the predetermined different timings is identified, x-y coordinate values, on the detection interface 400, of the detection light 17 emitted from the light source 10 at such timing are identified.</p><p id="p0050" num="0050">The control unit 40 detects a coordinate value of the instructing body 70 using a sensor signal based on the first reflected light 18 detected by the first light sensor 20 during a period for which the second light sensor 30 detects the second reflected light 19. In other words, a position of the instructing body 70 is determined using the first sensor signal A and the second sensor signal B which are received-light data obtained by the first light sensor 20 and the second light sensor 30 receiving light at the same timing. Specifically, the control unit 40 determines a position of the instructing body 70 using received-light data obtained by the first light sensor 20 receiving light at the same timing as the second light sensor 30 receives reflected light of an amount exceeding a predetermined threshold.</p><p id="p0051" num="0051">Furthermore, according to the detected coordinate value of the instructing body 70, the control unit 40 changes the mid-air image projected by the projector 150. Specifically, the control unit 40 may display a pointer at a position of the mid-air image according to the detected coordinate value of the instructing body 70 to change the mid-air image, or may rotate, move, or deform the mid-air image according to input of a pointer to change the mid-air image.</p><p id="p0052" num="0052">The projector 150 includes a display unit 50 and an image-forming unit 60 and projects a mid-air image to the detection region R. The projector 150 projects an image to the detection region R.</p><p id="p0053" num="0053">The display unit 50 is, for example, a display panel on which an image is displayed. A display surface of the display unit 50 crosses the first direction. The display unit 50 may be a liquid-crystal display unit, for example. The image may be, for example, an image<!-- EPO <DP n="12"> --> object with accurate three-dimensional features given through advanced shadow processing.</p><p id="p0054" num="0054">The image-forming unit 60 is an optical device panel that projects, as a mid-air image, an image displayed by the display unit 50, to the detection region R. In other words, the image-forming unit 60 projects a three-dimensional image to the detection region R by forming, in the detection region R, an image displayed by the display unit 50. The detection region R includes an image screen on which an image is formed by the image-forming unit 60. The image-forming unit 60 may be, for example, a dihedral corner reflector array disclosed in PTL 1. The image-forming unit 60 forms, in the detection region R, a mid-air image of a real image having a depth in the direction opposite to that of an original image. The control unit 40 defines a virtual touch screen having the same surface shape as that of the mid-air image so that a user can be given a virtual sense of touching the mid-air image.</p><p id="p0055" num="0055">Next, a functional structure of the input apparatus 100 is described.</p><p id="p0056" num="0056"><figref idrefs="f0003">FIG. 3</figref> is a block diagram illustrating one example of a functional structure of an input apparatus according to Embodiment 1.</p><p id="p0057" num="0057">As illustrated in <figref idrefs="f0003">FIG. 3</figref>, the light source 10 includes a laser driver 11, an infrared laser device 12, a collimating lens 13, a scan driver 14, an actuator 15, and a mirror 16.</p><p id="p0058" num="0058">The laser driver 11 drives the infrared laser device 12 under the control of the control unit 40. The infrared laser device 12 emits infrared laser light as the detection light 17. The collimating lens 13 converts into parallel light the incident infrared laser light emitted by the infrared laser device 12.</p><p id="p0059" num="0059">The scan driver 14 drives the actuator 15 under the control of the control unit 40. The actuator 15 changes a direction of the mirror 16, and raster scans the detection interface 400 with the detection light 17 reflected off the mirror 16. The raster scan by the detection interface 400 may include a primary raster scan in which the detection interface 400 is raster scanned in the X-axis direction<!-- EPO <DP n="13"> --> with the detection light 17 and a secondary raster scan in which the detection interface 400 is raster scanned in the Y-axis direction with the detection light 17 as illustrated in <figref idrefs="f0001">FIG. 1</figref>.</p><p id="p0060" num="0060">The first light sensor 20 includes a condensing lens 21 and a photoelectric conversion element 22.</p><p id="p0061" num="0061">The condensing lens 21 collects the first reflected light 18 which is the detection light 17 reflected off the instructing body 70, and focuses the first reflected light 18 to the photoelectric conversion element 22. The photoelectric conversion element 22 converts into a sensor signal the first reflected light 18 incident on the photoelectric conversion element 22 after travelling from the condensing lens 21.</p><p id="p0062" num="0062">The second light sensor 30 includes a condensing lens 31 and a photoelectric conversion element 32.</p><p id="p0063" num="0063">The condensing lens 31 collects the second reflected light 19 which is the detection light 17 reflected off the instructing body 70, and focuses the second reflected light 19 to the photoelectric conversion element 32. The photoelectric conversion element 32 converts into a sensor signal the second reflected light 19 incident on the photoelectric conversion element 32 after travelling from the condensing lens 31.</p><p id="p0064" num="0064">The control unit 40 is a controller that holds information in which the above-described predetermined different timings (that is, timings of sampling) and the coordinate values in the raster scan with the detection light 17 are associated, and identifies a position of the instructing body 70. The control unit 40 reflects the result of identifying a position of the instructing body 70 in content that is displayed by the display unit 50.</p><p id="p0065" num="0065">The control unit 40 may be specifically a computer system configured from a central processing unit (CPU) 41, a random access memory (RAM) 42, and a read only memory (ROM) 43, for example. A portion or all of the functions of the control unit 40 may be achieved by the CPU 41 executing a program (not illustrated in the drawings) recorded on the ROM 43 with the use of the RAM 42 as a working<!-- EPO <DP n="14"> --> memory. Moreover, a portion or all of the functions of the control unit 40 may be configured from a dedicated hardware circuit.</p><p id="p0066" num="0066">Next, a functional structure of the control unit 40 is described.</p><p id="p0067" num="0067"><figref idrefs="f0004">FIG. 4</figref> is a block diagram illustrating one example of a functional structure of a control unit according to Embodiment 1.</p><p id="p0068" num="0068">As illustrated in <figref idrefs="f0004">FIG. 4</figref>, the control unit 40 includes a coordinate detecting unit 110 and an image processing unit 120. The coordinate detecting unit 110 obtains the first sensor signal A from the first light sensor 20, the second sensor signal B from the second light sensor 30, and a synchronization signal for the raster scan by the light source 10, analyzes the obtained signals, and detects a coordinate value of the instructing body 70. The image processing unit 120 generates an image that is to be displayed by the display unit 50, based on the coordinate value of the instructing body 70 obtained by the coordinate detecting unit 110.</p><p id="p0069" num="0069">The coordinate detecting unit 110 includes a determining unit 111, a data generating unit 112, and a coordinate generating unit 113.</p><p id="p0070" num="0070">The determining unit 111 obtains the first sensor signal A from the first light sensor 20, the second sensor signal from the second light sensor 30, and the synchronization signal for the raster scan by the light source 10. The determining unit 111 determines, for each of the sensor signals obtained at different timings, whether or not the second sensor signal B exceeds a predetermined threshold. When the determining unit 111 determines that the second sensor signal B exceeds the predetermined threshold, the determining unit 111 identifies, among the first sensor signals A detected at the different timings, the first sensor signal A detected at a detection timing that is a timing at which the second sensor signal B that exceeds the predetermined threshold is detected.</p><p id="p0071" num="0071">The data generating unit 112 generates instructing body data representing the shape, size, peak intensity, and instructed coordinates of the instructing body 70, using the first sensor signal A detected at the detection timing and identified by the determining unit 111. The instructing body data includes coordinates<!-- EPO <DP n="15"> --> representing a position of a tip of the instructing body 70. Note that the coordinates included in the instructing body data are not limited to the coordinates representing the position of the tip of the instructing body 70 and may be previously determined coordinates that represent a reference position on the instructing body 70.</p><p id="p0072" num="0072">The coordinate generating unit 113 outputs coordinates representing the position of the tip of the instructing body 70 based on the instructing body data generated by the data generating unit 112.</p><p id="p0073" num="0073"><figref idrefs="f0005">FIG. 5A</figref> is an image obtained by converting the first sensor signal A in one raster scan. <figref idrefs="f0005">FIG. 5B</figref> is an image obtained by converting the second sensor signal B that exceeds the predetermined threshold among the second sensor signals B in one raster scan. <figref idrefs="f0006">FIG. 5C</figref> is an image obtained by converting the first sensor signal A detected at a detection timing. Herein, "one raster scan" is raster scanning of an entire region of the detection interface 400 one time.</p><p id="p0074" num="0074">As illustrated in <figref idrefs="f0005">FIG. 5A</figref>, the first sensor signal A obtained by the first light sensor 20 in one raster scan is represented as a first image 121. The first image 121 is whiter where the first sensor signal has a larger detection value, and is blacker where the first sensor signal has a smaller detection value. This means that the first reflected light 18 which is light reflected off the instructing body 70 with higher reflection intensity is drawn in a whiter color in the first image 121.</p><p id="p0075" num="0075">As illustrated in <figref idrefs="f0005">FIG. 5B</figref>, the second sensor signal B obtained by the second light sensor 30 in one raster scan is represented as a second image 122. The second image 122 shows whether or not the second sensor signal B exceeds the predetermined threshold, thus representing two values in black and white. In detail, the second image 122 is white where the second sensor signal B exceeds the predetermined threshold, and is black where the second sensor signal B is less than or equal to the predetermined threshold. Therefore, the use of the second image 122 makes it possible to<!-- EPO <DP n="16"> --> identify a specific region 122a represented by the second sensor signal B that exceeds the predetermined threshold.</p><p id="p0076" num="0076">The first image 121 and the second image 122 respectively represent the first sensor signal A and the second sensor signal B obtained during one raster scan. Specifically, the first image 121 is represented by detection values of the first sensor signals A detected at the predetermined different timings, and respective coordinate values identified based on the timings. The second image 122 is represented by the result of the determination on whether or not the detection values of the second sensor signal B detected at the predetermined different timings exceed the predetermined threshold, and the respective coordinate values identified based on the timings.</p><p id="p0077" num="0077">Subsequently, only the specific region 122a of <figref idrefs="f0005">FIG. 5B</figref> is clipped from the first image 121 of <figref idrefs="f0005">FIG. 5A</figref> to obtain a clipped image 123, and thus it is possible to obtain an image converted from the first sensor signal A detected at the detection timing. In this way, the control unit 40 is capable of obtaining the clipped image 123 from the first sensor signal A and the second sensor signal B obtained in one raster scan, and performs image processing on the clipped image 123, thereby generating the instructing body data representing the shape, size, peak intensity, and instructed coordinates of the instructing body 70. Therefore, as compared to the case where only the first sensor signal A is used to generate instructing body data, the image processing for generating instructing body data is performed on the clipped image 123 that is smaller, with the result that the processing load to detect a coordinate value of the tip of the instructing body 70 is lower.</p><p id="p0078" num="0078">The image processing is for detecting a region having the highest peak intensity in the image obtained as the sensor signal. Alternatively, the image processing is for detecting a region having a value exceeding a predetermined threshold in the image obtained as the sensor signal. Alternatively, the image processing is for detecting, as a coordinate value of the instructing body, a reference point in the region having high peak intensity or in the region having a value exceeding a predetermined threshold.<!-- EPO <DP n="17"> --></p><p id="p0079" num="0079">Next, operation of the control unit 40 is described.</p><p id="p0080" num="0080"><figref idrefs="f0007">FIG. 6</figref> is a flowchart illustrating a detecting process in which a coordinate value of the instructing body is detected.</p><p id="p0081" num="0081">In the detecting process, first, the control unit 40 causes the light source 10 to emit the detection light 17, and raster scans the detection interface 400 with the detection light 17 (S11).</p><p id="p0082" num="0082">Next, the control unit 40 obtains the first sensor signal A detected by the first light sensor 20 (S12).</p><p id="p0083" num="0083">Furthermore, the control unit 40 obtains the second sensor signal B detected by the second light sensor 30 (S13).</p><p id="p0084" num="0084">The control unit 40 detects a coordinate value of the instructing body 70 using the first sensor signal A and the second sensor signal B (S14). In detail, the control unit 40 detects a coordinate value of the instructing body 70 using a sensor signal based on the first reflected light 18 detected by the first light sensor 20 at a timing when the second light sensor 30 detects the second reflected light 19. Specifically, the control unit 40 determines whether or not the second reflected light 19 has been detected. Specifically, the control unit 40 uses the determining unit 111 to determine whether or not the second sensor signal B detected by the second light sensor 30 exceeds the predetermined threshold, thereby determining whether or not the second reflected light 19 has been detected. The control unit 40 then detects a coordinate value using the first sensor signal A of when it is determined that the second reflected light 19 has been detected.</p><p id="p0085" num="0085">In the input device 100 according to Embodiment 1, the detection light 17 is emitted toward the detection interface 400, the first light sensor 20 is disposed on the light source 10 side, and the second light sensor 30 is disposed at a position that allows the second light sensor 30 to detect the second reflected light 19 which is light that has been reflected off the instructing body 70 that has entered the detection region R. In other words, the first light sensor 20 detects the first reflected light 18 which is light reflected toward the light source 10 among light beams reflected off the instructing body 70, and the second light sensor 30 detects the second reflected<!-- EPO <DP n="18"> --> light 19 which is light reflected in the detection region R which is a limited region of the detection region for light to be detected by the first light sensor 20. The control unit 40 then detects a coordinate value of the instructing body 70 using a sensor signal that is based on the first reflected light 18 detected by the first light sensor 20 at a timing when the second light sensor 30 detects the second reflected light 19.</p><p id="p0086" num="0086">Thus, a coordinate value of the instructing body 70 is detected using not the whole detection result for the first reflected light 18 detected by the first light sensor 20, but the detection result for the first reflected light 18 detected at a timing when the second light sensor 30 detects the second reflected light 19 (that is, a part of the detection result for the first reflected light 18), and therefore a coordinate value of the instructing body can be accurately identified with reduced processing load. Furthermore, a position of the instructing body 70 in the depth direction (the Z-axis direction) can be detected by the second light sensor 30, and since a coordinate value of the instructing body is detected using the first sensor signal A detected at the detection timing of the second light sensor 30, the first sensor signal A from the instructing body 70 located outside of the detection region R of the second light sensor 30 is not used in the processing of detecting a coordinate value. In other words, a coordinate value of a part of the instructing body 70 is identified, the coordinate value of which is necessary, that is, which is included in the detection region R, among parts of the instructing body 70, and therefore it is possible to reduce the detection error of the first sensor signal A.</p><p id="p0087" num="0087">According to the input apparatus 100 according to Embodiment 1, a mid-air image is projected, and the projected mid-air image is changed according to a coordinate value of the instructing body 70, and therefore it is possible to give a user a sense of directly manipulating the mid-air image, for example.</p><heading id="h0012"><b>Embodiment 2</b></heading><p id="p0088" num="0088">Next, an input apparatus 100a according to Embodiment 2 is described.<!-- EPO <DP n="19"> --></p><p id="p0089" num="0089"><figref idrefs="f0008">FIG. 7</figref> is a block diagram illustrating one example of a functional structure of an input apparatus according to Embodiment 2. <figref idrefs="f0009">FIG. 8</figref> is a block diagram illustrating one example of a functional structure of a control unit according to Embodiment 2. <figref idrefs="f0010">FIG. 9</figref> is an illustration for use in describing a detection region of second light sensors in the input apparatus according to Embodiment 2.</p><p id="p0090" num="0090">An input apparatus 100a according to Embodiment 2 is different from the input apparatus 100 according to Embodiment 1 in that the input apparatus 100a includes second light sensors 30a, 30b, ..., and display units 50a, 50b, .... Furthermore, the control unit 40 according to Embodiment 2 is different from that according to Embodiment 1 in that the control unit 40 according to Embodiment 2 obtains the second sensor signals B1, B2, ..., obtained by the second light sensors 30a, 30b, ..., and performs the processing of detecting a coordinate value of the instructing body 70 using the obtained second sensor signals B1, B2, ..., and in that the control unit 40 according to Embodiment 2 outputs images to the display units 50a, 50b, ..., according to the detected coordinate value of the instructing body 70. The other structural elements are the same or similar as those according to Embodiment 1 and therefore denoted by the same reference signs, and descriptions thereof are omitted.</p><p id="p0091" num="0091">The second light sensors 30a, 30b, ..., are disposed at different positions in the first direction (the Z-axis direction). In other words, the second light sensors 30a, 30b, ..., are aligned in the Z-axis direction. For the second light sensors 30a, 30b, 30c, ..., detection interfaces 401, 402, and 403 are defined respectively as illustrated in <figref idrefs="f0010">FIG. 9</figref>. The detection interface 401 closest to a user is defined as a first layer, and detection interfaces farther from the user than the first layer are defined as a second layer, a third layer, ..., in sequence from one close to the user. Specifically, the detection interface 402 is defined as the second layer, and the detection interface 403 is defined as the third layer. The second light sensors 30a, 30b, and 30c detect the second reflected light which is light reflected off the instructing body 70 that has entered detection regions R1, R2, and<!-- EPO <DP n="20"> --> R3, respectively, provided on the light source 10 side relative to the detection interfaces 401, 402, and 403.</p><p id="p0092" num="0092">The control unit 40 detects a coordinate value of the instructing body 70 preferentially using the second reflected light detected by the second light sensor disposed close to the light source 10 among the second reflected light beams detected by the second light sensors 30a, 30b, .... Specifically, for example, when the second reflected light is detected by the second light sensor 30a and the second light sensor 30b, a coordinate value of the instructing body 70 is detected preferentially using the result of the detection by the second light sensor 30b disposed closer to the light source 10 (farther from a user).</p><p id="p0093" num="0093">The display units 50a, 50b, 50c, ..., are disposed each having a display surface crossing the Z-axis direction, and are aligned in the Z-axis direction. In other words, the display units 50a, 50b, 50c, ..., are aligned in a light emitted direction in which the light source 10 emits light, and each display a display image crossing the light emitted direction. The image-forming unit 60 projects three-dimensional images as mid-air images to the detection regions R1, R2, R3, ..., by forming, in the detection regions R1, R2, R3, ..., images displayed on the display units 50a, 50b, ... (<figref idrefs="f0011">FIG. 10A</figref>). As illustrated in <figref idrefs="f0011">FIG. 10A</figref>, the most front part of a solid is displayed on the display unit 50a closest to a user, and a part of the solid farther from the user is displayed on the display unit farther from the user. Consequently, the mid-air images are combined so that a three-dimensional image having a depth (<figref idrefs="f0011">FIG. 10B</figref>) becomes visible to the user.</p><p id="p0094" num="0094"><figref idrefs="f0011">FIG. 10A</figref> is an illustration for use in describing mid-air images that are formed based on images displayed on the respective display units 50a, 50b, 50c, ... according to images displayed on the detection interfaces 401, 402, and 403. <figref idrefs="f0011">FIG. 10B</figref> is an illustration for use in describing a three-dimensional image that becomes visible to a user as a result of combination of mid-air images.</p><p id="p0095" num="0095">As illustrated in <figref idrefs="f0010">FIG. 9</figref>, the second light sensors 30a, 30b, 30c, ..., and the display units 50a, 50b, 50c, ..., are provided at mutually<!-- EPO <DP n="21"> --> corresponding positions. Specifically, the positions of the second light sensor 30a and the display unit 50a in the Z-axis direction correspond to each other, the positions of the second light sensor 30b and the display unit 50b in the Z-axis direction correspond to each other, and the positions of the second light sensor 30c and the display unit 50c in the Z-axis direction correspond to each other.</p><p id="p0096" num="0096">Also in the input apparatus 100a according to Embodiment 2, a detecting process such as that described in Embodiment 1 is performed.</p><p id="p0097" num="0097"><figref idrefs="f0012">FIG. 11</figref> is a flowchart for use in describing a specific example of the detecting process that is performed in the input apparatus according to Embodiment 2.</p><p id="p0098" num="0098">First, the detecting process by the light source 10 performing one raster scan in the X-axis direction starts (S21). The following describes one raster scan in the X-axis direction among raster scans in the X-axis direction that are performed in one raster scan. At this time, the first sensor signal A is obtained as the result of the detection by the first light sensor 20 at predetermined different timings even during one raster scan in the X-axis direction. Processes in Step S21 to Step S33 are sequentially performed on the respective sensor signals detected at the predetermined different timings during one raster scan in the X-axis direction.</p><p id="p0099" num="0099">First, a detection status of the second light sensors 30a, 30b, ..., is obtained (S22).</p><p id="p0100" num="0100">Among the detection results of the second light sensors 30a, 30b, ..., the second light sensor that has the highest priority (that is, closest to the light source 10) in the detection results that exceed a predetermined threshold is selected (S23).</p><p id="p0101" num="0101">Next, it is determined whether or not a current signal is a signal that is detecting a tip of the instructing body 70 among the first sensor signals detected at the predetermined different timings (S24).</p><p id="p0102" num="0102"><figref idrefs="f0013">FIG. 12</figref> is an illustration related to the first sensor signal, for use in describing the process of detecting the tip of the instructing body. <figref idrefs="f0013">FIG. 12</figref> is a graph representing values of the first sensor<!-- EPO <DP n="22"> --> signal A obtained by the first light sensor 20 in one raster scan in the X-axis direction. In detail, although the first sensor signal A is represented as a continuous curve in <figref idrefs="f0013">FIG. 12</figref>, the continuous curve is actually made up of discretely plotted dots representing the first sensor signals A obtained at the predetermined different timings. In short, the curve in the graph of <figref idrefs="f0013">FIG. 12</figref> is obtained by curve approximation of the discretely plotted dots.</p><p id="p0103" num="0103">In Step S24, it is determined from the graph whether or not the following condition is satisfied: the instructing body 70 is detected at the first or higher layer, and the slope of this curve in the graph is positive, and when this condition is satisfied, the process transitions to the phase in which the tip is being detected. When the curve falls by one step from a higher layer to a lower layer among the layers while the tip is being detected, it is determined that the tip of the instructing body 70 is not being detected.</p><p id="p0104" num="0104">When it is determined that the tip of the instructing body 70 is not being detected (No in S24), the priority at the current timing and the priority at the last timing are compared (S25). The priority herein is defined by the position of, among the second light sensors 30a, 30b, ..., the second light sensor that is closest to the light source 10 when detecting the second reflected light. This means that specifically, the second layer has a higher priority than the first layer.</p><p id="p0105" num="0105">When it is determined that the current priority is higher than the last priority, detection of a coordinate value using the first sensor signal A detected at the current timing starts (S30).</p><p id="p0106" num="0106">When it is determined that the tip of the instructing body 70 is being detected (Yes in S24), the priority at the current timing and the priority at the last timing are compared (S26). The process in Step S26 is the same or similar as the process in Step S25.</p><p id="p0107" num="0107">When it is determined in Step S26 that the current priority is higher than the last priority, detected point data is reset (S27). Specifically, the first sensor signal A loaded on and before the last detection is deleted.<!-- EPO <DP n="23"> --></p><p id="p0108" num="0108">When it is determined in Step S26 that the current priority is the same as the last priority, the processing proceeds to the next Step S31.</p><p id="p0109" num="0109">When it is determined in Step S26 that the current priority is lower than the last priority, the coordinates detected using data detected on and before the last detection are set (S28). Specifically, for example, when the last priority is the third layer and the current priority is the second layer, the detected coordinates are set using data that has priorities up to the third layer and is detected on and before the last detection, and the processing is ended (S29).</p><p id="p0110" num="0110">When Step S30, S27 or S29 is ended or when it is determined in Step S26 that the current priority is the same as the last priority, loading of the first sensor signal A is started (S31).</p><p id="p0111" num="0111">Subsequently, data of at least one of the contour and the peak intensity of the instructing body 70 is calculated using the first sensor signal A (S32).</p><p id="p0112" num="0112">Next, when Step S32 is ended or when it is determined in Step S25 that the current priority is lower than the last priority, the detecting process by one raster scan in the X-axis direction is ended (S33), and the detecting process by one raster scan in the X-axis direction for the next row in the Y-axis direction is performed, which is repeated from Step S21 until there is no longer next row in the Y-axis direction.</p><p id="p0113" num="0113">When the processes in Step S21 to Step S33 are ended for all the raster scans in the X-axis direction, the coordinate system of the obtained coordinate data is transformed (S34). Specifically, since the range of the raster scan by the light source 10 with the detection light 17 is different for each of the detection interfaces 401, 402, 403, ..., the coordinate systems of the coordinate data obtained on the detection interfaces 401, 402, 403, ..., are transformed into a single coordinate system.</p><p id="p0114" num="0114">Lastly, the coordinate values after the transformation are output, and the processing is ended (S35).</p><p id="p0115" num="0115">Note that the processes in Step S21 to Step S35 are for obtaining one frame of one image subject to the raster scan, and are<!-- EPO <DP n="24"> --> supposed to be performed sequentially for frames thereof, meaning that once Step S35 is ended, the processing returns to Step S21 to repeat the processes.</p><p id="p0116" num="0116">The input apparatus 100a according to Embodiment 2 detects a coordinate value of the instructing body 70 preferentially using the second reflected light detected by the second light sensor disposed close to the light source 10 among the second reflected light beams. With the input apparatus 100a, a user performs input from a position facing the light source 10. This means that in the input apparatus 100a such as that described above, for example, since the coordinate position of the tip of a finger of a user is necessary, the first sensor signal A obtained in a front area close to the user is unnecessary among the results obtained from the second light sensors 30a, 30b, 30c, .... Thus, a priority is given to a coordinate value of a user's instruction point in a back area in the Z-axis direction (close to the light source 10) so that no processing is performed on unnecessary data detected in the front area close to the user, and therefore it is possible to minimize data of the first sensor signal A that is subject to the process of detecting coordinates. Accordingly, the processing load related to the detecting process can be reduced.</p><p id="p0117" num="0117">In the input apparatus 100a according to Embodiment 2, the light source 10 starts the raster scan from the second light sensors 30a, 30b, 30c, ..., side of the detection interfaces 401, 402, 403, ..., so that the raster scan on positions that will be in the shadows of the second light sensors 30a, 30b, 30c, ..., is performed after positions that are less likely to be in the shadows. With this, the result of the detection by the second light sensors 30a, 30b, 30c, ..., can be reliably obtained.</p><p id="p0118" num="0118">Furthermore, in the input apparatus 100a according to Embodiment 2, the display units 50a, 50b, 50c, ..., aligned in the Z-axis direction are used to display a three-dimensional image, and therefore it is possible to show a user the three-dimensional image that has a depth in the Z-axis direction.</p><p id="p0119" num="0119">Furthermore, in the input apparatus 100a according to Embodiment 2, the second light sensors 30a, 30b, 30c, ..., are<!-- EPO <DP n="25"> --> disposed at positions corresponding to the display units 50a, 50b, 50c, ..., and therefore it is sufficient that the processing of detecting a coordinate value is performed directly using the results of the detection by the second light sensors 30a, 30b, 30c, .... Consequently, it is possible to omit the coordinate transformation process for matching the coordinates of the three-dimensional image shown to a user with the coordinates of the instructing body.</p><heading id="h0013"><b>Other Embodiments</b></heading><p id="p0120" num="0120">The input apparatus according to embodiments of the present invention has hereinbefore been described, but the present invention is not limited to these embodiments.</p><p id="p0121" num="0121">For example, in the input apparatuses 100 and 100a according to the above-described embodiments, the second light sensors 30, 30a, 30b, 30c, ..., are disposed on the positive side of the Y-axis relative to the respective detection regions R, R1, R2, R3, ..., but the positions of the second light sensors 30, 30a, 30b, 30c, ..., are not limited to such positions on the positive side of the Y-axis and may be on the negative side of the Y-axis, or on the positive side of the X-axis, or on the negative side of the X-axis. In short, the position of the second light sensor is not limited to a position on the positive side of the Y-axis as long as it is located in an area in a direction substantially perpendicular to the Z-axis direction relative to the corresponding detection region.</p><p id="p0122" num="0122">Furthermore, for example, in the input apparatuses 100 and 100a according to the above-described embodiments, the detection regions R, R1, R2, R3, ..., of the second light sensors 30, 30a, 30b, 30c, ..., further expand with increasing distances from the second light sensors 30, 30a, 30b, 30c, ..., as illustrated in <figref idrefs="f0002">FIG. 2</figref> and <figref idrefs="f0010">FIG. 9</figref>, but this is not the only example. For example, the detection region of the second light sensor may be a detection region parallel to the Y-axis direction, which has the same width regardless of how close to or how far from the second light sensor.</p><p id="p0123" num="0123">Furthermore, for example, in the input apparatus 100a according to the above-described Embodiment 2, the second light sensors 30a, 30b, 30c, ..., are aligned in a row along the Z-axis<!-- EPO <DP n="26"> --> direction, but this is not the only example, and it may be that a part of the second light sensors is disposed to detect light reflected off the instructing body in the second direction extending along the detection interface, and a remaining part of the second light sensors is disposed to detect light reflected off the instructing body in the third direction different from the second direction extending along the detection interface. In other words, one of the second light sensors may be disposed at such a position that the second light sensor detects light reflected, in a display region (the detection interface) for the display unit corresponding to the second light sensor, in a direction different from a direction of reflected light for another display unit that does not correspond to the second light sensor.</p><p id="p0124" num="0124">Specifically, one of the second light sensors is positioned closer to the Y-axis direction than the detection region R1, and another one of the second light sensors is positioned closer to the X-axis direction than the detection region R2.</p><p id="p0125" num="0125">Furthermore, for example, although not particularly mentioned in the above-described embodiments, it may be that the first light sensor receives light reflected off the instructing body toward one side in the Y-axis direction and the second light sensor receives light reflected off the instructing body toward the Y-axis direction. Thus, it is preferred that the first light sensor be disposed at such a position that the first light sensor can receive light reflected off the instructing body toward the side in the direction on which the detection region of the second light sensor is located. With this, the first light sensor and the second light sensor are capable of receiving the reflected light which is the detection light emitted from the light source 10 and reflected off the instructing body 10.</p><p id="p0126" num="0126">The reasons are as follows. If the first light sensor receives light reflected off the instructing body toward the negative side of the Y-axis when light reflected off the instructing body is received by the second light sensor disposed on the positive side of the Y-axis, there is high possibility that one of the reflected light received by the second light sensor and the reflected light received by the first light<!-- EPO <DP n="27"> --> sensor is not reflected light originated from the detection light emitted from the light source 10. This leads to the risk of decrease in accuracy of detecting a position of the instructing body. Thus, it is preferred that the first light sensor and the second light sensor be disposed at such positions that the first light sensor and the second light sensor can receive reflected light which is the detection light emitted from the light source 10 and reflected off the instructing body.</p><p id="p0127" num="0127">Furthermore, for example, in the input apparatus 100a according to the above-described Embodiment 2, the second light sensors 30a, 30b, 30c, ..., may be aligned in two or more rows in the Z-axis direction.</p><p id="p0128" num="0128">Furthermore, for example, the light source 10 according to each of the above-described embodiments includes an optical system (for example, a mirror) that ultimately directs light toward the first direction. This means that the light source includes even a light source that emits light toward a direction different from the first direction as long as the optical system such as a mirror therein ultimately directs the light toward the first direction.</p><p id="p0129" num="0129">Moreover, each of the above-described apparatuses may more specifically be a computer system configured from a microprocessor, a ROM, a RAM, a hard disk drive, a display unit, a keyboard, and a mouse, for example. A computer program is stored in the RAM or the hard disk drive. Each of the apparatuses achieves its functions as a result of the microprocessor operating according to the computer program. Here, the computer program is configured of pieced together instruction codes indicating a command to the computer in order to achieve a given function.</p><p id="p0130" num="0130">A portion or all of the components of each of the above-described apparatuses may be configured from one system large scale integration (LSI). A system LSI is a super-multifunction LSI manufactured with components integrated on a single chip, and specifically is a computer system configured of a microprocessor, a ROM, and a RAM, for example. The computer program is stored in<!-- EPO <DP n="28"> --> the RAM. The system LSI achieves its function as a result of the microprocessor operating according to the computer program.</p><p id="p0131" num="0131">A portion or all of the components of each of the above-described apparatuses may each be configured from an IC card that is detachably attached to each apparatus or a stand-alone module. The IC card and the module are computer systems configured from a microprocessor, a ROM, and a RAM, for example. The IC card and the module may include the super-multifunction LSI described above. The IC card and the module achieve their functions as a result of the microprocessor operating according to the computer program. The IC card and the module may be tamperproof.</p><p id="p0132" num="0132">An aspect of the present invention may also be the above-described methods. Moreover, the present invention may also be a computer program realizing these methods with a computer, or a digital signal of the computer program.</p><p id="p0133" num="0133">Moreover, the present invention may also be realized as the computer program or the digital signal stored on a non-transitory computer-readable recording medium, such as a flexible disk, a hard disk, CD-ROM, MO, DVD, DVD-ROM, DVD-RAM, Blu-rayâ„¢ Disc (BD), or a semiconductor memory. The present invention may also be the digital signal stored on the above-mentioned non-transitory recording medium.</p><p id="p0134" num="0134">Moreover, the present invention may also be realized by transmitting the computer program or the digital signal, for example, via an electric communication line, a wireless or wired communication line, a network such as the Internet, or data broadcasting.</p><p id="p0135" num="0135">Moreover, the present invention may be a computer system including a memory storing the computer program and a microprocessor operating according to the computer program.</p><p id="p0136" num="0136">Moreover, the computer program or the digital signal may be implemented by an independent computer system by being stored on the non-transitory recording medium and transmitted, or sent via the network, for example.<!-- EPO <DP n="29"> --></p><p id="p0137" num="0137">Furthermore, the preceding embodiments and the preceding variation examples may be individually combined.</p><p id="p0138" num="0138">Although the present invention has been described and illustrated in detail, it is clearly understood that the same is by way of example only and is not to be taken by way of limitation, the scope of the present invention being limited only by the terms of the appended claims.</p><heading id="h0014"><b>Industrial Applicability</b></heading><p id="p0139" num="0139">The present invention is useful as an input apparatus or the like that is capable of accurately identifying a position of an instructing body in three-dimensional space with a simple structure and reduced processing load.</p><heading id="h0015"><b>Reference Signs List</b></heading><p id="p0140" num="0140"><dl id="dl0001" compact="compact"><dt>10</dt><dd>light source</dd><dt>11</dt><dd>laser driver</dd><dt>12</dt><dd>infrared laser device</dd><dt>13</dt><dd>collimating lens</dd><dt>14</dt><dd>scan driver</dd><dt>15</dt><dd>actuator</dd><dt>16</dt><dd>mirror</dd><dt>17</dt><dd>detection light</dd><dt>18</dt><dd>first reflected light</dd><dt>19</dt><dd>second reflected light</dd><dt>20</dt><dd>first light sensor</dd><dt>21</dt><dd>condensing lens</dd><dt>22</dt><dd>photoelectric conversion element</dd><dt>30, 30a to 30c</dt><dd>second light sensor</dd><dt>31</dt><dd>condensing lens</dd><dt>32</dt><dd>photoelectric conversion element</dd><dt>40</dt><dd>control unit</dd><dt>41</dt><dd>CPU</dd><dt>42</dt><dd>RAM</dd><dt>43</dt><dd>ROM<!-- EPO <DP n="30"> --></dd><dt>50, 50a to 50c</dt><dd>display unit</dd><dt>60</dt><dd>image-forming unit</dd><dt>70</dt><dd>instructing body</dd><dt>100, 100a</dt><dd>input apparatus</dd><dt>110</dt><dd>coordinate detecting unit</dd><dt>111</dt><dd>determining unit</dd><dt>112</dt><dd>data generating unit</dd><dt>113</dt><dd>coordinate generating unit</dd><dt>120</dt><dd>image processing unit</dd><dt>121</dt><dd>first image</dd><dt>122</dt><dd>second image</dd><dt>122a</dt><dd>specific region</dd><dt>123</dt><dd>clipped image</dd><dt>150</dt><dd>projector</dd><dt>400, 401, 402, 403</dt><dd>detection interface</dd></dl></p></description><claims mxw-id="PCLM90459530" lang="EN" load-source="patent-office"><!-- EPO <DP n="31"> --><claim id="c-en-0001" num="0001"><claim-text>An input apparatus comprising:
<claim-text>a light source that emits light toward a predetermined region;</claim-text>
<claim-text>a first light-receiving unit that receives light reflected off a subject toward a first region and outputs a first detection signal;</claim-text>
<claim-text>a second light-receiving unit that receives light reflected off the subject toward a second region in a direction substantially perpendicular to a direction from the predetermined region toward the first region, and outputs a second detection signal; and</claim-text>
<claim-text>a control unit that determines a position of the subject using the first detection signal during a period for which the second light-receiving unit detects the reflected light.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The input apparatus according to claim 1,<br/>
wherein the control unit determines the position of the subject using received-light data obtained by the first light-receiving unit receiving light at the same timing as the second light-receiving unit receives reflected light of an amount exceeding a predetermined threshold.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The input apparatus according to claim2, further comprising a display unit that displays an image; and<br/>
an image-forming unit that projects a three-dimensional image to the predetermined region by forming in the predetermined region the image displayed by the display unit.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The input apparatus according to claim 2,<br/>
wherein the display unit comprises display units, and<br/>
the display units are aligned in a light emitted direction in which the light source emits light, and each display a display image crossing the light emitted direction.<!-- EPO <DP n="32"> --></claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The input apparatus according to claim 3,<br/>
wherein the predetermined region includes an image screen on which the image is formed by the image-forming unit.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The input apparatus according to claim 4,<br/>
wherein the second light-receiving unit comprises second light-receiving units, and<br/>
each of the second light-receiving units is provided at a position corresponding to a different one of the display units.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The input apparatus according to claim 6,<br/>
wherein the second light-receiving units are disposed at positions that allow each of the second light-receiving units to detect light reflected, in a display region for a corresponding one of the display units, in a direction different from a direction of reflected light for another one of the display units.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The input apparatus according to claim 1,<br/>
wherein the first light-receiving unit receives light reflected off the subject toward one side in the direction substantially perpendicular to the direction from the predetermined region toward the first region, and<br/>
the second light-receiving unit receives light reflected off the subject toward the direction substantially perpendicular to the direction from the predetermined region toward the first region.</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>The input apparatus according to claim 2, further comprising a projector that projects an image to the predetermined region.<!-- EPO <DP n="33"> --></claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>An input apparatus comprising:
<claim-text>a light source that emits detection light toward a first direction, and raster scans a detection interface with the detection light, the detection interface being defined in space;</claim-text>
<claim-text>a first light sensor that is disposed closer to the light source than the detection interface, and detects first reflected light which is the detection light that has been reflected;</claim-text>
<claim-text>a second light sensor that detects second reflected light which is the detection light that has been reflected off an instructing body that has entered a detection region extending from the detection interface toward the light source; and</claim-text>
<claim-text>a control unit that detects a coordinate value of the instructing body using received-light data obtained by the first light sensor and the second light sensor receiving light at the same timing.</claim-text></claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The input apparatus according to claim 10,<br/>
wherein the second light sensor comprises second light sensors,<br/>
the second light sensors are disposed at different positions in the first direction, and<br/>
the control unit detects the coordinate value of the instructing body preferentially using, among second reflected light beams detected by the second light sensors, a second reflected light beam detected by, among the second light sensors, a second light sensor disposed close to the light source.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>The input apparatus according to claim 10, further comprising a projector that projects a mid-air image to the detection region,<br/>
<!-- EPO <DP n="34"> -->wherein the control unit changes the mid-air image projected by the projector, according to the detected coordinate value of the instructing body.</claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>The input apparatus according to claim 12,<br/>
wherein the projector includes:
<claim-text>a display units aligned in the first direction and each having a display surface crossing the first direction; and</claim-text>
<claim-text>an image-forming unit that projects a three-dimensional image as the mid-air image to the detection region by forming in the detection region images displayed on the display units.</claim-text></claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>The input apparatus according to claim 13,<br/>
wherein the second light sensors are provided at respective positions corresponding to the display units.</claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>The input apparatus according to claim 11,<br/>
wherein the second light sensors are aligned in the first direction.</claim-text></claim><claim id="c-en-0016" num="0016"><claim-text>The input apparatus according to claim 11,<br/>
wherein a part of the second light sensors is disposed to detect light reflected off the instructing body in a second direction extending along the detection interface, and<br/>
a remaining part of the second light sensors is disposed to detect light reflected off the instructing body in a third direction different from the second direction extending along the detection interface.</claim-text></claim><claim id="c-en-0017" num="0017"><claim-text>The input apparatus according to claim 10,<br/>
<!-- EPO <DP n="35"> -->wherein the light source starts the raster scan from an area of the detection interface that is on a side on which the second light sensor is provided.</claim-text></claim></claims><drawings mxw-id="PDW20422253" load-source="patent-office"><!-- EPO <DP n="36"> --><figure id="f0001" num="1"><img id="if0001" file="imgf0001.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0002" num="2"><img id="if0002" file="imgf0002.tif" wi="165" he="210" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> --><figure id="f0003" num="3"><img id="if0003" file="imgf0003.tif" wi="165" he="215" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> --><figure id="f0004" num="4"><img id="if0004" file="imgf0004.tif" wi="158" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> --><figure id="f0005" num="5A,5B"><img id="if0005" file="imgf0005.tif" wi="164" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> --><figure id="f0006" num="5C"><img id="if0006" file="imgf0006.tif" wi="46" he="91" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="42"> --><figure id="f0007" num="6"><img id="if0007" file="imgf0007.tif" wi="89" he="174" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="43"> --><figure id="f0008" num="7"><img id="if0008" file="imgf0008.tif" wi="161" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="44"> --><figure id="f0009" num="8"><img id="if0009" file="imgf0009.tif" wi="155" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="45"> --><figure id="f0010" num="9"><img id="if0010" file="imgf0010.tif" wi="165" he="183" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="46"> --><figure id="f0011" num="10A,10B"><img id="if0011" file="imgf0011.tif" wi="156" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="47"> --><figure id="f0012" num="11"><img id="if0012" file="imgf0012.tif" wi="157" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="48"> --><figure id="f0013" num="12"><img id="if0013" file="imgf0013.tif" wi="165" he="169" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="160" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="160" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
