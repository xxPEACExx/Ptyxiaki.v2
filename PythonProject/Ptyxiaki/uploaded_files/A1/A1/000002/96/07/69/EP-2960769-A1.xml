<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2960769-A1" country="EP" doc-number="2960769" kind="A1" date="20151230" family-id="51059295" file-reference-id="305014" date-produced="20180824" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160451508" ucid="EP-2960769-A1"><document-id><country>EP</country><doc-number>2960769</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14174048-A" is-representative="YES"><document-id mxw-id="PAPP193865984" load-source="docdb" format="epo"><country>EP</country><doc-number>14174048</doc-number><kind>A</kind><date>20140626</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193865985" load-source="patent-office" format="original"><country>EP</country><doc-number>14174048.0</doc-number><date>20140626</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162032114" ucid="EP-14174048-A" load-source="docdb"><document-id format="epo"><country>EP</country><doc-number>14174048</doc-number><kind>A</kind><date>20140626</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988521781" load-source="docdb">G06F   3/042       20060101ALI20140725BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988523967" load-source="docdb">G06F   3/041       20060101ALI20140725BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988525338" load-source="docdb">G06F   3/0488      20130101AFI20140725BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1987769960" load-source="docdb" scheme="CPC">G06F   3/041       20130101 LI20140723BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987779268" load-source="docdb" scheme="CPC">G06F   3/0425      20130101 LI20140723BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987783784" load-source="docdb" scheme="CPC">G06F   3/0488      20130101 FI20140723BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165545990" lang="DE" load-source="patent-office">Verfahren zum Bereitstellen von Eingabedaten unter Verwendung einer tastbaren Benutzerschnittstelle</invention-title><invention-title mxw-id="PT165545991" lang="EN" load-source="patent-office">Method for providing data input using a tangible user interface</invention-title><invention-title mxw-id="PT165545992" lang="FR" load-source="patent-office">Procédé pour fournir des données d'entrée en utilisant un interface utilisateur tangible</invention-title><citations><patent-citations><patcit mxw-id="PCIT335962093" load-source="docdb" ucid="US-20060066564-A1"><document-id format="epo"><country>US</country><doc-number>20060066564</doc-number><kind>A1</kind><date>20060330</date></document-id><sources><source name="SEA" category="I" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT389043393" load-source="docdb" ucid="US-6175954-B1"><document-id format="epo"><country>US</country><doc-number>6175954</doc-number><kind>B1</kind><date>20010116</date></document-id><sources><source name="SEA" category="XI" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>B. PIPER; C. RATTI; H. ISHII: "Illuminating Clay: A 3-D Tangible Interface for Landscape Analysis", CHI 2002, 20 April 2002 (2002-04-20), pages 355 - 362, XP001099426, DOI: doi:10.1145/503376.503439</text><sources><source mxw-id="PNPL72805842" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>E. W. PEDERSON; K. HORNBAEK: "Tangible Bots: Interaction with Active Tangibles in Tabletop Interfaces", CH 2011, 7 May 2011 (2011-05-07)</text><sources><source mxw-id="PNPL62638802" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>F. BOERS: "Applied Linguistics", vol. 21/4, 2000, OXFORD UNIVERSITY PRESS, article "Metaphor Awareness and Vocabulary Retention", pages: 553 - 571</text><sources><source mxw-id="PNPL62638803" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>H. ISHII: "The Tangible User Interface and its Evolution", COMM. OF THE ACM, vol. 51, no. 6, 2008, pages 32 - 36, XP058130524, DOI: doi:10.1145/1349026.1349034</text><sources><source mxw-id="PNPL72805843" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>J. PATTEN; B. RECHT; H. ISHII: "Audiopad: A Tag-based Interface for Musical Performance", NIME'02, PROCEEDINGS OF THE 2002 CONFERENCE ON NEW INTERFACES FOR MUSICAL EXPRESSION, 2002</text><sources><source mxw-id="PNPL62638805" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>J. UNDERKOFFLER; H. ISHII: "Urp: A Luminous-Tangible Workbench for Urban Planning and Design", CHI 99, 15 May 1999 (1999-05-15), pages 386 - 393, XP000894244, DOI: doi:10.1145/302979.303114</text><sources><source mxw-id="PNPL72805844" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>K. P. FISHKIN; PERS. UBIQUIT: "A taxonomy for and analysis of tangible interfaces", COMPUT., vol. 8, 2004, pages 347 - 358, XP058201444, DOI: doi:10.1007/s00779-004-0297-4</text><sources><source mxw-id="PNPL72805845" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>M. KALTENBRUNNER; R. BENCINA: "reacTIVision: A Computer-Vision Framework for Table-Based Tangible Interaction", TEI'07, 15 February 2007 (2007-02-15), pages 69 - 74, XP002579861</text><sources><source mxw-id="PNPL72805846" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>M. WEISS; J.D. HOLLAN; J. BORCHERS: "Tabletops - Horizontal Interactive Displays, Human-Computer Interaction Series, 149-170", 2010, SPRINGER-VERLAG, article "Augmenting Interactive Tabletops with Translucent Tangible Controls"</text><sources><source mxw-id="PNPL62638809" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>S. JORDA; G. GEIGER; M. ALONSO; M. KALTENBRUNNER: "The reacTable: Exploring the Synergy between Live Music Performance and Tabletop Tangible Interfaces", TEI'07, 15 February 2007 (2007-02-15), pages 139 - 146</text><sources><source mxw-id="PNPL62638810" load-source="docdb" name="APP"/></sources></nplcit><nplcit><text>V. MAQUIL; E. RAS; O. ZEPHIR: "Mensch &amp; Computer Workshopband", 2011, article "Understanding the characteristics of metaphors in tangible user interfaces", pages: 47 - 52</text><sources><source mxw-id="PNPL62638811" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103339793" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>LUXEMBOURG INST OF SCIENCE AND TECHNOLOGY LIST</last-name><address><country>LU</country></address></addressbook></applicant><applicant mxw-id="PPAR1103307784" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>LUXEMBOURG INSTITUTE OF SCIENCE AND TECHNOLOGY (LIST)</last-name></addressbook></applicant><applicant mxw-id="PPAR1101646182" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Luxembourg Institute of Science and Technology (LIST)</last-name><iid>101531335</iid><address><street>5, avenue des Hauts-Fourneaux</street><city>4362 Esch-sur-Alzette</city><country>LU</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103337509" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>TOBIAS ERIC</last-name><address><country>LU</country></address></addressbook></inventor><inventor mxw-id="PPAR1103318695" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>TOBIAS, ERIC</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647083" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>TOBIAS, ERIC</last-name><address><street>173, rue de Belvaux</street><city>4026 Esch-sur-Alzette</city><country>LU</country></address></addressbook></inventor><inventor mxw-id="PPAR1103325926" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>MAQUIL VALÉRIE</last-name><address><country>LU</country></address></addressbook></inventor><inventor mxw-id="PPAR1103334346" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>Maquil, Valérie</last-name></addressbook></inventor><inventor mxw-id="PPAR1101642702" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>Maquil, Valérie</last-name><address><street>21, rue Jean-Pierre Biermann</street><city>1268 Luxembourg</city><country>LU</country></address></addressbook></inventor><inventor mxw-id="PPAR1103329302" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>LATOUR THIBAUD</last-name><address><country>BE</country></address></addressbook></inventor><inventor mxw-id="PPAR1103311234" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>Latour, Thibaud</last-name></addressbook></inventor><inventor mxw-id="PPAR1101643990" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>Latour, Thibaud</last-name><address><street>42, rue Saint-Jean</street><city>6700 Arlon</city><country>BE</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101650489" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Lecomte &amp; Partners</last-name><iid>101278441</iid><address><street>P.O. Box 1623</street><city>1016 Luxembourg</city><country>LU</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660606774" load-source="docdb">AL</country><country mxw-id="DS660610980" load-source="docdb">AT</country><country mxw-id="DS660783657" load-source="docdb">BE</country><country mxw-id="DS660688295" load-source="docdb">BG</country><country mxw-id="DS660686099" load-source="docdb">CH</country><country mxw-id="DS660783658" load-source="docdb">CY</country><country mxw-id="DS660611447" load-source="docdb">CZ</country><country mxw-id="DS660606780" load-source="docdb">DE</country><country mxw-id="DS660783659" load-source="docdb">DK</country><country mxw-id="DS660783660" load-source="docdb">EE</country><country mxw-id="DS660685581" load-source="docdb">ES</country><country mxw-id="DS660688296" load-source="docdb">FI</country><country mxw-id="DS660688297" load-source="docdb">FR</country><country mxw-id="DS660606781" load-source="docdb">GB</country><country mxw-id="DS660783661" load-source="docdb">GR</country><country mxw-id="DS660606782" load-source="docdb">HR</country><country mxw-id="DS660611448" load-source="docdb">HU</country><country mxw-id="DS660686100" load-source="docdb">IE</country><country mxw-id="DS660783662" load-source="docdb">IS</country><country mxw-id="DS660688298" load-source="docdb">IT</country><country mxw-id="DS660783663" load-source="docdb">LI</country><country mxw-id="DS660688303" load-source="docdb">LT</country><country mxw-id="DS660610981" load-source="docdb">LU</country><country mxw-id="DS660688304" load-source="docdb">LV</country><country mxw-id="DS660688305" load-source="docdb">MC</country><country mxw-id="DS660610982" load-source="docdb">MK</country><country mxw-id="DS660610983" load-source="docdb">MT</country><country mxw-id="DS660688306" load-source="docdb">NL</country><country mxw-id="DS660686101" load-source="docdb">NO</country><country mxw-id="DS660688315" load-source="docdb">PL</country><country mxw-id="DS660685582" load-source="docdb">PT</country><country mxw-id="DS660606788" load-source="docdb">RO</country><country mxw-id="DS660685587" load-source="docdb">RS</country><country mxw-id="DS660688316" load-source="docdb">SE</country><country mxw-id="DS660692070" load-source="docdb">SI</country><country mxw-id="DS660686102" load-source="docdb">SK</country><country mxw-id="DS660686107" load-source="docdb">SM</country><country mxw-id="DS660783664" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166479878" lang="EN" load-source="patent-office"><p id="pa01" num="0001">The invention provides a method and device for using a Tangible User Interface, TUI, to collaboratively provide data input at a computing device. The method enables the use of strong metaphors for associating multiple predetermined choices in reply to an input request formulated by a software application that is executed by a computing device to which the TUI provides an input interface.
<img id="iaf01" file="imgaf001.tif" wi="129" he="61" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166759690" lang="EN" source="EPO" load-source="docdb"><p>The invention provides a method and device for using a Tangible User Interface, TUI, to collaboratively provide data input at a computing device. The method enables the use of strong metaphors for associating multiple predetermined choices in reply to an input request formulated by a software application that is executed by a computing device to which the TUI provides an input interface.</p></abstract><description mxw-id="PDES98404579" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001"><b>Technical field</b></heading><p id="p0001" num="0001">The invention relates to the field of Tangible User Interfaces, TUI. Specifically, the invention relates to a method for providing data input at a computing device, wherein the data input is based on multiple values provided by one or more users using tangible user interface elements.</p><heading id="h0002"><b>Background of the invention</b></heading><p id="p0002" num="0002">The area of human-computer interfaces relies heavily on the use of metaphors. For example, in the field of Graphical User Interfaces, GUI, symbols are commonly used as metaphors for implying the functionality of a user interface element. The use of efficient metaphors renders a user interface easy to manipulate and reduces the time and effort required by a user to get accustomed to using the interface. For example, a GUI button bearing the image of a floppy disk drive is instantly understood by a user as leading to the underlying software functionality of saving a document when the button is clicked using a pointing device. The stronger the user's association of the used metaphor with the associated functionality is, the more efficient an interface element will be.</p><p id="p0003" num="0003">Touch-based user interfaces have been able to propose several strong metaphors. As an example, the known feature of pinching fingers on an image displayed on a touch-sensitive display to zoom into the image, improves the usability of the software-implemented zoom functionality, as the user quickly associates the metaphor of the touch gesture to the underlying implied functionality.</p><p id="p0004" num="0004">Human beings are used to manipulating physical objects. Therefore, the potential of using a Tangible User Interface, TUI, which relies on the interaction of a user with physical objects to interact with software functionalities, has been identified as a good candidate for providing strong user interface metaphors. For a TUI to be usable in a natural way, using not only the user's visual capacities as traditional GUIs do, but also to convey information on a haptic and topological level, the interface needs to apply an efficient set of metaphors to imply functionality [1,2]. It is known that the application of metaphors works by designing the physical part of a Tangible User Interface, also called the tangible widgets, to imply a metaphorical context [3]. The implication of functionality relies mainly on the shape, color and texture of the objects that are used as tangible widgets. However, any physical<!-- EPO <DP n="2"> --> stimulus may be used to imply functionality, such as for example smells or sounds. The metaphorical context allows for the application of well-known metaphors to be used, mapping the source domain (containing a familiar concept such as rotating a radial or shaping clay) into the target domain (altering the state of a digital object).</p><p id="p0005" num="0005">Recent research in TUIs [4,5,6] has focused on the use of table-top surface-like devices. A milestone which has triggered a lot of research and implementations using tabletop TUIs has been reacTable [7], implementing an electronic music instrument. The work spent on the reacTable has been refined and distilled into the reacTIVision framework [8]. The framework provides facilities for tracking, using a below-table infrared camera, objects on the table via fiducial markers attached to their base.</p><p id="p0006" num="0006">Existing tabletop applications provide a variety of interaction components. They use the paradigm of direct manipulation [4] or convey a more generic meaning [9]. Most known approaches use a direct mapping between a single object that is manipulated and a single associated input data value. Some known examples [10] allow for dual hand input and have some specific interactions which specifically require the concurrent use of two pucks. Further, widgets that are able to alter their position or shape [11], and hence can provide haptic feedback, have been disclosed.</p><p id="p0007" num="0007">Tangible User Interfaces for assessing collaborative problems have only been marginally investigated in the prior art. To the best of the applicant's knowledge, none of the disclosed methods for using Tangible User Interfaces provides an efficient metaphor for enabling the collaborative input of data by multiple users in a collaborative way. An example of an application in which such data input is needed may be a collaborative choice application or a collaborative vote among possible data input values.</p><heading id="h0003"><b>Technical problem to be solved</b></heading><p id="p0008" num="0008">It is an objective of the present invention to provide a method and device for providing data input at a computing device using a tangible human-computer interface, which overcomes or alleviates at least some of the drawbacks of the prior art.<!-- EPO <DP n="3"> --></p><heading id="h0004"><b>Summary of the invention</b></heading><p id="p0009" num="0009">According to a first aspect of the invention, a method for providing data input at a computing device using a tangible human-computer interface is provided. The method comprises the steps of:
<ul><li>providing a tangible human-computer interface comprising physical support means having a plurality of support portions, processing means, and probing means capable of detecting physical objects on each of said support portions;</li><li>associating a predetermined data value with each one of said support portions;</li><li>probing the presence of any physical objects on any of said support portions, and</li><li>generating the data input and providing it at the computing device.</li></ul></p><p id="p0010" num="0010">The step of data input generation comprises associating the physical objects detected on a support portion with the corresponding predetermined data value.</p><p id="p0011" num="0011">The step of detecting at least one physical object may preferably comprise counting the number of objects detected on any of said support portions. Even more preferably, the step may comprise detecting physical properties of any of the detected objects, said properties comprising, for example, any of the shape, physical dimensions, weight, or color of the objects.</p><p id="p0012" num="0012">The step of data input generation may preferably comprise associating the number of detected physical objects on a portion and/or their detected physical properties with the corresponding predetermined data value.</p><p id="p0013" num="0013">The physical support means may preferably comprise a substantially planar surface, and said support portions may preferably be portions of said surface.</p><p id="p0014" num="0014">The probing means may preferably be optical probing means comprising image sensing means. Alternatively, the probing means may comprise means capable of detecting electromagnetic signals, or metal probing means.</p><p id="p0015" num="0015">The human-computer interface may further preferably comprise a substantially planar surface, wherein the probing means are arranged so that they are capable of detecting the presence of said physical support means and/or physical objects on said surface.</p><p id="p0016" num="0016">Said surface of the human-computer interface may preferably comprise the physical support means.<!-- EPO <DP n="4"> --></p><p id="p0017" num="0017">Further, the human-computing interface may preferably comprise feedback means. The feedback means may comprise visual feedback means provided by display means or projection means. The feedback means may further comprise audio feedback means or haptic feedback means. The feedback means may be operatively connected to the processing means.</p><p id="p0018" num="0018">The method may advantageously further comprise the step of providing feedback, wherein the feedback indicates the location of the support portions and/or the predetermined data values associated with said support portions.</p><p id="p0019" num="0019">The method may further preferably comprise the step of providing a feedback that is generated upon provision of said data input at said computing device.</p><p id="p0020" num="0020">The substantially planar surface of the human-computer interface may preferably be at least partially translucent. The probing means may preferably comprise image sensing means and optical means, and may further preferably be provided underneath said surface and comprise a field of view in which they are capable of detecting objects. The field of view may be directed towards said surface and said portions of the physical support means may be at least partially translucent, so that said probing means are capable of detecting a physical object on said portion from underneath, when the physical support means are placed on top of said surface.</p><p id="p0021" num="0021">The probing means may further preferably be configured for detecting physical properties of said objects.</p><p id="p0022" num="0022">More preferably, the probing means may further be configured for detecting the relative positions of objects that are detected on any given surface portion with respect to each other. The step of generating data input may comprise associating the detected objects with the corresponding predetermined data value depending on their detected relative positions.</p><p id="p0023" num="0023">Advantageously, the step of generating the input data may comprise the computation of a weighted average of the predetermined values, wherein the weights correspond to the number of physical objects detected on the support portions corresponding to said values. Further, the weights may preferably depend on the properties, physical or other, of the objects detected on the support portions corresponding to the predetermined values.<!-- EPO <DP n="5"> --> According to another aspect of the invention, a device for carrying out the method according to the invention is provided. The device comprises a tangible human-computer interface, a memory element, and processing means, wherein the human-computer interface comprises physical support means having a plurality of support portions and probing means capable of detecting physical objects on each of said support portions. The processing means are configured to:
<ul><li>associate a predetermined data value, stored in said memory element, with each one of said support portions;</li><li>probing, using said probing means, the presence of any physical objects on any of said support portions, and</li><li>generate data input and store it in said memory element, wherein the generation of the data input comprises associating the physical objects detected on a support portion with the corresponding predetermined data value.</li></ul></p><p id="p0024" num="0024">According to a further aspect of the invention, a computer capable of carrying out the method according to the invention is provided.</p><p id="p0025" num="0025">According to yet another aspect of the invention, a computer program is provided. The computer program comprises computer readable code, which when run on a computer, causes the computer to carry out the method according to the invention.</p><p id="p0026" num="0026">According to a further aspect of the invention, a computer program product comprising a computer-readable medium is provided. The computer program according to the invention is stored on the computer-readable medium.</p><p id="p0027" num="0027">The invention relates to Tangible User Interfaces and provides an efficient metaphor for enabling the collaborative input of data by multiple users. An example of an application in which the invention finds its use may be a collaborative choice application or a collaborative vote among possible data input values.</p><p id="p0028" num="0028">If the method is used by a single user, the user may provide multi-dimensional input data value in a natural way. For example, the user of a computing device may be confronted with a request from a software application that is executed on the device, for providing a combination of three predetermined color component values (Red, Green, and Blue). Using the invention, he may place objects onto three surface portions of the human-computer interface that represent the three options. The method of the invention allows to weight the Red, Green, and Blue components that are chosen and to provide the resulting combination<!-- EPO <DP n="6"> --> as input to the software application. This may for example be achieved either by considering the number of objects he puts on the respective surface portions, or for example by measuring their physical weight, or other physical properties. A visual feedback channel may be provided in such an application, wherein the resulting color mix is visualized instantly to the user. The user may in return modify the position of the objects to change the data input.</p><p id="p0029" num="0029">If the method is used by multiple users, collaborative choice or voting applications are enabled. For example, each one of a plurality of users of a software application that is executed on a computing device may use objects of a specific color and put them onto support portions representing predetermined data values. The overall number of objects detected as a consequence on the support portions may for example lead to the overall voting result, based on majority or weighted voting. The contribution of each user's input to the overall result may further be used as input by also taking the color of the detected objects on each support portion into account. Each user can manipulate their respective objects independently and concurrently with each other user. Specifically, each user may modify their contribution as they wish, i.e., incrementally by adding/removing a single object from the corresponding surface portion, or by adding/removing a plurality of objects at the same time. The interaction with the computing device is, therefore, enriched and rendered more natural by the present invention. The data input contribution provided by each user remains persistent through the use of physical objects. The position of the physical objects on the support means indicates the current data input contributions of each user at any time instant.</p><p id="p0030" num="0030">The method and device according to the invention enable collaborative data input and data manipulation at a computing device, which have not been achieved using known human-computer interfaces. The underlying principle of the claimed invention may find application in many specific data input situations, mainly wherein collaborative input by multiple users is required.</p><heading id="h0005"><b>Brief description of the drawings</b></heading><p id="p0031" num="0031">Several embodiments of the present invention are illustrated by way of figures, which do not limit the scope of the invention, wherein:
<ul><li><figref idrefs="f0001">Figure 1</figref> if a flow diagram illustrating the main method steps according to a preferred embodiment of the method according to the invention;<!-- EPO <DP n="7"> --></li><li><figref idrefs="f0001">Figure 2</figref> is a schematic illustration of a preferred embodiment of a device for implementing the method according to the invention;</li><li><figref idrefs="f0001">Figure 3</figref> is a schematic illustration of a preferred embodiment of a device for implementing the method according to the invention;</li><li><figref idrefs="f0001">Figure 4</figref> is a schematic illustration of a detail of a preferred embodiment of a device for implementing the method according to the invention;</li><li><figref idrefs="f0001">Figure 5</figref> is a schematic illustration of a detail of a preferred embodiment of a device for implementing the method according to the invention in a lateral cut view;</li><li><figref idrefs="f0002">Figures 6a, 6b and 6c</figref> are top views of a detail of a device for implementing the method according to the invention, according to three preferred embodiments.</li></ul></p><heading id="h0006"><b>Detailed description of the invention</b></heading><p id="p0032" num="0032">This section describes the invention in further detail based on preferred embodiments and on the figures, without limiting the scope of the invention to the disclosed examples.</p><p id="p0033" num="0033">Throughout the description, similar reference numbers will be used to indicate similar concepts across different embodiments of the invention. For example, references 100, 200, 300, 400 each indicate a tangible Human-Computer Interface as required by the invention, in four different embodiments. The described embodiments are provided as examples only. It should be understood that features disclosed for a specific embodiment may be combined with the features of other embodiments, unless the contrary is specifically stated.</p><p id="p0034" num="0034"><figref idrefs="f0001">Figure 1</figref> shows the main steps of the method according to the invention in a preferred embodiment. <figref idrefs="f0001">Figure 2</figref> illustrates elements of a device for implementing the method according to the invention. In a first step 10 a human-computer interface 100, specifically a Tangible User Interface, TUI, is provided. The TUI allows one or more users to provide input to a computing device 190, for which it acts as the interface. The interface comprises probing means 130 capable of detecting physical objects 140 on portions 112 of a physical support means 110. The physical support means may be a table top or a physical TUI widget placed upon a table top, or it may have any geometrical shape that allows it to support a physical object 140 detectable by said probing means. The probing means 130 are operatively connected to processing means 120 such as a central processing unit, CPU. The processing means 120 have read/write access to a non-illustrated memory element.<!-- EPO <DP n="8"> --></p><p id="p0035" num="0035">In a second step 20, a predetermined data value 114 is associated with each one of said support portions 112. In a scenario in which an application that is executed by the computing device 190 prompts a user for an answer, each value 114 may for example represent a possible answer to the question. The mapping between support portions 112 and predetermined data values 114 is stored in the memory element to which the processing means 120 have read access.</p><p id="p0036" num="0036">In a subsequent step 30, the presence of physical objects 140 on any of said support portions 112 is probed by said probing means 130. In a final step 40, data input 102 is generated and provided to the computing device 190. The generation of the data input comprises associating the physical objects 140 detected on a support portion 112 with the corresponding predetermined data value 114. This enables for example the selection of one of a plurality of predetermined answers to a question, or to weigh each of the predetermined values according to the number of detected objects on the corresponding support portion. The data input may for example be a weighted average of the predetermined values 114, wherein the weights are given as a function of the number of objects detected on the corresponding support portions. For specific applications, other useful combinations of the predetermined data values based on the detected presence of physical objects on the corresponding support portions will be applicable by the skilled person, without leaving the scope of the present invention.</p><p id="p0037" num="0037">As shown by the dashed lines on <figref idrefs="f0001">Figure 1</figref>, the method may be conditionally repeated by updating the predetermined data values in step 20, preferably based on the provided input in the earlier instance of the method.</p><p id="p0038" num="0038"><figref idrefs="f0001">Figure 3</figref> shows a further embodiment of the device according to the invention, wherein the human-computer interface 200 comprises a substantially planar surface 204, on which the physical support means 210 having the support portions 212 are placed. The planar surface 204 may for example be a table-top whereas the support means 210 are a physical TUI widget delimiting the regions corresponding to portions 212. A user may manipulate objects 240 on any one of the support portions 212, before passing the widget on to a different user, who in turn manipulates the objects on any of the support portions, whereby the two or more users collaboratively contribute to the data input. Probing means 230 are arranged so that they are capable of detecting the presence of the support means 210 as well as objects on the planar surface. The probing means may for example comprise optical probing means 232 such as image sensing means or image depth sensing means coupled to appropriate focusing lenses. The image sensing means may be provided by means of a<!-- EPO <DP n="9"> --> charge-coupled-device array, while depth sensing means may be provided by means of a time-of-flight camera. The field of view of such probing means, indicated by dashed lines on <figref idrefs="f0001">Figure 3</figref>, is in that case oriented so as to cover at least the part of the surface 204 onto which the support means 210 may be placed. Using the processing means 220, the detection of objects 240 based on the data provided by such probing means 230 is readily achieved. Image segmentation and object detection algorithms useful to that end are as such known in the art and will not be explained in further detail within the context of the present invention.</p><p id="p0039" num="0039"><figref idrefs="f0001">Figure 4</figref> illustrates another embodiment according to which the planar surface 304 comprises the physical support means 310 having the support portions 312. The planar surface may for example be a surface on which visual feedback is provided using feedback means operatively coupled to the processing means. Specifically, the surface may comprise a display device, which indicates the regions corresponding to the surface portions 312 to a user. The visual feedback may alternatively be provided using display projection means. In other embodiments, the location of the surface portions 312 may be indicated by haptic feedback means.</p><p id="p0040" num="0040">The feedback means may also be used to provide feedback on the generated input to one or more users who have provided the objects based on which the input has been generated. Such feedback may comprise optical feedback, haptic feedback, audio feedback, or any combinations thereof. By moving one or more objects from one surface portion 312 to another, the impact on the generated data input is in that case immediately appreciable by the user. The feedback may also comprise updating the predetermined values 314 associated to the support portions 312 as a function of the generated input data.</p><p id="p0041" num="0041"><figref idrefs="f0001">Figure 5</figref> schematically illustrates a lateral cut view of a further preferred embodiment of a device according to the invention. In this embodiment, the probing means 433 are arranged underneath the surface 404 on which the support means 410 are placed. In case the probing means 433 comprise optical probing means as described above, both the surface 404 and the regions of the support means corresponding to the support portion 412 are made of a material that is at least partially translucent. This is to make sure that an object 440 supported by the support means 410 is not occluded from the field of view of the probing means by either the support means 410 or the surface 404. The probing means 433 may for example be optical scanning means.<!-- EPO <DP n="10"> --></p><p id="p0042" num="0042">In all embodiments, detection of a support portion and of the presence of one or several objects on a support portion may be achieved by several techniques as such known in the art. For example, an object to be detected may be tagged with an optical marker or graphical tag easily recognizable by the optical probing means. The tag may be a simple high-contrast dot, or a combination of black and white squares.. The tags should be arranged on the objects so that they are within the field of view of and capable of being detected by the probing means. In the embodiment shown in <figref idrefs="f0001">Figure 3</figref>, the tag should be arranged on a side of an object 240 which is opposed to the side facing the support portion 212. Conversely, in the embodiment shown in <figref idrefs="f0001">Figure 5</figref>, the tag should be on the bottom side of the object 440, which corresponds to the side that contacts the support portion 412. The tags, if they uniquely identify each object, may also be used by the processing means to identify the objects placed on the support portions.</p><p id="p0043" num="0043">The tags may alternatively emit electromagnetic signals, which are detectable by the appropriately chosen probing means. Other tagging means are known in the art and applicable to the invention. Their specific description would, however, extend beyond the description of the invention as such.</p><p id="p0044" num="0044">During the data input generation step, the identity of each detected object on a detected support portion may lead to an identity-specific association of the object with the corresponding predetermined data value. For example, if multiple users provide objects on the support means, all objects of a particular user may be tagged using the same tag, while each user is attributed a distinct tag. The contribution of each user to the generated input value may then be straightforwardly tracked by the processing means.</p><p id="p0045" num="0045">A similar function may be achieved by using objects having different physical properties, such as size, shape, color or others, provided that the probing means used to detect the presence and identity of the objects are capable of probing such physical properties. Such probing means are well known in the art and are within the reach of the skilled person.</p><p id="p0046" num="0046">In yet another embodiment, the probing means are capable of identifying the relative positions of a plurality of objects on any of the support portions. The relative position of a plurality of objects may be used as described above to weigh the importance of the associated predetermined value during the step of generating the data input. Specifically, physical clustering of objects may be interpreted by the processing means as providing more weight to a given value.<!-- EPO <DP n="11"> --></p><p id="p0047" num="0047"><figref idrefs="f0002">Figures 6a, 6b and 6c</figref> provide exemplary embodiments of physical support means for supporting objects as required by the invention. These configurations are applicable to any embodiments described above. In <figref idrefs="f0002">Figure 6a</figref>, the physical support means 510 have a circular shape, wherein the portions 512 thereof, with which the predetermined values 514 are associated, are equiangular sectors of the circular shape. In <figref idrefs="f0002">Figure 6b</figref>, the support portions 612 with which the predetermined values 614 are associated do not provide an integral partition of the support means 610. In <figref idrefs="f0002">Figure 6c</figref>, the support means 710 comprise two disjoint support portions indicated by hatched rectangular features, which are associated with a first predetermined value. Two further disjoint support portions indicated by white rectangular features are associated with a second predetermined value, which is distinct from the first predetermined value.</p><p id="p0048" num="0048">The skilled person will be able to provide a computer program implementing some or all of the method steps according to the invention based on the provided description and the accompanying drawings. Such a computer program, when run on a computer, will lead the computer to execute the described method steps.</p><p id="p0049" num="0049">It should be understood that the detailed description of specific preferred embodiments is given by way of illustration only, since various changes and modifications within the scope of the invention will be apparent to the skilled man. The scope of protection is defined by the following set of claims.<!-- EPO <DP n="12"> --></p><heading id="h0007"><b>References</b></heading><p id="p0050" num="0050"><ol><li>1. "<nplcit id="ncit0001" npl-type="b"><text>Metaphor Awareness and Vocabulary Retention", F. Boers, Applied Linguistics 21/4: 553-571, Oxford University Press 2000</text></nplcit>.</li><li>2. "<nplcit id="ncit0002" npl-type="s"><text>Understanding the characteristics of metaphors in tangible user interfaces", V. Maquil, E. Ras and O. Zephir, Mensch &amp; Computer Workshopband, 47-52, 2011</text></nplcit>.</li><li>3. "<nplcit id="ncit0003" npl-type="s"><text>A taxonomy for and analysis of tangible interfaces", K. P. Fishkin, Pers. Ubiquit. Comput. (2004) 8: 347-358</text></nplcit>.</li><li>4. "<nplcit id="ncit0004" npl-type="s"><text>Urp: A Luminous-Tangible Workbench for Urban Planning and Design", J. Underkoffler and H. Ishii, CHI 99, 15-20 May 1999, 386-393</text></nplcit>.</li><li>5. "<nplcit id="ncit0005" npl-type="s"><text>The Tangible User Interface and its Evolution", H. Ishii, Comm. Of the ACM, June 2008/Vol. 51, No. 6, 32-36</text></nplcit>.</li><li>6. "<nplcit id="ncit0006" npl-type="s"><text>Illuminating Clay: A 3-D Tangible Interface for Landscape Analysis", B. Piper, C. Ratti and H. Ishii, CHI 2002, 20-25 April 2002, 355-362</text></nplcit>.</li><li>7. "<nplcit id="ncit0007" npl-type="s"><text>The reacTable: Exploring the Synergy between Live Music Performance and Tabletop Tangible Interfaces", S. Jordà, G. Geiger, M. Alonso and M. Kaltenbrunner, TEI'07, 15-17 Feb 2007, Baton Rouge, LA, USA, 139-146</text></nplcit>.</li><li>8. "<nplcit id="ncit0008" npl-type="s"><text>reacTIVision: A Computer-Vision Framework for Table-Based Tangible Interaction", M. Kaltenbrunner and R. Bencina, TEI'07, 15-17 Feb 2007, Baton Rouge, LA, USA, 69-74</text></nplcit>.</li><li>9. "<nplcit id="ncit0009" npl-type="b"><text>Augmenting Interactive Tabletops with Translucent Tangible Controls", M. Weiss, J.D. Hollan and J. Borchers, Tabletops - Horizontal Interactive Displays, Human-Computer Interaction Series, 149-170, Springer-Verlag 2010</text></nplcit>.</li><li>10. "<nplcit id="ncit0010" npl-type="s"><text>Audiopad: A Tag-based Interface for Musical Performance", J. Patten, B. Recht and H. Ishii, NIME'02, Proceedings of the 2002 conference on New interfaces for musical expression, 2002</text></nplcit>.</li><li>11. "<nplcit id="ncit0011" npl-type="s"><text>Tangible Bots: Interaction with Active Tangibles in Tabletop Interfaces", E. W. Pederson and K. Hornbaek, CHI 2011, May 7-12 2011, Vancouver BC, Canada</text></nplcit>.</li></ol></p></description><claims mxw-id="PCLM90459519" lang="EN" load-source="patent-office"><!-- EPO <DP n="13"> --><claim id="c-en-0001" num="0001"><claim-text>A method for providing data input (102, 202, 302, 402) at a computing device (190, 290, 390, 490) using a tangible human-computer interface (100, 200, 300, 400), wherein the method comprises the steps of:
<claim-text>- providing a tangible human-computer interface (100, 200, 300, 400) comprising physical support means (110, 210, 310, 410) having a plurality of support portions (112, 212, 312, 413), processing means (120, 220, 320, 420), and probing means (130, 230, 330, 430) capable of detecting physical objects (140, 240, 340, 440) on each of said support portions; (10)</claim-text>
<claim-text>- associating a predetermined data value (114, 214, 314, 414) with each one of said support portions (112, 212, 312, 412); (20)</claim-text>
<claim-text>- probing the presence of any physical objects (140, 240, 340, 440) on any of said support portions (112, 212, 312, 412); (30) and</claim-text>
<claim-text>- generating the data input (102, 202, 302, 402) and providing it at the computing device,<br/>
wherein the generation of the data input comprises associating the physical objects (140, 240, 340, 440) detected on a support portion (112, 212, 312, 412) with the corresponding predetermined data value (114, 214, 314, 414) (40).</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The method according to claim 1, wherein the physical support means (110) comprise a substantially planar surface, and wherein said support portions (112) are portions of said surface.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The method according to any of claims 1 or 2, wherein the human-computer interface (200) comprises a substantially planar surface (204), wherein probing means (230) are arranged so that they are capable of detecting the presence of said physical support means (210) and/or physical objects (240) on said surface (204).</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The method according to claim 3, wherein said surface of the human-computer interface (300) comprises said physical support means (310).</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The method according to any of claims 1 to 4, wherein the human-computing interface further comprises feedback means.<!-- EPO <DP n="14"> --></claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The method according to claims 4 and 5, wherein the method comprises the further step of providing feedback, wherein the feedback indicates the location of the support portions and/or the predetermined data values associated with said support portions.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The method according to any of claims 5 or 6, wherein the method further comprises the step of providing a feedback that is generated upon provision of said data input at said computing device.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>The method according to claim 3, wherein the substantially planar surface (404) of the human-computer interface (400) is at least partially translucent,<br/>
wherein said probing means (432) comprise image sensing means and optical means, which are provided underneath said surface (404), and which comprise a field of view in which they are capable of detecting objects, the field of view being directed towards said surface,<br/>
and further wherein said portions (412) of the physical support means (410) are at least partially translucent,<br/>
so that said probing means (430) are capable of detecting a physical object (440) on said portion (412) from underneath, when the physical support means (410) are placed on top of said surface (404).</claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>The method according to any of claims 1 to 8, wherein said probing means are further configured for detecting physical properties of said objects.</claim-text></claim><claim id="c-en-0010" num="0010"><claim-text>The method according to any of claims 1 to 9, wherein said probing means are further configured for detecting the relative positions of objects that are detected on any given surface portion with respect to each other, and wherein the step of generating data input comprises associating the detected objects with the corresponding predetermined data value depending on their detected relative positions.</claim-text></claim><claim id="c-en-0011" num="0011"><claim-text>The method according to any of claims 1 to 10, wherein said step of generating the input data comprises the computation of a weighted average of the predetermined values, wherein the weights correspond to the number of physical objects detected on the support portions corresponding to said values.</claim-text></claim><claim id="c-en-0012" num="0012"><claim-text>A device for carrying out the method according to any of claims 1 to 11, the device comprising a tangible human-computer interface, a memory element, and processing means, wherein the human-computer interface comprises physical support means<!-- EPO <DP n="15"> --> having a plurality of support portions and probing means capable of detecting physical objects on each of said support portions, and wherein the processing means are configured to
<claim-text>- associate a predetermined data value, stored in said memory element, with each one of said support portions;</claim-text>
<claim-text>- probe, using said probing means, the presence of any physical objects on any of said support portions, and</claim-text>
<claim-text>- generate data input and store it in said memory element, wherein the generation of the data input comprises associating the physical objects detected on a support portion with the corresponding predetermined data value.</claim-text></claim-text></claim><claim id="c-en-0013" num="0013"><claim-text>A computer capable of carrying out the method according to any of claims 1 to 11.</claim-text></claim><claim id="c-en-0014" num="0014"><claim-text>A computer program comprising computer readable code means, which when run on a computer, causes the computer to carry out the method according to any of claims 1 to 11.</claim-text></claim><claim id="c-en-0015" num="0015"><claim-text>A computer program product comprising a computer-readable medium on which the computer program according to claim 14 is stored.</claim-text></claim></claims><drawings mxw-id="PDW20422242" load-source="patent-office"><!-- EPO <DP n="16"> --><figure id="f0001" num="1,2,3,4,5"><img id="if0001" file="imgf0001.tif" wi="164" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="17"> --><figure id="f0002" num="6a,6b,6c"><img id="if0002" file="imgf0002.tif" wi="125" he="110" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="157" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="157" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
