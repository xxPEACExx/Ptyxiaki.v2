<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959360-A1" country="EP" doc-number="2959360" kind="A1" date="20151230" family-id="51391682" file-reference-id="255288" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160452609" ucid="EP-2959360-A1"><document-id><country>EP</country><doc-number>2959360</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13875658-A" is-representative="NO"><document-id mxw-id="PAPP193868186" load-source="patent-office" format="original"><country>EP</country><doc-number>13875658.0</doc-number><date>20130626</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193868187" load-source="docdb" format="epo"><country>EP</country><doc-number>13875658</doc-number><kind>A</kind><date>20130626</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162036707" ucid="US-2013047951-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2013047951</doc-number><kind>W</kind><date>20130626</date></document-id></priority-claim><priority-claim mxw-id="PPC162032320" ucid="US-201361768252-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201361768252</doc-number><kind>P</kind><date>20130222</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1890300773" load-source="ipcr">G06F   3/14        20060101ALI20160928BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1890306646" load-source="ipcr">G06F   3/041       20060101ALI20160928BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1890307200" load-source="ipcr">G06F   9/44        20060101ALI20160928BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1890307920" load-source="ipcr">G06F   3/01        20060101AFI20160928BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1913117552" load-source="docdb" scheme="CPC">H04M   1/72522     20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117553" load-source="docdb" scheme="CPC">G01C  21/00        20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117554" load-source="docdb" scheme="CPC">G06Q  10/00        20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117555" load-source="docdb" scheme="CPC">H04W   4/021       20130101 LA20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117556" load-source="docdb" scheme="CPC">G06F   9/44        20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117557" load-source="docdb" scheme="CPC">H04M2250/22        20130101 LA20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117558" load-source="docdb" scheme="CPC">G06F   3/14        20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117559" load-source="docdb" scheme="CPC">G08G   1/207       20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1913117560" load-source="docdb" scheme="CPC">G06F   3/04883     20130101 LI20160803BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987772095" load-source="docdb" scheme="CPC">G06F   3/04842     20130101 LI20151205BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987773088" load-source="docdb" scheme="CPC">G06F   3/04845     20130101 FI20151207BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987783763" load-source="docdb" scheme="CPC">G06F   3/0488      20130101 LI20151205BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987788208" load-source="docdb" scheme="CPC">G06F   3/0482      20130101 LI20151205BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987789146" load-source="docdb" scheme="CPC">G06F  17/30241     20130101 LI20151205BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987792292" load-source="docdb" scheme="CPC">G06F   3/04847     20130101 LI20151205BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165549293" lang="DE" load-source="patent-office">GEOFENCE-ERZEUGUNG AUF BERÜHRUNGSAKTIVIERTEN VORRICHTUNGEN</invention-title><invention-title mxw-id="PT165549294" lang="EN" load-source="patent-office">GEO-FENCE CREATION ON TOUCH-ENABLED DEVICES</invention-title><invention-title mxw-id="PT165549295" lang="FR" load-source="patent-office">CRÉATION DE GÉO-CLÔTURES SUR DES DISPOSITIFS À COMMANDE TACTILE</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103305613" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>INTEL CORP</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR1103335138" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>INTEL CORPORATION</last-name></addressbook></applicant><applicant mxw-id="PPAR1101653181" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Intel Corporation</last-name><iid>100147685</iid><address><street>2200 Mission College Boulevard</street><city>Santa Clara, CA 95054</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103328912" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>JOHNSON DERICK</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR1103336890" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>JOHNSON, Derick</last-name></addressbook></inventor><inventor mxw-id="PPAR1101651160" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>JOHNSON, Derick</last-name><address><street>126 Price Way</street><city>Folsom, California 95630</city><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR1103326146" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>BIRCH THOMAS A</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR1103317613" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>BIRCH, Thomas, A.</last-name></addressbook></inventor><inventor mxw-id="PPAR1101652673" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>BIRCH, Thomas, A.</last-name><address><street>4121 SW 43rd Ave.</street><city>Portland, Oregon 97221</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101644072" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Jennings, Vincent Louis</last-name><iid>101275397</iid><address><street>HGF Limited Fountain Precinct Balm Green</street><city>Sheffield S1 2JA</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2013047951-W"><document-id><country>US</country><doc-number>2013047951</doc-number><kind>W</kind><date>20130626</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014130072-A1"><document-id><country>WO</country><doc-number>2014130072</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660704064" load-source="docdb">AL</country><country mxw-id="DS660789124" load-source="docdb">AT</country><country mxw-id="DS660704066" load-source="docdb">BE</country><country mxw-id="DS660704504" load-source="docdb">BG</country><country mxw-id="DS660626638" load-source="docdb">CH</country><country mxw-id="DS660710621" load-source="docdb">CY</country><country mxw-id="DS660789125" load-source="docdb">CZ</country><country mxw-id="DS660704071" load-source="docdb">DE</country><country mxw-id="DS660710622" load-source="docdb">DK</country><country mxw-id="DS660710635" load-source="docdb">EE</country><country mxw-id="DS660625812" load-source="docdb">ES</country><country mxw-id="DS660704505" load-source="docdb">FI</country><country mxw-id="DS660704506" load-source="docdb">FR</country><country mxw-id="DS660704072" load-source="docdb">GB</country><country mxw-id="DS660710636" load-source="docdb">GR</country><country mxw-id="DS660704073" load-source="docdb">HR</country><country mxw-id="DS660789126" load-source="docdb">HU</country><country mxw-id="DS660626647" load-source="docdb">IE</country><country mxw-id="DS660704074" load-source="docdb">IS</country><country mxw-id="DS660704511" load-source="docdb">IT</country><country mxw-id="DS660710637" load-source="docdb">LI</country><country mxw-id="DS660726510" load-source="docdb">LT</country><country mxw-id="DS660789127" load-source="docdb">LU</country><country mxw-id="DS660726515" load-source="docdb">LV</country><country mxw-id="DS660726516" load-source="docdb">MC</country><country mxw-id="DS660625732" load-source="docdb">MK</country><country mxw-id="DS660625733" load-source="docdb">MT</country><country mxw-id="DS660789128" load-source="docdb">NL</country><country mxw-id="DS660625813" load-source="docdb">NO</country><country mxw-id="DS660626648" load-source="docdb">PL</country><country mxw-id="DS660726518" load-source="docdb">PT</country><country mxw-id="DS660789129" load-source="docdb">RO</country><country mxw-id="DS660726523" load-source="docdb">RS</country><country mxw-id="DS660626649" load-source="docdb">SE</country><country mxw-id="DS660726524" load-source="docdb">SI</country><country mxw-id="DS660625814" load-source="docdb">SK</country><country mxw-id="DS660626650" load-source="docdb">SM</country><country mxw-id="DS660625734" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139074852" ref-ucid="WO-2014130072-A1" lang="EN" load-source="patent-office"><p num="0000">Various systems and methods for creating geo-fences on touch-enabled devices are described herein. In an example, a map is displayed on a touch screen interface of a mobile device. User input is received from the touch screen interface, the user input resulting from a user touching the touch screen interface. An enclosed area is derived using the user input and the enclosed area is presented as a geo-fence object on the touch screen interface.</p></abstract><abstract mxw-id="PA139542371" ref-ucid="WO-2014130072-A1" lang="EN" source="national office" load-source="docdb"><p>Various systems and methods for creating geo-fences on touch-enabled devices are described herein. In an example, a map is displayed on a touch screen interface of a mobile device. User input is received from the touch screen interface, the user input resulting from a user touching the touch screen interface. An enclosed area is derived using the user input and the enclosed area is presented as a geo-fence object on the touch screen interface.</p></abstract><abstract mxw-id="PA139074853" ref-ucid="WO-2014130072-A1" lang="FR" load-source="patent-office"><p num="0000">La présente invention concerne divers systèmes et procédés permettant de créer des géo-clôtures sur des dispositifs à commande tactile. Dans un exemple, une carte s'affiche sur une interface à écran tactile d'un dispositif mobile. Une entrée d'utilisateur est reçue de l'interface à écran tactile, l'entrée d'utilisateur étant réalisée quand l'utilisateur touche l'interface à écran tactile. Une zone fermée est obtenue au moyen de l'entrée d'utilisateur, et la zone fermée est présentée en tant qu'objet de géo-clôture sur l'interface à écran tactile.</p></abstract><abstract mxw-id="PA139542372" ref-ucid="WO-2014130072-A1" lang="FR" source="national office" load-source="docdb"><p>La présente invention concerne divers systèmes et procédés permettant de créer des géo-clôtures sur des dispositifs à commande tactile. Dans un exemple, une carte s'affiche sur une interface à écran tactile d'un dispositif mobile. Une entrée d'utilisateur est reçue de l'interface à écran tactile, l'entrée d'utilisateur étant réalisée quand l'utilisateur touche l'interface à écran tactile. Une zone fermée est obtenue au moyen de l'entrée d'utilisateur, et la zone fermée est présentée en tant qu'objet de géo-clôture sur l'interface à écran tactile.</p></abstract><description mxw-id="PDES78477327" ref-ucid="WO-2014130072-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001"> GEO-FENCE CREATION ON TOUCH-ENABLED DEVICES </p><p id="p0002" num="0002">CLAIM OF PRIORITY </p><p id="p0003" num="0003">[0001] This patent application claims the benefit of U.S. Provisional Patent Application Serial No. 61/768,252, titled "GEO-FENCE CREATION ON TOUCH-ENABLED DEVICES," filed February 22, 2013, which is hereby incorporated by reference in its entirety. </p><p id="p0004" num="0004">TECHNICAL FIELD </p><p id="p0005" num="0005">[0002] Embodiments described herein generally relate to geo-fence technology and in particular, geo-fence creation on touch-enabled devices. </p><p id="p0006" num="0006">BACKGROUND </p><p id="p0007" num="0007"> [0003] The widespread availability of multi-functional mobile devices has resulted in having these devices as an integral medium for everyday activities. Mobile devices are capable of displaying maps, using position sensing technologies (e.g., global positioning system (GPS)), and providing alerts via text, graphics, audio, and the like. </p><p id="p0008" num="0008"> [0004] A geo-fence is a virtual boundary around a geographic location. When a location-aware mobile device enters or exits a geo-fenced area, various actions may occur, such as presenting a notification on the mobile device. Geo-fences are used in a variety of applications, including security, insurance, and recreation. </p><p id="p0009" num="0009">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0010" num="0010">[0005] In the drawings, which are not necessarily drawn to scale, like numerals may describe similar components in different views. Like numerals having different letter suffixes may represent different instances of similar components. Some embodiments are illustrated by way of example, and not limitation, in the figures of the accompanying drawings in which: </p><p id="p0011" num="0011"> [0006] FIG. 1 is a flowchart illustrating a method of creating a geo-fence according to an embodiment; 
<!-- EPO <DP n="3"/>-->
 [0007] FIGS. 2A-C illustrate a map and an implementation of creating a geo- fence on the map, according to an embodiment; </p><p id="p0012" num="0012"> [0008] FIG. 3 illustrates a mechanism that creates a dynamic polygonal representation of a geo-fence using two or more touch inputs, according to an embodiment; </p><p id="p0013" num="0013"> [0009] FIG. 4 illustrates a mechanism to create a geo-fence by dragging a touch input proximate to one or more roads or streets, according to an embodiment; </p><p id="p0014" num="0014"> [0010] FIGS. 5A-C illustrate another mechanism to create a geo-fence proximate to one or more roads or streets, according to an embodiment; </p><p id="p0015" num="0015"> [0011] FIG. 6 is a block diagram illustrating a mobile device, according to an embodiment; and </p><p id="p0016" num="0016"> [0012] FIG. 7 is a block diagram illustrating an example machine upon which any one or more of the techniques (e.g., methodologies) discussed herein may perform, according to an example embodiment. </p><p id="p0017" num="0017">DETAILED DESCRIPTION </p><p id="p0018" num="0018">[0013] As people increasingly rely on mobile devices in their day-to-day lives, creating and managing location-based applications from a mobile situation is correspondingly becoming more important. One useful technology for location- based applications is geo-fencing. Geo-fencing includes creating and managing a geo-fence, which is a virtual boundary of a geographical area. A geo-fence may be considered an object (e.g., a geo-fence object), such that the geo-fence may have various properties like objects in other disciplines, such as object-oriented programming. In this way, a geo-fence object may include attributes and functions. </p><p id="p0019" num="0019"> [0014] Attributes may include data, such as a name, unique identifier, coordinates defining the geo-fence (e.g., a latitude/longitude or a relative coordinate system), an active and expiration date/time, permissions, and the like. </p><p id="p0020" num="0020">[0015] Functions may include actions such as notifications on entry/exit of a geo-fence, application download or initiation upon entry/exit, or other actions that occur on a user' s mobile device or are caused to occur on another device associated with the user or on another user' s device. 
<!-- EPO <DP n="4"/>-->
 [0016] For example, a geo-fence may have attributes such as a name, "Memorial Day Sale" and a location, e.g., situated to enclose a store location at a mall. The geo-fence may have a beginning date of midnight, May 25, 2013 and an expiration date of midnight, May 28, 2013. The "Memorial Day Sale" geo- fence may have functions associated with it such that when a user enters the geo- fence a coupon notification is pushed to the user' s device. The coupon notification may include a mechanism that allows the user to navigate to a web page (e.g., embedded web link), which displays a coupon for the Memorial Day Sale at the store. </p><p id="p0021" num="0021">[0017] In order to take advantage of these types of location-based functions, one or more geo-fences have to first be created. While creating geo-fences on large form factor device, such as a desktop computer is generally easier because it allows the user to precisely plot the vertices of a geo-fence using a mouse, the move towards smaller, touch enabled devices, creates a need for an effective way to create digital boundaries on these smaller form factors. </p><p id="p0022" num="0022"> [0018] Geo-fencing is still a relatively new field and there are currently no applications that allow one to create precise geo-fences directly on a mobile device. The implementations disclosed herein make it much easier to create geo- fences on small form-factor or touch-enabled devices rather than use the same methods used on larger devices. </p><p id="p0023" num="0023"> [0019] FIG. 1 is a flowchart illustrating a method 100 of creating a geo-fence according to an embodiment. At block 102, a map is displayed on a touch screen interface of a mobile device. The map may be displayed in an interface with other controls, such as input controls to input a geo-fence' s attributes or details about a geo-fence' s functions. At block 104, a user input is received on the touch screen interface, the user input resulting from a user touching the touch screen interface. The user touch input may be in various forms including, but not limited to tapping, swiping, dragging, or touch- and-hold. In addition, the user touch input may be combined with one or more other types of user input including but not limited to a keyboard input, a mouse input, a voice input, and the like. At block 106, an enclosed area is derived from the user input. The enclosed area is discussed further below with reference to FIGS. 2-5. At block 108, the enclosed area is presented as a geo-fence object on the touch screen interface. For example, as illustrated in the figures discussed below, the geo-fence object may be depicted 
<!-- EPO <DP n="5"/>-->
 using a heavy line indicating the geo-fence boundary. Examples of various mechanism to create a geo-fence are discussed below with respect to FIGS. 2-5. </p><p id="p0024" num="0024">[0020] In an embodiment, the method 100 of defining the enclosed area using the user input includes determining a location of the user input on the map displayed. The method also includes identifying a first geographical structure proximate to the location of the user input and constructing a polygonal representation of the first geographical structure to define the enclosed area. </p><p id="p0025" num="0025">[0021] Turning to FIGS. 2A-C, in FIG. 2A, a map 200 is displayed and a user touch input point-of-contact 202 is depicted on the map 200. The point-of-contact 202 is proximate to a building 204. The building 204 is an example of a "first geographical structure." Thus, in an embodiment, the first geographical structure comprises a building. Other examples of geographical structures include but are not limited to bridges, parking lots, parks, recreational areas, monuments, and the like. </p><p id="p0026" num="0026">[0022] FIG. 2B depicts an example of a boundary 206 generated around the detected building. The boundary comprises a polygonal representation of the building's footprint. Constructing the polygonal representation (e.g. border) may be performed using various methods. In an embodiment, building details are obtained from a local or remote database. The building details may include a dataset of building polygons. In this situation, the nearest bounding polygon that encloses the user's touch point is returned as a geo-fence object. </p><p id="p0027" num="0027">[0023] In an embodiment, constructing the polygonal representation comprises using an edge detection process on map data corresponding to the map to identify edges of the first geographical structure. In an embodiment, the edge detection process may be a fast edge detection process. The map data may be filtered to identify edges surrounding the location of the user input and a corner detection process may be used to detect a plurality of vertices that define the geo-fence object. One mechanism for fast edge detection includes the use of screen scraping (e.g., capturing the contents displayed on a screen to an image). Thus, in an embodiment, using the fast edge detection process comprises screen scraping a portion of the map currently displayed on the touch screen interface to obtain a scraped image and processing the scraped image to determine edges. </p><p id="p0028" num="0028">[0024] After the polygonal representation is determined, in an embodiment, the vertices of the polygonal representation are illustrated to the user and may be used 
<!-- EPO <DP n="6"/>-->
 as input controls to move one or more vertices, thereby changing the size or shape of the polygonal representation. Such a mechanism is illustrated in FIG. 2C with vertices 208. </p><p id="p0029" num="0029"> [0025] While FIGS. 2A-C illustrate one building being selected and a corresponding geo-fence being calculated around the building, in some embodiments, the mechanism may be extended to two or more buildings. For examples, a user may select a first building and adjust the polygonal </p><p id="p0030" num="0030">representation of the geo-fence (e.g., border), and then select a second building in order to create a second geo-fence. The second geo-fence may be related to the first geo-fence (e.g., two areas with the shared geo-fence attributes or functions). Alternatively, the second geo-fence may be a distinct geo-fence. </p><p id="p0031" num="0031">[0026] Thus, in an embodiment, the method 100 comprises receiving a second user input on the touch screen interface, the second user input resulting from the user touching the touch screen interface. A second location of the second user input on the map is determined. A second geographical structure proximate to the second location of the second user input is identified and an aggregate polygonal representation including the first geographical structure and the second geographical structure is constructed to define the enclosed area. </p><p id="p0032" num="0032">[0027] In an embodiment, the first and second geographical structures are not adjacent to one another, and as a result, the polygonal representation comprises a plurality of polygonal areas. In an embodiment, the plurality of polygonal areas is linked as a single geo-fence object. In an alternative embodiment, the plurality of polygonal areas is distinct geo-fence objects. </p><p id="p0033" num="0033"> [0028] One mechanism for selecting multiple geographical structures may be to tap on two or more areas of a map. The tapping may be performed using a single finger in multiple successive taps, or using multiple fingers in a substantially combined tapping input. For example, a user may select a first geographical structure using an index finger by placing the index finger on a first place on the map and then select a second geographical structure using a ring finger by placing the ring finger on a second place on the map. Another mechanism for selecting multiple structures may be to use a dragging motion to select two or more geographical structures on the path of the dragging motion. Thus, in an embodiment, the second user input comprises a dragging motion that selects the first and second geographical structures. Other mechanisms may also 
<!-- EPO <DP n="7"/>-->
 be used to select several geographical structures, such as using a "join" function or a "split" function. For example, a user may enter a "join" mode (e.g., by clicking a button or other user interface control), such that any geographical structures selected by touch are aggregated with a previously-selected geographical structure to form an aggregate geo-fence. A split function may be used to disconnect any joined geographical structures. Another mechanism that may be used is a keyboard input combined with a touch input (e.g., CONTROL-tap or SHIFT-tap) to select multiple geographical structures. </p><p id="p0034" num="0034"> [0029] Turning now to the next figure, FIG. 3 illustrates a mechanism that creates a dynamic polygonal representation of a geo-fence using two or more touch inputs. In the example depicted in FIG. 3, a user has touched the user interface using two fingers and the shape being used is a rectangular polygon.</p><p id="p0035" num="0035">The user's touch points create the vertices 300 of the rectangular polygon 302.</p><p id="p0036" num="0036">After the user's manipulations, the rectangular polygon will be used to create a geo-fence of substantially similar size and shape. Based on the user's input of moving their fingers closer or farther from each other, the size of the rectangular polygon shrinks or grows accordingly. Additionally, by rotating the fingers around each other, the rectangular polygon rotates. </p><p id="p0037" num="0037"> [0030] Thus, in an embodiment, the method 100 of defining the enclosed area using the user input comprises detecting a first and second contact point from the user input, displaying a polygonal area based on the first and second contact points, the polygonal area formed by the first and second contact points, and using the displayed polygonal area to define the enclosed area. </p><p id="p0038" num="0038">[0031] As illustrated in FIG. 3, in an embodiment, the polygonal area 300 comprises a rectangle and the first and second contact points are used to adjust opposing vertices of the rectangle. However, other types of polygonal areas may be used, such as a circle, triangle, or square. Thus, in an embodiment, the polygonal area comprises one of a circle, triangle, or square, and the first contact point is used to anchor the center of the circle, triangle, or square, and the second contact point is used to adjust the size of the circle, triangle, or square. </p><p id="p0039" num="0039"> [0032] As discussed above, the user may resize the polygonal area using the vertices. Thus, in an embodiment, at least one of an expanding motion or a contracting motion of the first and second contact points relative to each other is received and the polygonal area is grown or shrunk, respectively. 
<!-- EPO <DP n="8"/>-->
 [0033] Also as discussed above, the user may rotate the polygonal area. Thus, in an embodiment, a rotating input motion from the user is detected, rotating the first and second contact points. The polygonal area is then displayed moving in a corresponding rotating motion. </p><p id="p0040" num="0040">[0034] Another mechanism includes using a third (or fourth, fifth, sixth, etc.) touch contact point from the user and then creating a polygonal area based on the touch points. For example, the touch points may be used as the vertices (e.g., three points to create a triangle, four to create a rectangular area), and the user may size, shape, and rotate the polygonal area by moving each finger, which will move a corresponding vertex. Thus, in an embodiment, the method 100 includes detecting a third contact point from the user input and adjusting a vertex of the polygonal area based on the third contact point. </p><p id="p0041" num="0041"> [0035] Turning now to the next figure, FIG. 4 illustrates a mechanism to create a geo-fence by dragging a touch input proximate to one or more roads or streets, according to an embodiment. As the user drags his fingers near streets a polygon will form, snapping to the streets (e.g., conforming), until the user closes the polygon by reaching the beginning of the drag. The area enclosed by the roads or streets is then used to construct a geo-fence. Thus, in FIG. 4, a user's dragging motion is depicted with a path 400. In the example depicted in FIG. 4, the user began the dragging motion at point 402 and dragged a finger along the path 400 to end of the path 404. Although not shown, when the user's dragging motion returns to substantially the same point as the beginning of the path (e.g., point 402), the polygonal representation is completed and the area is defined. </p><p id="p0042" num="0042">[0036] The vertices illustrated may be determined in various manners. For example, when a user pauses during tracing the path, a vertex may be created and displayed on the user interface at that point. Alternatively, when a user changes direction or alters course, a vertex may be created. Thus, in an embodiment, the first and second contact points are received as a portion of a dragging input mechanism from the user. </p><p id="p0043" num="0043">[0037] In an embodiment, the method 100 includes defining the enclosed area using the user input by detecting a first and second contact point from the user input, identifying a first road portion proximate to the first contact point, identifying a second road portion proximate to the second contact point; and forming a portion of a border of the enclosed area using the first and second road 
<!-- EPO <DP n="9"/>-->
 portions. Roads may be identified using various techniques, such as edge detection with screen scraping. </p><p id="p0044" num="0044"> [0038] Where FIG. 4 illustrates one mechanism for creating a geo-fence, FIGS. 5A-C illustrate another such mechanism. Similar to the road mapping mechanism used in FIG. 4, roads are identified in the mechanism illustrated in FIGS. 5A-C. However, instead of a dragging motion to snap the vertices to the roads, the user uses several touches on or near a road to create the polygonal representation. Thus, in FIG. 5 A, a user has successively touched vertex 500, vertex 502, vertex 504, and so on, until vertex 506. While the user touches the user interface, a dynamic area is formed after the user has touched three points (e.g., to create a triangular shape) and continuing until the user indicates that the shape is complete (e.g., by clicking on a "DONE" user input button control). FIGS. 5B and 5C illustrate how the polygonal area changes while the user continues to create new vertices with additional touches. </p><p id="p0045" num="0045">[0039] Thus, in an embodiment, a third contact point from the user input is detected. A third road portion proximate to the third contact point is detected, and the enclosed area is formed based on the first, second, and third road portions. </p><p id="p0046" num="0046">[0040] In any of the mechanisms illustrated or discussed with respect to FIGS. 4 or 5, the enclosed area may be formed by connecting roads around a </p><p id="p0047" num="0047">geographical structure and the geographical structure may then be associated with the geo-fence. </p><p id="p0048" num="0048"> [0041] In some embodiments, various optimization mechanisms may be used, such as approximating curved portions of a road or reducing vertices. Curves in roads may be quantized so that the polygon does not stray too far from the shape of the road; however a "maximum number of vertices" constraint may be enforced. Thus, in an embodiment, a curved road is quantized to approximate the shape of the road. In another embodiment, the number of vertices defining the enclosed area is reduced. </p><p id="p0049" num="0049"> [0042] While some embodiments have been discussed with respect to road snapping or street snapping, it is understood that the dragging or multiple vertex input mechanisms may be used to select areas that do not have roads. Such free form geo-fence creation may be useful to create an enclosed area where there are no roads, or where there are unconnected roads. 
<!-- EPO <DP n="10"/>-->
 [0043] In an embodiment, defining a geo-fence and the associated content with that geo-fence is managed on a server in the cloud. Such features may be implemented in smartphone, notebook, or a device with Ultrabook capabilities. The features may be provided in conjunction with various operating systems, such as Android® or Windows 8®. </p><p id="p0050" num="0050">Hardware Platform </p><p id="p0051" num="0051"> [0044] Embodiments may be implemented in one or a combination of hardware, firmware, and software. Embodiments may also be implemented as instructions stored on a machine-readable storage device, which may be read and executed by at least one processor to perform the operations described herein. A machine-readable storage device may include any non-transitory mechanism for storing information in a form readable by a machine (e.g., a computer). For example, a machine-readable storage device may include read-only memory (ROM), random-access memory (RAM), magnetic disk storage media, optical storage media, flash-memory devices, and other storage devices and media. </p><p id="p0052" num="0052">[0045] Examples, as described herein, may include, or may operate on, logic or a number of components, modules, or mechanisms. Modules are tangible entities (e.g., hardware) capable of performing specified operations and may be configured or arranged in a certain manner. In an example, circuits may be arranged (e.g., internally or with respect to external entities such as other circuits) in a specified manner as a module. In an example, the whole or part of one or more computer systems (e.g., a standalone, client or server computer system) or one or more hardware processors may be configured by firmware or software (e.g., instructions, an application portion, or an application) as a module that operates to perform specified operations. In an example, the software may reside on a machine-readable medium. In an example, the software, when executed by the underlying hardware of the module, causes the hardware to perform the specified operations. </p><p id="p0053" num="0053">[0046] Accordingly, the term "module" is understood to encompass a tangible entity, be that an entity that is physically constructed, specifically configured (e.g., hardwired), or temporarily (e.g., transitorily) configured (e.g., programmed) to operate in a specified manner or to perform part or all of any operation described herein. Considering examples in which modules are temporarily configured, each 
<!-- EPO <DP n="11"/>-->
 of the modules need not be instantiated at any one moment in time. For example, where the modules comprise a general-purpose hardware processor configured using software, the general-purpose hardware processor may be configured as respective different modules at different times. Software may accordingly configure a hardware processor, for example, to constitute a particular module at one instance of time and to constitute a different module at a different instance of time. </p><p id="p0054" num="0054"> [0047] FIG. 6 is a block diagram illustrating a mobile device 600, such as a user equipment (UE), a mobile station (MS), a mobile wireless device, a mobile communication device, a tablet, a handset, or other type of mobile wireless device. The mobile device 600 may include one or more antennas 602 configured to communicate with a base station (BS), an evolved Node B (eNB), or other type of wireless wide area network (WW AN) access point. The mobile device 600 may be configured to communicate using at least one wireless communication standard including 3 GPP LTE, WiMAX, High Speed Packet Access (HSPA), Bluetooth, and WiFi. The mobile device 600 may communicate using separate antennas for each wireless communication standard or shared antennas for multiple wireless communication standards. The mobile device 600 may communicate in a wireless local area network (WLAN), a wireless personal area network (WPAN), and/or a wireless wide area network (WW AN). </p><p id="p0055" num="0055"> [0048] The mobile device may include a microphone 604 and one more speakers 606 that may be used for audio input and output from the mobile device 600. The display screen 608 may be a liquid crystal display (LCD) screen, or other type of display screen such as an organic light emitting diode (OLED) display. The display screen 608 may be configured as a touch screen. The touch screen may use capacitive, resistive, or another type of touch screen technology. An application processor 610 and a graphics processor 612 may be coupled to internal non-transitory storage device (storage memory) 614 to provide processing and display capabilities. A keyboard (not shown) may be integrated with the mobile device 600 or wirelessly connected to the mobile device 600 to provide additional user input. A virtual keyboard may also be provided using a touch screen display 608. </p><p id="p0056" num="0056"> [0049] FIG. 7 is a block diagram illustrating a machine in the example form of a computer system 700, within which a set or sequence of instructions may be 
<!-- EPO <DP n="12"/>-->
 executed to cause the machine to perform any one of the methodologies discussed herein, according to an example embodiment. In alternative embodiments, the machine operates as a standalone device or may be connected (e.g., networked) to other machines. In a networked deployment, the machine may operate in the capacity of either a server or a client machine in server-client network </p><p id="p0057" num="0057">environments, or it may act as a peer machine in peer-to-peer (or distributed) network environments. The machine may be a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a mobile telephone, a web appliance, a network router, switch or bridge, or any machine capable of executing instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while only a single machine is illustrated, the term </p><p id="p0058" num="0058">"machine" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein. </p><p id="p0059" num="0059">[0050] Example computer system 700 includes at least one processor 702 (e.g., a central processing unit (CPU), a graphics processing unit (GPU) or both, processor cores, compute nodes, etc.), a main memory 704 and a static memory 706, which communicate with each other via a link 708 (e.g., bus). The computer system 700 may further include a video display unit 710, an alphanumeric input device 712 (e.g., a keyboard), and a user interface (UI) navigation device 714</p><p id="p0060" num="0060">(e.g., a mouse). In one embodiment, the video display unit 710, input device 712 and UI navigation device 714 are incorporated into a touch screen display. The computer system 700 may additionally include a storage device 716 (e.g., a drive unit), a signal generation device 718 (e.g., a speaker), a network interface device 720, and one or more sensors (not shown), such as a global positioning system (GPS) sensor, compass, accelerometer, or other sensor. </p><p id="p0061" num="0061"> [0051] The storage device 716 includes a machine-readable medium 722 on which is stored one or more sets of data structures and instructions 724 (e.g., software) embodying or utilized by any one or more of the methodologies or functions described herein. The instructions 724 may also reside, completely or at least partially, within the main memory 704, static memory 706, and/or within the processor 702 during execution thereof by the computer system 700, with the main memory 704, static memory 706, and the processor 702 also constituting machine-readable media. 
<!-- EPO <DP n="13"/>-->
 [0052] While the machine-readable medium 722 is illustrated in an example embodiment to be a single medium, the term "machine-readable medium" may include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more instructions 724. The term "machine-readable medium" shall also be taken to include any tangible medium that is capable of storing, encoding or carrying instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure or that is capable of storing, encoding or carrying data structures utilized by or associated with such instructions. The term "machine-readable medium" shall accordingly be taken to include, but not be limited to, solid-state memories, and optical and magnetic media. Specific examples of machine-readable media include non- volatile memory, including, by way of example, semiconductor memory devices (e.g., electrically programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM)) and flash memory devices; </p><p id="p0062" num="0062">magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. </p><p id="p0063" num="0063"> [0053] The instructions 724 may further be transmitted or received over a communications network 726 using a transmission medium via the network interface device 720 utilizing any one of a number of well-known transfer protocols (e.g., HTTP). Examples of communication networks include a local area network (LAN), a wide area network (WAN), the Internet, mobile telephone networks, plain old telephone (POTS) networks, and wireless data networks (e.g., Wi-Fi, 3G, and 4G LTE/LTE-A or WiMAX networks). The term "transmission medium" shall be taken to include any intangible medium that is capable of storing, encoding, or carrying instructions for execution by the machine, and includes digital or analog communications signals or other intangible medium to facilitate communication of such software. Additional Notes &amp; Examples: </p><p id="p0064" num="0064"> [0054] Example 1 includes subject matter for creating a geo-fence (such as a device, apparatus, or machine) comprising a touch screen display; an interface module adapted to: display a map on a touch screen display of a mobile device; and receive a user input from the touch screen display, the user input resulting 
<!-- EPO <DP n="14"/>-->
 from a user touching the touch screen display; and a geo-fence module adapted to derive an enclosed area using the user input, wherein the interface module is adapted to present the enclosed area as a geo-fence object on the touch screen display. </p><p id="p0065" num="0065">[0055] In Example 2, the subject matter of Example 1 may optionally include, wherein to derive the enclosed area using the user input, the geo-fence module is adapted to: determine a location of the user input on the map; identify a first geographical structure proximate to the location of the user input; and construct a polygonal representation of the first geographical structure to define the enclosed area. </p><p id="p0066" num="0066"> [0056] In Example 3 the subject matter of any one or more of Examples 1 to 2 may optionally include, wherein to construct the polygonal representation, the geo-fence module is adapted to: use a fast edge detection process on map data corresponding to the map to identify edges of the first geographical structure; filter the map data to identify edges surrounding the location of the user input; and use a corner detection process to detect a plurality of vertices that define the geo- fence object. </p><p id="p0067" num="0067"> [0057] In Example 4 the subject matter of any one or more of Examples 1 to 3 may optionally include, wherein to use the fast edge detection process, the geo- fence module is adapted to: screen scrape a portion of the map currently displayed on the touch screen display to obtain a scraped image; and process the scraped image to determine edges. </p><p id="p0068" num="0068"> [0058] In Example 5 the subject matter of any one or more of Examples 1 to 4 may optionally include, wherein the first geographical structure comprises a building. </p><p id="p0069" num="0069"> [0059] In Example 6 the subject matter of any one or more of Examples 1 to 5 may optionally include, wherein the interface module is adapted to receive a second user input from the touch screen display, the second user input resulting from the user touching the touch screen display; and wherein the geo-fence module is adapted to: determine a second location of the second user input on the map; identify a second geographical structure proximate to the second location of the second user input; and construct an aggregate polygonal representation including the first geographical structure and the second geographical structure to define the enclosed area. 
<!-- EPO <DP n="15"/>-->
 [0060] In Example 7 the subject matter of any one or more of Examples 1 to 6 may optionally include, wherein the first and second geographical structures are not adjacent to one another, and wherein the polygonal representation comprises a plurality of polygonal areas. </p><p id="p0070" num="0070">[0061] In Example 8 the subject matter of any one or more of Examples 1 to 7 may optionally include, wherein members of the plurality of polygonal areas are linked as a single geo-fence object. </p><p id="p0071" num="0071"> [0062] In Example 9 the subject matter of any one or more of Examples 1 to 8 may optionally include, wherein members of the plurality of polygonal areas are distinct geo-fence objects. </p><p id="p0072" num="0072"> [0063] In Example 10 the subject matter of any one or more of Examples 1 to 9 may optionally include, wherein the second user input comprises a dragging motion that selects the first and second geographical structures. </p><p id="p0073" num="0073">[0064] In Example 11 the subject matter of any one or more of Examples 1 to 10 may optionally include, wherein to derive the enclosed area using the user input, the geo-fence module is adapted to: detect, via the interface module, a first and second contact point from the user input; display, via the interface module, a polygonal area based on the first and second contact points, the polygonal area formed by the first and second contact points; and use the displayed polygonal area to define the enclosed area. </p><p id="p0074" num="0074"> [0065] In Example 12 the subject matter of any one or more of Examples 1 to</p><p id="p0075" num="0075">11 may optionally include, wherein the polygonal area comprises a rectangle and wherein the first and second contact points are used to adjust opposing vertices of the rectangle. </p><p id="p0076" num="0076">[0066] In Example 13 the subject matter of any one or more of Examples 1 to</p><p id="p0077" num="0077">12 may optionally include, wherein the polygonal area comprises one of a circle, triangle, or square, and wherein the first contact point is used to anchor the center of the circle, triangle, or square, and the second contact point is used to adjust the size of the circle, triangle, or square. </p><p id="p0078" num="0078">[0067] In Example 14 the subject matter of any one or more of Examples 1 to</p><p id="p0079" num="0079">13 may optionally include, wherein the interface module is adapted to: detect a third contact point from the user input; and adjust a vertex of the polygonal area based on the third contact point. 
<!-- EPO <DP n="16"/>-->
 [0068] In Example 15 the subject matter of any one or more of Examples 1 to</p><p id="p0080" num="0080">14 may optionally include, wherein the interface module is adapted to: detect a rotating input motion from the user, rotating the first and second contact points; and display the polygonal area moving in a corresponding rotating motion. </p><p id="p0081" num="0081">[0069] In Example 16 the subject matter of any one or more of Examples 1 to</p><p id="p0082" num="0082">15 may optionally include, wherein the interface module is adapted to: receive at least one of an expanding motion or a contracting motion of the first and second contact points relative to each other; and grow or shrink the polygonal area, respectively. </p><p id="p0083" num="0083">[0070] In Example 17 the subject matter of any one or more of Examples 1 to</p><p id="p0084" num="0084">16 may optionally include, wherein to derive the enclosed area using the user input, the geo-fence module is adapted to: detect, via the interface module, a first and second contact point from the user input; identify a first road portion proximate to the first contact point; identify a second road portion proximate to the second contact point; and form a portion of a border of the enclosed area using the first and second road portions. </p><p id="p0085" num="0085"> [0071] In Example 18 the subject matter of any one or more of Examples 1 to</p><p id="p0086" num="0086">17 may optionally include, wherein the interface module is adapted to detect a third contact point from the user input; and wherein the geo-fence module is adapted to: identify a third road portion proximate to the third contact point; and form the enclosed area based on the first, second, and third road portions. </p><p id="p0087" num="0087">[0072] In Example 19 the subject matter of any one or more of Examples 1 to</p><p id="p0088" num="0088">18 may optionally include, wherein the first and second contact points are received as a portion of a dragging input mechanism from the user. </p><p id="p0089" num="0089">[0073] In Example 20 the subject matter of any one or more of Examples 1 to</p><p id="p0090" num="0090">19 may optionally include, wherein the geo-fence module is adapted to: form the enclosed area by connecting roads around a geographical structure; and associate the geographical structure with the geo-fence. </p><p id="p0091" num="0091"> [0074] In Example 21 the subject matter of any one or more of Examples 1 to 20 may optionally include, wherein the geo-fence module is adapted to: quantize a curved road to approximate the shape of the road. </p><p id="p0092" num="0092"> [0075] In Example 22 the subject matter of any one or more of Examples 1 to 21 may optionally include, wherein the geo-fence module is adapted to: reduce the number of vertices defining the enclosed area. 
<!-- EPO <DP n="17"/>-->
 [0076] Example 23 includes or may optionally be combined with the subject matter of any one of Examples 1-22 to include subject matter for creating a geo- fence (such as a method, means for performing acts, machine readable medium including instructions that when performed by a machine cause the machine to performs acts, an apparatus configured to perform, or an apparatus with means for) comprising displaying a map on a touch screen interface of a mobile device; receiving a user input from the touch screen interface, the user input resulting from a user touching the touch screen interface; deriving an enclosed area using the user input; and presenting the enclosed area as a geo-fence object on the touch screen interface. </p><p id="p0093" num="0093"> [0077] In Example 24, the subject matter of Example 23 may optionally include, wherein deriving the enclosed area using the user input comprises: determining a location of the user input on the map; identifying a first geographical structure proximate to the location of the user input; and constructing a polygonal representation of the first geographical structure to define the enclosed area. </p><p id="p0094" num="0094"> [0078] In Example 25 the subject matter of any one or more of Examples 23 to</p><p id="p0095" num="0095">24 may optionally include, wherein constructing the polygonal representation comprises: using a fast edge detection process on map data corresponding to the map to identify edges of the first geographical structure; filtering the map data to identify edges surrounding the location of the user input; and using a corner detection process to detect a plurality of vertices that define the geo-fence object. </p><p id="p0096" num="0096">[0079] In Example 26 the subject matter of any one or more of Examples 23 to</p><p id="p0097" num="0097">25 may optionally include, wherein using the fast edge detection process comprises: screen scraping a portion of the map currently displayed on the touch screen interface to obtain a scraped image; and processing the scraped image to determine edges. </p><p id="p0098" num="0098"> [0080] In Example 27 the subject matter of any one or more of Examples 23 to</p><p id="p0099" num="0099">26 may optionally include, wherein the first geographical structure comprises a building. </p><p id="p0100" num="0100"> [0081] In Example 28 the subject matter of any one or more of Examples 23 to</p><p id="p0101" num="0101">27 may optionally include, receiving a second user input from the touch screen interface, the second user input resulting from the user touching the touch screen interface; determining a second location of the second user input on the map; 
<!-- EPO <DP n="18"/>-->
 identifying a second geographical structure proximate to the second location of the second user input; and constructing an aggregate polygonal representation including the first geographical structure and the second geographical structure to define the enclosed area. </p><p id="p0102" num="0102">[0082] In Example 29 the subject matter of any one or more of Examples 23 to 28 may optionally include, wherein the first and second geographical structures are not adjacent to one another, and wherein the polygonal representation comprises a plurality of polygonal areas. </p><p id="p0103" num="0103"> [0083] In Example 30 the subject matter of any one or more of Examples 23 to 29 may optionally include, wherein members of the plurality of polygonal areas are linked as a single geo-fence object. </p><p id="p0104" num="0104"> [0084] In Example 31 the subject matter of any one or more of Examples 23 to</p><p id="p0105" num="0105">30 may optionally include, wherein members of the plurality of polygonal areas are distinct geo-fence objects. </p><p id="p0106" num="0106">[0085] In Example 32 the subject matter of any one or more of Examples 23 to</p><p id="p0107" num="0107">31 may optionally include, wherein the second user input comprises a dragging motion that selects the first and second geographical structures. </p><p id="p0108" num="0108"> [0086] In Example 33 the subject matter of any one or more of Examples 23 to</p><p id="p0109" num="0109">32 may optionally include, wherein deriving the enclosed area using the user input comprises: detecting a first and second contact point from the user input; </p><p id="p0110" num="0110">displaying a polygonal area based on the first and second contact points, the polygonal area formed by the first and second contact points; and using the displayed polygonal area to define the enclosed area. </p><p id="p0111" num="0111"> [0087] In Example 34 the subject matter of any one or more of Examples 23 to 33 may optionally include, wherein the polygonal area comprises a rectangle and wherein the first and second contact points are used to adjust opposing vertices of the rectangle. </p><p id="p0112" num="0112"> [0088] In Example 35 the subject matter of any one or more of Examples 23 to 34 may optionally include, wherein the polygonal area comprises one of a circle, triangle, or square, and wherein the first contact point is used to anchor the center of the circle, triangle, or square, and the second contact point is used to adjust the size of the circle, triangle, or square. 
<!-- EPO <DP n="19"/>-->
 [0089] In Example 36 the subject matter of any one or more of Examples 23 to 35 may optionally include, detecting a third contact point from the user input; and adjusting a vertex of the polygonal area based on the third contact point. </p><p id="p0113" num="0113">[0090] In Example 37 the subject matter of any one or more of Examples 23 to 36 may optionally include, detecting a rotating input motion from the user, rotating the first and second contact points; and displaying the polygonal area moving in a corresponding rotating motion. </p><p id="p0114" num="0114"> [0091] In Example 38 the subject matter of any one or more of Examples 23 to</p><p id="p0115" num="0115">37 may optionally include, receiving at least one of an expanding motion or a contracting motion of the first and second contact points relative to each other; and growing or shrinking the polygonal area, respectively. </p><p id="p0116" num="0116"> [0092] In Example 39 the subject matter of any one or more of Examples 23 to</p><p id="p0117" num="0117">38 may optionally include, wherein deriving the enclosed area using the user input comprises: detecting a first and second contact point from the user input; </p><p id="p0118" num="0118">identifying a first road portion proximate to the first contact point; identifying a second road portion proximate to the second contact point; and forming a portion of a border of the enclosed area using the first and second road portions. </p><p id="p0119" num="0119">[0093] In Example 40 the subject matter of any one or more of Examples 23 to</p><p id="p0120" num="0120">39 may optionally include, detecting a third contact point from the user input; identifying a third road portion proximate to the third contact point; and forming the enclosed area based on the first, second, and third road portions. </p><p id="p0121" num="0121"> [0094] In Example 41 the subject matter of any one or more of Examples 23 to</p><p id="p0122" num="0122">40 may optionally include, wherein the first and second contact points are received as a portion of a dragging input mechanism from the user. </p><p id="p0123" num="0123">[0095] In Example 42 the subject matter of any one or more of Examples 23 to</p><p id="p0124" num="0124">41 may optionally include, forming the enclosed area by connecting roads around a geographical structure; and associating the geographical structure with the geo- fence. </p><p id="p0125" num="0125"> [0096] In Example 43 the subject matter of any one or more of Examples 23 to 42 may optionally include, quantizing a curved road to approximate the shape of the road. </p><p id="p0126" num="0126"> [0097] In Example 44 the subject matter of any one or more of Examples 23 to 43 may optionally include, reducing the number of vertices defining the enclosed area. 
<!-- EPO <DP n="20"/>-->
 [0098] Example 45 includes or may optionally be combined with the subject matter of any one of Examples 1 -44 to include a computer-readable medium including instructions that when performed by a machine cause the machine to performs any one of the examples of 1-44. </p><p id="p0127" num="0127">[0099] Example 46 includes or may optionally be combined with the subject matter of any one of Examples 1-44 to include subject matter for creating a geo- fence comprising means for performing any one of the examples of 1-44. </p><p id="p0128" num="0128">[00100] The above detailed description includes references to the accompanying drawings, which form a part of the detailed description. The drawings show, by way of illustration, specific embodiments that may be practiced. These embodiments are also referred to herein as "examples." Such examples may include elements in addition to those shown or described. However, also contemplated are examples that include the elements shown or described. </p><p id="p0129" num="0129">Moreover, also contemplate are examples using any combination or permutation of those elements shown or described (or one or more aspects thereof), either with respect to a particular example (or one or more aspects thereof), or with respect to other examples (or one or more aspects thereof) shown or described herein. </p><p id="p0130" num="0130">[00101] Publications, patents, and patent documents referred to in this document are incorporated by reference herein in their entirety, as though individually incorporated by reference. In the event of inconsistent usages between this document and those documents so incorporated by reference, the usage in the incorporated reference(s) are supplementary to that of this document; for irreconcilable inconsistencies, the usage in this document controls. </p><p id="p0131" num="0131">[00102] In this document, the terms "a" or "an" are used, as is common in patent documents, to include one or more than one, independent of any other instances or usages of "at least one" or "one or more." In this document, the term "or" is used to refer to a nonexclusive or, such that "A or B" includes "A but not B," "B but not A," and "A and B," unless otherwise indicated. In the appended claims, the terms "including" and "in which" are used as the plain- English equivalents of the respective terms "comprising" and "wherein." Also, in the following claims, the terms "including" and "comprising" are open-ended, that is, a system, device, article, or process that includes elements in addition to those listed after such a term in a claim are still deemed to fall within the scope of that claim. Moreover, in the following claims, the terms "first," "second," and "third," etc. are used 
<!-- EPO <DP n="21"/>-->
 merely as labels, and are not intended to suggest a numerical order for their objects. </p><p id="p0132" num="0132"> [00103] The above description is intended to be illustrative, and not restrictive. For example, the above-described examples (or one or more aspects thereof) may be used in combination with others. Other embodiments may be used, such as by one of ordinary skill in the art upon reviewing the above description. The Abstract is to allow the reader to quickly ascertain the nature of the technical disclosure, for example, to comply with 37 C.F.R. § 1.72(b) in the United States of America. It is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. Also, in the above Detailed Description, various features may be grouped together to streamline the disclosure. However, the claims may not set forth every feature disclosed herein as embodiments may feature a subset of said features. Further, embodiments may include fewer features than those disclosed in a particular example. Thus, the following claims are hereby incorporated into the Detailed Description, with a claim standing on its own as a separate embodiment. The scope of the embodiments disclosed herein is to be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled. 
</p></description><claims mxw-id="PCLM70077596" ref-ucid="WO-2014130072-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="22"/>-->CLAIMS  What is claimed is: </claim-statement><claim id="clm-0001" num="1"><claim-text>1. A system for creating a geo-fence, the system comprising: </claim-text><claim-text> a touch screen display; </claim-text><claim-text> an interface module adapted to: </claim-text><claim-text> display a map on a touch screen display of a mobile device; and receive a user input from the touch screen display, the user input resulting from a user touching the touch screen display; and </claim-text><claim-text> a geo-fence module adapted to derive an enclosed area using the user input, </claim-text><claim-text> wherein the interface module is adapted to present the enclosed area as a geo-fence object on the touch screen display. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The system of claim 1, wherein to derive the enclosed area using the user input, the geo-fence module is adapted to: </claim-text><claim-text> determining a location of the user input on the map; </claim-text><claim-text> identifying a first geographical structure proximate to the location of the user input; and </claim-text><claim-text> constructing a polygonal representation of the first geographical structure to define the enclosed area. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The system of claim 2, wherein to construct the polygonal representation, the geo-fence module is adapted to: </claim-text><claim-text> use an edge detection process on map data corresponding to the map to identify edges of the first geographical structure; </claim-text><claim-text> filter the map data to identify edges surrounding the location of the user input; and </claim-text><claim-text> use a corner detection process to detect a plurality of vertices that define the geo-fence object. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The system of claim 3, wherein to use the edge detection process, the geo- fence module is adapted to: 
<!-- EPO <DP n="23"/>-->
 screen scrape a portion of the map currently displayed on the touch screen display to obtain a scraped image; and </claim-text><claim-text> process the scraped image to determine edges. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The system of claim 2, wherein the first geographical structure comprises a building. </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The system of claim 2 or 5, wherein the interface module is adapted to receive a second user input from the touch screen display, the second user input resulting from the user touching the touch screen display; and </claim-text><claim-text> wherein the geo-fence module is adapted to: </claim-text><claim-text> determine a second location of the second user input on the map; identify a second geographical structure proximate to the second location of the second user input; and </claim-text><claim-text> construct an aggregate polygonal representation including the first geographical structure and the second geographical structure to define the enclosed area. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The system of claim 6, wherein the first and second geographical structures are not adjacent to one another, and wherein the polygonal </claim-text><claim-text>representation comprises a plurality of polygonal areas. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The system of claim 7, wherein members of the plurality of polygonal areas are linked as a single geo-fence object. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. The system of claim 7, wherein members of the plurality of polygonal areas are distinct geo-fence objects. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The system of claim 6, wherein the second user input comprises a dragging motion that selects the first and second geographical structures. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. The system of claim 1, wherein to derive the enclosed area using the user input, the geo-fence module is adapted to: 
<!-- EPO <DP n="24"/>-->
 detect, via the interface module, a first and second contact point from the user input; </claim-text><claim-text> display, via the interface module, a polygonal area based on the first and second contact points, the polygonal area formed by the first and second contact points; and </claim-text><claim-text> use the displayed polygonal area to define the enclosed area. </claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. The system of claim 11 , wherein the polygonal area comprises a rectangle and wherein the first and second contact points are used to adjust opposing vertices of the rectangle. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The system of claim 11 , wherein the polygonal area comprises one of a circle, triangle, or square, and wherein the first contact point is used to anchor the center of the circle, triangle, or square, and the second contact point is used to adjust the size of the circle, triangle, or square. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The system of claim 11, wherein the interface module is adapted to: detect a third contact point from the user input; and </claim-text><claim-text> adjust a vertex of the polygonal area based on the third contact point. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The system of claim 11 or 14, wherein the interface module is adapted to: detect a rotating input motion from the user, rotating the first and second contact points; and </claim-text><claim-text> display the polygonal area moving in a corresponding rotating motion. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. The system of claim 11 or 14, wherein the interface module is adapted to: receive at least one of an expanding motion or a contracting motion of the first and second contact points relative to each other; and </claim-text><claim-text> grow or shrink the polygonal area, respectively. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. The system of claim 1, wherein to derive the enclosed area using the user input, the geo-fence module is adapted to: </claim-text><claim-text> detect, via the interface module, a first and second contact point from the user input; 
<!-- EPO <DP n="25"/>-->
 identify a first road portion proximate to the first contact point; </claim-text><claim-text> identify a second road portion proximate to the second contact point; and form a portion of a border of the enclosed area using the first and second road portions. </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. The system of claim 17, wherein the interface module is adapted to detect a third contact point from the user input; and </claim-text><claim-text> wherein the geo-fence module is adapted to: </claim-text><claim-text> identify a third road portion proximate to the third contact point; and </claim-text><claim-text> form the enclosed area based on the first, second, and third road portions. </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The system of claim 17, wherein the first and second contact points are received as a portion of a dragging input mechanism from the user. </claim-text></claim><claim id="clm-0020" num="20"><claim-text>20. The system of claim 17 or 19, wherein the geo-fence module is adapted to: form the enclosed area by connecting roads around a geographical structure; and </claim-text><claim-text> associate the geographical structure with the geo-fence. </claim-text></claim><claim id="clm-0021" num="21"><claim-text>21. A method of creating a geo-fence, the method comprising: </claim-text><claim-text> displaying a map on a touch screen interface of a mobile device; </claim-text><claim-text> receiving a user input from the touch screen interface, the user input resulting from a user touching the touch screen interface; </claim-text><claim-text> deriving an enclosed area using the user input; and </claim-text><claim-text> presenting the enclosed area as a geo-fence object on the touch screen interface. </claim-text></claim><claim id="clm-0022" num="22"><claim-text>22. The method of claim 21, wherein deriving the enclosed area using the user input comprises: </claim-text><claim-text> detecting a first and second contact point from the user input; </claim-text><claim-text> identifying a first road portion proximate to the first contact point; </claim-text><claim-text>identifying a second road portion proximate to the second contact point; and 
<!-- EPO <DP n="26"/>-->
 forming a portion of a border of the enclosed area using the first and second road portions. </claim-text></claim><claim id="clm-0023" num="23"><claim-text>23. The method of claim 22, comprising: </claim-text><claim-text> quantizing a curved road to approximate the shape of the road. </claim-text></claim><claim id="clm-0024" num="24"><claim-text>24. The method of claim 23, comprising: </claim-text><claim-text> reducing the number of vertices defining the enclosed area. </claim-text></claim><claim id="clm-0025" num="25"><claim-text>25. A machine-readable medium including instructions for creating a geo- fence, which when executed by a machine, cause the machine to perform operations of any one of the method claims 21-24. </claim-text></claim><claim id="clm-0026" num="26"><claim-text>26. An apparatus comprising means for performing any of the methods of claims 21-24. 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
