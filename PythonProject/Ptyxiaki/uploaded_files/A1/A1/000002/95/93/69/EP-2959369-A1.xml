<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959369-A1" country="EP" doc-number="2959369" kind="A1" date="20151230" family-id="51352244" file-reference-id="299097" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160452600" ucid="EP-2959369-A1"><document-id><country>EP</country><doc-number>2959369</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14754442-A" is-representative="NO"><document-id mxw-id="PAPP193868168" load-source="patent-office" format="original"><country>EP</country><doc-number>14754442.3</doc-number><date>20140212</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193868169" load-source="docdb" format="epo"><country>EP</country><doc-number>14754442</doc-number><kind>A</kind><date>20140212</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162033037" ucid="US-201313770506-A" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201313770506</doc-number><kind>A</kind><date>20130219</date></document-id></priority-claim><priority-claim mxw-id="PPC162032538" ucid="US-2014015912-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>2014015912</doc-number><kind>W</kind><date>20140212</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1900002933" load-source="docdb">G06F   3/041       20060101ALI20160825BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900006855" load-source="docdb">G06F   3/0488      20130101AFI20160825BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900008243" load-source="docdb">G06F   3/14        20060101ALI20160825BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900008619" load-source="docdb">G06F   3/0484      20130101ALN20160825BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900008931" load-source="docdb">G06F   9/44        20060101ALN20160825BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900009085" load-source="docdb">G06F   3/048       20060101ALI20160825BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1687166108" load-source="docdb" scheme="CPC">G06F   9/451       20180201 LI20180219BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1823307363" load-source="docdb" scheme="CPC">G06F   3/04842     20130101 LI20170421BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1823307861" load-source="docdb" scheme="CPC">G06F   3/04886     20130101 FI20170421BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165549266" lang="DE" load-source="patent-office">HANDHABUNG VON ÜBERLASTETEN GESTEN</invention-title><invention-title mxw-id="PT165549267" lang="EN" load-source="patent-office">HANDLING OVERLOADED GESTURES</invention-title><invention-title mxw-id="PT165549268" lang="FR" load-source="patent-office">GESTION DE GESTES SURCHARGÉS</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103336233" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>FACEBOOK INC</last-name><address><country>US</country></address></addressbook></applicant><applicant mxw-id="PPAR1103315930" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>FACEBOOK, INC.</last-name></addressbook></applicant><applicant mxw-id="PPAR1101642728" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Facebook, Inc.</last-name><iid>101278072</iid><address><street>1601 Willow Road</street><city>Menlo Park, CA 94025</city><country>US</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103308731" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>AMERIGE BRIAN D</last-name><address><country>US</country></address></addressbook></inventor><inventor mxw-id="PPAR1103329080" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>AMERIGE, BRIAN D.</last-name></addressbook></inventor><inventor mxw-id="PPAR1101643664" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>AMERIGE, BRIAN D.</last-name><address><street>1601 Willow Road</street><city>Menlo Park, CA 94025</city><country>US</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101649736" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Schröer, Gernot H.</last-name><suffix>et al</suffix><iid>100036917</iid><address><street>Meissner, Bolte &amp; Partner GbR Bankgasse 3</street><city>90402 Nürnberg</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="US-2014015912-W"><document-id><country>US</country><doc-number>2014015912</doc-number><kind>W</kind><date>20140212</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014130306-A1"><document-id><country>WO</country><doc-number>2014130306</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660625677" load-source="docdb">AL</country><country mxw-id="DS660704413" load-source="docdb">AT</country><country mxw-id="DS660625683" load-source="docdb">BE</country><country mxw-id="DS660789077" load-source="docdb">BG</country><country mxw-id="DS660625548" load-source="docdb">CH</country><country mxw-id="DS660726251" load-source="docdb">CY</country><country mxw-id="DS660704414" load-source="docdb">CZ</country><country mxw-id="DS660625684" load-source="docdb">DE</country><country mxw-id="DS660726252" load-source="docdb">DK</country><country mxw-id="DS660726253" load-source="docdb">EE</country><country mxw-id="DS660704004" load-source="docdb">ES</country><country mxw-id="DS660789078" load-source="docdb">FI</country><country mxw-id="DS660789079" load-source="docdb">FR</country><country mxw-id="DS660625685" load-source="docdb">GB</country><country mxw-id="DS660726254" load-source="docdb">GR</country><country mxw-id="DS660625686" load-source="docdb">HR</country><country mxw-id="DS660704419" load-source="docdb">HU</country><country mxw-id="DS660625549" load-source="docdb">IE</country><country mxw-id="DS660726259" load-source="docdb">IS</country><country mxw-id="DS660789080" load-source="docdb">IT</country><country mxw-id="DS660726260" load-source="docdb">LI</country><country mxw-id="DS660626519" load-source="docdb">LT</country><country mxw-id="DS660710512" load-source="docdb">LU</country><country mxw-id="DS660626520" load-source="docdb">LV</country><country mxw-id="DS660626521" load-source="docdb">MC</country><country mxw-id="DS660710513" load-source="docdb">MK</country><country mxw-id="DS660710514" load-source="docdb">MT</country><country mxw-id="DS660704420" load-source="docdb">NL</country><country mxw-id="DS660625695" load-source="docdb">NO</country><country mxw-id="DS660726261" load-source="docdb">PL</country><country mxw-id="DS660626531" load-source="docdb">PT</country><country mxw-id="DS660704421" load-source="docdb">RO</country><country mxw-id="DS660626532" load-source="docdb">RS</country><country mxw-id="DS660726262" load-source="docdb">SE</country><country mxw-id="DS660789081" load-source="docdb">SI</country><country mxw-id="DS660625696" load-source="docdb">SK</country><country mxw-id="DS660726267" load-source="docdb">SM</country><country mxw-id="DS660710515" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139072485" ref-ucid="WO-2014130306-A1" lang="EN" load-source="patent-office"><p num="0000">In one embodiment, a method includes receiving a touch input within a particular region of a display area of the computing device. The display area presents a user interface (UI) including a number of views and an outside view. Each of the views corresponds to one or more regions of the display area. One or more of the views has a gesture recognizer configured to process a touch input detected within the regions of the display area associated with each of the views. The outside view has a gesture recognizer configured to process the touch input detected within the display area. The views are organized in a hierarchy. The method also includes determining whether a particular one of the views from among the views is set as a modal view.</p></abstract><abstract mxw-id="PA139540134" ref-ucid="WO-2014130306-A1" lang="EN" source="national office" load-source="docdb"><p>In one embodiment, a method includes receiving a touch input within a particular region of a display area of the computing device. The display area presents a user interface (UI) including a number of views and an outside view. Each of the views corresponds to one or more regions of the display area. One or more of the views has a gesture recognizer configured to process a touch input detected within the regions of the display area associated with each of the views. The outside view has a gesture recognizer configured to process the touch input detected within the display area. The views are organized in a hierarchy. The method also includes determining whether a particular one of the views from among the views is set as a modal view.</p></abstract><abstract mxw-id="PA139072486" ref-ucid="WO-2014130306-A1" lang="FR" load-source="patent-office"><p num="0000">Selon un mode de réalisation de l'invention, un procédé comprend une étape consistant à recevoir une entrée tactile à l'intérieur d'une région particulière d'une zone d'affichage du dispositif informatique. La zone d'affichage présente une interface utilisateur (UI) comprenant un certain nombre de vues et une vue extérieure. Chacune des vues correspond à une ou plusieurs régions de la zone d'affichage. Une ou plusieurs des vues sont dotées d'un dispositif de reconnaissance de geste conçu pour traiter une entrée tactile détectée à l'intérieur des régions de la zone d'affichage associée à chacune des vues. La vue extérieure est dotée d'un dispositif de reconnaissance de geste conçu pour traiter l'entrée tactile détectée à l'intérieur de la zone d'affichage. Les vues sont organisées suivant une hiérarchie. Le procédé comprend également une étape consistant à déterminer si une vue particulière parmi les vues est définie en tant que vue modale.</p></abstract><abstract mxw-id="PA139540135" ref-ucid="WO-2014130306-A1" lang="FR" source="national office" load-source="docdb"><p>Selon un mode de réalisation de l'invention, un procédé comprend une étape consistant à recevoir une entrée tactile à l'intérieur d'une région particulière d'une zone d'affichage du dispositif informatique. La zone d'affichage présente une interface utilisateur (UI) comprenant un certain nombre de vues et une vue extérieure. Chacune des vues correspond à une ou plusieurs régions de la zone d'affichage. Une ou plusieurs des vues sont dotées d'un dispositif de reconnaissance de geste conçu pour traiter une entrée tactile détectée à l'intérieur des régions de la zone d'affichage associée à chacune des vues. La vue extérieure est dotée d'un dispositif de reconnaissance de geste conçu pour traiter l'entrée tactile détectée à l'intérieur de la zone d'affichage. Les vues sont organisées suivant une hiérarchie. Le procédé comprend également une étape consistant à déterminer si une vue particulière parmi les vues est définie en tant que vue modale.</p></abstract><description mxw-id="PDES78476579" ref-ucid="WO-2014130306-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> HANDLING OVERLOADED GESTURES </p><p id="p0002" num="0002">TECHNICAL FIELD </p><p id="p0003" num="0003"> [1] This disclosure generally relates to mobile devices. </p><p id="p0004" num="0004">BACKGROUND </p><p id="p0005" num="0005"> [2] A mobile computing device— such as a smartphone, tablet computer, or laptop computer— may include functionality for determining its location, direction, or orientation, such as a GPS receiver, compass, or gyroscope. Such a device may also include functionality for wireless communication, such as BLUETOOTH communication, near-field communication (NFC), or infrared (IR) communication or communication with a wireless local area networks (WLANs) or cellular-telephone network. Such a device may also include one or more cameras, scanners, touchscreens, microphones, or speakers. Mobile computing devices may also execute software applications, such as games, web browsers, or social-networking applications. With social-networking applications, users may connect, communicate, and share information with other users in their social networks. </p><p id="p0006" num="0006">SUMMARY OF PARTICULAR EMBODIMENTS </p><p id="p0007" num="0007">[3] In particular embodiments, a user interface of a mobile device application that comprises a nested hierarchy of views may enable a particular view to over-ride the default behavior of a hit-test view. For example, the default behavior of the hit test may allocate a touch input to the lowest view that contains the area of the touch input. In particular embodiments, a view higher in the hierarchy may express it's interest in "winning" the hit test or process the touch input. The view that over-rides the default hit test view may be known as the modal view. In particular embodiments, a modal view may be eligible to process a touch input within the modal view and force the hit test view to defer processing of the touch input to the modal view. 
<!-- EPO <DP n="4"/>-->
 An example modal view is the full screen view of a photo in an application. Any touch of the photo should be processed by the photo view. In one example, modality may be implemented using run-time swizzling to over-ride methods within the mobile iOS for touch input processing. </p><p id="p0008" num="0008">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0009" num="0009">[4] FIGURE 1 illustrates an example mobile device. </p><p id="p0010" num="0010"> [5] FIGURE 2 illustrates an example wireframe of an example user interface with an example nested hierarchy of views. </p><p id="p0011" num="0011"> [6] FIGURE 3 illustrates an example method for handling overloaded gestures. </p><p id="p0012" num="0012"> [7] FIGURE 4 illustrates an example computing system. </p><p id="p0013" num="0013">DESCRIPTION OF EXAMPLE EMBODIMENTS </p><p id="p0014" num="0014">[8] FIGURE 1 illustrates an example mobile computing device. In particular embodiments, the client system may be a mobile computing device 10 as described above. This disclosure contemplates mobile computing device 10 taking any suitable physical form. In particular embodiments, mobile computing device 10 may be a computing system as described below. As example and not by way of limitation, mobile computing device 10 may be a single- board computer system (SBC) (such as, for example, a computer-on-module (COM) or system- on-module (SOM)), a laptop or notebook computer system, a mobile telephone, a smartphone, a personal digital assistant (PDA), a tablet computer system, or a combination of two or more of these. In particular embodiments, mobile computing device 10 may have a touch sensor 12 as an input component. In the example of FIGURE 1, touch sensor 12 is incorporated on a front surface of mobile computing device 10. In the case of capacitive touch sensors, there may be two types of electrodes: transmitting and receiving. These electrodes may be connected to a controller designed to drive the transmitting electrodes with electrical pulses and measure the changes in capacitance from the receiving electrodes caused by a touch or proximity input. In 
<!-- EPO <DP n="5"/>-->
 the example of FIGURE 1, one or more antennae 14A-B may be incorporated into one or more sides of mobile computing device 10. Antennae 14A-B are components that convert electric current into radio waves, and vice versa. During transmission of signals, a transmitter applies an oscillating radio frequency (RF) electric current to terminals of antenna 14A-B, and antenna 14A-B radiates the energy of the applied the current as electromagnetic (EM) waves. During reception of signals, antennae 14A-B convert the power of an incoming EM wave into a voltage at the terminals of antennae 14A-B. The voltage may be transmitted to a receiver for amplification. </p><p id="p0015" num="0015"> [9] Mobile computing device 10 many include a communication component coupled to antennae 14A-B for communicating with an Ethernet or other wire-based network or a wireless NIC (WNIC), wireless adapter for communicating with a wireless network, such as for example a WI-FI network or modem for communicating with a cellular network, such third generation mobile telecommunications (3G), or Long Term Evolution (LTE) network. This disclosure contemplates any suitable network and any suitable communication component 20 for it. As an example and not by way of limitation, mobile computing device 10 may communicate with an ad hoc network, a personal area network (PAN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), or one or more portions of the Internet or a combination of two or more of these. One or more portions of one or more of these networks may be wired or wireless. As another example, mobile computing device 10 may communicate with a wireless PAN (WPAN) (such as, for example, a BLUETOOTH WPAN), a WI-FI network, a WI-MAX network, a cellular telephone network (such as, for example, a Global System for Mobile Communications (GSM), 3G, or LTE network), or other suitable wireless network or a combination of two or more of these. Mobile computing device 10 may include any suitable communication component for any of these networks, where appropriate. </p><p id="p0016" num="0016"> [10] In particular embodiments, an application executed on mobile computing device 10 may provide a user interface (UI) on a display of mobile computing device 10. As described below, the user of mobile computing device 10 may interact with the UI of a particular application through touch inputs detected by touch sensor 12. In particular embodiments, the 
<!-- EPO <DP n="6"/>-->
 application may correlate one or more touch inputs detected by touch sensor 12 with one or more interactions with the application. Although this disclosure illustrates and describes handling gestures on a particular type of computing device with a touch sensor, this disclosure contemplates handling gestures by any suitable type of computing device with a touch sensor, such as for example, a personal computer, tablet computer, connected television, or smartphone. </p><p id="p0017" num="0017"> [11] FIGURE 2 illustrates an example wireframe of an example user interface with an example nested hierarchy of views. In particular embodiments, the UI of an application displayed in display area 54 of mobile computing device 10 may include one or more views 50A-C that each correspond to a particular region of display area 54. Furthermore, a particular view 50A-C may handle processing one or more touch inputs detected within the associated region of display area 54. In particular embodiments, the touch inputs may be processed through a gesture recognizer associated with each view 50A-C. In particular embodiments, views 50A-C with overlapping regions of display area 54 may be organized into a nested hierarchy based at least in part on the size of the corresponding area of display area 54. In the example of FIGURE 2, view 50C has a portion of its corresponding area of display 54 that overlaps the corresponding area of view 50B. Moreover, a region of view 50B overlaps the area of display area 54 that corresponds to view 5 OA. As an example and not by way of limitation, a particular nested hierarchy of views 50A-C may process touch inputs by assigning the highest priority for a view 50A-C lowest in the hierarchy that encompasses the touch input and does not substantially encompass another view 50A-C within it. For example, a touch input 56 within a particular region of display area 54 corresponding to views 50A-C of the hierarchy may be processed by view 5 OA. Although this disclosure illustrates and describes a particular configuration of a particular number of views associated with particularly shaped regions of the display area, this disclosure contemplates any suitable configuration of any suitable number of views associated with any suitably shaped region. </p><p id="p0018" num="0018"> [12] In particular embodiments, the hierarchy of views 50A-C of an application may be modified such that touch input 56 may be processed by a view 50B-C higher in the nested hierarchy than view 5 OA, such as for example one of views 50B-C. As an example and not by 
<!-- EPO <DP n="7"/>-->
 way of limitation, an application may modify the ranking of the gesture recognizer associated with each view 50A-C within the hierarchy such that the gesture recognizer associated with a particular higher level view (e.g. 50B-C) or a "modal" view may process touch input 56. As an example and not by way of limitation, a particular view (e.g. 50C) may be designated the modal view and the application may force the gesture recognizer associated with view 50A to defer processing of touch input 56 to the gesture recognizer associated with the modal view (e.g. 50C). Furthermore, the application may designate the modal view by setting a modal view flag associated with the particular view (e.g. 50C). Although this disclosure describes a particular view being a modal view, this disclosure contemplates designating any suitable view as being the modal view. </p><p id="p0019" num="0019"> [13] In particular embodiments, the UI of a particular application executed on mobile computing device 10 may include an outside view 52 in addition to views 50A-C. Outside view 52 may have an associated area of coverage that encompasses a region outside the boundaries of display area 54. In particular embodiments, setting the modal view flag associated with a modal property by the application executed on mobile computing device 10 retargets the gesture recognizer associated with a particular modal view 50A-C to the gesture recognizer associated with outside view 52, thereby enabling the gesture recognizer associated with the particular view 50A-C to be eligible to process touch input 56 within any region of display area 54 instead of being confined to processing touch inputs within the region of display area 54 associated with the particular view 50A-C. As described below, associating the gesture recognizer of the particular modal view with the gesture recognizer of outside view 52 ensures the gesture recognizer of the particular modal view is eligible to process any touch input 56 performed within display area 54. In particular embodiments, the gesture recognizer associated with the modal view may set a modal view flag to indicate modal gesture recognizer will over-ride the nest hierarchy of views 50A-C. </p><p id="p0020" num="0020"> [14] In particular embodiments, an application may query each view 50A-C and outside view 52 of the hierarchy to determine one or more views 50A-C and outside view 52 eligible to process touch input 56, in response to detecting touch input 56. In particular 
<!-- EPO <DP n="8"/>-->
 embodiments, the application executed on mobile computing device 10 may determine that one or more eligible views 50A-C have an associated gesture recognizer that is configured to process touch input 56. As described above, the gesture recognizer associated with a modal view may be retargeted to the gesture recognizer associated with outside view 52. The application executed on mobile computing device 10 may allow the gesture recognizer associated with the modal view (e.g. 50C) to prevent any of the remaining non-modal views (e.g. 50A-B) from processing touch input 56 and restrict the remaining non-modal views (e.g. 50A-B) from preventing the modal view from processing touch input 56 through outside view 52. In response to detecting touch input 56, each eligible gesture recognizer in the hierarchy is queried from lowest to highest view 50A-C and outside view 52 of the hierarchy to determine if any view 50A-C may prevent another view 50A-C from processing the detected touch input 56. In the example of FIGURE 2, initially the application will query the gesture recognizer associated with view 5 OA, then the gesture recognizer of views 50B-C according to the hierarchy of views described above. As an example and not by way of limitation, the gesture recognizer associated with view 50A may query the gesture recognizer associated with view 50B whether the gesture recognizer associated with view 50B may prevent the gesture recognizer associated with view 50A from processing touch input 56. As described above, the gesture recognizer associated with outside view 52 may prevent non-modal views (e.g. 50A-B) from processing touch input 56 and restrict non-modal views (e.g. 50A-B) from preventing any view 50A-B from processing touch input 56, thereby modifying the nested hierarchy such that touch input 56 is processed by the retargeted gesture recognizer associated with the modal view (e.g. 50C). As an example and not by way of limitation, modifying the nested hierarchy as described above may be implemented through any suitable method-swapping protocol, such as for example, run-time swizzling. As another example, a custom gesture recognizer associated with each view 50A-C may enforce a modified hierarchy described above. </p><p id="p0021" num="0021"> [15] FIGURE 3 illustrates an example method for handling overloaded gestures. The method may start at step 300, where a computing device receives a touch input within a particular region of a display area of the computing device. In particular embodiments, the 
<!-- EPO <DP n="9"/>-->
 display area presents a user interface (UI) comprising a plurality of views and an outside view. In particular embodiments, each of the views corresponds to one or more regions of the display area. In particular embodiments, one or more of the views has a gesture recognizer configured to process a touch input detected within the regions of the display area associated with each of the views, the outside view having a gesture recognizer configured to process the touch input detected within the display area, the views being organized in a hierarchy. Step 302 determines, by the computing device, whether a particular one of the views from among the plurality of views is set as a modal view. Step 304 retargets, by the computing device, the gesture recognizer associated with the particular view to the gesture recognizer associated with the outside view in response to determining the particular view is set as the modal view. At step 306 the computing device prevents the gesture recognizers of one or more non-modal views from processing the touch input within the particular region in response to a presence of the modal view. At step 308 the computing device processes the touch input within the particular region through the gesture recognizer associated with the modal view in response to the presence of the modal view, at which point the method may end. Although this disclosure describes and illustrates particular steps of the method of FIGURE 3 as occurring in a particular order, this disclosure contemplates any suitable steps of the method of FIGURE 3 occurring in any suitable order. Moreover, although this disclosure describes and illustrates particular components carrying out particular steps of the method of FIGURE 3, this disclosure contemplates any suitable combination of any suitable components carrying out any suitable steps of the method of FIGURE 3. </p><p id="p0022" num="0022"> [16] FIGURE 4 illustrates example computing system. In particular embodiments, one or more computer systems 60 perform one or more steps of one or more methods described or illustrated herein. In particular embodiments, one or more computer systems 60 provide functionality described or illustrated herein. In particular embodiments, software running on one or more computer systems 60 performs one or more steps of one or more methods described or illustrated herein or provides functionality described or illustrated herein. Particular embodiments include one or more portions of one or more computer systems 60. Herein, reference to a computer system may encompass a computing device, where appropriate. 
<!-- EPO <DP n="10"/>-->
 Moreover, reference to a computer system may encompass one or more computer systems, where appropriate. </p><p id="p0023" num="0023"> [17] This disclosure contemplates any suitable number of computer systems 60. This disclosure contemplates computer system 60 taking any suitable physical form. As example and not by way of limitation, computer system 60 may be an embedded computer system, a system- on-chip (SOC), a single-board computer system (SBC) (such as, for example, a computer-on- module (COM) or system-on-module (SOM)), a desktop computer system, a laptop or notebook computer system, an interactive kiosk, a mainframe, a mesh of computer systems, a mobile telephone, a personal digital assistant (PDA), a server, a tablet computer system, or a combination of two or more of these. Where appropriate, computer system 60 may include one or more computer systems 60; be unitary or distributed; span multiple locations; span multiple machines; span multiple data centers; or reside in a cloud, which may include one or more cloud components in one or more networks. Where appropriate, one or more computer systems 60 may perform without substantial spatial or temporal limitation one or more steps of one or more methods described or illustrated herein. As an example and not by way of limitation, one or more computer systems 60 may perform in real time or in batch mode one or more steps of one or more methods described or illustrated herein. One or more computer systems 60 may perform at different times or at different locations one or more steps of one or more methods described or illustrated herein, where appropriate. </p><p id="p0024" num="0024"> [18] In particular embodiments, computer system 60 includes a processor 62, memory 64, storage 66, an input/output (I/O) interface 68, a communication interface 70, and a bus 72. Although this disclosure describes and illustrates a particular computer system having a particular number of particular components in a particular arrangement, this disclosure contemplates any suitable computer system having any suitable number of any suitable components in any suitable arrangement. </p><p id="p0025" num="0025"> [19] In particular embodiments, processor 62 includes hardware for executing instructions, such as those making up a computer program. As an example and not by way of limitation, to execute instructions, processor 62 may retrieve (or fetch) the instructions from an 
<!-- EPO <DP n="11"/>-->
 internal register, an internal cache, memory 64, or storage 66; decode and execute them; and then write one or more results to an internal register, an internal cache, memory 64, or storage 66. In particular embodiments, processor 62 may include one or more internal caches for data, instructions, or addresses. This disclosure contemplates processor 62 including any suitable number of any suitable internal caches, where appropriate. As an example and not by way of limitation, processor 62 may include one or more instruction caches, one or more data caches, and one or more translation lookaside buffers (TLBs). Instructions in the instruction caches may be copies of instructions in memory 64 or storage 66, and the instruction caches may speed up retrieval of those instructions by processor 62. Data in the data caches may be copies of data in memory 64 or storage 66 for instructions executing at processor 62 to operate on; the results of previous instructions executed at processor 62 for access by subsequent instructions executing at processor 62 or for writing to memory 64 or storage 66; or other suitable data. The data caches may speed up read or write operations by processor 62. The TLBs may speed up virtual-address translation for processor 62. In particular embodiments, processor 62 may include one or more internal registers for data, instructions, or addresses. This disclosure contemplates processor 62 including any suitable number of any suitable internal registers, where appropriate. Where appropriate, processor 62 may include one or more arithmetic logic units (ALUs); be a multi- core processor; or include one or more processors 62. Although this disclosure describes and illustrates a particular processor, this disclosure contemplates any suitable processor. </p><p id="p0026" num="0026"> [20] In particular embodiments, memory 64 includes main memory for storing instructions for processor 62 to execute or data for processor 62 to operate on. As an example and not by way of limitation, computer system 60 may load instructions from storage 66 or another source (such as, for example, another computer system 60) to memory 64. Processor 62 may then load the instructions from memory 64 to an internal register or internal cache. To execute the instructions, processor 62 may retrieve the instructions from the internal register or internal cache and decode them. During or after execution of the instructions, processor 62 may write one or more results (which may be intermediate or final results) to the internal register or internal cache. Processor 62 may then write one or more of those results to memory 64. In 
<!-- EPO <DP n="12"/>-->
 particular embodiments, processor 62 executes only instructions in one or more internal registers or internal caches or in memory 64 (as opposed to storage 66 or elsewhere) and operates only on data in one or more internal registers or internal caches or in memory 64 (as opposed to storage 66 or elsewhere). One or more memory buses (which may each include an address bus and a data bus) may couple processor 62 to memory 64. Bus 72 may include one or more memory buses, as described below. In particular embodiments, one or more memory management units (MMUs) reside between processor 62 and memory 64 and facilitate accesses to memory 64 requested by processor 62. In particular embodiments, memory 64 includes random access memory (RAM). This RAM may be volatile memory, where appropriate Where appropriate, this RAM may be dynamic RAM (DRAM) or static RAM (SRAM). Moreover, where appropriate, this RAM may be single-ported or multi-ported RAM. This disclosure contemplates any suitable RAM. Memory 64 may include one or more memories 64, where appropriate. Although this disclosure describes and illustrates particular memory, this disclosure contemplates any suitable memory. </p><p id="p0027" num="0027"> [21] In particular embodiments, storage 66 includes mass storage for data or instructions. As an example and not by way of limitation, storage 66 may include a hard disk drive (HDD), a floppy disk drive, flash memory, an optical disc, a magneto-optical disc, magnetic tape, or a Universal Serial Bus (USB) drive or a combination of two or more of these. Storage 66 may include removable or non-removable (or fixed) media, where appropriate. Storage 66 may be internal or external to computer system 60, where appropriate. In particular embodiments, storage 66 is non-volatile, solid-state memory. In particular embodiments, storage 66 includes read-only memory (ROM). Where appropriate, this ROM may be mask-programmed ROM, programmable ROM (PROM), erasable PROM (EPROM), electrically erasable PROM (EEPROM), electrically alterable ROM (EAROM), or flash memory or a combination of two or more of these. This disclosure contemplates mass storage 66 taking any suitable physical form. Storage 66 may include one or more storage control units facilitating communication between processor 62 and storage 66, where appropriate. Where appropriate, storage 66 may include one or more storages 66. Although this disclosure describes and illustrates particular storage, this disclosure contemplates any suitable storage. 
<!-- EPO <DP n="13"/>-->
 [22] In particular embodiments, I/O interface 68 includes hardware, software, or both providing one or more interfaces for communication between computer system 60 and one or more I/O devices. Computer system 60 may include one or more of these I/O devices, where appropriate. One or more of these I/O devices may enable communication between a person and computer system 60. As an example and not by way of limitation, an I/O device may include a keyboard, keypad, microphone, monitor, mouse, printer, scanner, speaker, still camera, stylus, tablet, touch screen, trackball, video camera, another suitable I/O device or a combination of two or more of these. An I/O device may include one or more sensors. This disclosure contemplates any suitable I/O devices and any suitable I/O interfaces 68 for them. Where appropriate, I/O interface 68 may include one or more device or software drivers enabling processor 62 to drive one or more of these I/O devices. I/O interface 68 may include one or more I/O interfaces 68, where appropriate. Although this disclosure describes and illustrates a particular I/O interface, this disclosure contemplates any suitable I/O interface. </p><p id="p0028" num="0028"> [23] In particular embodiments, communication interface 70 includes hardware, software, or both providing one or more interfaces for communication (such as for example, packet-based communication) between computer system 60 and one or more other computer systems 60 or one or more networks. As an example and not by way of limitation, communication interface 70 may include a network interface controller (NIC) or network adapter for communicating with an Ethernet or other wire-based network or a wireless NIC (WNIC) or wireless adapter for communicating with a wireless network, such as a WI-FI network. This disclosure contemplates any suitable network and any suitable communication interface 70 for it. As an example and not by way of limitation, computer system 60 may communicate with an ad hoc network, a personal area network (PAN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), or one or more portions of the Internet or a combination of two or more of these. One or more portions of one or more of these networks may be wired or wireless. As an example, computer system 60 may communicate with a wireless PAN (WPAN) (such as for example, a BLUETOOTH WPAN), a WI-FI network, a WI-MAX network, a cellular telephone network (such as, for example, a Global System for Mobile 
<!-- EPO <DP n="14"/>-->
 Communications (GSM) network), or other suitable wireless network or a combination of two or more of these. Computer system 60 may include any suitable communication interface 70 for any of these networks, where appropriate. Communication interface 70 may include one or more communication interfaces 70, where appropriate. Although this disclosure describes and illustrates a particular communication interface, this disclosure contemplates any suitable communication interface. </p><p id="p0029" num="0029"> [24] In particular embodiments, bus 72 includes hardware, software, or both coupling components of computer system 60 to each other. As an example and not by way of limitation, bus 72 may include an Accelerated Graphics Port (AGP) or other graphics bus, an Enhanced Industry Standard Architecture (EISA) bus, a front-side bus (FSB), a HYPERTRANSPORT (HT) interconnect, an Industry Standard Architecture (ISA) bus, an INFINIBAND interconnect, a low-pin-count (LPC) bus, a memory bus, a Micro Channel Architecture (MCA) bus, a Peripheral Component Interconnect (PCI) bus, a PCI-Express (PCIe) bus, a serial advanced technology attachment (SATA) bus, a Video Electronics Standards Association local (VLB) bus, or another suitable bus or a combination of two or more of these. Bus 72 may include one or more buses 72, where appropriate. Although this disclosure describes and illustrates a particular bus, this disclosure contemplates any suitable bus or interconnect. </p><p id="p0030" num="0030"> [25] Herein, a computer-readable non-transitory storage medium or media may include one or more semiconductor-based or other integrated circuits (ICs) (such, as for example, field- programmable gate arrays (FPGAs) or application-specific ICs (ASICs)), hard disk drives (HDDs), hybrid hard drives (HHDs), optical discs, optical disc drives (ODDs), magneto-optical discs, magneto-optical drives, floppy diskettes, floppy disk drives (FDDs), magnetic tapes, solid- state drives (SSDs), RAM-drives, SECURE DIGITAL cards or drives, any other suitable computer-readable non-transitory storage media, or any suitable combination of two or more of these, where appropriate. A computer-readable non-transitory storage medium may be volatile, non-volatile, or a combination of volatile and non-volatile, where appropriate. </p><p id="p0031" num="0031"> [26] Herein, "or" is inclusive and not exclusive, unless expressly indicated otherwise or indicated otherwise by context. Therefore, herein, "A or B" means "A, B, or both," unless 
<!-- EPO <DP n="15"/>-->
 expressly indicated otherwise or indicated otherwise by context. Moreover, "and" is both joint and several, unless expressly indicated otherwise or indicated otherwise by context. Therefore, herein, "A and B" means "A and B, jointly or severally," unless expressly indicated otherwise or indicated otherwise by context. </p><p id="p0032" num="0032"> [27] The scope of this disclosure encompasses all changes, substitutions, variations, alterations, and modifications to the example embodiments described or illustrated herein that a person having ordinary skill in the art would comprehend. The scope of this disclosure is not limited to the example embodiments described or illustrated herein. Moreover, although this disclosure describes and illustrates respective embodiments herein as including particular components, elements, functions, operations, or steps, any of these embodiments may include any combination or permutation of any of the components, elements, functions, operations, or steps described or illustrated anywhere herein that a person having ordinary skill in the art would comprehend. Furthermore, reference in the appended claims to an apparatus or system or a component of an apparatus or system being adapted to, arranged to, capable of, configured to, enabled to, operable to, or operative to perform a particular function encompasses that apparatus, system, component, whether or not it or that particular function is activated, turned on, or unlocked, as long as that apparatus, system, or component is so adapted, arranged, capable, configured, enabled, operable, or operative. 
</p></description><claims mxw-id="PCLM70076823" ref-ucid="WO-2014130306-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="16"/>-->WHAT IS CLAIMED IS: </claim-statement><claim id="clm-0001" num="1"><claim-text> 1. A method comprising: </claim-text><claim-text> by a computing device, receiving a touch input within a particular region of a display area of the computing device, the display area presenting a user interface (UI) comprising a plurality of views and an outside view, each of the views corresponding to one or more regions of the display area, one or more of the views having a gesture recognizer configured to process a touch input detected within the regions of the display area associated with each of the views, the outside view having a gesture recognizer configured to process the touch input detected within the display area, the views being organized in a hierarchy; </claim-text><claim-text> by the computing device, determining whether a particular one of the views from among the plurality of views is set as a modal view; </claim-text><claim-text> by the computing device, retargeting the gesture recognizer associated with the particular view to the gesture recognizer associated with the outside view in response to determining the particular view is set as the modal view; </claim-text><claim-text> by the computing device, preventing the gesture recognizers of one or more non-modal views from processing the touch input within the particular region in response to a presence of the modal view; and </claim-text><claim-text> by the computing device, processing the touch input within the particular region through the gesture recognizer associated with the modal view in response to the presence of the modal view. </claim-text></claim><claim id="clm-0002" num="2"><claim-text>2. The method of Claim 1, further comprising: </claim-text><claim-text> by the computing device, determining one or more of eligible views from among the plurality of views, each of the eligible views having one or more corresponding regions of the display area that encompasses the particular region containing the touch input; </claim-text><claim-text> by the computing device, determining a lowest eligible view in the hierarchy based at least in part on a size of one or more of the regions associated with each eligible view; and </claim-text><claim-text> by the computing device, processing the touch input within the particular region through 
<!-- EPO <DP n="17"/>-->
 the gesture recognizer associated with the lowest eligible view in response to an absence of the modal view. </claim-text></claim><claim id="clm-0003" num="3"><claim-text>3. The method of Claim 1, wherein the computing device is a mobile computing device. </claim-text></claim><claim id="clm-0004" num="4"><claim-text>4. The method of Claim 3, wherein the method is executed by an application of the mobile computing device. </claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method of Claim 1, further comprising: </claim-text><claim-text> preventing the gesture recognizers of one or more non-modal views from processing the touch input within the particular regions in response to the presence of the modal view; and </claim-text><claim-text> restricting the non-modal views from preventing the gesture recognizer associated with the modal view from processing the touch input in response to the presence of the modal view. </claim-text></claim><claim id="clm-0006" num="6"><claim-text>6. The method of Claim 1, selecting the particular one of the views comprises setting a modal view flag of the particular one of the views. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method of Claim 1, wherein the retargeting of the gesture recognizer comprises mapping the gesture recognizer associated with the modal view to the gesture recognizer associated with the outside view through run-time swizzling. </claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The method of Claim 1 , wherein the modal view is higher in the hierarchy than one or more of the non-modal views eligible to process the touch input. </claim-text></claim><claim id="clm-0009" num="9"><claim-text>9. One or more computer-readable non-transitory storage media embodying software configured when executed to: </claim-text><claim-text> receive a touch input within a particular region of a display area of a computing device, 
<!-- EPO <DP n="18"/>-->
 the display area presenting a user interface (UI) comprising a plurality of views and an outside view, each of the views corresponding to one or more regions of the display area, one or more of the views having a gesture recognizer configured to process a touch input detected within the regions of the display area associated with each of the views, the outside view having a gesture recognizer configured to process the touch input detected within the display area, the views being organized in a hierarchy; </claim-text><claim-text> determine whether a particular one of the views from among the plurality of views is set as a modal view; </claim-text><claim-text> retarget the gesture recognizer associated with the particular view to the gesture recognizer associated with the outside view in response to determining the particular view is set as the modal view; </claim-text><claim-text> prevent the gesture recognizers of one or more non-modal views from processing the touch input within the particular region in response to a presence of the modal view; and </claim-text><claim-text> process the touch input within the particular region through the gesture recognizer associated with the modal view in response to the presence of the modal view. </claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. The media of Claim 9, wherein the software is further configured to: </claim-text><claim-text> determine one or more of eligible views from among the plurality of views, each of the eligible views having one or more corresponding regions of the display area that encompasses the particular region containing the touch input; </claim-text><claim-text> determine a lowest eligible view in the hierarchy based at least in part on a size of one or more of the regions associated with each eligible view; and </claim-text><claim-text> process the touch input within the particular region through the gesture recognizer associated with the lowest eligible view in response to an absence of the modal view. </claim-text></claim><claim id="clm-0011" num="11"><claim-text>11. The media of Claim 9, wherein the computing device is a mobile computing device. 
<!-- EPO <DP n="19"/>-->
</claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. The media of Claim 11, wherein the software is an application of the mobile computing device. </claim-text></claim><claim id="clm-0013" num="13"><claim-text>13. The media of Claim 9, wherein the software is further configured to: </claim-text><claim-text> prevent the gesture recognizers of one or more non-modal views from processing the touch input within the particular regions in response to the presence of the modal view; and </claim-text><claim-text> restrict the non-modal views from preventing the gesture recognizer associated with the modal view from processing the touch input in response to the presence of the modal view. </claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The media of Claim 9, wherein the software is further configured to set a modal view flag of the particular one of the views. </claim-text></claim><claim id="clm-0015" num="15"><claim-text>15. The media of Claim 9, wherein the software is further configured to map the gesture recognizer associated with the modal view to the gesture recognizer associated with the outside view through run-time swizzling. </claim-text></claim><claim id="clm-0016" num="16"><claim-text>16. The media of Claim 9, wherein the modal view is higher in the hierarchy than one or more of the non-modal views eligible to process the touch input. </claim-text></claim><claim id="clm-0017" num="17"><claim-text>17. A device comprising : </claim-text><claim-text> a processor; and </claim-text><claim-text> one or more computer-readable non-transitory storage media coupled to the processor and embodying software that: </claim-text><claim-text> receive a touch input within a particular region of a display area of the device, the display area presenting a user interface (UI) comprising a plurality of views and an outside view, each of the views corresponding to one or more regions of the display area, one or more of the views having a gesture recognizer configured to process a touch input detected within the regions of the display area associated with each of the views, the 
<!-- EPO <DP n="20"/>-->
 outside view having a gesture recognizer configured to process the touch input detected within the display area, the views being organized in a hierarchy; </claim-text><claim-text> determine whether a particular one of the views from among the plurality of views is set as a modal view; </claim-text><claim-text> retarget the gesture recognizer associated with the particular view to the gesture recognizer associated with the outside view in response to determining the particular view is set as the modal view; </claim-text><claim-text> prevent the gesture recognizers of one or more non-modal views from processing the touch input within the particular region in response to a presence of the modal view; and </claim-text><claim-text> process the touch input within the particular region through the gesture recognizer associated with the modal view in response to the presence of the modal view. </claim-text></claim><claim id="clm-0018" num="18"><claim-text>18. The device of Claim 17, wherein the software is further configured to: </claim-text><claim-text> determine one or more of eligible views from among the plurality of views, each of the eligible views having one or more corresponding regions of the display area that encompasses the particular region containing the touch input; </claim-text><claim-text> determine a lowest eligible view in the hierarchy based at least in part on a size of one or more of the regions associated with each eligible view; and </claim-text><claim-text> process the touch input within the particular region through the gesture recognizer associated with the lowest eligible view in response to an absence of the modal view. </claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The device of Claim 17, wherein the software is further configured to: </claim-text><claim-text> prevent the gesture recognizers of one or more non-modal views from processing the touch input within the particular regions in response to the presence of the modal view; and </claim-text><claim-text> restrict the non-modal views from preventing the gesture recognizer associated with the modal view from processing the touch input in response to the presence of the modal view. 
<!-- EPO <DP n="21"/>-->
</claim-text></claim><claim id="clm-0020" num="20"><claim-text>20. The device of Claim 17, wherein the software is further configured to set a modal flag of the particular one of the views. 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
