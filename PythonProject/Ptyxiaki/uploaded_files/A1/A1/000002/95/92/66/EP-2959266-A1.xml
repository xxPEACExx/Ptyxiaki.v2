<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959266-A1" country="EP" doc-number="2959266" kind="A1" date="20151230" family-id="51390565" file-reference-id="312929" date-produced="20180825" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160452865" ucid="EP-2959266-A1"><document-id><country>EP</country><doc-number>2959266</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13875994-A" is-representative="NO"><document-id mxw-id="PAPP193868698" load-source="docdb" format="epo"><country>EP</country><doc-number>13875994</doc-number><kind>A</kind><date>20130225</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193868699" load-source="patent-office" format="original"><country>EP</country><doc-number>13875994.9</doc-number><date>20130225</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162035336" ucid="IB-2013051496-W" linkage-type="A" load-source="docdb"><document-id format="epo"><country>IB</country><doc-number>2013051496</doc-number><kind>W</kind><date>20130225</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1832231580" load-source="docdb">G08G   1/0969      20060101ALI20170327BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1832231581" load-source="docdb">G01C  21/36        20060101ALI20170327BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1832231582" load-source="docdb">G01C  21/34        20060101ALI20170327BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1832231583" load-source="docdb">G01C  21/26        20060101ALI20170327BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1832231584" load-source="docdb">G01C  21/00        20060101AFI20170327BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1761062643" load-source="docdb" scheme="CPC">G01C  21/26        20130101 LI20170929BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1761062644" load-source="docdb" scheme="CPC">G01C  21/3602      20130101 LI20170929BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1761062645" load-source="docdb" scheme="CPC">G01C  21/12        20130101 LI20170929BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984016834" load-source="docdb" scheme="CPC">H04N   7/181       20130101 LI20160114BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984018763" load-source="docdb" scheme="CPC">G01C  21/34        20130101 FI20160115BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984022674" load-source="docdb" scheme="CPC">G01S  19/13        20130101 LI20160114BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984023897" load-source="docdb" scheme="CPC">H04N   5/247       20130101 LI20160114BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984024849" load-source="docdb" scheme="CPC">H04N   5/33        20130101 LI20160114BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165550061" lang="DE" load-source="patent-office">INTELLIGENTE VIDEONAVIGATION FÜR KRAFTFAHRZEUGE</invention-title><invention-title mxw-id="PT165550062" lang="EN" load-source="patent-office">INTELLIGENT VIDEO NAVIGATION FOR AUTOMOBILES</invention-title><invention-title mxw-id="PT165550063" lang="FR" load-source="patent-office">NAVIGATION VIDÉO INTELLIGENTE POUR AUTOMOBILES</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103333211" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CONTINENTAL AUTOMOTIVE GMBH</last-name><address><country>DE</country></address></addressbook></applicant><applicant mxw-id="PPAR1103331722" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CONTINENTAL AUTOMOTIVE GMBH</last-name></addressbook></applicant><applicant mxw-id="PPAR1101642576" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Continental Automotive GmbH</last-name><iid>101081062</iid><address><street>Vahrenwalder Strasse 9</street><city>30165 Hannover</city><country>DE</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103313493" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>CHIA WEI MING DAN</last-name><address><country>SG</country></address></addressbook></inventor><inventor mxw-id="PPAR1103324206" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>CHIA, WEI MING DAN</last-name></addressbook></inventor><inventor mxw-id="PPAR1101652804" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>CHIA, WEI MING DAN</last-name><address><street>Blk 126 Bishan Street 12 03-145</street><city>Singapore 570126</city><country>SG</country></address></addressbook></inventor><inventor mxw-id="PPAR1103327027" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>LIM CHUN HOCK</last-name><address><country>SG</country></address></addressbook></inventor><inventor mxw-id="PPAR1103309156" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>LIM, Chun Hock</last-name></addressbook></inventor><inventor mxw-id="PPAR1101652034" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>LIM, Chun Hock</last-name><address><street>Blk 9 Jalan Membina 18-03</street><city>Singapore 169483</city><country>SG</country></address></addressbook></inventor><inventor mxw-id="PPAR1103314252" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>KONG BEE CHING</last-name><address><country>SG</country></address></addressbook></inventor><inventor mxw-id="PPAR1103302795" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>KONG, Bee Ching</last-name></addressbook></inventor><inventor mxw-id="PPAR1101641244" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>KONG, Bee Ching</last-name><address><street>Blk 989D Jurong West Street 93 08-659</street><city>Singapore 644989</city><country>SG</country></address></addressbook></inventor></inventors></parties><international-convention-data><pct-or-regional-filing-data ucid="IB-2013051496-W"><document-id><country>IB</country><doc-number>2013051496</doc-number><kind>W</kind><date>20130225</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014128532-A1"><document-id><country>WO</country><doc-number>2014128532</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660630168" load-source="docdb">AL</country><country mxw-id="DS660630212" load-source="docdb">AT</country><country mxw-id="DS660630170" load-source="docdb">BE</country><country mxw-id="DS660714721" load-source="docdb">BG</country><country mxw-id="DS660708008" load-source="docdb">CH</country><country mxw-id="DS660629163" load-source="docdb">CY</country><country mxw-id="DS660630213" load-source="docdb">CZ</country><country mxw-id="DS660790398" load-source="docdb">DE</country><country mxw-id="DS660630179" load-source="docdb">DK</country><country mxw-id="DS660629164" load-source="docdb">EE</country><country mxw-id="DS660733291" load-source="docdb">ES</country><country mxw-id="DS660714722" load-source="docdb">FI</country><country mxw-id="DS660714731" load-source="docdb">FR</country><country mxw-id="DS660790399" load-source="docdb">GB</country><country mxw-id="DS660630180" load-source="docdb">GR</country><country mxw-id="DS660630181" load-source="docdb">HR</country><country mxw-id="DS660629165" load-source="docdb">HU</country><country mxw-id="DS660708009" load-source="docdb">IE</country><country mxw-id="DS660630182" load-source="docdb">IS</country><country mxw-id="DS660714732" load-source="docdb">IT</country><country mxw-id="DS660629166" load-source="docdb">LI</country><country mxw-id="DS660790400" load-source="docdb">LT</country><country mxw-id="DS660630214" load-source="docdb">LU</country><country mxw-id="DS660790401" load-source="docdb">LV</country><country mxw-id="DS660790402" load-source="docdb">MC</country><country mxw-id="DS660709918" load-source="docdb">MK</country><country mxw-id="DS660709923" load-source="docdb">MT</country><country mxw-id="DS660733292" load-source="docdb">NL</country><country mxw-id="DS660709924" load-source="docdb">NO</country><country mxw-id="DS660733293" load-source="docdb">PL</country><country mxw-id="DS660714733" load-source="docdb">PT</country><country mxw-id="DS660733294" load-source="docdb">RO</country><country mxw-id="DS660714734" load-source="docdb">RS</country><country mxw-id="DS660733319" load-source="docdb">SE</country><country mxw-id="DS660708010" load-source="docdb">SI</country><country mxw-id="DS660709925" load-source="docdb">SK</country><country mxw-id="DS660709926" load-source="docdb">SM</country><country mxw-id="DS660630223" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139077769" ref-ucid="WO-2014128532-A1" lang="EN" load-source="patent-office"><p num="0000">The application provides a method of vehicle navigation guidance. The method comprises a data collection, a first position determination, and a second position determination. The data collection includes receiving at least three satellite positioning data from at least three satellites and receiving road marking data from a driver assistance device. After this, the first position determination is performed. The first position determination includes calculating a first position of a vehicle according to the satellite positioning data and to the road marking data. The second position determination is performed afterward. The second position determination includes determining a turning action of the vehicle according to the road marking data and determining an acceleration force of the vehicle according to the road marking data. The calculating of a second position of the vehicle is then done according to the first position, to the turning action, and to the acceleration force.</p></abstract><abstract mxw-id="PA139544503" ref-ucid="WO-2014128532-A1" lang="EN" source="national office" load-source="docdb"><p>The application provides a method of vehicle navigation guidance. The method comprises a data collection, a first position determination, and a second position determination. The data collection includes receiving at least three satellite positioning data from at least three satellites and receiving road marking data from a driver assistance device. After this, the first position determination is performed. The first position determination includes calculating a first position of a vehicle according to the satellite positioning data and to the road marking data. The second position determination is performed afterward. The second position determination includes determining a turning action of the vehicle according to the road marking data and determining an acceleration force of the vehicle according to the road marking data. The calculating of a second position of the vehicle is then done according to the first position, to the turning action, and to the acceleration force.</p></abstract><abstract mxw-id="PA139077770" ref-ucid="WO-2014128532-A1" lang="FR" load-source="patent-office"><p num="0000">La présente invention concerne un procédé de guidage de navigation d'un véhicule. Le procédé comprend un recueil de données, la détermination d'une première position et la détermination d'une seconde position. Le recueil de données comprend la réception d'au moins trois données de positionnement satellite à partir d'au moins trois satellites et la réception de données de marquage de route à partir d'un dispositif d'assistance à la conduite. Après cela, la détermination de la première position est effectuée. La détermination de la première position comprend le calcul d'une première position d'un véhicule en fonction des données de positionnement satellite et des données de marquage de la route. La détermination de la seconde position est effectuée par la suite. La détermination de la seconde position comprend la détermination d'une action de virage du véhicule en fonction des données de marquage de route et la détermination d'une force d'accélération du véhicule en fonction des données de marquage de route. Le calcul d'une seconde position du véhicule est ensuite réalisé en fonction de la première position, de l'action de virage et de la force d'accélération.</p></abstract><abstract mxw-id="PA139544504" ref-ucid="WO-2014128532-A1" lang="FR" source="national office" load-source="docdb"><p>La présente invention concerne un procédé de guidage de navigation d'un véhicule. Le procédé comprend un recueil de données, la détermination d'une première position et la détermination d'une seconde position. Le recueil de données comprend la réception d'au moins trois données de positionnement satellite à partir d'au moins trois satellites et la réception de données de marquage de route à partir d'un dispositif d'assistance à la conduite. Après cela, la détermination de la première position est effectuée. La détermination de la première position comprend le calcul d'une première position d'un véhicule en fonction des données de positionnement satellite et des données de marquage de la route. La détermination de la seconde position est effectuée par la suite. La détermination de la seconde position comprend la détermination d'une action de virage du véhicule en fonction des données de marquage de route et la détermination d'une force d'accélération du véhicule en fonction des données de marquage de route. Le calcul d'une seconde position du véhicule est ensuite réalisé en fonction de la première position, de l'action de virage et de la force d'accélération.</p></abstract><description mxw-id="PDES78477497" ref-ucid="WO-2014128532-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001">INTELLIGENT VIDEO NAVIGATION FOR AUTOMOBILES </p><p id="p0002" num="0002">This application relates to a navigation device for providing route guidance for an automobile. </p><p id="p0003" num="0003">Many automobiles have navigation systems for aiding its driv- ers to reach their destination. Each navigation system has a Global positioning system (GPS) receiver with a gyroscope and with an accelerometer. The GPS receiver receives signals from a few GPS satellites to provide location information of the automobile. The gyroscope detects turning actions of the auto- mobile while the accelerometer detects and provides accelera- tion force of the automobile. </p><p id="p0004" num="0004">It is an object of the application to provide an improved vehicle navigation device. </p><p id="p0005" num="0005">The application provides a method of providing navigation guidance for a vehicle. The vehicle is used for transporting human beings or goods. The navigation guidance often uses audio and visual means to provide directions to the driver for assisting the driver to reach his desired destination. </p><p id="p0006" num="0006">The method includes a first step for collecting data of the vehicle, a second step for determining a first position of the vehicle, and a third step for determining a second position of the vehicle. </p><p id="p0007" num="0007">The first step for collecting data of the vehicle includes an act of receiving at least three satellite positioning data from at least three corresponding positioning satellites. The first step also includes receiving road marking data from a driver assistance device of the vehicle. The driver assistance device is also known as an Advanced Driver Assistance System 
<!-- EPO <DP n="3"/>-->
 (ADAS) . The driver assistance device has image sensors that can include visual camera sensors and radar sensors. The road marking data can refer to road lane arrow marking data, to road lane marking data, to road boundary marking data, or to road direction turning data. </p><p id="p0008" num="0008">After the first step, the second step for determining a first position of the vehicle is performed. The second step comprises calculating a first position of a vehicle according to the received satellite positioning data and to the received road marking data. For most purposes, data from three satellites is used to determine the first position of the vehicle while data from a fourth satellite can be used to verify the position determined by the three satellites and to correct any error in the determined position. </p><p id="p0009" num="0009">The third step for determining a second position of the vehicle is then performed after the second step. This said third step includes determining a turning action of the vehicle according to the road marking data. In other words, this said third step determines whether the vehicle is turning to the left or is turning to the right. </p><p id="p0010" num="0010">This step of determining the turning action is intended for improving positional accuracy while saving or avoiding cost of a gyroscope, which can be used to determine turning actions. By avoiding the use of the gyroscope, this step of determining saves the cost of implementing gyroscope. </p><p id="p0011" num="0011">This third step also includes determining an acceleration force of the vehicle according to the road marking data. The magnitude of the acceleration force of the vehicle can be used to determine the speed of the vehicle while the direction of 
<!-- EPO <DP n="4"/>-->
the acceleration force of the vehicle can be used to determine the inclination of the vehicle. </p><p id="p0012" num="0012">The speed and the inclination of the vehicle can then be used to differentiate between stacked parallel roads. By determining the acceleration force from the road marking data, this step is unlike most other that use the accelerometer to determine the acceleration force. In other words, this step of determining avoids the use of the accelerometer and thus saves the cost of the accelerometer. </p><p id="p0013" num="0013">A second position of the vehicle is then calculated according to the first position, to the turning action, and to the acceleration force. </p><p id="p0014" num="0014">In short, this method usually may not require additional hardware as the driver assistance devices is readily available in certain vehicles. The method provides vehicle navigation guidance in a covered facility where the vehicle cannot receive satellite signals. This method is also able to differentiate or distinguish between two stacked parallel roads. Most other methods of navigation that uses only satellite signals are unable to differentiate between stacked parallel roads. This also method avoids use of gyroscope and also avoids use of accelerometer, wherein the gyroscope is often used to detect turning actions of a vehicle while the accelerometer is often used to detect and to provide acceleration force of the vehicle. Use of these devices would increase cost of the navigation guidance device. </p><p id="p0015" num="0015">The method often includes a step of determining or detecting a changing of road lane of the vehicle according to the road marking data. The road marking data is used to determine the changing of road lanes of the vehicle. Most navigation device 
<!-- EPO <DP n="5"/>-->
that uses data just from the satellite positioning data is unable to determine the changing of the vehicle road lanes because the satellite positioning data is often not accurate enough to determine the road lane on which the vehicle is positioned . </p><p id="p0016" num="0016">The method can include a step of generating a map database. </p><p id="p0017" num="0017">This step comprises receiving image data from the driver assistance device. After this, one or more data of objects that surround the vehicle are detected in the image data. Position data corresponding to the detected object are later assigned to the detected object data. The object data with the corresponding position data are afterward stored in a memory device. A collection of the object data with the respectively position data later forms the map database. </p><p id="p0018" num="0018">The detecting of the object data can comprise a step of detecting a bus stop or of detecting a building of interest, such as petrol station, even though other type of objects, such as road marking and road signage, can also be detected. </p><p id="p0019" num="0019">The method can include the step of a route video conferencing. The vehicle can be in a location for which a navigation device of the vehicle does not have map data. This step then allows the driver of the vehicle to receive route guidance from a remote route guidance provider. This step comprises transmitting an image data of the surrounding of the vehicles to a routing service location. The image data can comprise a photograph or video information. The image data is later received by the routing service location. A routing guidance according the received image data is then transmitted from the routing service location back to the vehicle. 
<!-- EPO <DP n="6"/>-->
The application provides a vehicle navigation device for providing route guidance . </p><p id="p0020" num="0020">The vehicle navigation device comprises a satellite positioning data receiver, an image data receiving port for connecting to a vehicle driver assistance device, a navigation display port, and a navigation audio port. The driver assistance device is also known as an Advanced Driver Assistance System. The vehicle navigation device also comprises a processor that is connected communicatively to the satellite positioning data receiver, to the driver assistance device image data receiver port, to the display port, and to the audio port. </p><p id="p0021" num="0021">The vehicle navigation device provides a data collection mode, a first position determination mode, and a second position determination mode. </p><p id="p0022" num="0022">In the data collection mode, the satellite positioning data receiver receives at least three satellite positioning data from at least three corresponding satellites while the image data receiving port obtains road marking data from the driver assistance device. </p><p id="p0023" num="0023">In the first position determination mode, the processor calculates a first position of a vehicle according to the received satellite positioning data and the received road marking data. </p><p id="p0024" num="0024">In the second position determination mode, the processor determines at least one turning action of the vehicle according to the road marking data, and the processor determining at least one acceleration force of the vehicle according to the road marking data. The processor then calculates a second position of the vehicle according to the calculated first posi- 
<!-- EPO <DP n="7"/>-->
tion, to the determined turning action, and to the determined acceleration force. </p><p id="p0025" num="0025">The second position determination mode often comprises the step of the processor determining a tilt of the vehicle according to a direction of the acceleration force. </p><p id="p0026" num="0026">The vehicle navigation device can provide a map generation mode. In the map generation mode, the processor receives image data from the driver assistance device image data receiving port. The processor then detects at least one data of an object in the image data, wherein the object is positioned in the surrounding of the vehicle. After this, the processor assigns position data to the respective detected object data. The processor later stores the object data with the position data . </p><p id="p0027" num="0027">The application also provides a vehicle. The vehicle comprises a driver assistance device and the above navigation device being connected to the driver assistance device. The driver assistance device comprises at least two vision image sensors for providing image data. </p><p id="p0028" num="0028">Different implementations of the vision image sensors are possible. In one implementation, the vision image sensors comprise an infrared sensor. In another implementation, the vision image sensors comprise a camera sensor. </p><p id="p0029" num="0029">The driver assistance device often includes at least two radar image sensors for providing image data. One radar image sensor can be provided at a left side of the vehicle while the other radar image sensor can be provided at a right side of the vehicle . 
<!-- EPO <DP n="8"/>-->
The image data can comprise road marking data, such as road lane arrow marking data, road lane marking data, road boundary marking data, or road direction turning data. </p><p id="p0030" num="0030">The image data often include data of objects that are surrounding the vehicle. </p><p id="p0031" num="0031">In summary, the application provides a navigation device for an automobile or vehicle. The navigation device includes a Global Positioning System (GPS) receiver, vision sensors and radar sensors, and a memory device. The vision sensors and the radar sensors can be a part of a component of the automobile, known as an Advance Driver Assistance System (ADAS) . The memory device has map data as well as other information. </p><p id="p0032" num="0032">In use, the navigation device is installed in the automobile. The automobile travels on roads to transport goods and people. </p><p id="p0033" num="0033">The navigation device uses information from the ADAS and information from the GPS receiver to aid navigation of the automobile . </p><p id="p0034" num="0034">The ADAS provides digital images of the surrounding of the automobile. The digital images comprise digital visual images and digital radio wave images. The vision sensors provides the digital visual images, which relate to objects placed in front of the vision sensors. Similarly, the radar sensors provides the digital radio wave images, which relate to objects placed in front of the radar sensors . </p><p id="p0035" num="0035">The GPS receiver receives signals from a few GPS satellites for providing location information of the automobile. 
<!-- EPO <DP n="9"/>-->
In one aspect of the application, both the GPS receiver location data and the ADAS image data are used to determine the location of the automobile. The GPS receiver location data is used to determine area that the automobile is located. The ADAS image data is used to determine which part of the area that the vehicle is positioned. This is unlike most other navigation units, which provide location data with this resolution . In one example, the GPS receiver location data is used to determine which part of a road that the automobile is located. The ADAS image data is used to determine which lane of the said road on which the vehicle is positioned. The ADAS image data can be used to detect changes of the road lanes. </p><p id="p0036" num="0036">The navigation device also provides a Personalized Map Data (PMD) to complement the map data. As the vehicle travels, the vision sensors and the radar sensors generate image data of the surrounding. The image data is then placed in an image da- tabase of the memory device. It is later extracted from the recorded image data to generate the PMD. </p><p id="p0037" num="0037">As an example, features of the surrounding, which is extracted from the recorded image data, includes speed limit signage da- ta, traffic light location data, speed camera location data as well as other road signage data. </p><p id="p0038" num="0038">When a map supplier provides a new map data, the PMD can also be used to complement the new map data. </p><p id="p0039" num="0039">In another aspect of the application, the navigation device is used to differentiate between two stacked parallel expressways. The stacked parallel expressways appear as one single road in a 2D (two-dimensional) map and the location information from 
<!-- EPO <DP n="10"/>-->
the GPS receiver often does not differentiate between the par allel expressways. </p><p id="p0040" num="0040">Before the vehicle reaches the parallel expressways, the navi gation device identifies the road and the road lane on which the vehicle is travelling. The navigation device also identifies changes of the road lanes. When the vehicle reaches the parallel expressways, the navigation device is then able to determine which of the parallel expressway that the vehicle i travelling using these identified data. </p><p id="p0041" num="0041">This differentiation is unlike other differentiations, which uses an accelerometer . The use of the accelerometer would require an additional piece of hardware that increases cost of the navigation device. In contrast, the ADAS is readily avail able in many vehicles and would increase cost of the navigation device. </p><p id="p0042" num="0042">The navigation device can also provide information about the parked location of the vehicle in a covered parking facility. The navigation device determines the location of vehicle usin the GPS satellite signals before the vehicle enters the parking facility. The parking facility may be covered such that the navigation device does not receive the GPS satellite signals inside the parking facility. </p><p id="p0043" num="0043">The vehicle then enters the parking facility and travels to its parking lot while the vision sensors detect the turning movements of the vehicle in the parking facility. The navigation device also receives speed information of the vehicle in the parking facility. Using the detected turning movements, the previously known location of the vehicle outside the park ing facility, as well as the speed of the vehicle inside the parking facility, the navigation device then determines its 
<!-- EPO <DP n="11"/>-->
parking location. The navigation device later sends the parking location information to the driver via a communication means, such as Bluetooth. </p><p id="p0044" num="0044">The navigation device can also provide digital pictures of its parking lot for easy reference. The vision sensors take a picture of the vehicle parking lot as well as pictures of locations surrounding the parking lot. The navigation device may turn on the automobile headlight if the ambient light is low to brighten the areas around the vehicle for taking the pictures. These pictures are afterward sent to a mobile phone of a driver of the vehicle via Bluetooth or via other communication means. Afterwards, the driver is able to locate the vehicle easily using the above sent information without needing to remember the parking location. This is especially useful when the parking facility has a large number of parking lots. </p><p id="p0045" num="0045">The navigation device provides navigation teleconferencing using a video feed. The vision sensors provide a stream of images of the front of the vehicle. These images are then sent to another destination to seek advice or guidance regarding navigation to a desired location. The map data may have information regarding its present location because the present location is new. Because of this, the map data may be used to provide guidance to the desired location. </p><p id="p0046" num="0046">Fig. 1 illustrates functional blocks of a navigation device, </p><p id="p0047" num="0047"> Fig. 2 illustrates an automobile that is installed with the navigation device of Fig. 1, </p><p id="p0048" num="0048"> Fig. 3 illustrates a flow chart of the navigation device of Fig. 1 for determining road lanes of the automobile of Fig. 2, 
<!-- EPO <DP n="12"/>-->
Fig. 4 illustrates a flow chart of the navigation device of Fig. 1 for generating a Personalized Map Database (PMD) , </p><p id="p0049" num="0049"> Fig. 5 illustrates a flow chart of the navigation device of Fig. 1 for differentiating between two stacked parallel roads while avoiding use of the accelerometer, and </p><p id="p0050" num="0050"> Fig. 6 illustrates a flow chart of the navigation device of Fig. 1 for determining a location of the automobile in a covered facility and for locating the automobile 40 in a parking area. </p><p id="p0051" num="0051">In the following description, details are provided to describe embodiments of the application. It shall be apparent one skilled in the art, however, that the embodiments may b' practiced without such details. </p><p id="p0052" num="0052">Some parts of the embodiments have similar parts. The similar parts may have the same names or the similar part numbers. The description of one similar part also applies by reference to another similar parts, where appropriate, thereby reducing repetition of text without limiting the disclosure. </p><p id="p0053" num="0053">Fig. 1 shows functional blocks of a navigation device 10. The navigation device 10 includes an ECU (Engine Control Unit) unit 12 of an ADAS (Advance Driver Assistance System) . The ADAS ECU unit 12 is connected communicatively to a multimedia navigation unit 16 by a MOST (Media Oriented Systems </p><p id="p0054" num="0054">Transport) data bus 18 and by a CAN (Controller Area Network) data bus 21. </p><p id="p0055" num="0055">The ADAS ECU unit 12 includes an ADAS module 23, a plurality of camera modules 25, and a plurality of radar modules 27, wherein the camera modules 25 and the radar modules 27 are 
<!-- EPO <DP n="13"/>-->
connected to the ADAS module 23. The camera modules 25 are also called vision sensors while the radar modules 27 are also called radar sensors. </p><p id="p0056" num="0056">The navigation unit 16 includes a multi-media module 30. The multi-media module 30 has an integrated display 32, a Bluetooth (BT) communication port 34, and a GPS (Global positioning system) receiver 38. The BT communication port 34 is connected to an external display module 36, which is communicatively connected to a BT communication port of a mobile phone 37. </p><p id="p0057" num="0057">The navigation unit 16 also includes a processor 72 and a memory device 74, wherein the processor 72 is connected to the memory device 74 and to other modules. The memory device 74 stores a database of map data. </p><p id="p0058" num="0058">In use, the navigation device 10 is installed in an automobile or vehicle 40, which is illustrated in Fig. 2. The vehicle 40 travels on roads or expressways for transporting human beings or goods. </p><p id="p0059" num="0059">The MOST data bus 18 and the CAN data bus 21 allow transfer of data between the ADAS ECU unit 12 and the multimedia navigation unit 16. The data is also referred to here as information. </p><p id="p0060" num="0060">The vision sensors 25 are distributed around the vehicle 40 such that the vision sensors 25 are directed to the front, the rear, the left rear side, and the right rear side of the vehicle 40, as shown in Fig. 2. The vision sensors 25 take digital visual images or pictures of objects that placed in front of the vision sensors 25. The vision sensors 25 also transmit data of the digital visual images to the ADAS module 23. 
<!-- EPO <DP n="14"/>-->
In particular, the vision sensors 25 are directed to take digital visual images of the road and road lanes on which the vehicle 40 is placed. The vision sensors 25 are also directed to take digital visual images of features of the road surrounding the vehicle 40, such as traffic lights and road signage. </p><p id="p0061" num="0061">Similarly, the radar sensors 27 are distributed around the vehicle 40, wherein the radar sensors 27 are directed to the front, the front left side, the front right side, the rear, the rear left side, and the rear right side of the vehicle 40, as illustrated in the Fig. 2. The radar sensors 27 generate radio waves and receive the radio waves reflected by objects that are placed in front of the radar sensors 27. The reflected radio waves are used to form radio wave images of the objects. The radar sensors 27 also transmit data of the radio wave images to the ADAS module 23. The vision sensors 25 also transmit data of the digital visual images to the ADAS module 23. </p><p id="p0062" num="0062">Specifically, the radar sensors 27 are directed to take digital radio waves images of the road on which the vehicle 40 is placed. The radar sensors 27 are also directed to take digital radio waves images of features of the road surrounding the vehicle 40. </p><p id="p0063" num="0063">In a general sense, the vision sensors 25 include infrared sensors and camera sensors. </p><p id="p0064" num="0064">The GPS receiver 38 receives signals from GPS satellites when an unobstructed line of sight exists between the GPS satellites and the GPS receiver 38. The GPS receiver 38 also transmits location data of the navigation device 10, wherein the location data are derived from these satellite signals. 
<!-- EPO <DP n="15"/>-->
The processor 72 uses the received satellite signals to determine the location of the vehicle 40, which can be located on a part of a certain road. The processor 72 also uses image data from the ADAS module 23 to detect which lane of the road that the vehicle 40 is presently located. </p><p id="p0065" num="0065">In one implementation, the GPS receiver provides location data with an accuracy of a few hundred meters. Coupling this location data with the image data from the ADAS module 23, the processor 72 can determine the position of the vehicle 40 with an accuracy of a few meters. </p><p id="p0066" num="0066">The map data of a region includes geographical positional data of locations of the region and routing information among the locations . </p><p id="p0067" num="0067">The processor 72 uses the GPS receiver location data and the ADAS image data to determine present location of the vehicle 40. The processor 72 also determines a route to a destination from the present location. The processor 72 provides audio and visual route guidance to a user of the vehicle 40 for reaching the destination. The guidance includes instructions to move the vehicle 40 to the left or the right in order to position the vehicle 40 for entering another road. </p><p id="p0068" num="0068">The processor 72 also uses data from the ADAS 23 to determine the turning motions of the vehicle 40. The processor 72 can determine whether the vehicle 40 is turning to the left or to the right. </p><p id="p0069" num="0069">The processor 72 also uses data from the ADAS 23 to determine objects around the vehicle 40, such as speed limit signage, traffic light, speed camera, and other road signage. 
<!-- EPO <DP n="16"/>-->
The processor 72 can also use information received from the radar sensors 27 to increase reliability of the information provided by the vision sensors 25. This is because the radio wave images are mostly unaffected by bad weather conditions while the visual images can be affected by bad weather conditions. In effect, this allows the information provided by the vision sensors 25 to be resilience or accurate even during changing weather conditions . </p><p id="p0070" num="0070">The processor 72 can also use image data from the ADAS module</p><p id="p0071" num="0071">23 to provide vehicle features such as Adaptive Cruise Control (ACC) , Blind Sport Detection (BSD) , Rear Cross Traffic Alert (RCTA) , Emergency Brake Assist (EBA) , Intelligent Headlamp</p><p id="p0072" num="0072">Control (IHC) , Lane Departure Warning (LDW) , and Traffic Sign</p><p id="p0073" num="0073">Recognition (TSR) . </p><p id="p0074" num="0074">The Emergency Brake Assist (EBA) refers to an automobile braking technology that increases that increases braking pressure in an emergency. </p><p id="p0075" num="0075">The Blind Sport Detection (BSD) provides warning to the driver when there are vehicles in the blind spot of the side-view mirror of the vehicle. </p><p id="p0076" num="0076">The Adaptive Cruise Control (ACC) uses radar sensors for monitoring the vehicle in front and for adjusting the speed of the vehicle to keep it at a preset distance behind the vehicle ahead, even in most fog and rain conditions. When the traffic begins to move again, the Adaptive Cruise Control (ACC) accelerates the vehicle up to the preferred cruising speed if possible. In other words, the Adaptive Cruise Control (ACC) controls the stop and go of the vehicle. 
<!-- EPO <DP n="17"/>-->
The Lane Departure Warning (LDW) provides warning a driver when the vehicle begins to move out of its lane, unless a turn signal is switched on in that the direction of the vehicle movement . </p><p id="p0077" num="0077">The Traffic Sign Recognition (TSR) allows a vehicle to recognize the traffic signs on the road, such as speed limit, children, or turn ahead traffic signs. The Traffic Sign Recognition (TSR) identifies traffic signs and then displays them on the instrument panel of the vehicle. The Traffic Sign Recognition (TSR) may flash the display when the vehicle is over the speed limit for helping the driver to legally and safely. </p><p id="p0078" num="0078">The Intelligent Headlamp Control (IHC) monitors oncoming vehicles for controlling the headlamp of the vehicle such that the headlamps provide lighting while its high beams do not blind other road users. </p><p id="p0079" num="0079">In one implementation, the vision sensors 25 include infrared sensors that are used for providing Emergency Brake Assist (EBA) . The vision sensors 25 also include camera sensors that are used for providing Lane Departure Warning (LDW) , Traffic Sign Recognition (TSR) , and Intelligent Headlamp Control (IHC) . </p><p id="p0080" num="0080">The radar sensors 27 include mid-range to long-range radar sensors with operating frequency of 77 GHz that are used for providing Adaptive Cruise Control (ACC) and for providing Emergency Brake Assist (EBA) . 
<!-- EPO <DP n="18"/>-->
The radar sensors 27 include short-range radar sensors with operating frequency of 24 GHz that are used for providing </p><p id="p0081" num="0081">Blind Sport Detection (BSD) . The integrated display 32 acts to display navigation guidance information for a user of the navigation device 10. </p><p id="p0082" num="0082">The BT communication port 34 transfers information between the multi-media module 30 and the mobile phone 37 through the dis- play module 36. </p><p id="p0083" num="0083">Fig. 3 shows a flow chart 80 of the navigation device 10 for determining road lanes of the automobile 40. The road lanes are determined using the image data of the ADAS module 23. </p><p id="p0084" num="0084">The flow chart 80 shows a step 82 of a driver driving the vehicle 40 on a road or expressway. The flow chart 80 also shows a step 83 of the navigation device 10 using the map database to provide route guidance instructions to the driver. The route guidance instructs the driver to move the vehicle 40 to the left or to the right. This moving is intended to position the vehicle 40 for turning to a slip road or for turning at a road junction, in order to enter another road. The steps 82 and 83 are then followed by a step 84 of the driver moving the vehicle 40 to the left or to the right, in accordance with the provided route guidance. </p><p id="p0085" num="0085">The navigation device 10 then comprehends or checks which road lane that the vehicle is positioned currently , in a step 87. This checking is done by determining the currently positioned road lane based on the image data from the ADAS module 23. The determination allows the navigation device 10 to know actually, 
<!-- EPO <DP n="19"/>-->
where the vehicle 40 is positioned . It also knows whether the vehicle 40 has moved as instructed by the navigation device 10. </p><p id="p0086" num="0086">The checking ceases when data from the GPS receiver 38 indi- cate that the vehicle 40 has reached the said slip road or junction, as shown in a step 92. </p><p id="p0087" num="0087">When the vehicle 40 is travelling on another road, the navigation device 10 uses data from map database in a step 96 and continues to provide route guidance to a further road, as shown in a step 94. </p><p id="p0088" num="0088">In short, the flow chart 80 provides a closed loop guidance in that the step 87 provides the checking which road lane that the automobile 40 is actually placed. In effect, the navigation device 10 knows whether the vehicle 40 is positioned correctly. This is especially important where the road has many turns and junctions that the driver has to navigate. By detecting the current road lane and then providing the corre- sponding advice for navigating, potential accidents can be avoided. This method of navigation also reduces driving stress while allowing the driver to relax. </p><p id="p0089" num="0089">Such guidance is different from guidance provided by other navigation devices that is an open-loop system. </p><p id="p0090" num="0090">Fig. 4 shows illustrates a flow chart 100 of the navigation device 10 for generating a Personalized Map Database (PMD) for improving map guidance. The PMD is generated using the map da- ta and the ADAS image data. The flow chart 100 acts to populate the PMD with live data. </p><p id="p0091" num="0091">Initially, the navigation device 10 receives the map data in a step 101. The navigation device 10 then starts road detection 
<!-- EPO <DP n="20"/>-->
and road data collection in a step 102 if the location data of the vehicle 40, which is derived from the ADAS image data and the GPS receiver location data, is not present in the map data. The road detection continues in a step 104, as long as the said the present vehicle location data is not present in the map data. The processor 72 then records the ADAS image data and the GPS receiver location data. The processor 72 later places the recorded data in the PMD, in a step 109 </p><p id="p0092" num="0092">The processor 72 afterward correlates or compares the PMD data with the map data in a step 111 to determine similarities or differences. The similarities affirm which part of the map data is correct while the differences indicate that which of the map data needs to be corrected and replaced with the PMD data. The identified differences are then recorded for use together with the map data to improve route navigation device making the route navigation more intelligent. The comparison may show changes of the road which are not shown in the map data. The comparison may also show new information, such as new road signage, which are not present in the map data and which can be used to complement and improve the map data. This compared information are then stored in the PMD for later use. The navigation system 10 then provides guidance according to the map data with the PMD data. </p><p id="p0093" num="0093">The guidance includes generation of a route to a destination, in a step 115. This also includes the use a routing algorithm to generate the route, in a step 117. Following the step 117, a step of providing audio and visual guidance is then performed, in a step 119. 
<!-- EPO <DP n="21"/>-->
As an example, when the automobile 40 travels on a new road that is not present in the map database, the processor 72 detects this and records the detected vision sensor information and the detected radar information together with the GPS re- ceiver location data in the PMD. The navigation device 10 later uses information from the map data together with the recorded PMD data, and not just the original map data, for guidance . This method provides a self-learning system and also replaces the need for map updates, which the map vendor may be provide infrequently . </p><p id="p0094" num="0094">The PMD is kept as a separate database such that it can use with the new map data, whenever the new map data is provided from the vendor. </p><p id="p0095" num="0095">Fig. 5 shows a flow chart 130 of the navigation device 10 for differentiating between two stacked parallel roads. The dif- ferentiating does not require use an accelerometer . </p><p id="p0096" num="0096">The flow chart 130 includes a step 132 of identifying the present location of the automobile 40 using the GPS receiver data. The processor 72 also uses the ADAS image data to recognize the road lane where the automobile is located, in a step 135. The step 135 also includes adding a time stamp or time record to the location data of the automobile 40 for keeping a time location record of the automobile 40. After this, the ADAS image data is searched for a splitting of the road to the two different stacked parallel roads, in a step 136. If a road split is detected in the ADAS image data, the existence of this road split in the map data is checked, 
<!-- EPO <DP n="22"/>-->
in the step 139. If this road split is not shown in the map data, this information is added to the PMD. </p><p id="p0097" num="0097">By noting the route taken after the road split, the parallel road taken by the vehicle 40 can be determined. The parallel road can also be determined based on its road feature, which can be identified using the ADAS image data. </p><p id="p0098" num="0098">Following this, the road for travelling after the road split is determined, in a step 141. A step 143 of continuing to identify current location of the vehicle 40 is then performed. This step 143 is performed until the automobile 40 stops moving. The step 143 continues to performed when the automobile 40 starts moving again. </p><p id="p0099" num="0099">This way of navigating uses data from the GPS receiver 38 together with the image data from the ADAS module 23 to detect roads and lanes of the roads travelled by the automobile 40. The navigation device 10 uses the ADAS image data to identify the start of stacked parallel roads. The navigation device 10 also uses the ADAS image data to differentiate between stacked parallel roads. The differentiation is possible because the different stacked parallel roads have different visual features. In this manner, the navigation device 10 provides clear route guidance through the stacked parallel roads. </p><p id="p0100" num="0100">The navigation avoids use of the accelerometer for providing gravitational level information for differentially between stacked roads parallel by the automobile 40. </p><p id="p0101" num="0101">Fig. 6 shows a flow chart 150 of the navigation device 10 for determining a location of the automobile 40 in a covered facility and for locating the parking lot of the automobile 40. 
<!-- EPO <DP n="23"/>-->
The steps for determining a parking location of the automobile 40 in the covered facility are described below </p><p id="p0102" num="0102">The processor 72 detects the road on which the vehicle 40 is placed using the ADAS image data, in a step 153. After this, the processor 72 checks whether the automobile 40 is performing parking, in a step 157. The processor 72 also checks whether, accordingly to the map data, the automobile 40 is entering a covered parking facility, in a step 159. </p><p id="p0103" num="0103">The processor 72 then checks whether the ambient light is low and whether the signals received by the GPS receiver are weak, in a step 161. In a covered parking facility, the ambient light may be low and the GPS receiver signals may be weak. If the ambient light is low, the processor 72 switches on the headlights of the automobile 41 to facilitate the taking of the ADAS image data, in a step 164. </p><p id="p0104" num="0104">The processor 72 later uses the ADAS image data in the parking facility for determining the turning motions of the automobile 40, in a step 167. </p><p id="p0105" num="0105">The processor 72 may also uses data from a gyroscope to determine the turning motions, although this is not necessary here, as the image data from the ADAS module 23 can perform this. </p><p id="p0106" num="0106">The location of the automobile can then be calculated based on the last known location of the automobile, which is outside the covered facility, based on the recorded turning motions of the automobile in the covered facility, and based on the speed of the automobile in the covered facility. </p><p id="p0107" num="0107">The steps for locating the parking lot of the vehicle 40 are described below. 
<!-- EPO <DP n="24"/>-->
The processor 72 later determines whether the automobile 40 is entering a parking lot using image data from the ADAS module 23, in a step 170. The processor 72 then takes a picture of the parking lot, using the ADAS module 23, before the automobile stops moving, in a step 173. The picture is derived from the image data of the ADAS module 23. The processor 72 later stores the photograph of the parking lot in a parking location database, in a step 176. </p><p id="p0108" num="0108">Following this, the processor 72 sends the parking lot photograph to the mobile phone 37 of the driver via a communication means, such as Bluetooth or USB technology, in a step 180. The mobile phone 37 is illustrated in Fig. 1. The driver can afterward locate easily the automobile 40 in the parking area by using the parking lot photograph. </p><p id="p0109" num="0109">A method of the navigation device 10 to provide navigation using teleconferencing is described below. The teleconferencing uses images of the ADAS module 23. </p><p id="p0110" num="0110">This method is intended for a driver at a particular location who wants to reach a certain destination while the map data of the navigation device 10 does not have data of the present location and/or data of the desired destination. </p><p id="p0111" num="0111">The navigation device 10 sends a digital picture of the current location to a staff at a routing service location. The navigation device 10 also informs the service staff of its destination. The service staff then provides route guidance to the driver according the received information. The communication between the navigation device 10 and the service staff can be done using mobile phone technology. 
<!-- EPO <DP n="25"/>-->
This method allows the driver to reach the destination without distracting the driver. </p><p id="p0112" num="0112">This method is especially useful where no specific Points of Interest (POI) or landmarks exist around the destination. </p><p id="p0113" num="0113">In another embodiment, both voice and video data are streamed together using data packet technology via a communication channel from the navigation device 10 to a receiver. This allows the receiver to receive live images as well while also allowing the driver to receive voice guidance. </p><p id="p0114" num="0114">In summary, these embodiments provide a navigation product with a GPS receiver, vision sensors and radar sensors for navigation of automobiles. The vision sensors and the radar sensors can be a part of an ADAS, which exists in many automobiles. Present ADAS often merely provides multiple safety functions for the automobile uses vision and radar methodology. </p><p id="p0115" num="0115">This navigation product provides location data with accuracy better than location data, which is provided by the GPS re- ceiver . </p><p id="p0116" num="0116">The vision sensors enable differentiation between parallel highways without use of the accelerometer . </p><p id="p0117" num="0117">The vision sensors also recognize accurately the roads taken by the automobile for matching with the map data of the navigation product. This matching is then used to improve profiles of the roads for improving route navigation. The matching can also removes the need for map data updates. 
<!-- EPO <DP n="26"/>-->
The vision sensor also provides navigation video conferencing for showing the automobile current location to another loca- tion for seeking guidance to a desired location. </p><p id="p0118" num="0118">Although the above description contains much specificity, this should not be construed as limiting the scope of the embodiments but merely providing illustration of the foreseeable embodiments. The above stated advantages of the embodiments should not be construed especially as limiting the scope of the embodiments but merely to explain possible achievements if the described embodiments are put into practice. Thus, the scope of the embodiments should be determined by the claims and their equivalents, rather than by the examples given. 
<!-- EPO <DP n="27"/>-->
</p><p id="p0119" num="0119">Reference numbers </p><p id="p0120" num="0120"> 10 navigation device </p><p id="p0121" num="0121"> 12 ECU unit </p><p id="p0122" num="0122"> 16 multimedia navigation unit</p><p id="p0123" num="0123">18 MOST data bus </p><p id="p0124" num="0124"> 21 CAN data bus </p><p id="p0125" num="0125"> 23 ADAS module </p><p id="p0126" num="0126"> 25 vision sensor </p><p id="p0127" num="0127"> 27 radar sensor </p><p id="p0128" num="0128"> 30 multi-media module </p><p id="p0129" num="0129"> 32 integrated display </p><p id="p0130" num="0130"> 34 Bluetooth communication port</p><p id="p0131" num="0131">36 external display module</p><p id="p0132" num="0132">28 GPS receiver </p><p id="p0133" num="0133"> 70 vehicle </p><p id="p0134" num="0134"> 72 processor </p><p id="p0135" num="0135"> 74 memory device </p><p id="p0136" num="0136"> 80 flow chart </p><p id="p0137" num="0137"> 82 step </p><p id="p0138" num="0138"> 83 step </p><p id="p0139" num="0139"> 84 step </p><p id="p0140" num="0140"> 87 step </p><p id="p0141" num="0141"> 89 step </p><p id="p0142" num="0142"> 92 step </p><p id="p0143" num="0143"> 94 step </p><p id="p0144" num="0144"> 96 step </p><p id="p0145" num="0145"> 100 flow chart </p><p id="p0146" num="0146"> 101 step </p><p id="p0147" num="0147"> 102 step </p><p id="p0148" num="0148"> 104 step </p><p id="p0149" num="0149"> 107 step </p><p id="p0150" num="0150"> 109 step </p><p id="p0151" num="0151"> 111 step </p><p id="p0152" num="0152"> 115 step 
<!-- EPO <DP n="28"/>-->
117 step</p><p id="p0153" num="0153">119 step</p><p id="p0154" num="0154">130 flow chart</p><p id="p0155" num="0155">132 step</p><p id="p0156" num="0156">135 step</p><p id="p0157" num="0157">136 step 139 step 141 step 143 step</p><p id="p0158" num="0158">150 flow chart</p><p id="p0159" num="0159">153 step</p><p id="p0160" num="0160">157 step</p><p id="p0161" num="0161">159 step</p><p id="p0162" num="0162">161 step</p><p id="p0163" num="0163">164 step</p><p id="p0164" num="0164">167 step</p><p id="p0165" num="0165">170 step</p><p id="p0166" num="0166">173 step</p><p id="p0167" num="0167">176 step</p><p id="p0168" num="0168">180 step 
</p></description><claims mxw-id="PCLM70077772" ref-ucid="WO-2014128532-A1" lang="EN" load-source="patent-office"><claim id="clm-0001" num="0001"><!-- EPO <DP n="29"/>--><claim-text/><claim-text>Claims </claim-text><claim-text>A method of vehicle navigation guidance, the method comprising </claim-text><claim-text> receiving at least three satellite positioning data, receiving road marking data from a driver assistance device, </claim-text><claim-text> calculating a first position of a vehicle according to the at least three satellite positioning data and to the road marking data, </claim-text><claim-text> determining a turning action of the vehicle according to the road marking data, </claim-text><claim-text> determining an acceleration force of the vehicle according to the road marking data, and </claim-text><claim-text> calculating a second position of the vehicle according to the first position, to the turning action, and to the acceleration force. </claim-text><claim-text>2. The method according to claim 1 further comprising </claim-text><claim-text> determining a changing of lane of the vehicle according to the road marking data. </claim-text><claim-text>3. The method according to claim 1 or 2, further comprising generating a map database that comprises </claim-text><claim-text> receiving image data from the driver assistance device, </claim-text><claim-text> detecting at least one object data surrounding the vehicle in the image data, </claim-text><claim-text> assigning a position data to the object data, and storing the object data with the position data. </claim-text><claim-text>The method according to claim 3, wherein </claim-text><claim-text> the detecting of the object data comprises </claim-text><claim-text> detecting a bus stop. 
<!-- EPO <DP n="30"/>-->
 The method according to claim 3 or 4, wherein the detecting of the obj ect data comprises </claim-text><claim-text> detecting a building of interest . </claim-text><claim-text>The method according one of claims 1 to 5 further comprising </claim-text><claim-text> a video conferencing that comprises </claim-text><claim-text> transmitting an image data of the surrounding of the vehicle , </claim-text><claim-text> receiving the image data by a routing service location, and </claim-text><claim-text> transmitting a routing guidance according the image data from the routing service location to the vehicle . </claim-text><claim-text>7. A vehicle navigation device for providing route guidance, the vehicle navigation device comprising </claim-text><claim-text> a satellite positioning data receiver, </claim-text><claim-text> a driver assistance device image data receiving port, a navigation display port, </claim-text><claim-text> a navigation audio port, and </claim-text><claim-text> a processor communicatively connecting to the satellite positioning receiver, to the driver assistance device image data receiver port, to the display port, and to the audio port, wherein </claim-text><claim-text> the vehicle navigation device provides </claim-text><claim-text> a data collection mode, </claim-text><claim-text> a first position determination mode, and </claim-text><claim-text> a second position determination mode, </claim-text><claim-text> in the data collection mode, </claim-text><claim-text> the satellite positioning data receiver receives at least three satellite positioning data, and 
<!-- EPO <DP n="31"/>-->
 the driver assistance device image data receiving port obtains road marking data, </claim-text><claim-text>in the first position determination mode, </claim-text><claim-text> the processor calculates a first position of a vehi- cle according to the at least three satellite positioning data and the road marking data, and </claim-text><claim-text>in the second position determination mode, </claim-text><claim-text> the processor determines at least one turning action of the vehicle according to the road marking data, - the processor determines at least one acceleration force of the vehicle according to the road marking data, and </claim-text><claim-text> the processor calculates a second position of the vehicle according to the first position, to the turning action, and to the acceleration force. </claim-text><claim-text>The vehicle navigation device according to claim 7, wherein </claim-text><claim-text>the second position determination mode further comprises the processor determining a tilt of the vehicle according to a direction of the acceleration force. </claim-text><claim-text>The vehicle navigation device according to claim 7 or 8, wherein </claim-text><claim-text>the vehicle navigation device further provides a map gen eration mode, </claim-text><claim-text>in the map generation mode, </claim-text><claim-text> the processor receives image data from the driver assistance device image data receiving port, the processor detects at least one object data in the image data, </claim-text><claim-text> the processor assigns a position data to the object data, and 
<!-- EPO <DP n="32"/>-->
 the processor stores the object data with the position data. </claim-text><claim-text>10. A vehicle comprising </claim-text><claim-text> a driver assistance device that comprises at least two vision image sensors for providing image data and a navigation device according to one of claims 7 to</claim-text><claim-text>9 being connected to the driver assistance device. 11. The vehicle according to claim 10, wherein </claim-text><claim-text> the vision image sensors comprises an infrared sensor. </claim-text><claim-text>12. The vehicle according to claim 10 or 11, wherein </claim-text><claim-text> the vision image sensors comprises a camera sensor. </claim-text><claim-text>13. The vehicle according to one of claims 10 to 12, wherein the driver assistance device further comprises at least two radar image sensors for providing image data. 14. The vehicle according to one of claims 10 to 13, wherein the image data comprises road marking data. </claim-text><claim-text>15. The vehicle according to one of claims 10 to 14, wherein the image data comprises data of objects surrounding the vehicle. 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
