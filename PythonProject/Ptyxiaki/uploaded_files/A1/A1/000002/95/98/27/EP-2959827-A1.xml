<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959827-A1" country="EP" doc-number="2959827" kind="A1" date="20151230" family-id="53284006" file-reference-id="290962" date-produced="20180825" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160452851" ucid="EP-2959827-A1"><document-id><country>EP</country><doc-number>2959827</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-15168937-A" is-representative="YES"><document-id mxw-id="PAPP193868670" load-source="patent-office" format="original"><country>EP</country><doc-number>15168937.9</doc-number><date>20150522</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193868671" load-source="docdb" format="epo"><country>EP</country><doc-number>15168937</doc-number><kind>A</kind><date>20150522</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162036073" ucid="JP-2014130291-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2014130291</doc-number><kind>A</kind><date>20140625</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988520864" load-source="docdb">A61B   5/11        20060101ALI20151119BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988521814" load-source="docdb">A61B   5/00        20060101AFI20151119BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988521937" load-source="docdb">A61B   5/0245      20060101ALI20151119BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988525809" load-source="docdb">A61B   7/00        20060101ALI20151119BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988526476" load-source="docdb">A61B   5/08        20060101ALI20151119BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1740201995" load-source="docdb" scheme="CPC">A61B   5/0816      20130101 LI20171101BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1946269281" load-source="docdb" scheme="CPC">A61B   5/4818      20130101 LI20160427BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1965254682" load-source="docdb" scheme="CPC">A61B   7/003       20130101 LI20151110BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697652" load-source="docdb" scheme="CPC">A61B   5/113       20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699193" load-source="docdb" scheme="CPC">A61B   5/024       20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699464" load-source="docdb" scheme="CPC">A61B   5/4806      20130101 FI20160106BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699515" load-source="docdb" scheme="CPC">A61B   5/6823      20130101 LI20160106BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984702441" load-source="docdb" scheme="CPC">A61B   5/0402      20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984704353" load-source="docdb" scheme="CPC">A61B   5/0205      20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984704995" load-source="docdb" scheme="CPC">A61B   5/02055     20130101 LI20151231BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987768290" load-source="docdb" scheme="CPC">A61B   5/08        20130101 LA20151110BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987780269" load-source="docdb" scheme="CPC">A61B   5/1116      20130101 LI20151110BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987784670" load-source="docdb" scheme="CPC">A61B   5/7278      20130101 LI20151112BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987790001" load-source="docdb" scheme="CPC">A61B   5/4815      20130101 LI20151112BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987797711" load-source="docdb" scheme="CPC">A61B2562/0219      20130101 LA20151112BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987799014" load-source="docdb" scheme="CPC">A61B   5/0245      20130101 LA20151110BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165550019" lang="DE" load-source="patent-office">SCHLAFZUSTANDSSCHÄTZVORRICHTUNG, VERFAHREN UND SPEICHERMEDIUM</invention-title><invention-title mxw-id="PT165550020" lang="EN" load-source="patent-office">SLEEP STATE ESTIMATION DEVICE, METHOD AND STORAGE MEDIUM</invention-title><invention-title mxw-id="PT165550021" lang="FR" load-source="patent-office">DISPOSITIF D'ESTIMATION D'ÉTAT DE SOMMEIL, PROCÉDÉ ET SUPPORT DE STOCKAGE</invention-title><citations><patent-citations><patcit mxw-id="PCIT371137931" load-source="docdb" ucid="US-20090062628-A1"><document-id format="epo"><country>US</country><doc-number>20090062628</doc-number><kind>A1</kind><date>20090305</date></document-id><sources><source name="SEA" category="XI" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335742246" load-source="docdb" ucid="US-20100298899-A1"><document-id format="epo"><country>US</country><doc-number>20100298899</doc-number><kind>A1</kind><date>20101125</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT371137932" load-source="docdb" ucid="US-20110105915-A1"><document-id format="epo"><country>US</country><doc-number>20110105915</doc-number><kind>A1</kind><date>20110505</date></document-id><sources><source name="SEA" category="XA" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335738560" load-source="docdb" ucid="US-7753861-B1"><document-id format="epo"><country>US</country><doc-number>7753861</doc-number><kind>B1</kind><date>20100713</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335744651" load-source="docdb" ucid="WO-2004073494-A2"><document-id format="epo"><country>WO</country><doc-number>2004073494</doc-number><kind>A2</kind><date>20040902</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335742550" load-source="docdb" ucid="WO-2010105203-A2"><document-id format="epo"><country>WO</country><doc-number>2010105203</doc-number><kind>A2</kind><date>20100916</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit><patcit mxw-id="PCIT335743388" load-source="docdb" ucid="WO-2011032132-A2"><document-id format="epo"><country>WO</country><doc-number>2011032132</doc-number><kind>A2</kind><date>20110317</date></document-id><sources><source name="SEA" category="X" created-by-npl="N"/></sources></patcit></patent-citations><non-patent-citations><nplcit><text>None</text><sources><source mxw-id="PNPL67567986" load-source="docdb" name="APP"/></sources></nplcit></non-patent-citations></citations></technical-data><parties><applicants><applicant mxw-id="PPAR1103332650" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>TOSHIBA KK</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR1103336956" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KABUSHIKI KAISHA TOSHIBA</last-name></addressbook></applicant><applicant mxw-id="PPAR1101641146" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Kabushiki Kaisha Toshiba</last-name><iid>101017825</iid><address><street>1-1 Shibaura 1-chome</street><city>Minato-ku Tokyo 105-8001</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103327206" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>TAKAKURA JUNYA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103316333" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>TAKAKURA, JUNYA</last-name></addressbook></inventor><inventor mxw-id="PPAR1101650942" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>TAKAKURA, JUNYA</last-name><address><street>c/o IP Div. Toshiba Corp. (K.K. TOSHIBA) 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo 105-8001</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103307542" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>NAKAYAMA KANAKO</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103327457" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>NAKAYAMA, KANAKO</last-name></addressbook></inventor><inventor mxw-id="PPAR1101642518" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>NAKAYAMA, KANAKO</last-name><address><street>c/o IP Div. Toshiba Corp. (K.K. TOSHIBA) 1-1, Shibaura 1-chome Minato-ku</street><city>Tokyo 105-8001</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103318749" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>YAMAUCHI YASUNOBU</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103343740" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>YAMAUCHI, YASUNOBU</last-name></addressbook></inventor><inventor mxw-id="PPAR1101645728" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>YAMAUCHI, YASUNOBU</last-name><address><street>c/o IP Div. Toshiba Corp. (K.K. TOSHIBA) 1-1, Shibaura 1-chome Minato-ku</street><city>TTokyo 105-8001</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101643241" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Noble, Nicholas</last-name><suffix>et al</suffix><iid>101226434</iid><address><street>Kilburn &amp; Strode LLP 20 Red Lion Street</street><city>London WC1R 4PJ</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><designated-states><ep-contracting-states><country mxw-id="DS660790325" load-source="docdb">AL</country><country mxw-id="DS660707868" load-source="docdb">AT</country><country mxw-id="DS660790327" load-source="docdb">BE</country><country mxw-id="DS660628955" load-source="docdb">BG</country><country mxw-id="DS660733016" load-source="docdb">CH</country><country mxw-id="DS660629921" load-source="docdb">CY</country><country mxw-id="DS660707869" load-source="docdb">CZ</country><country mxw-id="DS660790328" load-source="docdb">DE</country><country mxw-id="DS660629922" load-source="docdb">DK</country><country mxw-id="DS660629935" load-source="docdb">EE</country><country mxw-id="DS660709779" load-source="docdb">ES</country><country mxw-id="DS660628956" load-source="docdb">FI</country><country mxw-id="DS660628957" load-source="docdb">FR</country><country mxw-id="DS660790329" load-source="docdb">GB</country><country mxw-id="DS660629936" load-source="docdb">GR</country><country mxw-id="DS660790330" load-source="docdb">HR</country><country mxw-id="DS660707870" load-source="docdb">HU</country><country mxw-id="DS660733017" load-source="docdb">IE</country><country mxw-id="DS660629937" load-source="docdb">IS</country><country mxw-id="DS660628958" load-source="docdb">IT</country><country mxw-id="DS660629938" load-source="docdb">LI</country><country mxw-id="DS660630037" load-source="docdb">LT</country><country mxw-id="DS660707875" load-source="docdb">LU</country><country mxw-id="DS660630038" load-source="docdb">LV</country><country mxw-id="DS660630047" load-source="docdb">MC</country><country mxw-id="DS660714550" load-source="docdb">MK</country><country mxw-id="DS660714555" load-source="docdb">MT</country><country mxw-id="DS660707876" load-source="docdb">NL</country><country mxw-id="DS660709780" load-source="docdb">NO</country><country mxw-id="DS660733018" load-source="docdb">PL</country><country mxw-id="DS660630049" load-source="docdb">PT</country><country mxw-id="DS660707877" load-source="docdb">RO</country><country mxw-id="DS660630050" load-source="docdb">RS</country><country mxw-id="DS660733027" load-source="docdb">SE</country><country mxw-id="DS660630059" load-source="docdb">SI</country><country mxw-id="DS660709781" load-source="docdb">SK</country><country mxw-id="DS660733028" load-source="docdb">SM</country><country mxw-id="DS660714556" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data></bibliographic-data><abstract mxw-id="PA166480949" lang="EN" load-source="patent-office"><p id="pa01" num="0001">According to one embodiment, a sleep state estimation device (10) attached to a user during sleep and used is provided. The device includes a first detector (111), a first estimation module (112), a second detector (113), and a second estimation module (114). The first detector is configured to detect a first signal to estimate a first state of the user. The first estimation module is configured to estimate the user's first state, based on the first signal. The second detector is configured to detect a second signal to estimate a second state of the user other than the user's first state. The second estimation module is configured to estimate the user's second state by using the second signal, based on an estimation method determined in accordance with the first signal.
<img id="iaf01" file="imgaf001.tif" wi="113" he="91" img-content="drawing" img-format="tif"/></p></abstract><abstract mxw-id="PA166760761" lang="EN" source="EPO" load-source="docdb"><p>According to one embodiment, a sleep state estimation device (10) attached to a user during sleep and used is provided. The device includes a first detector (111), a first estimation module (112), a second detector (113), and a second estimation module (114). The first detector is configured to detect a first signal to estimate a first state of the user. The first estimation module is configured to estimate the user's first state, based on the first signal. The second detector is configured to detect a second signal to estimate a second state of the user other than the user's first state. The second estimation module is configured to estimate the user's second state by using the second signal, based on an estimation method determined in accordance with the first signal.</p></abstract><description mxw-id="PDES98405650" lang="EN" load-source="patent-office"><!-- EPO <DP n="1"> --><heading id="h0001">FIELD</heading><p id="p0001" num="0001">Embodiments described herein relate generally to a sleep state estimation device, a method and a storage medium.</p><heading id="h0002">BACKGROUND</heading><p id="p0002" num="0002">In general, a user's state during sleep (hereinafter referred to as a sleep state) is estimated by using a body position sensor, a snoring sensor, etc., for diagnosis of, for example, sleep apnea syndrome, etc.</p><p id="p0003" num="0003">In this case, the body sensor is attached to, for example, the user's chest, etc., and the snoring sensor is attached to, for example, the user's throat, etc. Sleeping with a plurality of sensors separately attached at different points is very troublesome for the user.</p><p id="p0004" num="0004">Recently, a sleep state estimation device (sleep sensor) including a plurality of sensors used to estimate the user's sleep state has been developed. According to such a sleep state estimation device, not only the user's body position and respiratory state during sleep, but also other conditions such as electrocardiographic activity, pulse and body temperature can be estimated. In addition, the inconvenience for the user can be reduced since separately attaching a plurality of sensors as mentioned above is unnecessary.</p><p id="p0005" num="0005">The above-explained sleep state estimation device is capable of estimating various states as the user's sleep state but, the estimation accuracy may be low since each state is estimated by using a signal alone<!-- EPO <DP n="2"> --> detected by the sensor corresponding to the state.</p><heading id="h0003">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p0006" num="0006"><ul><li><figref idrefs="f0001">FIG. 1</figref> is an illustration showing an example of use of a sleep state estimation device of a first embodiment.</li><li><figref idrefs="f0001">FIG. 2</figref> is a plan view showing an example of an attachment surface of the sleep state estimation device.</li><li><figref idrefs="f0002">FIG. 3</figref> is an illustration showing an example of attachment of the sleep state estimation device.</li><li><figref idrefs="f0002">FIG. 4</figref> is a diagram showing an example of a system configuration of the sleep state estimation device.</li><li><figref idrefs="f0003">FIG. 5</figref> is a diagram showing an example of a structure of an electrocardiographic sensor module shown in <figref idrefs="f0002">FIG. 4</figref>.</li><li><figref idrefs="f0003">FIG. 6</figref> is a diagram showing an example of a structure of an acceleration sensor module shown in <figref idrefs="f0002">FIG. 4</figref>.</li><li><figref idrefs="f0004">FIG. 7</figref> is a block diagram mainly showing an example of a functional structure of the sleep state estimation device of the first embodiment.</li><li><figref idrefs="f0004">FIG. 8</figref> is a flowchart showing an example of a procedure of body position estimation processing.</li><li><figref idrefs="f0005">FIG. 9</figref> is a flowchart showing an example of a procedure of respiratory state estimation processing.</li><li><figref idrefs="f0005">FIG. 10</figref> is a block diagram mainly showing an example of a functional structure of a sleep state estimation device of a second embodiment.</li><li><figref idrefs="f0006">FIG. 11</figref> is a flowchart showing an example of a procedure of heart rate estimation processing.</li><li><figref idrefs="f0007">FIG. 12</figref> is a diagram showing an example of a system configuration of a sleep state estimation device of a third embodiment.</li><li><figref idrefs="f0008">FIG. 13</figref> is a block diagram mainly showing an example of a functional structure of the sleep state estimation device of the third embodiment.</li><li><figref idrefs="f0008">FIG. 14</figref> is a flowchart showing an example of a<!-- EPO <DP n="3"> --> procedure of respiratory state estimation processing in the third embodiment.</li></ul></p><heading id="h0004">DETAILED DESCRIPTION</heading><p id="p0007" num="0007">Various embodiments will be described hereinafter with reference to the accompanying drawings.</p><p id="p0008" num="0008">In general, according to one embodiment, a sleep state estimation device attached to a user during sleep and used is provided. The device includes a first detector, a first estimation module, a second detector, and a second estimation module. The first detector is configured to detect a first signal to estimate a first state of the user. The first estimation module is configured to estimate the user's first state, based on the first signal. The second detector is configured to detect a second signal to estimate a second state of the user other than the user's first state. The second estimation module is configured to estimate the user's second state by using the second signal, based on an estimation method determined in accordance with the first signal.</p><heading id="h0005">(First Embodiment)</heading><p id="p0009" num="0009"><figref idrefs="f0001">FIG. 1</figref> shows an example of use of a sleep state estimation device of the First Embodiment. The sleep state estimation device 10 shown in <figref idrefs="f0001">FIG. 1</figref> is a small, lightweight and thin device employed to estimate a user's state during sleep (hereinafter called a sleep state), and is used by attaching an attachment surface of the sleep state estimation device 10 on a chest, etc., of the user during sleep.</p><p id="p0010" num="0010">In the present embodiment, the user's sleep state estimated by the sleep state estimation device 10 indicates, for example, a heart rate, a body position (position and attitude of body), a respiratory state, etc., of the user during sleep.</p><p id="p0011" num="0011">In the following explanations, axes horizontal to the attachment surface and orthogonal to each other are called an x-axis and a y-axis, and an axis orthogonal<!-- EPO <DP n="4"> --> to the x-axis and the y-axis (i.e., an axis normal to the attachment surface) is called a z-axis, relative to the attachment of the sleep state estimation device 10. In this case, the sleep state estimation device 10 is assumed to be attached such that the x-axis corresponds to an axis in a lateral direction of the user's body, the y-axis corresponds to an axis in a vertical direction of the user's body, and the z-axis corresponds to an axis in a front and back direction of the user, as shown in <figref idrefs="f0001">FIG. 1</figref>.</p><p id="p0012" num="0012"><figref idrefs="f0001">FIG. 2</figref> is a plan view showing an example of the attachment surface (i.e., back surface) of the sleep state estimation device 10. The sleep state estimation device 10 is, for example, in an ellipsoidal shape (or a substantially rectangular shape) having a longitudinal axis of approximately several centimeters, and two electrocardiographic electrodes 11 and 12 are arranged on the attachment surface of the sleep state estimation device 10 as shown in <figref idrefs="f0001">FIG. 2</figref>. The electrocardiographic electrodes 11 and 12 are spaced apart (arranged in close vicinity of both ends) along the longitudinal axis of the attachment surface of the sleep state estimation device 10.</p><p id="p0013" num="0013">The attachment surface of the sleep state estimation device 10 contacts (skin of) the user via an electrically conductive gel 14 arranged at a position in contact with each of the electrocardiographic electrodes 11 and 12, as shown in <figref idrefs="f0002">FIG. 3</figref>. The electrically conductive gel (material) 14 may be adhesive or the sleep state estimation device 10 may be attached to the user with a double-sided tape, a belt or the like. The gel 14 is thick and elastic.</p><p id="p0014" num="0014"><figref idrefs="f0002">FIG. 4</figref> is a diagram showing a system configuration of the sleep state estimation device 10. The sleep state estimation device 10 includes a CPU 101, a nonvolatile memory 102, a main memory 103, a BIOS-ROM 104, a system controller 105, an electrocardiographic<!-- EPO <DP n="5"> --> sensor module 106, an acceleration sensor module 107, a BT module 108, an EC 109, etc., as shown in <figref idrefs="f0002">FIG. 4</figref>.</p><p id="p0015" num="0015">The CPU 101 is a processor configured to control operations of respective components in the sleep state estimation device 10. The CPU 101 executes various types of software loaded from the nonvolatile memory 102, which is a storage device, to the main memory 103.</p><p id="p0016" num="0016">The CPU 101 also executes a Basic Input/Output System (BIOS) stored in the BIOS-ROM 104. The BIOS is a program for hardware control.</p><p id="p0017" num="0017">The system controller 105 is a bridge device which makes connection between the CPU 101 and various components. The CPU 101, the nonvolatile memory 102, the main memory 103, the BIOS-ROM 104, the electrocardiographic sensor module 106, the acceleration sensor module 107, the BT module 108, the EC 109, etc., are connected to the system controller 105.</p><p id="p0018" num="0018">The electrocardiographic sensor module 106 is a module which includes an electrocardiographic sensor capable of sensing an electrocardiographic signal and which is employed to estimate the user's heart rate during sleep. The acceleration sensor module 107 is a module which includes an acceleration sensor capable of sensing an acceleration signal and which is employed to estimate the user's body position and respiratory state during sleep. Structures of the electrocardiographic sensor module 106 and the acceleration sensor module 107 will be described later.</p><p id="p0019" num="0019">The BT module 108 is a module configured to execute wireless communication of Bluetooth(TM) with a Bluetooth-enabled device. The Bluetooth-enabled device implies, for example, a smartphone, a tablet computer, a personal computer (PC) or the like.</p><p id="p0020" num="0020">The EC 109 is an electric power management controller which executes electric power management of the sleep state estimation device 10.<!-- EPO <DP n="6"> --></p><p id="p0021" num="0021">The sleep state estimation device 10 of the present embodiment may includes a plurality of types of sensors capable of sensing a pulse wave, a body temperature, etc., besides the electrocardiographic sensor and the acceleration sensor, though not shown in <figref idrefs="f0001">FIG. 2</figref>.</p><p id="p0022" num="0022">The sleep state estimation device 10 thus includes a plurality of sensors in a housing but, since analog front-ends of the plurality of sensors are different in specification for the respective sensors, the device is required to have both flexibility and high performance and may be upsized. In the present embodiment, however, a module which is several millimeters square is implemented by integrating a plurality of analog front-ends, the CPU 101, etc., on a single chip by pseudo-SoC technology. The pseudo-SoC technology establishes compatibility of miniaturization corresponding to SoC and degree of freedom corresponding to SiP by integrating components on a wafer. The small, lightweight (ten grams or so) and thin (several millimeters or so) sleep state estimation device 10 can be implemented by connecting a small number of peripheral components such as an antenna and a battery to the module. Implementation of miniaturization of the sleep state estimation device 10 using pseudo-SoC technology is explained here, but the miniaturization can be implemented with, for example, an LSI, etc.</p><p id="p0023" num="0023"><figref idrefs="f0003">FIG. 5</figref> shows a structure of the electrocardiographic sensor module 106 shown in <figref idrefs="f0002">FIG. 4</figref>. In the electrocardiographic sensor module 106, the electrocardiographic electrodes 11 and 12 are connected to an electrocardiographic sensor 106a which is the analog front-end for electrocardiogram, as shown in <figref idrefs="f0003">FIG. 5</figref>.</p><p id="p0024" num="0024">The electrocardiographic sensor 106a is a sensor which senses an electrocardiographic signal indicating<!-- EPO <DP n="7"> --> an electrocardiographic state of the user to which the sleep state estimation device 10 is attached. The electrocardiographic signal sensed by the electrocardiographic sensor 106a includes, for example, a time-series signal obtained by sampling a potential difference between the electrocardiographic electrodes 11 and 12. According to the electrocardiographic signal, an electrocardiogram (electrocardiographic waveform), etc., can be obtained.</p><p id="p0025" num="0025"><figref idrefs="f0003">FIG. 6</figref> shows a structure of the acceleration sensor module 107 shown in <figref idrefs="f0002">FIG. 4</figref>. The acceleration sensor module 107 includes an acceleration sensor 107a, a high-pass filter 107b, an amplifier circuit 107c, an A/D converter 107d, etc., as shown in <figref idrefs="f0003">FIG. 6</figref>.</p><p id="p0026" num="0026">The acceleration sensor 107a is a sensor which senses an acceleration signal indicating acceleration which acts on the sleep state estimation device 10. The acceleration sensor 107a is assumed to be, for example, a triaxial acceleration sensor (three-dimensional acceleration sensor) capable of sensing an acceleration signal of each axial direction of three orthogonal axes (i.e., the above-described x-, y- and z-axes). In addition, the acceleration signal sensed (output) by the acceleration sensor 107a is an analog signal.</p><p id="p0027" num="0027">The sleep state estimation device 10 is shaped in, for example, an ellipsoid as explained above, but (the acceleration signal produced according to) vibration can hardly be sensed in vicinity of a center of the sleep state estimation device 10. The acceleration sensor 107a is therefore located at, for example, a position displaced from the center of the sleep state estimation device 10 to detect the acceleration signal more correctly.</p><p id="p0028" num="0028">The high-pass filter 107b extracts an alternating-current component of the acceleration signal from the acceleration signal sensed by the acceleration sensor<!-- EPO <DP n="8"> --> 107a.</p><p id="p0029" num="0029">The amplifier circuit (analog circuit) 107c amplifies the alternating-current component extracted by the high-pass filter 107b.</p><p id="p0030" num="0030">The A/D converter 107d converts the acceleration signal (analog signal) sensed by the acceleration sensor 107a into a digital signal. In addition, the A/D converter 107d converts the alternating-current component of the acceleration signal amplified by the amplifier circuit 107c into a digital signal.</p><p id="p0031" num="0031"><figref idrefs="f0004">FIG. 7</figref> is a block diagram mainly showing a functional configuration of the sleep state estimation device 10 of the present embodiment. The sleep state estimation device 10 includes an acceleration signal detector 111, a body position estimation module 112, a respiratory signal detector 113, and a respiratory state estimation module 114 as shown in <figref idrefs="f0004">FIG. 7</figref>.</p><p id="p0032" num="0032">The acceleration signal detector 111 is a functional module implemented by the acceleration sensor 107a, which detects the acceleration signal by the acceleration sensor 107a as a signal (first signal) to estimate the user's body position (first state). The acceleration sensor 107a of the present embodiment is a triaxial acceleration sensor as explained above. For this reason, the acceleration signal detector 111 detects the acceleration signal representing the acceleration in each axial direction of the orthogonal x-, y- and z-axes.</p><p id="p0033" num="0033">Each of the acceleration signals detected by the acceleration signal detector 111 includes a direct-current component mainly representing a gravitational acceleration, and the alternating-current component mainly representing the acceleration produced by the user's conditions (for example, respiratory state).</p><p id="p0034" num="0034">In the following explanations, an acceleration signal (first acceleration signal) indicating the acceleration of the x-axis (first axis) direction, of<!-- EPO <DP n="9"> --> the acceleration signals detected by the acceleration signal detector 111, is called an x-axis acceleration signal, an acceleration signal (second acceleration signal) indicating the acceleration of the y-axis (second axis) direction is called a y-axis acceleration signal, and an acceleration signal (third acceleration signal) indicating the acceleration of the z-axis (third axis) direction is called a z-axis acceleration signal.</p><p id="p0035" num="0035">The body position estimation module 112 estimates the user's body position during sleep, based on the acceleration signals (x-axis acceleration signal, y-axis acceleration signal, and z-axis acceleration signal) detected by the acceleration signal detector 111. The user's body position estimated by the body position estimation module 112 includes, for example, a supine position, a prone position, a lateral position, etc.</p><p id="p0036" num="0036">The respiratory signal detector 113 is a functional module implemented by the high-pass filter 107b and the amplifier circuit 107c. The respiratory signal detector 113 cuts the direct-current component of each acceleration signal detected by the acceleration signal detector 111, by the high-pass filter 107b and extracts the alternating-current component (high-frequency component) of the acceleration signal. In addition, the respiratory signal detector 113 amplifies the extracted alternating-current component of the acceleration signal by the amplifier circuit 107c. The respiratory signal detector 113 thereby detects the amplified alternating-current component of the acceleration signal as the signal (second signal) for estimation of a state (second state) other than the user's body position. In the following explanations, the signal detected by the respiratory signal detector 113 (i.e., the amplified alternating-current component of the<!-- EPO <DP n="10"> --> acceleration signal) is called the respiratory signal.</p><p id="p0037" num="0037">The respiratory state estimation module 114 estimates the user's respiratory state during sleep, based on the acceleration signals detected by the acceleration signal detector 111 and the respiratory signals detected by the respiratory signal detector 113. The user's respiratory state estimated by the respiratory state estimation module 114 includes, for example, presence or absence of a symptom of snoring during sleep, etc.</p><p id="p0038" num="0038">Next, an operation of the sleep state estimation device 10 attached to the user's chest, etc., to estimate the user's sleep state will be explained.</p><p id="p0039" num="0039">In the present embodiment, when the acceleration signals (x-axis acceleration signal, y-axis acceleration signal and z-axis acceleration signal) are detected by the acceleration signal detector 111 (acceleration sensor 107a), processing of estimating the user's body position during sleep (hereinafter called body position estimation processing) and processing of estimating the user's respiratory state during sleep (hereinafter called respiratory state estimation processing) as the user's sleep state are executed. The body position estimation processing and the respiratory state estimation processing will be hereinafter explained. The body position estimation processing and the respiratory state estimation processing are assumed to be periodically executed during the user's sleep period.</p><p id="p0040" num="0040">First, a procedure of the body position estimation processing will be explained with reference to a flowchart of <figref idrefs="f0004">FIG. 8</figref>.</p><p id="p0041" num="0041">In the body position estimation processing, the acceleration signals detected by the acceleration signal detector 111 are converted into digital signals by the A/D converter 107d (step S1). The acceleration signals thus converted into the digital signals are<!-- EPO <DP n="11"> --> supplied to the body position estimation module 112.</p><p id="p0042" num="0042">Next, the body position estimation module 112 estimates the user's body position during sleep, based on the acceleration signals converted into the digital signals by the A/D converter 107d (step S2).</p><p id="p0043" num="0043">The processing of step S2 will be explained more specifically. Each of the acceleration signals (x-axis acceleration signal, y-axis acceleration signal, and z-axis acceleration signal) detected by the acceleration signal detector 111 includes the direct-current component (gravitational acceleration), and the direction of the gravitational acceleration to the sleep state estimation device 10 can be computed from (the direct-current component of) each of the acceleration signals. The user's body position is estimated based on the direction of the gravitational acceleration to the sleep state estimation device 10 thus computed.</p><p id="p0044" num="0044">More specifically, when the direction of the gravitational acceleration to the sleep state estimation device 10 is a user's backward direction on the z-axis (i.e., user's back surface direction), the user's body position is assumed to be the supine position. In contrast, when the direction of the gravitational acceleration to the sleep state estimation device 10 is a user's forward direction on the z-axis (i.e., user's front surface direction), the user's body position is assumed to be the prone position. In addition, when the direction of the gravitational acceleration to the sleep state estimation device 10 is the x-axis direction (i.e., user's lateral direction), the user's body position is assumed to be the lateral position.</p><p id="p0045" num="0045">When the user's body position is estimated in step S2, the body position estimation module 112 outputs the estimation result to an external Bluetooth-enabled device (smartphone, tablet computer, PC, etc.) via, for<!-- EPO <DP n="12"> --> example, the BT module 108, etc. (step S3). The estimation result thus output to the Bluetooth-enabled device is provided (presented) to, for example, the user, etc., by the device.</p><p id="p0046" num="0046">Next, a procedure of the respiratory state estimation processing will be explained with reference to a flowchart of <figref idrefs="f0005">FIG. 9</figref>.</p><p id="p0047" num="0047">In the respiratory state estimation processing, the respiratory signal detector 113 executes filtering using the high-pass filter 107b, for each of the acceleration signals detected by the acceleration signal detector 111. According to this processing, the respiratory signal detector 113 extracts the alternating-current component of the x-axis acceleration signal, the alternating-current component of the y-axis acceleration signal, and the alternating-current component of the z-axis acceleration signal, from the respective acceleration signals (x-axis acceleration signal, y-axis acceleration signal, and z-axis acceleration signal) detected by the acceleration signal detector 111 (step S11).</p><p id="p0048" num="0048">The A/D converter 107d in the present embodiment needs to A/D-convert both (the direct-current component of) each of the acceleration signals detected by the acceleration signal detector 111 and the alternating-current component of the acceleration signal, to estimate the user's body position and the respiratory state. In this case, the alternating-current component of each of the acceleration signals is extremely smaller in amplitude than the acceleration signal. More specifically, detection of the acceleration signal within a range of ±1G is required for estimation of the body position, but the magnitude of the acceleration signal detected for estimation of the respiratory state is, for example, several milliG or smaller. In contrast, resolution of the A/D converter 107d which can be mounted on the sleep state estimation device 10<!-- EPO <DP n="13"> --> is, for example, approximately 10 bit.</p><p id="p0049" num="0049">In other words, the alternating-current component of each of the acceleration signals extracted by using the high-pass filter 107b cannot be processed by the A/D converter 107d as it is. Thus, the respiratory signal detector 113 amplifies the extracted alternating-current component of each of the acceleration signals, by the amplifier circuit 107c (step S12). In this case, an amplification rate of the alternating-current component of each of the acceleration signals is assumed to be adjusted in accordance with a dynamic range of the A/D converter 107d such that the alternating-current component can be processed even by the resolution of the A/D converter 107d.</p><p id="p0050" num="0050">The alternating-current component of each of the acceleration signals thus amplified by the respiratory signal detector 113 (i.e., the respiratory signal detected by the respiratory signal detector 113) is converted into a digital signal by the A/D converter 107d (step S13). The respiratory signal converted into the digital signal is supplied to the respiratory state estimation module 114.</p><p id="p0051" num="0051">Next, the respiratory state estimation module 114 estimates the user's respiratory state (presence or absence of a symptom of snoring) during sleep, based on the respiratory signals (alternating-current components of the acceleration signals) converted into the digital signals by the A/D converter 107d (step S14).</p><p id="p0052" num="0052">The processing in step S14 will be explained more specifically. In this case, the respiratory state estimation module 114 estimates the presence or absence of a symptom of snoring based on, for example, a variable computed based on the respiratory signal.</p><p id="p0053" num="0053">A variable (Y) to thus estimate the presence or absence of a symptom of snoring is computed by, for example, an expression such as<!-- EPO <DP n="14"> --> "Kx(Axac)<sup>2</sup>+Ky(Ayac)<sup>2</sup>+Kz(Azac)<sup>2</sup>".</p><p id="p0054" num="0054">In this expression, Axac indicates the alternating-current component of the x-axis acceleration signal. Similarly, Ayac and Azac indicate the alternating-current component of the y-axis acceleration signal and the alternating-current component of the z-axis acceleration signal, respectively. Kx, Ky and Kz indicate values representing weights on the x-axis acceleration signal (Axac), the y-axis acceleration signal (Ayac) and the z-axis acceleration signal (Azac), respectively. In other words, the variable to estimate the presence or absence of a symptom of snoring is computed based on the alternating-current component of the x-axis acceleration signal, the alternating-current component of the y-axis acceleration signal, and the alternating-current component of the z-axis acceleration signal, which are weighted by Kx, Ky and Kz, respectively.</p><p id="p0055" num="0055">When the sleep state estimation device 10 is attached to the user by the electrically conductive gel 14, vibration may be damped, and a main axis of the vibration may be changed according to the directions of the gravitational accelerations, since the gel 14 is thick and elastic. Thus, in the present embodiment, the weights (i.e., Kx, Ky and Kz) on the alternating-current component of the x-axis acceleration signal, the alternating-current component of the y-axis acceleration signal, and the alternating-current component of the z-axis acceleration signal, are varied in response to the gravitational accelerations (direct-current components of the respective acceleration signals).</p><p id="p0056" num="0056">In this case, magnitudes of Kx, Ky and Kz are set to be varied in proportion to the direct-current component Axdc of the x-axis acceleration signal, the direct-current component Aydc of the y-axis acceleration signal, and the direct-current component<!-- EPO <DP n="15"> --> Azdc of the alternating current component of the z-axis acceleration signal, respectively. In other words, a ratio of Kx, Ky and Kz is varied in accordance with a ratio of the direct-current component Axdc of the x-axis acceleration signal, the direct-current component Aydc of the y-axis acceleration signal, and the direct-current component Azdc of the z-axis acceleration signal. Alternatively, a relationship among Kx, Ky and Kz or a relationship among Axdc, Aydc, and Azdc may not be a simply proportional relationship, but may be determined by a nonlinear function.</p><p id="p0057" num="0057">The direct-current component Axdc of the x-axis acceleration signal, the direct-current component Aydc of the y-axis acceleration signal, and the direct-current component Azdc of the z-axis acceleration signal (i.e., low frequency components) are assumed to be extracted by, for example, a low-pass filter, etc. In addition, a function for uniquely determining Kx, Ky and Kz according to the direct-current component Axdc of the x-axis acceleration signal, the direct-current component Aydc of the y-axis acceleration signal, and the direct-current component Azdc of the z-axis acceleration signal is assumed to be prestored in, for example, the sleep state estimation device 10.</p><p id="p0058" num="0058">In step S14, when the variable computed as explained above is equal to or greater than a predetermined value, it can be estimated that the user has a symptom of snoring.</p><p id="p0059" num="0059">When the user's respiratory state is thus estimated, the respiratory state estimation module 114 outputs (transmits) the estimation result to an external Bluetooth-enabled device via, for example, the above-explained BT module 108, etc. (step S15). The estimation result thus output to the Bluetooth-enabled device is provided (presented) to, for example, the user, etc., by the device. The estimation result output in step S15 may include, for example, a degree<!-- EPO <DP n="16"> --> of snoring caused by a symptom of snoring besides the presence or absence of the symptom of snoring.</p><p id="p0060" num="0060">When the above-explained respiratory state estimation processing is executed, Kx, Ky and Kz determined in step S14 are assumed to be held in, for example, the sleep state estimation device 10. According to this, when the respiratory state estimation processing is executed again but the user's body position is not changed, the user's respiratory state can be estimated by using Kx, Ky and Kz held in the sleep state estimation device 10, without determining Kx, Ky and Kz again.</p><p id="p0061" num="0061">The body position estimation processing and the respiratory state estimation processing have been mainly explained in the present embodiment, but the user's heart rate can also be estimated by using the electrocardiographic sensor module 106 (electrocardiographic sensor 106a) in the sleep state estimation device 10 of the present embodiment. Furthermore, a respiratory rate, etc., can also be estimated by connecting a sensor configured to sense impedance to the electrocardiographic electrodes 11 and 12 and sensing variation in the impedance between the electrodes by the sensor.</p><p id="p0062" num="0062">In the present embodiment, as explained above, the acceleration signals representing the acceleration acting on the sleep state estimation device 10 are detected and the user's body position is estimated based on the detected acceleration signals (gravitational accelerations). Moreover, in the present embodiment, the signal (respiration signal) to estimate the user's respiratory state (state other than the user's body position) is detected, and the user's respiratory state is estimated based on the detected acceleration signals and the respiration signal. More specifically, in the present embodiment, the alternating-current components of the acceleration<!-- EPO <DP n="17"> --> signals are weighted according to the direct-current components of the acceleration signals in the directions of the three orthogonal axes, respectively, and the user's respiratory state (presence or absence of a symptom of snoring) is estimated based on the weighted alternating-current components of the acceleration signals. In other words, in the present embodiment, the acceleration signals (gravitational accelerations) which are the signals to estimate the user's body position are used when the presence or absence of a symptom of snoring is estimated (or the variable used for the estimation is computed), and a processing method (weight value) for estimating the user's respiratory state is varied according to the directions of the gravitational accelerations (i.e., user's body position).</p><p id="p0063" num="0063">In the present embodiment, since influence from the elasticity of the electrically conductive gel 14 and the gravitational accelerations can be excluded and the sensitivity in vibration detection can be highly maintained by the structure, accuracy in estimation of the user's sleep state (presence or absence of a symptom of snoring) can be improved.</p><p id="p0064" num="0064">In addition, in the present embodiment, since the user's respiratory state is estimated by using the alternating-current components extracted from the acceleration signals detected by the acceleration signal detector 111, sensors for estimating the user's body position and the user's respiratory state do not need to be provided separately. In the present embodiment, the miniaturization, reduction of the weight, and thinning of the sleep state estimation device 10 can be thereby implemented.</p><p id="p0065" num="0065">Moreover, in the present embodiment, since processing can be executed appropriately with the A/D converter 107d of low resolution by amplifying the alternating-current components extracted from the<!-- EPO <DP n="18"> --> acceleration signals detected by the acceleration signal detector 111, an expensive A/D converter 107d does not need to be employed and reduction of manufacturing costs can be implemented.</p><p id="p0066" num="0066">Use of the sleep state estimation device 10 attached to the user's chest is explained in the present embodiment but, for example, if the user's body position and the user's respiratory state during sleep can be estimated with the acceleration signals sensed by the acceleration sensor 107a, the sleep state estimation device 10 may be attached to the other site.</p><p id="p0067" num="0067">In addition, outputting the estimation result on the user's body position and the user's respiratory state to the Bluetooth-enabled device has been explained in the present embodiment, but the estimation result may be output to, for example, an external server device, etc., which functions as a cloud server providing cloud computing service over a wireless LAN, etc., and stored in the server device. Alternatively, the estimation result may be stored in the sleep state estimation device 10.</p><p id="p0068" num="0068">Moreover, the sleep state estimation device 10 including the body position estimation module 112 and the respiratory state estimation module 114 is explained in the present embodiment, but the processing executed by the body position estimation module 112 and the respiratory state estimation module 114 may be executed by, for example, an external Bluetooth-enabled device, a server device or the like.</p><heading id="h0006">(Second Embodiment)</heading><p id="p0069" num="0069">Next, a second embodiment will be described.</p><p id="p0070" num="0070">The present embodiment is different from the First Embodiment with respect to a feature of estimating the user's heart rate during sleep by using the user's body position estimated based on an acceleration signal. In other words, the user's sleep state estimated by a sleep state estimation device of the present embodiment<!-- EPO <DP n="19"> --> includes, for example, a body position and a heart rate of the user during sleep.</p><p id="p0071" num="0071"><figref idrefs="f0005">FIG. 10</figref> is a block diagram mainly showing a functional configuration of the sleep state estimation device 20 of the present embodiment. In <figref idrefs="f0005">FIG. 10</figref>, portions like or similar to those shown in <figref idrefs="f0004">FIG. 7</figref> are denoted by the same reference numbers and symbols, and detailed explanations are omitted. Portions different from <figref idrefs="f0004">FIG. 7</figref> will be mainly explained. Since the system configuration of the sleep state estimation device 20 of the present embodiment is the same as that of the First Embodiment, the device will be explained as needed with reference to <figref idrefs="f0002 f0003">FIG. 4 to FIG. 6</figref>, etc.</p><p id="p0072" num="0072">The sleep state estimation device 20 includes an electrocardiographic signal detector 211, a storage 212 and a heart rate estimation module 213 besides the acceleration signal detector 111 and the body position estimation module 112 of the First Embodiment, as shown in <figref idrefs="f0005">FIG. 10</figref>.</p><p id="p0073" num="0073">The electrocardiographic signal detector 211 is a functional module implemented by the electrocardiographic sensor 106a, and detects the electrocardiographic signal by the electrocardiographic sensor 106a.</p><p id="p0074" num="0074">The storage module 212 stores information on a plurality of processing methods (contents) for estimating the user's heart rate, based on the electrocardiographic signal detected by the electrocardiographic signal detector 211. In other words, the processing methods corresponding to the respective user's body positions estimated by the body position estimation module 112 are defined in the storage 212. As the plurality of processing methods (contents), for example, a plurality of arbitrary, publicly known methods such as simple peak detection, Pans &amp; Tompkins and pattern matching can be employed. Alternatively, a plurality of parameters (for example,<!-- EPO <DP n="20"> --> threshold values) used in the at least one of the methods may be prepared. In addition, correspondence between the plurality of processing methods (contents) and the user's body positions may be varied in accordance with each of user's characteristics (orientation of a heart, composition of chest tissues, etc., different in individual persons).</p><p id="p0075" num="0075">The heart rate estimation module 213 determines (selects) the processing method corresponding to the user's body position estimated by the body position estimation module 112, of the plurality of processing methods defined in the storage 212. The heart rate estimation module 213 estimates the user's heart rate by executing processing based on the determined processing method for the electrocardiographic signal detected by the electrocardiographic signal detector 211.</p><p id="p0076" num="0076">Next, an operation of the sleep state estimation device 20 attached to the user's chest, etc., to estimate the user's sleep state will be explained.</p><p id="p0077" num="0077">In the present embodiment, when acceleration signals are detected by the acceleration signal detector 111 (acceleration sensor 107a), the processing of estimating the user's body position shown in <figref idrefs="f0004">FIG. 8</figref> is executed, similarly to the First Embodiment. Furthermore, processing of estimating the user's heart rate during sleep (hereinafter referred to as heart rate estimation processing) is executed besides body position estimation processing, in the present embodiment. The heart rate estimation processing is assumed to be periodically executed during the user's sleep period, similarly to the above-explained body position estimation processing.</p><p id="p0078" num="0078">Next, a procedure of the heart rate estimation processing will be explained with reference to a flowchart of <figref idrefs="f0006">FIG. 11</figref>.</p><p id="p0079" num="0079">In the heart rate estimation processing, the<!-- EPO <DP n="21"> --> electrocardiographic signal detector 211 detects the electrocardiographic signal indicating electrocardiogram of the user to which the sleep state estimation device 20 is attached, by using the electrocardiographic sensor 106a (step S21).</p><p id="p0080" num="0080">When the electrocardiographic signal is thus detected by the electrocardiographic signal detector 211, the heart rate estimation module 213 obtains the user's body position estimated by the body position estimation processing, from the body position estimation module 112 (step S22).</p><p id="p0081" num="0081">When a small sensor having a short distance between the electrocardiographic electrodes such as the sleep state estimation device 20 of the present embodiment is attached to a chest, a waveform of the electrocardiogram obtained from the electrocardiographic signal (electrocardiographic waveform) is greatly varied according to the user's body position (attitude). In addition, the heart rate is estimated from the electrocardiographic signal (waveform of the electrocardiogram) in the present embodiment, but an optimum processing method (algorithms, parameters, etc.) for estimating the heart rate is changed depending on the waveform of the electrocardiogram. Thus, in the present embodiment, the processing method for estimating the heart rate (estimation method) is changed in accordance with the estimated user's body position.</p><p id="p0082" num="0082">In this case, the heart rate estimation module 213 determines the processing method corresponding to the user's body position obtained in step S22 (i.e., processing method for appropriately processing the waveform of the electrocardiogram estimated to be obtained in accordance with the user's body position), by referring to the storage 212 (step S23).</p><p id="p0083" num="0083">Next, the heart rate estimation module 213 executes the processing based on the processing method<!-- EPO <DP n="22"> --> determined in step S23 (i.e., processing corresponding to the estimated user's body position), based on the electrocardiographic signal detected by the electrocardiographic signal detector 211. According to this, for example, a time-series heart rate is computed and the user's heart rate is estimated (step S24).</p><p id="p0084" num="0084">When the user's heart rate is thus estimated, the heart rate estimation module 213 outputs the estimation result to, for example, an external Bluetooth-enabled device, a server device, or the like (step S25).</p><p id="p0085" num="0085">When the above-explained heart rate estimation processing is executed, the processing method determined in step S23 is assumed to be held in, for example, the sleep state estimation device 10. According to this, when the heart rate estimation processing is executed again but the user's body position is not changed, the user's heart rate can be estimated based on the processing method held in the sleep state estimation device 10, without determining the processing method again.</p><p id="p0086" num="0086">In the present embodiment, as explained above, the user's heart rate is estimated by detecting the electrocardiographic signal indicating the user's electrocardiogram as the signal for estimating the user's heart rate and by executing the processing corresponding to the user's body position estimated based on the acceleration signals. In other words, in the present embodiment, accuracy in estimation of the user's sleep state (heart rate) can be enhanced since the heart rate can be estimated by using the optimum algorithm, parameter, etc., corresponding to the user's body position by changing the processing method for estimating the heart rate in accordance with the user's body position.</p><p id="p0087" num="0087">The present embodiment is explained on the assumption that the processing method for estimating the user's heart rate is uniquely determined in<!-- EPO <DP n="23"> --> accordance with the user's body position, but the processing method often cannot be uniquely determined in accordance with the user's body position (i.e., a plurality of processing methods correspond to the user's body position). In this case, all types of processing are executed in a plurality of processing methods according to the user's body position, and the processing method which obtains a most desirable processing result of all the processing results is adopted. The time-series heart rate is computed as the processing result in the present embodiment, but desirability of the processing result is evaluated based on, for example, whether the computed heart rate falls within a predetermined range or not (for example, a rate of a value which falls within the range), etc. In this structure, the user's heart rate can be estimated by using the optimum algorithm, parameter, etc., even if the processing method cannot be uniquely determined based on the user's body position.</p><p id="p0088" num="0088">In addition, in the present embodiment, the acceleration sensor module 107 may not includes the high-pass filter 107b or the amplifier circuit 107c since the acceleration signal is used for estimation of the user's body position alone and the alternating-current component does not need to be extracted from the acceleration signal.</p><p id="p0089" num="0089">The acceleration sensor 107a outputs the analog signal in the First Embodiment, but the acceleration sensor 107a of the present embodiment may output the digital signal since the acceleration signal is used for estimation of the user's body position alone in the present embodiment.</p><p id="p0090" num="0090">Use of the sleep state estimation device 20 attached to the user's chest is explained in the present embodiment, but the sleep state estimation device 20 may be attached to the other site if, for example, the user's body position during sleep can be<!-- EPO <DP n="24"> --> estimated with the acceleration signals sensed by the acceleration sensor 107a and the user's heart rate during sleep can be estimated with the electrocardiographic signal sensed by the electrocardiographic sensor 106a.</p><p id="p0091" num="0091">Moreover, the sleep state estimation device 20 including the body position estimation module 112 and the heart rate estimation module 213 is explained in the present embodiment, but the processing executed by the body position estimation module 112 and the heart rate estimation module 213 may be executed by, for example, an external Bluetooth-enabled device, a server device or the like.</p><heading id="h0007">(Third Embodiment)</heading><p id="p0092" num="0092">Next, a third embodiment will be described. The present embodiment is different from the First Embodiment with respect to a feature of using a microphone to estimate a user's respiratory state (presence or absence of a symptom of snoring).</p><p id="p0093" num="0093"><figref idrefs="f0007">FIG. 12</figref> is a block diagram mainly showing a functional configuration of the sleep state estimation device 30 of the present embodiment. In <figref idrefs="f0007">FIG. 12</figref>, portions like or similar to those shown in <figref idrefs="f0002">FIG. 4</figref> are denoted by the same reference numbers and symbols, and detailed explanations are omitted. Portions different from <figref idrefs="f0002">FIG. 4</figref> will be mainly explained.</p><p id="p0094" num="0094">The sleep state estimation device 30 further includes a microphone 301 besides the system configuration of the First Embodiment, as shown in <figref idrefs="f0007">FIG. 12</figref>.</p><p id="p0095" num="0095">The microphone 301 is used to estimate the user's respiratory state during sleep, and senses a sound signal produced in response to the user's respiratory state. According to the microphone 301, sound around the sleep state estimation device 30 can be converted into an electrical signal (sound signal).</p><p id="p0096" num="0096"><figref idrefs="f0008">FIG. 13</figref> is a block diagram mainly showing a<!-- EPO <DP n="25"> --> functional configuration of the sleep state estimation device 30 of the present embodiment. In <figref idrefs="f0008">FIG. 13</figref>, portions like or similar to those shown in <figref idrefs="f0004">FIG. 7</figref> are denoted by the same reference numbers and symbols, and detailed explanations are omitted. Portions different from <figref idrefs="f0004">FIG. 7</figref> will be mainly explained.</p><p id="p0097" num="0097">The sleep state estimation device 30 includes a sound signal detector 311, a storage 312 and a respiratory state estimation module 313 besides the acceleration signal detector 111 and the body position estimation module 112 of the First Embodiment, as shown in <figref idrefs="f0008">FIG. 13</figref>.</p><p id="p0098" num="0098">The sound signal detector 311 is a functional module implemented by the microphone 301, and detects the sound signal by using the microphone 301. The sound signal detected by the sound signal detector 311 includes, for example, a signal of a respiratory sound produced by the user's symptom of snoring during sleep (hereinafter called a snoring signal), other noise or the like as the sound signal produced in response to the user's respiratory state.</p><p id="p0099" num="0099">The storage 312 stores information on a plurality of processing methods (contents) for estimating the user's respiratory rate (for example, presence or absence of a symptom of snoring), based on the sound signal detected by the sound signal detector 311. In other words, the processing methods corresponding to the respective user's body positions estimated by the body position estimation module 112 are defined in the storage 312. The processing method defined in the storage 312 includes, for example, a processing method for removing noise from the sound signal detected by the sound signal detector 311, or the like. As the plurality of processing methods (contents) for removing the noise, for example, a plurality of arbitrary, publicly known techniques employed for removal of noise from an acoustic signal, such as an infinite impulse<!-- EPO <DP n="26"> --> response (IIR) filter, an adaptive filter and nonnegative matrix factorization (NMF) can be employed. Alternatively, a plurality of parameters (for example, filter coefficients) used in the at least one of the methods may be prepared.</p><p id="p0100" num="0100">In addition, correspondence between the plurality of processing methods (contents) and the user's body positions may be varied in accordance with a positional relationship with a noise source in an environment in which the sleep state estimation device 30 is used. More specifically, for example, in an environment in which a TV is located on a user's right side and an air-conditioner is located on a user's left side, a processing method suitable for removing noise from the TV is employed when the user's body position is a right-side lateral position while a processing method suitable for removing noise from the air-conditioner is employed when the user's body position is a left-side lateral position.</p><p id="p0101" num="0101">The respiratory state estimation module 313 determines (selects) the processing method corresponding to the user's body position as estimated by the body position estimation module 112, of the plurality of processing methods defined in the storage module 312. The respiratory state estimation module 313 estimates the user's respiratory state by executing processing (for example, noise removing processing) based on the determined processing method for the sound signal detected by the sound signal detector 311.</p><p id="p0102" num="0102">Next, an operation of the sleep state estimation device 30 attached to the user's chest, etc., to estimate the user's sleep state will be explained.</p><p id="p0103" num="0103">In the present embodiment, when acceleration signals are detected by the acceleration signal detector 111 (acceleration sensor 107a), the processing of estimating the user's body position shown in <figref idrefs="f0004">FIG. 8</figref> is executed, similarly to the First Embodiment.<!-- EPO <DP n="27"> --> Furthermore, processing of estimating the user's respiratory state during sleep (hereinafter referred to as respiratory state estimation processing) is executed besides body position estimation processing, in the present embodiment. The respiratory state estimation processing is assumed to be periodically executed during the user's sleep period, similarly to the above-explained body position estimation processing.</p><p id="p0104" num="0104">Next, a procedure of the respiratory state estimation processing of the present embodiment will be explained with reference to a flowchart of <figref idrefs="f0008">FIG. 14</figref>.</p><p id="p0105" num="0105">In the respiratory state estimation processing, the sound signal detector 311 detects the sound signal indicating the sound around the sleep state estimation device 30 by using the microphone 301 (step S31).</p><p id="p0106" num="0106">When the sound signal is thus detected by the sound signal detector 311, the respiratory state estimation module 313 obtains the user's body position estimated by the body position estimation processing, from the body position estimation module 112 (step S32).</p><p id="p0107" num="0107">When the sound signal is detected by using the microphone 301 in the sleep state estimation device 30 of the present embodiment, the type of the noise included in the sound signal is changed by change in the orientation of the sleep state estimation device 30. In the present embodiment, the respiratory state is estimated from the sound signal in the present embodiment, but an optimum processing method (algorithms, parameters, etc.) for estimating the respiratory state is changed when the type of the noise included in the sound signal is changed. Thus, in the present embodiment, the processing method for removing the noise included in the sound signal is changed in accordance with the estimated user's body position.</p><p id="p0108" num="0108">In this case, the respiratory state estimation module 313 determines the processing method<!-- EPO <DP n="28"> --> corresponding to the user's body position obtained in step S32 (i.e., processing method for appropriately removing the noise estimated to be included in the sound signal in accordance with the user's body position), by referring to the storage 312 (step S33).</p><p id="p0109" num="0109">More specifically, for example, when the TV is located on the user's right side as explained above, and when the user's body position is the right-side lateral position, the sound signal detected by the sound signal detector 311 may include noise caused by sound from the TV besides the snoring signal. In this case, the processing method for removing the noise caused by the sound from the TV is determined as the processing method corresponding to the user's body position.</p><p id="p0110" num="0110">Next, the respiratory state estimation module 313 removes the noise by executing processing (noise removing processing) based on the determined processing method for the sound signal detected by the sound signal detector 311, and estimates the user's respiratory state based on the sound signal from which the noise is removed (step S34). In this case, when the sound signal from which the noise is removed is analyzed and the sound signal is determined to include the snoring signal, it can be estimated that the user has the symptom of snoring.</p><p id="p0111" num="0111">When the user's respiratory status is thus estimated, the respiratory state estimation module 313 outputs the estimation result to, for example, an external Bluetooth-enabled device, a server device, or the like (step S35).</p><p id="p0112" num="0112">Presence or absence of a symptom of snoring alone is explained for convenience but, for example, the degree of snoring, etc., in the symptom of snoring may be included as the estimation result on the user's respiratory state, as explained in the First Embodiment.<!-- EPO <DP n="29"> --></p><p id="p0113" num="0113">When the above-explained respiratory state estimation processing is executed, the processing method determined in step S33 is assumed to be held in, for example, the sleep state estimation device 30. According to this, when the respiratory state estimation processing is executed again but the user's body position is not changed, the user's respiratory state can be estimated based on the processing method held in the sleep state estimation device 30, without determining the processing method again.</p><p id="p0114" num="0114">In the present embodiment, as explained above, the user's respiratory state (presence or absence of a symptom of snoring) is estimated by detecting the sound signal produced in response to the user's respiratory state as the signal for estimating the user's respiratory state and by executing the processing corresponding to the user's body position estimated based on the acceleration signals. In other words, in the present embodiment, accuracy in estimation of the user's sleep state (respiratory state) can be enhanced since the respiratory state can be estimated by using the optimum algorithm, parameter, etc., corresponding to the user's body position by varying the processing method for estimating the respiratory state in accordance with the user's body position.</p><p id="p0115" num="0115">The present embodiment is explained on the assumption that the processing method for estimating the user's respiratory state is uniquely determined in accordance with the user's body position, but the processing method often cannot be uniquely determined in accordance with the user's body position (i.e., a plurality of processing methods correspond to the user's body position). In this case, all types of processing (noise removing processing) are executed in a plurality of processing methods according to the user's body position, and the processing method which obtains a most desirable processing result of all the<!-- EPO <DP n="30"> --> processing results is adopted, as explained in the Second Embodiment. The desirability of the processing result in this case is evaluated based on, for example, whether the shape of the noise-removed frequency spectrum falls within a predetermined range or not, etc. In this structure, the user's respiratory state can be estimated by using the optimum algorithm, parameter, etc. even if the processing method cannot be uniquely determined based on the user's body position.</p><p id="p0116" num="0116">In addition, in the present embodiment, the acceleration sensor module 107 may not includes the high-pass filter 107b or the amplifier circuit 107c since the alternating-current component does not need to be extracted from the acceleration signal. In addition, the acceleration sensor 107a of the present embodiment may output the analog signal or may output the digital signal, similarly to the Second Embodiment.</p><p id="p0117" num="0117">Use of the sleep state estimation device 30 attached to the user's chest is explained in the present embodiment, but the sleep state estimation device 30 may be attached to the other site if, for example, the user's body position during sleep can be estimated with the acceleration signals sensed by the acceleration sensor 107a and the user's respiratory state during sleep can be estimated with the sound signal sensed by the microphone 301.</p><p id="p0118" num="0118">Moreover, the sleep state estimation device 30 including the body position estimation module 112 and the respiratory state estimation module 313 is explained in the present embodiment, but the processing executed by the body position estimation module 112 and the respiratory state estimation module 313 may be executed by, for example, an external Bluetooth-enabled device, a server device or the like.</p><p id="p0119" num="0119">The user's respiratory state or heart rate is estimated as the user's state other than the body position in each of the above-explained embodiments<!-- EPO <DP n="31"> --> but, for example, if the estimation accuracy can be enhanced by using the acceleration signals (or estimation results of user's body positions) sensed by the acceleration sensor 107a, each of the embodiments may be applied to a case where a state other than these states is estimated. In addition, the electrocardiographic sensor 106a, the acceleration sensor 107a and the microphone 301 are used in each of the above-explained embodiments but, if the accuracy in estimation of the user's respiratory state can be enhanced by combining (signals detected by) a plurality of sensors, each of the embodiments may be applied to a case of using a sensor other than these sensors.</p><p id="p0120" num="0120">In each of the above-explained embodiments, the sleep state estimation device is attached by the electrically conductive gel 14 having elasticity, but the sleep state estimation device may be attached to the user in the other methods.</p><p id="p0121" num="0121">Since the processing of each of the above-explained embodiments can be implemented by a computer program, the same advantage as those of the embodiments can easily achieved by installing the computer program in a computer and executing the computer program.</p><p id="p0122" num="0122">According to at least one of the above-explained embodiments, a sleep state estimation device, a method and a storage medium capable of enhancing the accuracy in estimation of the user's respiratory state can be provided.</p><p id="p0123" num="0123">While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel embodiments described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the embodiments described herein may be made without departing from the spirit of the inventions. The accompanying claims and their<!-- EPO <DP n="32"> --> equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions.</p></description><claims mxw-id="PCLM90460615" lang="EN" load-source="patent-office"><!-- EPO <DP n="33"> --><claim id="c-en-0001" num="0001"><claim-text>A sleep state estimation device (10) attached to a user during sleep and used, <b>characterized by</b> comprising:
<claim-text>a first detector (111) configured to detect a first signal to estimate a first state of the user;</claim-text>
<claim-text>a first estimation module (112) configured to estimate the user's first state, based on the first signal;</claim-text>
<claim-text>a second detector (113) configured to detect a second signal to estimate a second state of the user other than the user's first state; and</claim-text>
<claim-text>a second estimation module (114) configured to estimate the user's second state by using the second signal, based on an estimation method determined in accordance with the first signal.</claim-text></claim-text></claim><claim id="c-en-0002" num="0002"><claim-text>The device of Claim 1, <b>characterized in that</b><br/>
the first detector is further configured to detect an acceleration signal indicating acceleration acting on the device, and<br/>
the first estimation module is further configured to estimate a body position of the user, based on the acceleration signal.</claim-text></claim><claim id="c-en-0003" num="0003"><claim-text>The device of Claim 2, <b>characterized in that</b><br/>
the first detector is further configured to detect first to third acceleration signals indicating accelerations in directions of first to third axes orthogonal to each other, respectively,<br/>
the second detector is further configured to detect alternating-current components of the first to third acceleration signals from the respective first to third acceleration signals, and<br/>
the second estimation module is further configured to weight the alternating-current components of the first to third acceleration signals in accordance with direct-current components of the first to third<!-- EPO <DP n="34"> --> acceleration signals, respectively, and estimate a respiratory state of the user, based on the weighted alternating-current components of the first to third acceleration signals.</claim-text></claim><claim id="c-en-0004" num="0004"><claim-text>The device of Claim 3, <b>characterized in that</b><br/>
the second detector includes an amplifier (107c) configured to amplify each of the alternating-current components of the first to third acceleration signals.</claim-text></claim><claim id="c-en-0005" num="0005"><claim-text>The device of Claim 3, <b>characterized by</b> further comprising<br/>
an electrically conductive material (14) having elasticity which allows the device to be attached to the user.</claim-text></claim><claim id="c-en-0006" num="0006"><claim-text>The device of Claim 2, <b>characterized in that</b><br/>
<br/>
the second detector (211) is further configured to detect an electrocardiographic signal indicating electrocardiogram of the user, and<br/>
the second estimation module (213) is further configured to estimate a heart rate of the user by executing a processing corresponding to the user's body position estimated based on the acceleration signal, of a plurality of processing for estimating the user's heart rate, based on the electrocardiographic signal.</claim-text></claim><claim id="c-en-0007" num="0007"><claim-text>The device of Claim 2, <b>characterized in that</b><br/>
the second detector (311) is further configured to detect a sound signal produced in accordance with a respiratory state of the user, and<br/>
the second estimation module (313) is further configured to estimate the user's respiratory state by executing a processing corresponding to the user's body position estimated based on the acceleration signal, of a plurality of processing for estimating the user's respiratory state, based on the sound signal.</claim-text></claim><claim id="c-en-0008" num="0008"><claim-text>A sleep state estimation method executed by a sleep state estimation device (10) attached to a user during sleep and used, the method <b>characterized by</b> comprising:<!-- EPO <DP n="35"> -->
<claim-text>detecting a first signal to estimate a first state of the user;</claim-text>
<claim-text>estimating the user's first state, based on the first signal;</claim-text>
<claim-text>detecting a second signal to estimate a second state of the user other than the user's first state; and</claim-text>
<claim-text>estimating the user's second state by using the second signal, based on an estimation method determined in accordance with the first signal.</claim-text></claim-text></claim><claim id="c-en-0009" num="0009"><claim-text>A non-transitory computer-readable storage medium having stored thereon a computer program which is executable by a computer of a sleep state estimation device (10) comprising a first detector configured to detect a first signal to estimate a first state of a user during sleep, and a second detector configured to detect a second signal to estimate a second state of the user other than the user's first state, the computer program <b>characterized by</b> comprising instructions capable of causing the computer to execute functions of:
<claim-text>estimating the user's first state, based on the first signal; and</claim-text>
<claim-text>estimating the user's second state by using the second signal, based on an estimation method determined in accordance with the first signal.</claim-text></claim-text></claim></claims><drawings mxw-id="PDW20423233" load-source="patent-office"><!-- EPO <DP n="36"> --><figure id="f0001" num="1,2"><img id="if0001" file="imgf0001.tif" wi="97" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="37"> --><figure id="f0002" num="3,4"><img id="if0002" file="imgf0002.tif" wi="164" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="38"> --><figure id="f0003" num="5,6"><img id="if0003" file="imgf0003.tif" wi="162" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="39"> --><figure id="f0004" num="7,8"><img id="if0004" file="imgf0004.tif" wi="120" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="40"> --><figure id="f0005" num="9,10"><img id="if0005" file="imgf0005.tif" wi="137" he="233" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="41"> --><figure id="f0006" num="11"><img id="if0006" file="imgf0006.tif" wi="99" he="151" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="42"> --><figure id="f0007" num="12"><img id="if0007" file="imgf0007.tif" wi="165" he="172" img-content="drawing" img-format="tif"/></figure><!-- EPO <DP n="43"> --><figure id="f0008" num="13,14"><img id="if0008" file="imgf0008.tif" wi="129" he="233" img-content="drawing" img-format="tif"/></figure></drawings><search-report-data><doc-page id="srep0001" file="srep0001.tif" wi="164" he="233" type="tif"/><doc-page id="srep0002" file="srep0002.tif" wi="164" he="233" type="tif"/><doc-page id="srep0003" file="srep0003.tif" wi="164" he="233" type="tif"/></search-report-data><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
