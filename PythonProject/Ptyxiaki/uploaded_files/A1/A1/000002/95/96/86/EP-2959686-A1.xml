<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959686-A1" country="EP" doc-number="2959686" kind="A1" date="20151230" family-id="51390972" file-reference-id="312929" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160452490" ucid="EP-2959686-A1"><document-id><country>EP</country><doc-number>2959686</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14754311-A" is-representative="NO"><document-id mxw-id="PAPP193867948" load-source="patent-office" format="original"><country>EP</country><doc-number>14754311.0</doc-number><date>20140218</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193867949" load-source="docdb" format="epo"><country>EP</country><doc-number>14754311</doc-number><kind>A</kind><date>20140218</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162030939" ucid="JP-2013031106-A" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2013031106</doc-number><kind>A</kind><date>20130220</date></document-id></priority-claim><priority-claim mxw-id="PPC162030606" ucid="JP-2014000816-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>JP</country><doc-number>2014000816</doc-number><kind>W</kind><date>20140218</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1900003326" load-source="docdb">G02B  27/22        20060101ALI20160826BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900005858" load-source="docdb">H04N  13/04        20060101AFI20160826BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1900007225" load-source="docdb">H04N  13/02        20060101ALI20160826BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1627894506" load-source="docdb" scheme="CPC">H04N  13/279       20180501 FI20180612BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1645074701" load-source="docdb" scheme="CPC">H04N  13/31        20180501 LI20180504BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1645076851" load-source="docdb" scheme="CPC">H04N  13/296       20180501 LI20180504BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1645080219" load-source="docdb" scheme="CPC">H04N  13/275       20180501 LI20180504BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1987776469" load-source="docdb" scheme="CPC">G02B  27/225       20130101 LI20151122BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165548936" lang="DE" load-source="patent-office">STEREOSKOPISCHES BILDAUSGABESYSTEM</invention-title><invention-title mxw-id="PT165548937" lang="EN" load-source="patent-office">STEREOSCOPIC IMAGE OUTPUT SYSTEM</invention-title><invention-title mxw-id="PT165548938" lang="FR" load-source="patent-office">SYSTÈME DE SORTIE D'IMAGE STÉRÉOSCOPIQUE</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103312163" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>GEO TECHNICAL LAB CO LTD</last-name><address><country>JP</country></address></addressbook></applicant><applicant mxw-id="PPAR1103336882" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>GEO TECHNICAL LABORATORY CO., LTD.</last-name></addressbook></applicant><applicant mxw-id="PPAR1101652215" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Geo Technical Laboratory Co., Ltd.</last-name><iid>101208053</iid><address><street>1-26, Hakataekihigashi 3-chome Hakata-ku</street><city>Fukuoka-shi, Fukuoka 812-0013</city><country>JP</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103314084" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>KISHIKAWA KIYONARI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103317228" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>KISHIKAWA, KIYONARI</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647833" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>KISHIKAWA, KIYONARI</last-name><address><street>c/o GEO TECHNICAL LABORATORY CO. LTD. 1-26 Hakaraekihigashi 3-Chome Hakata-ku</street><city>Fukuoka-shi Fukuoka 812-0013</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103308208" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>TOMITAKA TSUBASA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103337255" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>TOMITAKA, Tsubasa</last-name></addressbook></inventor><inventor mxw-id="PPAR1101648049" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>TOMITAKA, Tsubasa</last-name><address><street>c/o GEO TECHNICAL LABORATORY CO. LTD. 1-26 Hakaraekihigashi 3-Chome Hakata-ku</street><city>Fukuoka-shi Fukuoka 812-0013</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103315513" load-source="docdb" sequence="3" format="epo"><addressbook><last-name>ADA MASAYA</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103331088" load-source="docdb" sequence="3" format="intermediate"><addressbook><last-name>ADA, MASAYA</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647643" load-source="patent-office" sequence="3" format="original"><addressbook><last-name>ADA, MASAYA</last-name><address><street>c/o GEO TECHNICAL LABORATORY CO. LTD. 1-26 Hakaraekihigashi 3-Chome Hakata-ku</street><city>Fukuoka-shi Fukuoka 812-0013</city><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103305867" load-source="docdb" sequence="4" format="epo"><addressbook><last-name>KIMURA TATSUJI</last-name><address><country>JP</country></address></addressbook></inventor><inventor mxw-id="PPAR1103330216" load-source="docdb" sequence="4" format="intermediate"><addressbook><last-name>KIMURA, TATSUJI</last-name></addressbook></inventor><inventor mxw-id="PPAR1101640850" load-source="patent-office" sequence="4" format="original"><addressbook><last-name>KIMURA, TATSUJI</last-name><address><street>c/o GEO TECHNICAL LABORATORY CO. LTD. 1-26 Hakaraekihigashi 3-Chome Hakata-ku</street><city>Fukuoka-shi Fukuoka 812-0013</city><country>JP</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101645964" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Eder, Christian</last-name><iid>101277731</iid><address><street>Eder Schieschke &amp; Partner mbB Patentanwälte Elisabethstrasse 34/II</street><city>80796 München</city><country>DE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="JP-2014000816-W"><document-id><country>JP</country><doc-number>2014000816</doc-number><kind>W</kind><date>20140218</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014129174-A1"><document-id><country>WO</country><doc-number>2014129174</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660788533" load-source="docdb">AL</country><country mxw-id="DS660702731" load-source="docdb">AT</country><country mxw-id="DS660708849" load-source="docdb">BE</country><country mxw-id="DS660623367" load-source="docdb">BG</country><country mxw-id="DS660624267" load-source="docdb">CH</country><country mxw-id="DS660708850" load-source="docdb">CY</country><country mxw-id="DS660702732" load-source="docdb">CZ</country><country mxw-id="DS660788535" load-source="docdb">DE</country><country mxw-id="DS660708851" load-source="docdb">DK</country><country mxw-id="DS660708852" load-source="docdb">EE</country><country mxw-id="DS660624882" load-source="docdb">ES</country><country mxw-id="DS660623368" load-source="docdb">FI</country><country mxw-id="DS660624268" load-source="docdb">FR</country><country mxw-id="DS660788536" load-source="docdb">GB</country><country mxw-id="DS660708853" load-source="docdb">GR</country><country mxw-id="DS660788537" load-source="docdb">HR</country><country mxw-id="DS660702733" load-source="docdb">HU</country><country mxw-id="DS660624269" load-source="docdb">IE</country><country mxw-id="DS660708854" load-source="docdb">IS</country><country mxw-id="DS660623369" load-source="docdb">IT</country><country mxw-id="DS660708863" load-source="docdb">LI</country><country mxw-id="DS660623370" load-source="docdb">LT</country><country mxw-id="DS660723367" load-source="docdb">LU</country><country mxw-id="DS660623379" load-source="docdb">LV</country><country mxw-id="DS660623380" load-source="docdb">MC</country><country mxw-id="DS660723368" load-source="docdb">MK</country><country mxw-id="DS660723369" load-source="docdb">MT</country><country mxw-id="DS660624887" load-source="docdb">NL</country><country mxw-id="DS660702751" load-source="docdb">NO</country><country mxw-id="DS660624888" load-source="docdb">PL</country><country mxw-id="DS660723370" load-source="docdb">PT</country><country mxw-id="DS660624270" load-source="docdb">RO</country><country mxw-id="DS660723387" load-source="docdb">RS</country><country mxw-id="DS660624889" load-source="docdb">SE</country><country mxw-id="DS660702734" load-source="docdb">SI</country><country mxw-id="DS660702752" load-source="docdb">SK</country><country mxw-id="DS660702753" load-source="docdb">SM</country><country mxw-id="DS660623381" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139076578" ref-ucid="WO-2014129174-A1" lang="EN" load-source="patent-office"><p num="0000">The object of this invention is to realize stereoscopic vision of an image using parallel projection. A terminal 300 provided with a display capable of stereoscopic vision by visually recognizing an image for the right eye and an image for the left eye by the right eye and the left eye, respectively, is prepared. As map data 211 to be a background, a result of parallel projection of a three-dimensional model is generated as image data for all the regions. At this time, by setting a projection condition giving parallax for the right eye/left eye, parallel projection data for the right eye/left eye can be prepared for all the regions. In images for the left eye/right eye generated as above, reference lines are vectors in different directions as arrows FAL and FAR. Therefore, if a user instructs scrolling to an arrow SR direction on a map, correction is made in the scrolling direction of each of the images for the left eye/right eye, and scrolling is performed in individual directions in each image. As a result, in a state in which appropriate scrolling can be made, stereoscopic vision using parallel projection can be realized.</p></abstract><abstract mxw-id="PA139543600" ref-ucid="WO-2014129174-A1" lang="EN" source="national office" load-source="docdb"><p>The object of this invention is to realize stereoscopic vision of an image using parallel projection. A terminal 300 provided with a display capable of stereoscopic vision by visually recognizing an image for the right eye and an image for the left eye by the right eye and the left eye, respectively, is prepared. As map data 211 to be a background, a result of parallel projection of a three-dimensional model is generated as image data for all the regions. At this time, by setting a projection condition giving parallax for the right eye/left eye, parallel projection data for the right eye/left eye can be prepared for all the regions. In images for the left eye/right eye generated as above, reference lines are vectors in different directions as arrows FAL and FAR. Therefore, if a user instructs scrolling to an arrow SR direction on a map, correction is made in the scrolling direction of each of the images for the left eye/right eye, and scrolling is performed in individual directions in each image. As a result, in a state in which appropriate scrolling can be made, stereoscopic vision using parallel projection can be realized.</p></abstract><abstract mxw-id="PA139076579" ref-ucid="WO-2014129174-A1" lang="FR" load-source="patent-office"><p num="0000">Cette invention a pour objet de réaliser une vision stéréoscopique d'une image à l'aide d'une projection parallèle. Un terminal 300 pourvu d'un dispositif d'affichage permettant une vision stéréoscopique par reconnaissance visuelle d'une image pour l'œil droit et d'une image pour l'œil gauche par l'œil droit et l'œil gauche, respectivement, est préparé. A titre de données de carte 211 devant constituer un fond, un résultat de projection parallèle d'un modèle tridimensionnel est généré à titre de données d'image pour toutes les régions. A ce moment, par réglage d'une condition de projection donnant une parallaxe pour l'œil droit/l'œil gauche, des données de projection parallèle pour l'œil droit/l'œil gauche peuvent être préparées pour toutes les régions. Dans des images pour l'œil gauche/l'œil droit générées de la manière décrite plus haut, des lignes de référence sont des vecteurs dans des directions différentes comme indiqué par des flèches FAL et FAR. En conséquence, si un utilisateur donne une instruction de défilement dans la direction d'une flèche SR sur une carte, une correction est effectuée dans la direction de défilement dans chacune des images pour l'œil gauche/l'œil droit, et un défilement est effectué dans des directions individuelles dans chaque image. En résultat, dans un état dans lequel un défilement approprié peut être effectué, une vision stéréoscopique peut être réalisée à l'aide d'une projection parallèle.</p></abstract><abstract mxw-id="PA139543601" ref-ucid="WO-2014129174-A1" lang="FR" source="national office" load-source="docdb"><p>Cette invention a pour objet de réaliser une vision stéréoscopique d'une image à l'aide d'une projection parallèle. Un terminal 300 pourvu d'un dispositif d'affichage permettant une vision stéréoscopique par reconnaissance visuelle d'une image pour l'œil droit et d'une image pour l'œil gauche par l'œil droit et l'œil gauche, respectivement, est préparé. A titre de données de carte 211 devant constituer un fond, un résultat de projection parallèle d'un modèle tridimensionnel est généré à titre de données d'image pour toutes les régions. A ce moment, par réglage d'une condition de projection donnant une parallaxe pour l'œil droit/l'œil gauche, des données de projection parallèle pour l'œil droit/l'œil gauche peuvent être préparées pour toutes les régions. Dans des images pour l'œil gauche/l'œil droit générées de la manière décrite plus haut, des lignes de référence sont des vecteurs dans des directions différentes comme indiqué par des flèches FAL et FAR. En conséquence, si un utilisateur donne une instruction de défilement dans la direction d'une flèche SR sur une carte, une correction est effectuée dans la direction de défilement dans chacune des images pour l'œil gauche/l'œil droit, et un défilement est effectué dans des directions individuelles dans chaque image. En résultat, dans un état dans lequel un défilement approprié peut être effectué, une vision stéréoscopique peut être réalisée à l'aide d'une projection parallèle.</p></abstract><description mxw-id="PDES78479334" ref-ucid="WO-2014129174-A1" lang="EN" load-source="patent-office"><invention-title lang="EN">STEREOSCOPIC IMAGE OUTPUT SYSTEM</invention-title><technical-field><p num="0001">
              The present invention relates to a stereoscopic image output system for performing stereoscopic vision by outputting an image for the right eye and an image for the left eye with parallax.
</p></technical-field><background-art><p num="0002">
              In an electronic map used in a navigation apparatus, a computer screen and the like, a three-dimensional map expressing features such as a building in a three-dimensional manner is used in some cases.  The three-dimensional map is usually expressed by drawing a three-dimensional model in a three-dimensional manner by using a perspective projection and the like.<br/>
              Here, the three-dimensional map includes a large number of features, and the number of three-dimensional models is also large and thus, a load on drawing processing of the three-dimensional map might become extremely high.  As a method for reducing such load, Patent Literature 1 discloses a technology in which a projection image obtained by parallel projection of a three-dimensional model in advance is constructed as a two-dimensional image database, and the three-dimensional map is displayed by using it.
</p><p num="0003">
              On the other hand, a technology of stereoscopic vision by displaying an image for the right eye and an image for the left eye with parallax has spread in recent years, and a display for realizing such stereoscopic vision has begun to be utilized widely. <br/>
              Patent Literatures 2 and 3 are technologies relating to such stereoscopic vision.  Patent Literature 2 discloses a technology of realizing stereoscopic vision by applying correction processing for eliminating a perspective of an image obtained by perspective projection.  Patent Literature 3 discloses a technology of realizing stereoscopic vision by shifting a two-dimensional object such as an icon to right or left so as to give parallax. <br/>
              By applying these stereoscopic vision technologies, a depth feeling of the three-dimensional map can be felt more real, and usability of the three-dimensional map can be improved.
</p></background-art><citation-list><patent-literature><p num="0004"><patcit num="1"><text>Japanese Unexamined Patent Application Publication No. 2012-150823</text></patcit><patcit num="2"><text>Japanese Unexamined Patent Application Publication No. 2009-211718</text></patcit><patcit num="3"><text>Japanese Unexamined Patent Application Publication No. 2012-174237</text></patcit></p></patent-literature></citation-list><?BRFSUM description="Brief Summary" end="lead"?><summary-of-invention><tech-problem><p num="0005">
              However, in order to realize stereoscopic vision, two virtual cameras for right eye and left eye need to be installed at viewpoint of the three-dimensional map so as to generate an image for the left eye and an image for the right eye by performing perspective projection by the respective cameras, whereby a processing load twice the usual perspective projection is required.  Since the three dimensional map might be displayed on a terminal with low processing capacity such as a navigation apparatus or a portable terminal, an increase in the processing load for drawing is an unignorable problem.<br/>
              In order to reduce the processing load, realization of stereoscopic vision by using an image obtained by parallel projection can be considered, but an image obtained by projecting a three-dimensional model can not realize stereoscopic vision only by being shifted to right or left, unlike a two-dimensional object.  Moreover, as long as it is used as a map, even when a center point in map display is moved and the map is scrolled, an image for the left eye and an image for the right eye should be fused so that stereoscopic vision is realized.<br/>
              Such problem similarly occurs not only when a map is displayed electronically but also when an arbitrary spot in the map is segmented, and an image for the left eye and an image for the right eye are printed so as to realize stereoscopic vision.  Moreover, this is a common problem not only in maps but also in an output of an image capable of stereoscopic vision of various three-dimensional models such as a virtual space generated by computer graphics and the like and design models for machines, buildings and the like.  The present invention is made in view of these problems and has an object to realize stereoscopic vision of an image using parallel projection. 
</p></tech-problem><tech-solution><p num="0006">
The present invention is a stereoscopic image output system for stereoscopic vision of an image which can be constituted to include:<br/>
a stereoscopic vision output unit for realizing stereoscopic vision by outputting an image for the left eye and an image for the right eye with parallax that can be visually recognized by the left eye and the right eye, respectively;<br/>
an image database storage unit for storing parallel projection data for the left eye and parallel projection data for the right eye as two-dimensional drawing data obtained by projecting a three-dimensional model to be a target of stereoscopic vision on a plane by parallel projection from diagonal directions inclined from a vertical direction by predetermined projection angles respectively set so as to generate parallax between the left and right eyes; and<br/>
an output control unit for outputting the image for the left eye and the image for the right eye with parallax for outputting an image in a specified output range to the stereoscopic vision output unit on the basis of the parallel projection data for the left eye and the parallel projection data for the right eye, in which<br/>
if the output range moves, the output control unit determines moving directions of the image for the left eye and the image for the right eye individually, reads the parallel projection data for the left eye and the parallel projection data for the right eye in accordance with the moving directions and outputs the image for the left eye and the image for the right eye.
</p><p num="0007">
              According to the present invention, by individually setting the projection angle so as to give parallax to the left and right eyes, stereoscopic vision can be realized also in parallel projection.  That is, in the parallel projection, projections are made in the diagonal directions inclined from the vertical direction in order to perform projection with a three-dimensional feeling.  This projection direction can be expressed by a pitch angle indicating an inclination to a first reference direction from the vertical direction and a yaw angle indicating an inclination to a plane including the vertical angle and the first reference direction.  By inverting the signs of the yaw angles of the left and right eyes, parallax can be given to the both eyes.  In the parallel projection, there is no concept of a viewpoint, and a virtual camera positions corresponding to the left and right eyes can not be set, but as described above, by making the projection direction different between the left and right eyes, stereoscopic vision can be realized.<br/>
              Since there is no specific viewpoint in the parallel projection, a projection image, that is, parallel projection data can be generated in advance for all the regions.  When stereoscopic vision is to be realized, too, since images for the left and right eyes are generated by parallel projection, respectively, such advantage is not lost.  Therefore, according to the present invention, only by reading and outputting the parallel projection data for the left and right eyes prepared in advance as appropriate, there is an advantage that stereoscopic vision can be realized with a light load.
</p><p num="0008">
              In the present invention, while stereoscopic vision is realized by parallel projection as above, moving directions of the image for the left eye and the image for the right eye are further determined individually when the output range is moved.  In the present invention, since the projection direction of the parallel projection is different between the left eye and the right eye, even if a coordinate value in the image for the left eye and a coordinate value in the image for the right eye match each other, the coordinate values do not necessarily indicate the identical spot.  Therefore, when the output ranges of the images are to be moved, if the image for the left eye and the image for the right eye are both moved in the same direction by the same moving amount, a phenomenon that different output ranges are outputted for the left and right eyes occurs.  In the present invention, based on the feature of the images for the left and right eyes generated by the parallel projection, by individually determining the moving directions for the image for the left eye and the image for the right eye, occurrence of a discrepancy in the output ranges between the both images can be suppressed, and stereoscopic vision can be continued.
</p><p num="0009">
              As a three-dimensional model to be a target of stereoscopic vision in the present invention, various three-dimensional models including a map, a virtual space generated in computer graphics and the like and design models such as machines, buildings and the like can be considered.  Particularly, as the three-dimensional model in the case of stereoscopic vision of a map, a model three-dimensionally expressing a shape of a feature such as an artificial object, a natural object and the like is applicable.<br/>
              Moreover, as the stereoscopic output unit of the present invention, for example, a stereoscopic display displaying the image for the left eye and the image for the right eye so that they can be individually recognized by the left and right eyes, respectively, can be used.  A printing apparatus for arranging and printing the image for the left eye and the image for the right eye such that stereoscopic vision is realized by being placed behind a lens for stereoscopic vision called a lenticular may be used.  It may be a printing apparatus for printing by simply aligning the image for the left eye and the image for the right eye on the left and right.
</p><p num="0010">
              In the present invention, the moving directions of the image for the left eye and the image for the right eye can be determined by various methods, for example,<br/>
              the output control unit may determine the moving directions of the image for the left eye and the image for right eye on the basis of parameters of the parallel projection.<br/>
              A difference in the moving directions between the image for the left eye and the image for the right eye when the output ranges are moved is caused by a difference in the parameters of the parallel projection in the both images.  Therefore, by considering the parameters of the parallel projection, the moving directions for matching the output ranges of the image for the left eye and the image for the right eye can be determined with accuracy.  The moving direction can be calculated by a method in which coordinate axes for the image for the left eye and the image for the right eye converted from the two-dimensional coordinate axis indicating a plane when seen vertically below on the basis of the parameters of the parallel projection are acquired and calculation is made on this coordinate conversion, for example.  The moving direction may be calculated for each movement of the output range but in the parallel projection, since the moving direction is common for all the regions, the moving directions for the left eye and the right eye may be acquired in advance.
</p><p num="0011">
              In the present invention,<br/>
              the image data storage unit further stores two-dimensional object data for outputting a two-dimensional object into the image; and<br/>
              the output control unit may output the image for the left eye and the image for the right eye with parallax by ofsetting the two-dimensional object data to left or right with respect to a predetermined output position.<br/>
              As the two-dimensional object, characters indicating the name of the feature and other three dimensional models, various symbols and the like are applicable.<br/>
              By giving parallax by a method different from the three-dimensional model to such a two-dimensional object, the two-dimensional object can be expressed in the stereoscopic image with a light load.  Moreover, since in the two-dimensional object, a depth feeling can be changed by changing a displacement amount in left or right, the depth feeling can be changed in accordance with the type of the object or the like, and versatile stereoscopic vision can be realized.<br/>
              In the above described aspect, the parallax may be given to the two-dimensional object when the image for the left eye and the image for the right eye are outputted.  Moreover, in a state including image data of the two-dimensional object arranged with the parallax between the left and right eyes in advance, parallel projection data for the left eye and parallel projection data for the right eye may be prepared.
</p><p num="0012">
              In the present invention, all of the above described various features do not necessarily have to be provided, and the present invention may be constituted by omitting a part of it or by combination as appropriate.<br/>
              Moreover, the present invention may be also constituted in various aspects other than the configuration as a stereoscopic image output system.<br/>
              For example, the present invention may be configured as an image data generating device for generating image data for a stereoscopic image output system which outputs a stereoscopic image by outputting an image for the left eye and an image for the right eye with parallax so that they can be visually recognized by the left eye and the right eye, respectively, provided with:<br/>
              a 3D image database storage unit for storing a three-dimensional model to be a target of a stereoscopic vision and two-dimensional object data for outputting a two-dimensional object in the stereoscopic image;<br/>
              a parallel projection unit for generating parallel projection data for the left eye and parallel projection data for the right eye as two-dimensional drawing data obtained by projecting the three-dimensional model on a plane by parallel projection from diagonal directions inclined from a vertical direction by predetermined projection angles respectively set so as to generate parallax between the left and right eyes by the unit of a mesh having a predetermined size;<br/>
              a two-dimensional object image data generating unit for generating two-dimensional object image data for the left eye and two-dimensional object image data for the right eye by giving parallax by ofsetting the two-dimensional object data to left or right with respect to a predetermined output position; and<br/>
              a superposition processing unit for generating image data for the left eye and image data for the right eye by superposing the generated two-dimensional object image data for the left eye and two-dimensional object image data for the right eye on the parallel projection data for the left eye and parallel projection data for the right eye at positions considering the parallax.<br/>
              According to such an aspect, the parallel projection data for the left and right eyes can be generated by parallel projection on the basis of the three-dimensional model.  Moreover, parallax can be given by ofsetting the two-dimensional object data to left or right so that the two-dimensional object image data for the left and right eyes can be generated.   Then, the image data for the left and right eyes in which the both are superposed can be also generated.  In this way, by reading and outputting data corresponding to the output range from the image data for the left and right eyes, a stereoscopic image including the two-dimensional object can be outputted with a light load.
</p><p num="0013">
              The present invention may be also configured as a stereoscopic image output method for outputting a stereoscopic image by a computer or may be configured as a computer program for allowing a computer to execute such output.  Moreover, the present invention may be also configured as a computer-readable recording medium recording such computer program.   As the recording medium, various mediums such as a flexible disk, a CD-ROM, a magneto-optical disk, an IC card, a ROM cartridge, a punch card, a printed matter on which codes such as a barcode are printed, an internal storage apparatus (memory such as a RAM and a ROM) and an external storage apparatus of a computer and the like can be used.<br/>
 
</p></tech-solution></summary-of-invention><?BRFSUM description="Brief Summary" end="tail"?><description-of-drawings><p num="0014"><figref num="1">Figs. 1 are explanatory diagrams illustrating a principle of stereoscopic vision by parallel projection.</figref><figref num="2">Figs. 2 are explanatory diagrams illustrating an example of parallel projection data for right eye/left eye.</figref><figref num="3">Figs. 3 are explanatory diagrams illustrating a method (1) for setting parallel projection parameters.</figref><figref num="4">Figs. 4 are explanatory diagrams illustrating a method (2) for setting the parallel projection parameter.</figref><figref num="5">Figs. 5 are explanatory diagrams illustrating a relationship between the parallel projection and a mesh.</figref><figref num="6">Figs. 6 are explanatory diagrams illustrating an influence given by the parallel projection to scrolling.</figref><figref num="7">Fig. 7 is an explanatory diagram illustrating a configuration of a stereoscopic map display system.</figref><figref num="8">Figs. 8 are explanatory diagrams illustrating arrangement of feature data.</figref><figref num="9">Fig. 9 is a flowchart of feature data generation processing.</figref><figref num="10">Fig. 10 is a flowchart of map display processing.</figref><figref num="11">Fig. 11 is a flowchart of scrolling processing.</figref><figref num="12">Figs. 12 are explanatory diagrams illustrating arrangement examples of a character image in a variation.</figref><figref num="13">Fig. 13 is a flowchart of feature data generation processing in the variation.</figref><figref num="14">Figs. 14 are explanatory diagrams illustrating a feature data example in the variation.</figref></p></description-of-drawings><description-of-embodiments><embodiments-example ex-num="1"><p num="0015">
              Regarding the present invention, an example configured as a stereoscopic map display system for displaying a stereoscopic map as an example in stereoscopic images will be described.  Here, a system for displaying a map in a three-dimensional manner and capable of stereoscopic vision by parallel projection and also for displaying characters and the like drawn on that will be exemplified capable of stereoscopic vision.  In a meaning not of a map drawn simply three-dimensionally but of a map capable of stereoscopic vision, a map displayed in the example will be referred to as a stereoscopic map.<br/>
              Moreover, while the map is displayed three-dimensionally, characters, symbols and the like are two-dimensional data, and thus, they might be also referred to as a two-dimensional object in the example below.
</p><p num="0016">
A. Principle of stereoscopic vision by parallel projection:<br/>
              First, a principle for realizing stereoscopic vision by parallel projection will be described.<br/>
              Figs. 1 are explanatory diagrams illustrating the principle of stereoscopic vision by parallel projection.  As illustrated in Fig. 1A, three axes are defined.  That is, the x-axis and the z-axis are defined on a horizontal plane, and the y-axis is defined in a vertical lower direction.  The x-axis, y-axis, and z-axis are a right-handed coordinate system.  As illustrated, a camera is placed vertically above a feature, and parallel projection is performed so that a two-dimensional map is drawn.  The parallel projection referred to in this example is projection with the camera inclined from this state.<br/>
              In the parallel projection, there is no viewpoint, unlike perspective projection.  In this description, if a term of a camera is used in relation with parallel projection, a projection direction is schematically indicated.<br/>
              In Fig. 1A, if the camera is rotated around the x-axis, it is equal to parallel projection with diagonal inclination from the vertical direction, so the rotation around the x-axis indicates a pitch angle.   Moreover, if the camera is rotated in the y-axis direction, it changes an azimuth of the parallel projection in a horizontal direction, so the rotation around the y-axis indicates a projection azimuth.  Then, if the camera is rotated around the z-axis, parallax can be given as described below.<br/>
              Fig. 1B illustrates a reason why parallax is generated.  Fig. 1B is a state in which a feature is seen in the z-axis direction, that is, in a state in which the z-axis is perpendicular to the figure plane.  The parallax is a difference in visual-line directions generated by a difference in positions of the right eye and the left eye when this feature is seen in the y-axis direction from vertically above.  Therefore, by projecting at a camera position CR corresponding to a state seen from the right eye and a camera position CL corresponding to a state seen from the left eye to a reference camera position CC in the figure, images for the right eye/left eye with parallax can be generated.  The parallax, that is, a rotation angle Delta around the z-axis can be set arbitrarily, but as an angle that can give parallax without sense of discomfort, approximately 20 degrees can be set, for example.<br/>
              By performing parallel projection with consideration to the parallax illustrated in Fig. 1B in addition to the projection angle and the projection azimuth as described above, images for the right eye/left eye capable of stereoscopic vision can be generated even by the parallel projection.
</p><p num="0017">
              Figs. 2 are explanatory diagrams illustrating an example of parallel projection data for the right eye/left eye.  Fig. 2A illustrates parallel projection data for the right eye, and Fig. 2B illustrates parallel projection data for the left eye.  In the respective images, the features are displayed three-dimensionally by parallel projection.  For example, if a region A1 and a region B1 as well as a region A2 and a region B2 are compared, respectively, a difference in parallax for the right eye/left eye can be recognized from how a side wall of a building is drawn or the like.  By using the parallel projection data for the right eye/left eye prepared as above, a three-dimensional map can be viewed three-dimensionally.
</p><p num="0018">
              B. Influence by parallel projection:<br/>
              Figs. 3 are explanatory diagrams illustrating a method (1) for setting parallel projection parameters.<br/>
              Fig. 3A illustrates an example of a three-dimensional model.  Here, as illustrated, a model of three-dimensional shapes of alphabets A to I, each having a height, will be described as an example.<br/>
              Fig. 3B illustrates a state of parallel projection if a camera CA is set perpendicularly downward.  As illustrated, the camera CA for parallel projection is set just above a region F1 including the alphabet E at the center by being directed vertically downward.  As described above, since there is no specific viewpoint in parallel projection, the camera CA illustrated in Fig. 3B only schematically illustrates a projection direction of the parallel projection.<br/>
              Fig. 3C illustrates a projection result obtained by the parallel projection in Fig. 3B.  By projection perpendicularly downward, a planar projection result is obtained as illustrated in Fig. 3C.  That is, so-called two-dimensional map is obtained with respect to the three-dimensional model as illustrated in Fig. 3A.
</p><p num="0019">
              Fig. 3D illustrates a state in which the camera CA is inclined diagonally to a direction of the alphabet B only by the pitch angle from the vertically downward.  As a result, a region F2 projected by the camera CA has a rectangular shape extended in an inclined direction according to the pitch angle.<br/>
              Fig. 3E illustrates a projection result if the camera CA is inclined as in Fig. 3D.
</p><p num="0020">
              Figs. 4 are explanatory diagrams illustrating a method (2) for setting parallel projection parameters.<br/>
              Fig. 4A illustrates a state obtained by shifting the yaw angle from the state inclined only by the pitch angle as in Fig. 3D, that is, a state in which the model is rotated only by an angle Delta illustrated in Fig. 1B.  By shifting the yaw angle as above, a projected region F3 has a parallelogram as illustrated.<br/>
              Fig. 4B illustrates a projection result obtained by parallel projection in a state of Fig. 4A.  By performing parallel projection from a diagonal direction by shifting the pitch angle and the yaw angle as above, the three-dimensional model can be expressed three-dimensionally.
</p><p num="0021">
              Fig. 4C illustrates an image for the left eye generated by the above described parallel projection, and Fig. 4D illustrates an image for the right eye.  In stereoscopic vision by parallel projection, parallax is given by inverting the signs of the yaw angles in Fig. 4B of the left and right eyes, as is known from a region surrounding the alphabet E in the figure, the projected region has a parallelogram in a diagonally left direction for the image for the left eye and a parallelogram in a diagonally right direction for the image for the right eye.
</p><p num="0022">
              Figs. 5 are explanatory diagrams illustrating a relationship between parallel projection and a mesh.  Map data is usually prepared by being divided into rectangular regions, each having a certain size called mesh.  Figs. 5 illustrate an influence if stereoscopic vision by parallel projection is applied to a map divided into the meshes as above.<br/>
              Fig. 5A illustrates a state in which a map divided into meshes is parallelly projected.  As described in Figs. 4, if being parallelly projected, a region projected by the camera CA becomes a parallelogram such as regions FA and FB.  Assuming that a result of parallel projection of a certain mesh is expressed as the region FA, the result of parallel projection of a mesh adjacent in an arrow D direction is expressed as the region FB.  That is, before the parallel projection, rectangular meshes are aligned, but after the parallel projection, each mesh is deformed into a parallelogram and causes a discrepancy as indicated between the regions FA and FB. <br/>
              Fig. 5B is an enlarged diagram of the regions FA and FB.  Only a shape of each region is illustrated.  As described above, a discrepancy having an offset amount OST is generated between sides SA and SB in contact with the regions FA and FB.  The distance OST is determined in accordance with parallel projection parameters, that is, a pitch angle PA and a yaw angle Delta at the parallel projection.  That is, a mesh of a unit length can be expressed as follows by a coordinate conversion of sequentially rotating only by the pitch angle PA and a yaw angle Delta:<br/>
              OST = sinDelta &amp;#8729; sinPA &amp;#8729; cosPA<br/>
              Therefore, if the regions FA and RB are to be aligned, shifting by the offset amount OST in an arrow A direction is necessary.<br/>
              Fig. 5C illustrates a state in which the regions FA and FB are arranged without alignment.  That is, the regions FA and FB are simply arranged, even though there is a discrepancy between them as illustrated in Fig. 5A.  Fig. 5A is to show the shape of the region, and positions of the alphabets A to I are illustrated in a state before the parallel projection, but actually, if parallel projection is performed for each mesh, a discrepancy is generated between images at a boundary B between the regions FA and FB as illustrated in Fig. 5C.<br/>
              The pitch angle PA and the yaw angle Delta used in the parallel projection can be set arbitrarily, but as parameters used in the parallel projection for realizing stereoscopic vision, it may be approximately set such that the pitch angle PA = 22.5 degrees and the yaw angle Delta = 20 degrees, for example.  If such parallel projection parameters are used, the discrepancy illustrated in Fig. 5C is known to be contained in approximately several pixels.
</p><p num="0023">
              Figs. 6 are explanatory diagrams illustrating an influence given by the parallel projection to scrolling.<br/>
              As illustrated in Fig. 6A, assume that a display range is moved from the alphabet B to E on a two-dimensional map, that is, in an arrow SR direction.<br/>
              Fig. 6B illustrates images for the left eye FAL and FBL and images for the right eye FAR and FBR, obtained by parallel projection for realizing stereoscopic vision.  In a state before scrolling, it is assumed that the alphabet B portion in the regions FAL and FAR illustrated in a lower side is displayed.  At this time, parallax between the image FAL for the left eye and the image FAR for the right eye is a distance L1, and it is assumed that fusion of the both is realized by this parallax, and stereoscopic vision is enabled.<br/>
              Subsequently, the display range is moved in the SR direction.  At this time, as illustrated in Fig. 4 above, it is assumed that the display range is simply moved in the arrow SR direction similarly to a two-dimensional plane, without considering that each of the images FAL and FBL for the left eye and the images FAR and FBR for the right eye has its original rectangular region projected to a parallelogram region by parallel projection.  Then, the display range should have been moved along the alignment of the alphabets B, E, H, and B, but in the images FAL and FBL for the left eye, many useless portions are included on the right side of the alignment, while in the images FAR and FBR for the right eye, many useless portions are included on the left side to the contrary.  As a result, in the alphabet B portion, for example, parallax L2 becomes larger than the initial parallax L1, fusion can not be achieved, and stereoscopic vision can not be realized.  That is, the alphabet B is visually recognized only in a doubly shifted state.  In order to avoid such a phenomenon, the moving direction of the display range needs to be set individually as arrows SRL and SRR, considering that the parallel projection is performed to a parallelogram in the images FAL and FBL for the left eye and the images FAR and FBR for the right eye, respectively.<br/>
              Such phenomenon is caused by preparation of the images FAL and FBL for the left eye and the images FAR and FBR for the right eye in advance for all the regions.  In this example, as described above, by setting the scrolling direction individually for the image for the left eye and the image for the right eye, scrolling is achieved while stereoscopic vision is realized.
</p><p num="0024">
              B. System configuration:<br/>
              Fig. 7 is an explanatory diagram illustrating a configuration of a stereoscopic map display system.<br/>
              Fig. 7 illustrates a configuration example for displaying a map on a display 300d of a terminal 300 on the basis of map data provided from a server 200 via a network NE2 or the like.  A smartphone is used as the terminal 300, but a cell phone, a personal computer, a navigation apparatus and the like may be also used.  Moreover, the three-dimensional stereoscopic map display system may be also configured as a system operating standalone other than the system composed of the terminal 300 and the server 200.<br/>
              In the figure, a data generating apparatus 100 for generating map data is also illustrated.<br/>
              The display 300d of the terminal 300 has a stereoscopic vision function which can display the image for the right eye and the image for the left eye so that they can be visually recognized by the right eye and the left eye, respectively.  In this example, the display 300d capable of stereoscopic vision by so-called naked eyes is used, but a device for stereoscopic vision by using glasses for stereoscopic vision and the like may be also used.
</p><p num="0025">
              In the terminal 300, various functional blocks operating under a main control unit 304 are constituted.  In this example, the main control unit 304 and each of the functional blocks are configured by installing software which realizes the respective functions, but a part of or the whole of them may be configured by using hardware.<br/>
              A transmission/reception unit 301 conducts communication with the server 200 via the network NE2.  In this example, transmission/reception of map data and commands for displaying a stereoscopic map is mainly conducted.<br/>
              A command input unit 302 inputs an instruction from a user via with an operation of a button or a touch panel and the like.  As an instruction in this example, specification of a display range, expansion/contraction of a three-dimensional map, setting of a place for departure and a destination when a route guidance is to be given and the like can be cited.<br/>
              A GPS input unit 303 obtains a coordinate value of latitude and longitude on the basis of a GPS (Global Positioning System) signal.  Moreover, in the route guidance, an advancing direction is calculated on the basis of a change in the latitude/longitude.<br/>
              A map information storage unit 305 is a buffer for temporarily storing map data provided from the server 200.  If a map to be displayed is continuously changing as in the case of a route guidance, map data of a range which is not sufficient with the map information storage unit 305 is received from the server 200 so as to display a map.<br/>
              A display control unit 306 displays a stereoscopic map on the display 300d of the terminal 300 by using the map data stored in the map information storage unit 305.<br/>
              A scrolling control unit 307 determines a scrolling direction with respect to each of an image for the left eye and an image for the right eye when a scrolling instruction is given from a user and realizes scrolling while stereoscopic vision is maintained.
</p><p num="0026">
              In the server 200, illustrated functional blocks are configured.  In this example, these functional blocks are configured by installing software which realizes the respective functions but a part of or the whole of them may be configured by hardware.<br/>
              A map database 210 is a database for displaying a stereoscopic map.  In this example, map data including feature data 211, a two-dimensional object 212, and network data 213 is stored.  The network data 213 can be omitted.<br/>
              The feature data 211 is parallel projection data for displaying features such as roads, buildings and the like three-dimensionally and capable of stereoscopic vision, and is two-dimensional polygon data obtained by parallelly projecting a three-dimensional model of a feature for the right eye/left eye, respectively, by changing a projection condition.  That is, as the feature data 211, parallel projection data for the right eye as two-dimensional image data obtained by parallel projection under the condition for the right eye and parallel projection data for the left eye as two-dimensional image data obtained by parallel projection under the condition for the left eye for one map region are stored.<br/>
              The two-dimensional object 212 is characters expressing a name of a feature, a name of a place, guidance information and the like which should be displayed on the map other than the features, map symbols/traffic restriction signs, symbol data indicating a current position in route guidance, polygon data of arrows constituting a route and the like.  Except those with unstable display positions such as the current position or route, the two-dimensional object 212 stores data such as characters and symbols which should be displayed and display positions associated with each other.  The display position may be a position on a three-dimensional space or may be a position coordinate on a projection image parallelly projected.  Moreover, regarding the two-dimensional object 212 associated with a specific feature such as the feature name, data indicating association with the feature is also stored.<br/>
              In this example, the two-dimensional object 212 is to give parallax at display, but as a variation, it may be configured capable of stereoscopic vision with parallax determined in advance.  In such a case, the two-dimensional object 212 may be stored in a format of the image for the right eye and the image for the left eye capable of stereoscopic vision.  Alternatively, image data in a state in which the feature data 211 and the two-dimensional object 212 are superposed can be stored as the feature data 211.<br/>
              The network data 213 is data expressing roads by a collection of nodes and links.  The node is data corresponding to an intersection of the roads and an end point of the road.  The link is a line segment connecting a node to another node and is data corresponding to the road.  In this example, positions of the node and the link constituting the network data 213 are determined by three-dimensional data of latitude, longitude, and height.<br/>
              The transmission/reception unit 201 conducts transmission/reception of data with the terminal 300 via the network NE2.  In this example, transmission/reception of the map data and the command for displaying the three-dimensional map is mainly conducted.  Moreover, the transmission/reception unit 201 also conducts communication with the data generating apparatus 100 via a network NE1.  In this example, send/receive of the generated map data is mainly conducted.<br/>
              The database control unit 202 controls reading and writing of data with respect to the map database 210.<br/>
              A route search unit 203 makes a route search by using the network data 213 in the map database 210.  For the route search, algorithm of Dijkstra or the like can be used.  As described above, arrows indicating a route obtained by the route search and the like also fall under the two-dimensional objects.
</p><p num="0027">
              In the data generating apparatus 100, illustrated functional blocks are configured.  In this example, these functional blocks are configured by installing software which realizes the respective functions in a personal computer, but a part of or the whole of them may be configured by using hardware.<br/>
              A transmission/reception unit 105 sends/receives data to/from the server 200 via the network NE1.<br/>
             A command input unit 101 inputs an instruction of the operator via a keyboard or the like.  In this example, the specifications of regions where the map data should be generated, specifications of the parallel projection parameters and the like are included.  The parallel projection parameters refer to the pitch angle and the yaw angle when parallel projection is performed.  The pitch angle means an amount of inclination from the vertical direction in projection.  The yaw angle means an angle (see Fig. 1) inclined in different directions for left and right eyes in order to give parallax.<br/>
              A 3D map database 104 is a database for storing the three-dimensional model used for generating the map data.  For features such as roads and buildings, electronic data indicating the three-dimensional shapes is stored.  Moreover, two-dimensional objects such as characters and symbols which should be displayed on the map are also stored.<br/>
              A parallel projection unit 102 generates feature data by drawing using parallel projection on the basis of the 3D map database 104.  The drawn projection image is stored as parallel projection data 103 and stored in the feature data 211 of the map database 210 of the server 200 via the transmission/reception unit 105.  A projection parameter modification unit 106 modifies the specified parallel projection parameters so that the sign of a yaw angle value is inverted when the parallel projection is performed, and the parallel projection parameters for the right eye/left eye are set.  In this way, the image for the right eye and the image for the left eye for stereoscopic vision can be generated, respectively.
</p><p num="0028">
              Figs. 8 are explanatory diagrams illustrating arrangement of the feature data.<br/>
              In Fig. 8A, a mesh of the 3D map data before parallel projection is indicated by a solid line, an image for the left eye by a broken line, and an image for the right eye by a one-dot chain line in superposition.  On the lower right, an enlarged view of two meshes is shown.  In this example, the map data before parallel projection is prepared for each mesh aligned in a rectangular region as illustrated in regions M1 and M2 in the enlarged view.  If each mesh is parallelly projected, each one is projected to a parallelogram region like images M1L and M2L for the left eye and images M1R and M2R for the right eye.  In this example, the image for the left eye and the image for the right eye each having a parallelogram shape generated by the unit of mesh as above are stored without being aligned as illustrated in Fig. 8A.  Therefore, if seen precisely, as illustrated in Fig. 5C, there is a discrepancy of approximately several pixels between the parallelogram regions.<br/>
              Fig. 8B is an arrangement example of the feature data as a variation.  In this example, a discrepancy generated between the parallelogram regions generated for each mesh are matched and arranged.  As a result, the meshes M1 and M2 aligned vertically in the figure before parallel projection are aligned in a diagonally left direction like the regions M1L and M2L for the image for the left eye and in a diagonally right direction like the regions M1R and M2R for the image for the right eye.  The feature data may be prepared in a format in which a discrepancy between the regions is matched as above.
</p><p num="0029">
              C. Feature data generation processing:<br/>
              Subsequently, processing for generating feature data 211, that is, two-dimensional polygon data obtained by parallelly projecting the three-dimensional model of the feature for the right eye/left eye by changing the projection condition, respectively, will be described.  This processing is processing executed mainly by the parallel projection unit 102 of the data generating apparatus 100 and processing executed by a CPU of the data generating apparatus 100 in terms of hardware.<br/>
              Fig. 9 is a flowchart of feature data generation processing.<br/>
              When the processing is started, the data generating apparatus 100 specifies a mesh to be processed and inputs the parallel projection parameters (Step S10).  As a method for specifying a mesh, an index unique to the mesh, a coordinate of the mesh and the like can be used.  A method in which the mesh containing a coordinate value of a point specified by the operator on the map is analyzed by the data generating apparatus 100, and the mesh is set as a mesh to be processed.<br/>
             The parallel projection parameters are a pitch angle and a projection azimuth.  At this stage, parallax, that is, a yaw angle is set to 0 degrees.  These may be specified by the operator each time the feature data is generated or a default value may be used.<br/>
              The projection azimuth may be any one azimuth but in this example, parallel projection is performed for each of 8 azimuths obtained by shifting the azimuth by 45 degrees each so as to generate the feature data.  By preparing the feature data in multiple azimuths as above, even if a blind spot such as behind a building is generated in any one of the projection azimuths, there is an advantage that display avoiding the blind spot can be realized by using the other projection azimuths.<br/>
              The parallel projection parameters can be arbitrarily set for either one of the pitch angle and the projection azimuth and can take various values as in a method of taking a single value, a method of using a plurality of values as parametric and the like.
</p><p num="0030">
              Subsequently, the data generating apparatus 100 reads the 3D map database for a target mesh and a mesh in a predetermined range around the target mesh (Step S20).  The reason for reading also the mesh in the predetermined range around it is as follows.<br/>
              In this example, the feature data is generated by parallelly projecting a three-dimensional model included in the 3D map database from a diagonal direction inclined by predetermined projection angles with respect to the vertical direction.  In the case of the parallel projection from the diagonal direction as above, a part of the feature present in the mesh around the mesh to be processed might be projected.  On the contrary, if the parallel projection is performed by using the 3D map database only for the target mesh, it lacks a projection of the feature present in another mesh, and appropriate feature data can not be obtained.  In order to avoid this, in this example, the mesh around the target mesh is also read.  The reading range can be arbitrarily set, but in this example, 3D map data belonging to meshes within 2 sections from the target mesh is read.
</p><p num="0031">
              Subsequently, the data generating apparatus 100 sets parallax for the left eye and parallax for the right eye, that is, the respective yaw angles (Step S30).  As illustrated in Fig. 1B above, the yaw angle is given as parallel projection parameters, and parallel projection is performed.  In order to give parallax, signs of directions of the parallax for the left eye and the parallax for the right eye are set opposite to each other.  This processing corresponds to processing of the projection parameter modification unit 106.  Moreover, the offset amount OST is calculated by using the parallel projection parameters set as above, that is, the pitch angle and the yaw angle.  The offset amount is, as illustrated in Fig. 5B, a numeral value indicating a discrepancy between the parallelogram regions corresponding to each of the meshes after the parallel projection and is used for determining a scrolling direction in the images for the right/left eye as will be described later.<br/>
              The data generating apparatus 100 generates the image for the left eye by performing parallel projection by using the parallax for the left eye set as above (Step S40) and generates the image for the right eye by performing parallel projection by using the parallax for the right eye (Step S50).  The images generated, respectively, are two-dimensional image data expressing the feature three-dimensionally by parallel projection.  The images for the left eye/right eye generated by parallel projection are called parallel projection data for the left eye/right eye in some cases.<br/>
              The data generating apparatus 100 segments a region corresponding to the target mesh from the images for the left eye/right eye obtained as above, respectively (Step S60), and stores it with the offset amount OST as the feature data made of the image data for the left eye and the image data for the right eye (Step S70).  The image data is stored as two-dimensional polygon data, but may also be stored as raster data.  Moreover, in segmenting and storing of the image data for the left eye/right eye, each polygon may be provided with attributes such as a name, a position, a shape and the like.<br/>
              By executing the above processing for all the projection azimuths and all the meshes, the data generating apparatus 100 can provide the feature data 211 of this example.
</p><p num="0032">
              D. Map display processing:<br/>
              D1. Entire processing:<br/>
              Fig. 10 is a flowchart of map display processing.  Here, an example of processing in which a map to be a background is displayed to be capable of stereoscopic vision in accordance with the spot and azimuth specified by the user, and a two-dimensional object such as a character and the like is displayed to be capable of stereoscopic vision in front of it is illustrated.  This processing can be also used as route guidance display by being used together with the route search.<br/>
              The map display processing is processing executed by the main control unit 304 and the display control unit 306 of the terminal 300 and is processing executed by a CPU of the terminal in terms of hardware.
</p><p num="0033">
              In this processing, first, the terminal 300 inputs a display spot, an azimuth, and a range specified by the user (Step S100).  The display spot may use a current position obtained by GPS, for example.<br/>
              Then, the terminal 300 executes scrolling processing in accordance with the specified display position and the like (Step S200).  Though contents of the scrolling processing will be described later, this is processing in which, if the specified display spot has been moved from the previous spot, that is, if scrolling is specified by the user, scrolling directions suitable for the image for the left eye and the image for the right eye are determined, and the display spot is set for each of the image for the left eye and the image for the right eye.
</p><p num="0034">
              The terminal 300 reads parallel projection data for the left eye/right eye from the map information storage unit 305 in accordance with a result of the scrolling processing (Step S300).  If data of a region not accumulated in the map information storage unit 305 is needed, the terminal 300 downloads the data from the server 200.<br/>
              Here, in the parallel projection data for the left eye/right eye, in this example, as illustrated in Fig. 8A, a discrepancy only by the offset amount OST is generated between each of the parallelogram regions.  Thus, when the parallel projection data for the left eye/right eye is read at Step S300, the terminal 300 arranges it with a shift only by the offset amount OST between the regions in a direction to solve this discrepancy.  As illustrated in the figure, since there is a discrepancy only by the offset amount OST between images for the left eye PL1 and PL2, the region PL2 is shifted to the left side only by the offset amount OST so as to match the region PL1.  Since there is a discrepancy also between images for the right eye PR1 and PR2, the region PR2 is shifted to the right side only by the offset amount OST so as to match the region PR1.
</p><p num="0035">
              Subsequently, the terminal 300 similarly reads data of the two-dimensional objects (characters, symbols (including map symbols/traffic restriction signs), current position, a route display and the like) from the map information storage unit 305 (Step S310).  Regarding the two-dimensional objects, only those required may be read depending on the function of the map and the like.<br/>
              Since a display position is specified in the three-dimensional space for the two-dimensional object, the terminal 300 applies coordinate conversion processing to the read two-dimensional object and moreover, gives parallax by shifting the position of the two-dimensional object to left or right for each of the image for the left eye and the image for the right eye so as to generate two-dimensional object images for the left eye/right eye (Step S320).<br/>
              The above described coordinate conversion can be achieved by rotating the three-dimensional position information around the y-axis only by a projection azimuth on the basis of the pitch angle and the projection azimuth as parallel projection parameters and then, by acquiring a coordinate conversion matrix for rotation around the x-axis only by the pitch angle and by having this coordinate conversion matrix act on the position of the two-dimensional object.
</p><p num="0036">
              The parallax for the left eye/right eye of the two-dimensional object can be made on the basis of setting of a display depth.  The display depth may be the same for all the two-dimensional objects or may be changed in accordance with the type of the two-dimensional objects.  For example, it may be so configured that priority is set in order of the character, symbol, and route display so that the characters are displayed on the foremost side and then, the symbols indicating the current position and traffic restriction, and the route display is displayed on the depth in this order.  Moreover, if the two-dimensional objects are overlapped, their display depths may be made different from each other.<br/>
              By setting the display depth as above, the two-dimensional object is set at a three-dimensional position corresponding to a set display depth h, and a rotation matrix by the parallax (yaw angle) Delta is made to act so as to obtain displacement of an x-coordinate, which becomes a shifting amount of the two-dimensional object for the right eye/left eye.  At this time, the shifting amount is a function of the display depth h and can be given by shifting amount = h&amp;#8729; tan Delta.
</p><p num="0037">
              Lastly, the terminal 300 displays the images for the left eye/right eye, that is, the feature and the two-dimensional object in a superposed manner (Step S330) and finishes the map display processing .  By recognizing the image for the right eye displayed as above by the right eye and the image for the left eye by the left eye, the user can stereoscopically view the map of the background and the two-dimensional object.<br/>
              The parallel projection data for the left eye/right eye is only two-dimensional polygon data after parallel projection has been performed, and thus, stereoscopic vision can be realized with a light load only by drawing a polygon in accordance with the obtained data without performing projection processing in the processing at Step S501.
</p><p num="0038">
              D2. Scrolling processing:<br/>
              Fig. 11 is a flowchart of the scrolling processing.  It is processing corresponding to Step S200 in the map display processing (Fig. 10) and corresponds to processing executed by the scrolling setting unit 307 of the terminal 300.<br/>
              When this processing is started, the terminal 300 inputs a scrolling vector (Step S210).  The scrolling vector is a vector in a moving direction of the map, that is, a vector going from the preceding display spot to a newly instructed display spot.  This scrolling vector corresponds to the moving direction in a three-dimensional model before parallel projection since the user with stereoscopic vision specifies a direction to be visually recognized on its stereoscopic map.
</p><p num="0039">
              The terminal 300 subsequently corrects a scrolling direction and sets scrolling vectors for the left eye/right eye (Step S220).<br/>
              A method of correcting the scrolling direction is illustrated in the figure.  In a diagram illustrated on the right side in the figure, assuming that the scrolling is specified in a vertical v-axis direction in the three-dimensional model, for example, since the mesh is changed to a parallelogram by parallel projection, the scrolling direction corresponding to the specified scrolling is a direction along a diagonal side in this parallelogram region.  That is, the scrolling direction in the v-axis direction needs to be corrected only by a correction angle GA.  This angle GA can be acquired on the basis of a ratio of a distance VM in the v-axis direction of the mesh to the offset amount OST.  That is, it is tan GA = OST/VM.  Since the correction angle GA is a value uniquely determined when the parallel projection parameters (pitch angle, yaw angle) are determined, it may be calculated in advance similarly to the offset amount OST.<br/>
              The terminal 300 corrects a scrolling vector VS on the basis of the correction angle GA as illustrated in a diagram on the left side.  That is, for the image for the right eye, a vector VSR in a direction obtained by rotating the scrolling vector VS clockwise only by the correction angle GA is a scrolling direction, while for the image for the left eye, a vector VSL in a direction obtained by rotation to the opposite direction only by the correction angle GA is a scrolling direction.<br/>
              The direction for scrolling needs to be corrected due to the influence of the parallel projection as above, but an advance distance in the v-axis direction needs to be matched for the left/right eyes.  Therefore, sizes of the scrolling vectors VSL and VSR for the images for the left eye/right eye are set so that the scrolling vectors VSL and VSR for the images for the left eye/right eye and a tip end of the scrolling vector VS are aligned on a straight line in parallel with the u-axis. 
</p><p num="0040">
              The terminal 300 sets the display spot after scrolling for each of the images for the left eye/right eye on the basis of the scrolling vectors VSL and VSR for the images for the left eye/right eye set by the processing above (Step S230) and finishes the scrolling processing.
</p><p num="0041">
              E. Effects:<br/>
              According to the stereoscopic map display system of this example described above, the stereoscopic vision can be realized by parallel projection.  In the parallel projection, the images for the left eye/right eye for stereoscopic vision can be generated for all the regions in advance regardless of a position of a viewpoint, and stereoscopic vision can be realized with an extremely light load.<br/>
              Moreover, in this example, since the scrolling direction is individually set for the images for the left eye/right eye when scrolling is instructed, an influence caused by deformation of the original mesh into a parallelogram by parallel projection can be avoided, and scrolling in a state in which stereoscopic vision is maintained can be realized.
</p><p num="0042">
              F. Variation:<br/>
              In the example, the feature data and the two-dimensional object are prepared separately, and a method of giving parallax to the two-dimensional object at the time when display is exemplified.  On the other hand, an aspect in which the images for the left eye/right eye are prepared in a state in which the two-dimensional object and the feature data are superposed may be employed.  An example using such method is illustrated as a variation.
</p><p num="0043">
             Figs. 12 are explanatory diagrams illustrating arrangement examples of a character image in the variation.  As described above, a problem occurring if the two-dimensional object is superposed on the feature data in advance will be described.<br/>
              Fig. 12A illustrates an influence of parallel projection to the two-dimensional object.  As illustrated on the left side in Fig. 12A, the meshes in the map are deformed into parallelogram regions M1 and M2 by parallel projection.  At the time of the parallel projection, the regions M1 and M2 are stored in a state in which a discrepancy with the offset amount OST is generated between them, as illustrated on the right side in the figure, but in the map display processing, this offset amount is corrected (see Step S300 in Fig. 10) and displayed in an aligned state as illustrated on the left side in Fig. 12A.<br/>
              In such a case, assume that the characters CH are displayed on a boundary portion between the regions M1 and M2 as a hatched portion in the figure.  The characters CH are assumed to be superposed as an image on the regions M1 and M2 in advance.  At this time, the characters CH should be displayed without a discrepancy on the boundary between the regions M1 and M2 as illustrated on the left side in Fig. 12A with the offset amount corrected.  Therefore, in the feature data of each of the regions M1 and M2 before the offset amount is corrected (state on the right side in Fig. 12A), characters CH1 and CH2 need to be in a state with a discrepancy by an amount corresponding to the offset amount OST on the boundary of the regions M2 and M2.<br/>
              That is, if the characters CH are drawn in a state in which the regions M1 and M2 are stored and superposed on the image data of the feature as illustrated in Fig. 12B, when the offset amount is corrected as in Fig. 12A, the characters CH would have a discrepancy between the boundaries.<br/>
              In the variation, while the offset amount is reflected at a display position of the character in the vicinity of the boundary as above, the feature data in which the image data of the feature and the image of the character are superposed is generated.
</p><p num="0044">
              Fig. 13 is a flowchart of the feature data generation processing in the variation. <br/>
              Processing for generating an image for the left eye and an image for the right eye expressing a feature by using 3D map data is the same as Steps S10 to S50 in the example (Fig. 9).  Subsequently, the data generating apparatus 100 reads the two-dimensional objects (characters, symbols) to be displayed in the map (Step S51).  Here, instead of the two-dimensional object dynamically moving when the map is displayed as the current position or the route guidance display, a two-dimensional object that can be prepared as map data in advance is a target.<br/>
              Then, the feature data generating apparatus 100 performs coordinate conversion of the two-dimensional object and generates two-dimensional object images for the left eye/right eye (Step S52).  This coordinate conversion is processing to convert a display position of the two-dimensional object to a display position in the images for the left eye/right eye on the basis of the parallel projection parameters.  The coordinate conversion also includes processing of shifting the display position of the two-dimensional object for the images for the left eye/right eye to left or right in order to give parallax.<br/>
              If the two-dimensional object image is generated for each of the images for the left eye/right eye, the feature data generating apparatus 100 arranges the images for the left eye/right eye, considering the offset amount OST, and pastes the two-dimensional object images for the left eye/right eye (Step S53).  And then, the feature data generating apparatus 100 segments a region corresponding to the target mesh from each of the images for the left eye/right eye (Step S60) and stores the feature data and the offset amount OST (Step S70).<br/>
              By means of the processing at Step S53 and after, the characters CH are arranged with a discrepancy between the character regions M1 and M2 as illustrated on the right side in Fig. 12A.  However, when this processing is performed, there is no need to perform processing of dividing the images of the characters CH into each region and shifting them or the like.  As described above, the images for the left eye/right eye are generated for each region, and a method for generating a parallel projection image including the periphery of the target mesh and of segmenting the regions M1, M2 and the like corresponding to the target meshes is used as its method.  Therefore, when the region M2 is to be generated, it is only necessary to arrange the entire character image including a portion protruding the region M2 at a position of CH1 on the right side in Fig. 12A.  By segmenting the portion corresponding to the region M2 from this, feature data including only the portion of the character CH1 is generated without performing complicated image processing.  The same applies to the region M1.  When the region M1 is to be generated, it is only necessary to arrange the entire character image at the position of the characters CH2 and then, to segment a portion corresponding to the region M1.
</p><p num="0045">
              Figs. 14 are explanatory diagrams illustrating a feature data example in the variation.<br/>
              Fig. 14A illustrates feature data before correction for the offset amount is made when the map is to be displayed.  Here, the region is expressed having a rectangular shape.  Such state can be easily realized by segmenting not in a parallelogram but in a rectangular shape in segmentation (Step S60) in the feature data generation processing (Fig. 13).<br/>
              In Fig. 14A, since correction for the offset amount has not been made, boundaries BH and BV are aligned linearly.  However, it is known that character strings Toranomon 35 Mori Building and Chishakuin-Betsuin-Shinpukuji across the boundary BH have a horizontal discrepancy between an upper side and a lower side of the boundary BH.  This is a state corresponding to a discrepancy between the characters CH1 and Ch2 indicated on the right side in Fig. 12A above.<br/>
              Fig. 14B illustrates a state in which the offset amount has been corrected.  That is, boundaries BV1 and BV2 in the longitudinal direction have a discrepancy in the vertical direction of the boundary BH.  As the result of the correction of the offset amount as above, the discrepancy in the character string across the boundary BH is modified, and the character string is displayed in a state without a discrepancy.
</p><p num="0046">
              According to the variation described above, similarly to the example, the stereoscopic vision can be realized with a light load using the images for the left eye/right eye generated by parallel projection in advance, and since it is generated within the images for the left eye/right eye including also the characters, the processing load for displaying characters can be also reduced, and the stereoscopic vision can be realized with a lighter processing load.  Moreover, in the variation, since the characters are arranged on the basis of the correction for the offset amount at display in advance, characters can be displayed without a discrepancy at the boundary between each region at the display.<br/>
              In the variation, the two-dimensional objects which move during display such as the current position, route guidance and the like may be further included.  These two-dimensional objects can be displayed by giving parallax at the display similarly to the example.
</p><p num="0047">
              The examples of the present invention have been described.  The stereoscopic map display system does not necessarily have to be provided with all the functions in the above described examples, but only a part of them may be realized.  Moreover, an additional function may be provided in the above described contents.<br/>
              The present invention is not limited to the above described examples, and it is needless to say that various configurations can be employed within a range not departing from the gist thereof.  For example, the portion constituted by hardware in the examples may be constituted by software and vice versa.  Moreover, not only a map but various stereoscopic images can be used as targets.  Furthermore, an output of a stereoscopic image is not limited to display on a display but may be printed.
</p></embodiments-example></description-of-embodiments><industrial-applicability><p num="0048">
              The present invention can be used to provide a stereoscopic image by using parallel projection.
</p></industrial-applicability><reference-signs-list><p num="0049">
100        data generating apparatus <br/>
101        command input unit <br/>
102        parallel projection unit <br/>
103        parallel projection data <br/>
104        3D map database <br/>
105        transmission/reception unit <br/>
106        projection parameter modification unit <br/>
200        server <br/>
201        transmission/reception unit <br/>
202        database control unit <br/>
203        route search unit <br/>
210        map database <br/>
211        feature data <br/>
212        two-dimensional object <br/>
213        network data <br/>
300        terminal<br/>
300d      display<br/>
301        transmission/reception unit <br/>
302        command input unit <br/>
303        GPS input unit <br/>
304        main control unit <br/>
305        map information storage unit <br/>
306        display control unit <br/>
307        scrolling control unit 
</p></reference-signs-list></description><claims mxw-id="PCLM70079639" ref-ucid="WO-2014129174-A1" lang="EN" load-source="patent-office"><claim num="1"><claim-text>
A stereoscopic image output system for stereoscopic vision of an image, comprising:<br/>
a stereoscopic vision output unit for realizing stereoscopic vision by outputting an image for the left eye and an image for the right eye with parallax so that the images can be visually recognized by the left eye and the right eye, respectively;<br/>
an image database storage unit for storing parallel projection data for the left eye and parallel projection data for the right eye as two-dimensional drawing data obtained by projecting a three-dimensional model to be a target of stereoscopic vision on a plane by parallel projection from diagonal directions inclined from a vertical direction by predetermined projection angles respectively set so as to generate parallax between the left and right eyes; and<br/>
an output control unit for outputting the image for the left eye and the image for the right eye with parallax for outputting an image in a specified output range to the stereoscopic vision output unit on the basis of the parallel projection data for the left eye and the parallel projection data for the right eye, wherein<br/>
if the output range moves, the output control unit determines moving directions of the image for the left eye and the image for the right eye individually, reads the parallel projection data for the left eye and the parallel projection data for the right eye in accordance with the moving directions and outputs the image for the left eye and the image for the right eye.
</claim-text></claim><claim num="2"><claim-text>
              The stereoscopic image output system according to claim 1, wherein<br/>
the output control unit determines the moving directions of the image for the left eye and the image for the right eye on the basis of parameters of the parallel projection.
</claim-text></claim><claim num="3"><claim-text>
              The stereoscopic image output system according to claim 1 or 2, wherein<br/>
the image data storage unit further stores two-dimensional object data for outputting a two-dimensional object into the image; and<br/>
              and the output control unit outputs the image for the left eye and the image for the right eye by giving parallax by ofsetting the two-dimensional object data to left or right with respect to a predetermined output position.
</claim-text></claim><claim num="4"><claim-text>
An image data generating apparatus for generating image data for a stereoscopic image output system which outputs a stereoscopic image by outputting an image for the left eye and an image for the right eye with parallax so that the images can be visually recognized by the left eye and the right eye, respectively, comprising:<br/>
              a 3D image database storage unit for storing a three-dimensional model to be a target of a stereoscopic vision and two-dimensional object data for outputting a two-dimensional object into the stereoscopic image;<br/>
              a parallel projection unit for generating parallel projection data for the left eye and parallel projection data for the right eye as two-dimensional drawing data obtained by projecting the three-dimensional model on a plane by parallel projection form diagonal directions inclined from a vertical direction by predetermined projection angles respectively set so as to generate parallax between the left and right eyes by the unit of a mesh having a predetermined size;<br/>
              a two-dimensional object image data generating unit for generating two-dimensional object image data for the left eye and two-dimensional object image data for the right eye by giving parallax by ofsetting the two-dimensional object data to left or right with respect to a predetermined output position; and<br/>
              a superposition processing unit for generating image data for the left eye and image data for the right eye by superposing the generated two-dimensional object image data for the left eye and the two-dimensional object image data for the right eye on the parallel projection data for the left eye and the parallel projection data for the right eye at positions considering the respective parallaxes.
</claim-text></claim><claim num="5"><claim-text>
              A method for outputting a stereoscopic image for stereoscopic vision of an image by a computer using a stereoscopic vision output unit for realizing stereoscopic vision by outputting an image for the left eye and an image for the right eye with parallax so that the images can be visually recognized by the left eye and the right eye, respectively, comprising steps of:<br/>
              reading data by the computer from an image database storage unit storing parallel projection data for the left eye and parallel projection data for the right eye as two-dimensional drawing data obtained by projecting a three-dimensional model to be a target of stereoscopic vision on a plane by parallel projection from diagonal directions inclined from a vertical direction by predetermined projection angles respectively set so as to generate parallax between the left and right eyes; and<br/>
              output control by the computer for outputting the image for the left eye and image for the right eye with parallax for outputting an image in a specified output range to the stereoscopic vision output unit on the basis of the parallel projection data for the left eye and the parallel projection data for the right eye, wherein<br/>
              in the output control step, if the output range moves, moving directions of the image for the left eye and the image for the right eye are determined individually, the parallel projection data for the left eye and the parallel projection data for the right eye are read in accordance with the moving directions, and the image for the left eye and the image for the right eye are outputted.
</claim-text></claim><claim num="6"><claim-text>
              A computer program for stereoscopic vision of an image by using a stereoscopic vision output unit for realizing stereoscopic vision by outputting an image for the left eye and an image for the right eye with parallax so that the images can be visually recognized by the left eye and the right eye, respectively, allowing a computer to realize functions of:<br/>
              reading data from an image database storage unit storing parallel projection data for the left eye and parallel projection data for the right eye as two-dimensional drawing data obtained by projecting a three-dimensional model to be a target of stereoscopic vision on a plane by parallel projection from diagonal directions inclined from a vertical direction by a predetermined projection angles respectively set so as to generate parallax between the left and right eyes;<br/>
outputting the image for the left eye and image for the right eye with parallax for outputting an image in a specified output range to the stereoscopic vision output unit on the basis of the parallel projection data for the left eye and the parallel projection data for the right eye; and<br/>
if the output range moves, determining moving directions of the image for the left eye and image for the right eye individually, reading the parallel projection data for the left eye and the parallel projection data for the right eye in accordance with the moving directions, and outputting the image for the left eye and image for the right eye.
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
