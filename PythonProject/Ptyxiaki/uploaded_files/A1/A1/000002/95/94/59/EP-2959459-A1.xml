<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959459-A1" country="EP" doc-number="2959459" kind="A1" date="20151230" family-id="51390414" file-reference-id="311904" date-produced="20180825" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160453202" ucid="EP-2959459-A1"><document-id><country>EP</country><doc-number>2959459</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14754127-A" is-representative="NO"><document-id mxw-id="PAPP193869372" load-source="patent-office" format="original"><country>EP</country><doc-number>14754127.0</doc-number><date>20140225</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193869373" load-source="docdb" format="epo"><country>EP</country><doc-number>14754127</doc-number><kind>A</kind><date>20140225</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162027719" ucid="AU-2013900667-A" load-source="docdb"><document-id format="epo"><country>AU</country><doc-number>2013900667</doc-number><kind>A</kind><date>20130225</date></document-id></priority-claim><priority-claim mxw-id="PPC162027119" ucid="AU-2014000174-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>AU</country><doc-number>2014000174</doc-number><kind>W</kind><date>20140225</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1875757769" load-source="docdb">G06T  15/04        20110101ALI20161115BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1875757770" load-source="docdb">G06T  15/00        20110101AFI20161115BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1875757771" load-source="docdb">G03B  35/16        20060101ALI20161115BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1642409230" load-source="docdb" scheme="CPC">H04N  13/25        20180501 LI20180510BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1702753123" load-source="docdb" scheme="CPC">G06T  15/04        20130101 LI20180131BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1702753124" load-source="docdb" scheme="CPC">G06T   7/90        20170101 LI20180131BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1702753125" load-source="docdb" scheme="CPC">G06T   7/593       20170101 LI20180131BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1702753126" load-source="docdb" scheme="CPC">G03B  35/02        20130101 LI20180131BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984696643" load-source="docdb" scheme="CPC">G06T  17/00        20130101 LI20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697904" load-source="docdb" scheme="CPC">G06T  11/008       20130101 LI20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699434" load-source="docdb" scheme="CPC">G06T2207/10048     20130101 LA20160108BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700357" load-source="docdb" scheme="CPC">G06T  15/06        20130101 LI20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700489" load-source="docdb" scheme="CPC">H04N   5/33        20130101 FI20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984702846" load-source="docdb" scheme="CPC">G06T2207/10028     20130101 LA20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984705271" load-source="docdb" scheme="CPC">G06T2207/10012     20130101 LA20160108BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165551072" lang="DE" load-source="patent-office">3D-ABBILDUNGSVERFAHREN UND -SYSTEM</invention-title><invention-title mxw-id="PT165551073" lang="EN" load-source="patent-office">3D IMAGING METHOD AND SYSTEM</invention-title><invention-title mxw-id="PT165551074" lang="FR" load-source="patent-office">PROCÉDÉ ET SYSTÈME D'IMAGERIE 3D</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103307573" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>COMMW SCIENT IND RES ORG</last-name><address><country>AU</country></address></addressbook></applicant><applicant mxw-id="PPAR1103328891" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>COMMONWEALTH SCIENTIFIC AND INDUSTRIAL RESEARCH ORGANISATION</last-name></addressbook></applicant><applicant mxw-id="PPAR1101646220" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Commonwealth Scientific and Industrial Research Organisation</last-name><iid>101337427</iid><address><street>Limestone Avenue</street><city>Campbell, ACT 2612</city><country>AU</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103332283" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>MOGHADAM PEYMAN</last-name><address><country>AU</country></address></addressbook></inventor><inventor mxw-id="PPAR1103317139" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>MOGHADAM, Peyman</last-name></addressbook></inventor><inventor mxw-id="PPAR1101643303" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>MOGHADAM, Peyman</last-name><address><street>1 Technology Court</street><city>Pullenvale, Queensland 4069</city><country>AU</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101647456" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Zacco Sweden AB</last-name><iid>101480799</iid><address><street>P.O. Box 5581</street><city>114 85 Stockholm</city><country>SE</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="AU-2014000174-W"><document-id><country>AU</country><doc-number>2014000174</doc-number><kind>W</kind><date>20140225</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014127431-A1"><document-id><country>WO</country><doc-number>2014127431</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660633630" load-source="docdb">AL</country><country mxw-id="DS660792077" load-source="docdb">AT</country><country mxw-id="DS660633640" load-source="docdb">BE</country><country mxw-id="DS660716621" load-source="docdb">BG</country><country mxw-id="DS660741954" load-source="docdb">CH</country><country mxw-id="DS660636731" load-source="docdb">CY</country><country mxw-id="DS660792078" load-source="docdb">CZ</country><country mxw-id="DS660633641" load-source="docdb">DE</country><country mxw-id="DS660636732" load-source="docdb">DK</country><country mxw-id="DS660636733" load-source="docdb">EE</country><country mxw-id="DS660712883" load-source="docdb">ES</country><country mxw-id="DS660716622" load-source="docdb">FI</country><country mxw-id="DS660716627" load-source="docdb">FR</country><country mxw-id="DS660633642" load-source="docdb">GB</country><country mxw-id="DS660636734" load-source="docdb">GR</country><country mxw-id="DS660633651" load-source="docdb">HR</country><country mxw-id="DS660792079" load-source="docdb">HU</country><country mxw-id="DS660741955" load-source="docdb">IE</country><country mxw-id="DS660636739" load-source="docdb">IS</country><country mxw-id="DS660716628" load-source="docdb">IT</country><country mxw-id="DS660636740" load-source="docdb">LI</country><country mxw-id="DS660634844" load-source="docdb">LT</country><country mxw-id="DS660792080" load-source="docdb">LU</country><country mxw-id="DS660634845" load-source="docdb">LV</country><country mxw-id="DS660634846" load-source="docdb">MC</country><country mxw-id="DS660722597" load-source="docdb">MK</country><country mxw-id="DS660722598" load-source="docdb">MT</country><country mxw-id="DS660792081" load-source="docdb">NL</country><country mxw-id="DS660633652" load-source="docdb">NO</country><country mxw-id="DS660636741" load-source="docdb">PL</country><country mxw-id="DS660634852" load-source="docdb">PT</country><country mxw-id="DS660792082" load-source="docdb">RO</country><country mxw-id="DS660634853" load-source="docdb">RS</country><country mxw-id="DS660636742" load-source="docdb">SE</country><country mxw-id="DS660634854" load-source="docdb">SI</country><country mxw-id="DS660633653" load-source="docdb">SK</country><country mxw-id="DS660636759" load-source="docdb">SM</country><country mxw-id="DS660722603" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139081499" ref-ucid="WO-2014127431-A1" lang="EN" load-source="patent-office"><p num="0000">A system for generating a three-dimensional model of an object, the system including a portable hand-held imaging device including a housing, a plurality of sensors attached to the housing and at least one electronic processing device coupled to the plurality of sensors. The electronic processing device determines from the plurality of sensors, whilst the imaging device is in at least two different poses relative to the object, range data indicative of a range of the object and thermal infrared data indicative of a thermal infrared image of the object, generates a three-dimensional model based at least partially upon the range data from the at least two different poses and associates model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object.</p></abstract><abstract mxw-id="PA139547826" ref-ucid="WO-2014127431-A1" lang="EN" source="national office" load-source="docdb"><p>A system for generating a three-dimensional model of an object, the system including a portable hand-held imaging device including a housing, a plurality of sensors attached to the housing and at least one electronic processing device coupled to the plurality of sensors. The electronic processing device determines from the plurality of sensors, whilst the imaging device is in at least two different poses relative to the object, range data indicative of a range of the object and thermal infrared data indicative of a thermal infrared image of the object, generates a three-dimensional model based at least partially upon the range data from the at least two different poses and associates model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object.</p></abstract><abstract mxw-id="PA139081500" ref-ucid="WO-2014127431-A1" lang="FR" load-source="patent-office"><p num="0000">La présente invention concerne un système permettant de générer un modèle en trois dimensions d'un objet, le système incluant un dispositif d'imagerie portable et portatif équipé d'un boîtier, d'une pluralité de capteurs fixés au boîtier et d'au moins un dispositif de traitement électronique couplé à la pluralité de capteurs. Le dispositif de traitement électronique détermine, à partir de la pluralité de capteurs, tandis que le dispositif d'imagerie se trouve dans au moins deux poses différentes par rapport à l'objet, des données de portée indiquant une portée de l'objet et des données thermiques infrarouges indiquant une image infrarouge de l'objet ; le dispositif de traitement électronique génère un modèle en trois dimensions basé au moins en partie sur les données de portée des deux ou plus de poses différentes et associe les données de modèle déduites des données thermiques infrarouges issues des deux ou plus de poses différentes avec le modèle en trois dimensions pour fournir ainsi un modèle infrarouge thermique en trois dimensions de l'objet.</p></abstract><abstract mxw-id="PA139547827" ref-ucid="WO-2014127431-A1" lang="FR" source="national office" load-source="docdb"><p>La présente invention concerne un système permettant de générer un modèle en trois dimensions d'un objet, le système incluant un dispositif d'imagerie portable et portatif équipé d'un boîtier, d'une pluralité de capteurs fixés au boîtier et d'au moins un dispositif de traitement électronique couplé à la pluralité de capteurs. Le dispositif de traitement électronique détermine, à partir de la pluralité de capteurs, tandis que le dispositif d'imagerie se trouve dans au moins deux poses différentes par rapport à l'objet, des données de portée indiquant une portée de l'objet et des données thermiques infrarouges indiquant une image infrarouge de l'objet ; le dispositif de traitement électronique génère un modèle en trois dimensions basé au moins en partie sur les données de portée des deux ou plus de poses différentes et associe les données de modèle déduites des données thermiques infrarouges issues des deux ou plus de poses différentes avec le modèle en trois dimensions pour fournir ainsi un modèle infrarouge thermique en trois dimensions de l'objet.</p></abstract><description mxw-id="PDES78478651" ref-ucid="WO-2014127431-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="3"/>--><p id="p0001" num="0001"> 3D IMAGING METHOD AND SYSTEM FIELD OF THE INVENTION </p><p id="p0002" num="0002"> [0001] The present invention relates to three-dimensional imaging. In particular, although not exclusively, the invention relates to a system and method for three-dimensional imaging utilising thermal infrared image data. </p><p id="p0003" num="0003">BACKGROUND TO THE INVENTION </p><p id="p0004" num="0004"> [0002] The reference in this specification to any prior publication (or information derived from it), or to any matter which is known, is not, and should not be taken as an acknowledgment or admission or any form of suggestion that the prior publication (or information derived from it) or known matter forms part of the common general knowledge in the field of endeavour to which this specification relates. </p><p id="p0005" num="0005">[0003] Energy sustainability is a major challenge of the 21st century in the face of climate change. In order to reduce environmental impact such as greenhouse gas emissions, changes are required not only on the supply side of the energy chain but also on the demand side in reducing energy usage and improving energy efficiency. </p><p id="p0006" num="0006">[0004] Energy auditing is therefore often required to assess energy efficiency of buildings. Thermal infrared imaging is used for energy auditing assessments, in which energy flow is analysed for the purpose of identifying thermal infrared irregularities such as thermal infrared defects, air leakages, heat losses and thermal infrared bridges, energy wastage and energy inefficiency. Such energy audits rely on two-dimensional thermal infrared images of relevant parts of the building, such as furnaces, windows and other areas where energy inefficiencies are common. </p><p id="p0007" num="0007">[0005] A thermal infrared image sensor captures emitted or reflected thermal infrared radiation from objects and typically represents the thermal infrared data as a colour-mapped image, or a group of such images. The thermal infrared data is mapped to colour values to simplify visualisation of otherwise invisible thermal infrared radiation. 
<!-- EPO <DP n="4"/>-->
 [0006] A problem with such energy auditing systems of the prior art is that they lack information on the location and orientation of objects with reference to each other, particularly across separate images. Generally, discrete objects are viewed separately, and are not considered in the context of other objects. </p><p id="p0008" num="0008">[0007] A further problem with such energy auditing systems of the prior art is that the thermal infrared imaging indirectly measures a temperature of an object by capturing thermal infrared radiation from a surface of the object. The captured thermal infrared radiation can, however, comprise a thermal infrared reflection from another object. The problem is exaggerated in windows and other objects having a "shiny" surface, as thermal infrared radiation is more likely to be reflected. </p><p id="p0009" num="0009">[0008] Thermal infrared imaging has also been demonstrated to be an effective tool for medical diagnosis. Thermal infrared cameras are able to discern abnormal temperature patterns, making them useful for medical diagnosis, treatment, examination and general health monitoring. Additionally, thermal infrared imaging is non-invasive, does not require skin contact and is radiation free. Medical applications of thermal infrared imaging include breast cancer detection, neonatal health monitoring, and neuro-imaging. </p><p id="p0010" num="0010">[0009] Medical thermal infrared imaging typically involves viewing a two-dimensional image of a person (or a part of a person). A problem with medical thermal infrared imaging of the prior art is that imaging artefacts are often present on edges of objects, objects far from the sensor, or areas where an angle of incidence is great. Similarly, it is often difficult for a medical practitioner to analyse a static two-dimensional thermal infrared image and compare thermal infrared images taken at different times, distances and angles </p><p id="p0011" num="0011">[0010] Certain systems have attempted to overcome some of these problems by using three- dimensional thermal infrared imaging. Such systems typically utilize an array of fixed- position thermal infrared sensors, which together are able to generate a three-dimensional thermal infrared model. 
<!-- EPO <DP n="5"/>-->
 [0011] One problem with such systems is that they are costly, as several thermal infrared sensors are required. A further problem with such systems is that they are unable to map arbitrarily sized structures. This can, for example, result in a different configuration being required for full-body analysis than for analysis of a smaller body part such as a foot. </p><p id="p0012" num="0012">[0012] Yet a further problem of prior art medical thermal infrared imaging systems is that they may need to be repositioned and recalibrated at several locations around the patient in order to take multiple views. Calibration of these systems is also complex and generally requires the expertise of a trained engineer. </p><p id="p0013" num="0013">[0013] A general problem with thermal infrared imaging of the prior art is that the thermal infrared image data often varies depending on an angle and distance between the object and the camera during capture. This prevents quantitative and repeatable measurements unless monitoring is performed at the exact same position and angle with respect to the object at different times. </p><p id="p0014" num="0014">[0014] Accordingly, there is a need for an improved three-dimensional imaging method and system. </p><p id="p0015" num="0015">SUMMARY OF THE INVENTION </p><p id="p0016" num="0016"> [0015] The present invention seeks to provide consumers with improvements and advantages over the above described prior art, and/or overcome and alleviate one or more of the above described disadvantages of the prior art, and/or provide a useful commercial choice. </p><p id="p0017" num="0017">[0016] According to one aspect the present invention seeks to provide a system for generating a three-dimensional model of an object, the system including: </p><p id="p0018" num="0018"> a portable hand-held imaging device including: </p><p id="p0019" num="0019"> a housing; </p><p id="p0020" num="0020"> a plurality of sensors attached to the housing; </p><p id="p0021" num="0021"> at least one electronic processing device coupled to the plurality of sensors, wherein the electronic processing device: 
<!-- EPO <DP n="6"/>-->
 W </p><p id="p0022" num="0022">. 4 . determines from the plurality of sensors and whilst the imaging device is in at least two different poses relative to the object: </p><p id="p0023" num="0023"> range data indicative of a range of the object; </p><p id="p0024" num="0024"> thermal infrared data indicative of a thermal infrared image of the object; generates a three-dimensional model based at least partially upon the range data from the at least two different poses; and </p><p id="p0025" num="0025"> associates model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object. </p><p id="p0026" num="0026">[0017] Typically the electronic processing device: </p><p id="p0027" num="0027"> determines colour data indicative of a visible image of the object; and </p><p id="p0028" num="0028"> associates model data derived from the visible image data from the at least two different poses with the three dimensional model to thereby provide a three dimensional colour model of the object. </p><p id="p0029" num="0029">[0018] Typically the electronic processing device generates a three dimensional fused thermal infrared and colour model of the object. </p><p id="p0030" num="0030">[0019] Typically the system includes: </p><p id="p0031" num="0031"> a range sensor attached to the housing that senses a range of the object: </p><p id="p0032" num="0032"> a visible light image sensor attached to the housing that senses a visible image of the object; and </p><p id="p0033" num="0033"> a thermal infrared image sensor attached to the housing that senses a thermal infrared image of the object. </p><p id="p0034" num="0034">[0020] Typically the electronic processing device: </p><p id="p0035" num="0035"> determines a relative pose of the hand-held device; and, </p><p id="p0036" num="0036"> uses the relative pose to generate the three dimensional model. 
<!-- EPO <DP n="7"/>-->
 [0021] Typically the electronic processing device determines the relative pose of the handheld device using at least one of range data, image data and thermal infrared data from at least some of the different poses. </p><p id="p0037" num="0037">[0022] Typically the electronic processing device: </p><p id="p0038" num="0038"> estimates a timing skew between the range sensor and at least one of a thermal infrared image sensor and a visible light image sensor; and </p><p id="p0039" num="0039"> * estimates a pose associated with the image data and/or the thermal infrared data based upon the skew. </p><p id="p0040" num="0040">[0023] Typically the electronic processing device: </p><p id="p0041" num="0041"> determines a device pose using at least one of the range sensor and a pose sensor; determines a path of movement of the hand-held device; </p><p id="p0042" num="0042"> determines a visible light image sensor pose and a thermal infrared image sensor pose based on the device pose and the path of movement. </p><p id="p0043" num="0043">[0024] Typically the system includes a pose sensor and wherein the electronic processing device determines the relative pose using pose data from the pose sensor. </p><p id="p0044" num="0044">[0025] Typically the pose sensor is at least one of an orientation sensor and an inertial measurement unit. </p><p id="p0045" num="0045">[0026] Typically the electronic processing device: </p><p id="p0046" num="0046"> selects an object surface part; </p><p id="p0047" num="0047"> identifies images of the selected object surface part whilst the imaging device is in at least two different poses relative to the object; </p><p id="p0048" num="0048"> identifies at least one image pixel corresponding to the selected object surface part for each identified image; and, </p><p id="p0049" num="0049"> uses the identified image pixels to determine the model data. 
<!-- EPO <DP n="8"/>-->
 [0027] Typically for a thermal infrared image the model data includes model thermal infrared data indicative of a temperature and for a visible image the model data includes model colour data indicative of a colour. </p><p id="p0050" num="0050">[0028] Typically the electronic processing device associates the data with a voxel in the three dimensional model corresponding to the selected object surface part. </p><p id="p0051" num="0051">[0029] Typically the selected object surface part is a vertex. </p><p id="p0052" num="0052">[0030] Typically the electronic processing device determines the model data using a weighted sum of data associated with the identified pixels. </p><p id="p0053" num="0053">[0031] Typically the electronic processing device determines the weighted sum using a confidence score associated with the identified pixels. </p><p id="p0054" num="0054">[0032] Typically the electronic processing device determines the confidence score based on factors including at least one of: </p><p id="p0055" num="0055"> a velocity of a sensor when the image was captured; </p><p id="p0056" num="0056"> a position of the one or more pixels in a field of view of the sensor; </p><p id="p0057" num="0057"> an angle between a ray and an object surface, the ray extending from the object surface part to the sensor; and, </p><p id="p0058" num="0058"> the range when the image was captured. </p><p id="p0059" num="0059">[0033] Typically, for a thermal infrared image, the electronic processing device determines the confidence score based on a temperature value associated with the identified pixels. </p><p id="p0060" num="0060">[0034] According to one aspect the present invention seeks to provide a method of generating a three-dimensional model of an object, the method including, in an electronic processing device: </p><p id="p0061" num="0061"> determining from the plurality of sensors and whilst an imaging device is in at least two different poses relative to the object: </p><p id="p0062" num="0062"> range data indicative of a range of the object; </p><p id="p0063" num="0063"> thermal infrared data indicative of a thermal infrared image of the object; 
<!-- EPO <DP n="9"/>-->
 generating a three-dimensional model based at least partially upon the range data from the at least two different poses; and </p><p id="p0064" num="0064"> associating model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object. </p><p id="p0065" num="0065">[0035] According to one aspect, the invention resides in a system for generating a three- dimensional model of an object, the system including: </p><p id="p0066" num="0066"> a portable imaging device including: </p><p id="p0067" num="0067"> a housing; </p><p id="p0068" num="0068"> a range sensor attached to the housing; </p><p id="p0069" num="0069"> a visible light image sensor attached to the housing; and </p><p id="p0070" num="0070"> an thermal infrared image sensor attached to the housing, wherein the range sensor, the visible light image sensor and the thermal infrared image sensor have overlapping fields of view; </p><p id="p0071" num="0071"> at least one processor coupled to the range sensor, the visible light image sensor and the thermal infrared image sensor of the portable imaging device; and </p><p id="p0072" num="0072"> a memory, coupled to the at least one processor, including instruction code executable by the processor for: </p><p id="p0073" num="0073"> receiving range data from the range sensor, image data from the visible light image sensor, and thermal infrared data from the thermal infrared image sensor; </p><p id="p0074" num="0074"> generating a three-dimensional model based upon the range data captured from several positions; and </p><p id="p0075" num="0075"> associating data from the image data and the thermal infrared data, from at least two positions, with the three dimensional model. </p><p id="p0076" num="0076">[0036] Preferably, associating the image data and the thermal infrared data with the three- dimensional model comprises estimating a timing skew between the range sensor and at least one of the visible light image sensor and the thermal infrared image sensor, and estimating a pose associated with the image data and/or the thermal infrared data based upon the skew. 
<!-- EPO <DP n="10"/>-->
 [0037] Preferably, thermal infrared data is associated with the three-dimensional model based at least partly upon an' angle of incidence. Suitably, thermal infrared data is associated with the three-dimensional model further based upon a capture distance, environmental data, such as temperature data, or any other data indicative of a confidence of the thermal infrared data. </p><p id="p0077" num="0077">[0038] Preferably, the portable imaging device further comprises an orientation sensor. </p><p id="p0078" num="0078">[0039] Preferably, the system further comprises at least one environmental sensor, wherein the thermal infrared data is associated with the three-dimensional model based upon data from the at least one environmental sensor. Suitably, the at least one environmental sensor comprises a carbon dioxide sensor, a humidity sensor and/or a temperature sensor. </p><p id="p0079" num="0079">[0040] Preferably, the system further comprises an audio sensor for recording an operator's voice during operation of the system. </p><p id="p0080" num="0080">[0041] Preferably, the housing of the portable imaging device comprises an elongate portion, wherein the range sensor, visible light image sensor and thermal infrared image sensor are attached to an end of the elongate portion. </p><p id="p0081" num="0081">[0042] Preferably, the housing of the portable imaging device further comprises a trigger for activating at least one of the range sensor, the visible light image sensor and the thermal infrared image sensor. </p><p id="p0082" num="0082">[0043] Preferably, the at least one processor is coupled to the range sensor, visible light image sensor and thermal infrared image sensor of the portable imaging device at least in part by a wireless data communication link. </p><p id="p0083" num="0083">[0044] Preferably, the thermal infrared image sensor comprises a thermal infrared sensor. </p><p id="p0084" num="0084">[0045] Preferably, the memory further includes instruction code executable by the processor for: 
<!-- EPO <DP n="11"/>-->
 generating an output image corresponding to a view of the three-dimensional model, the output image including at least part of the associated image data superimposed on the model and at least part of the associated thermal infrared data superimposed on the model. </p><p id="p0085" num="0085">[0046] Preferably, the output image is generated by rendering at least part of the associated visible light image data on a first portion of the model and at least part of the associated thermal infrared data on a second portion of the model. </p><p id="p0086" num="0086">[0047] Preferably, the three-dimensional model corresponds to at least a part of a person's body and the associated image data includes skin tones of the person. </p><p id="p0087" num="0087">[0048] Preferably, the memory further includes instruction code executable by the processor for refining the three-dimension model based upon the image data and/or the thermal infrared data. </p><p id="p0088" num="0088">[0049] Preferably, the a portable imaging device further includes a display, for displaying data captured from one or more of the range sensor, visible light image sensor and thermal infrared image sensor, in real time as it is captured. </p><p id="p0089" num="0089">[0050] According to a second aspect, the invention resides in a method of generating a three- dimensional model of an object, the method including: </p><p id="p0090" num="0090"> receiving, from a range sensor, a visible light image sensor and an thermal infrared image sensor, range data, image data and thermal infrared data, wherein the range sensor, the visible light image sensor and the thermal infrared image sensor have overlapping fields of view, and wherein the range data, image data and thermal infrared data each correspond to data from at least two different positions of the range sensor, visible light image sensor and thermal infrared image sensor; </p><p id="p0091" num="0091"> generating a three-dimensional model based upon the range data; and </p><p id="p0092" num="0092"> associating data from the image data and the thermal infrared data, corresponding to at least two different positions, with the three dimensional model. 
<!-- EPO <DP n="12"/>-->
 [0051] Preferably, the at least two different positions comprise arbitrary positions, wherein at least a part of an object visible in a first position of the at least two different positions is visible in a second position of the at least two positions. </p><p id="p0093" num="0093">[0052] Preferably, the arbitrary positions comprise overlapping views caused by movement of the range sensor, the visible light image sensor and the thermal infrared image sensor across the object. </p><p id="p0094" num="0094">[0053] Preferably, the method further comprises generating an output image corresponding to a view of the three dimensional model, the output image including at least part of the associated image data superimposed on the model and at least part of the associated thermal infrared data superimposed on the model. </p><p id="p0095" num="0095">[0054] It will be appreciated that the different aspects of the invention and their respective features can be used in conjunction or interchangeably and reference to them as separate aspects is not intended to be limiting. </p><p id="p0096" num="0096">BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0097" num="0097"> [0055] To assist in understanding the invention and to enable a person skilled in the art to put the invention into practical effect, preferred embodiments of the invention are described below by way of example only with reference to the accompanying drawings, in which: </p><p id="p0098" num="0098">[0056] FIG. 1 illustrates a system for generating a model of an object, according to an embodiment of the present invention; </p><p id="p0099" num="0099"> [0057] FIG. 2a illustrates a perspective view of a portable imaging device of the system of FIG. 1 ; </p><p id="p0100" num="0100"> [0058] FIG. 2b illustrates a side view of the portable imaging device of FIG. 2a; </p><p id="p0101" num="0101"> [0059] FIG. 3a illustrates a perspective view of a portable imaging device, according to an alternative embodiment of the present invention; </p><p id="p0102" num="0102"> [0060] FIG. 3b illustrates a side view of the portable imaging device of FIG. 3a; </p><p id="p0103" num="0103"> [0061] FIG. 4 schematically illustrates a system for generating a model of an object, according to an embodiment of the present invention. 
<!-- EPO <DP n="13"/>-->
 [0062] FIG. 5 illustrates a screenshot of a display of the system of FIG. 1 , illustrating a rendering of a three-dimensional model with image data and thermal infrared data; </p><p id="p0104" num="0104">[0063] FIG. 6 diagrammatically illustrates the computing apparatus of the system of FIG. 1 , according to an embodiment of the present invention; and </p><p id="p0105" num="0105"> [0064] FIG. 7 illustrates a method of generating a three-dimensional model of an object, according to an embodiment of the present invention. </p><p id="p0106" num="0106"> [0065] Those skilled in the art will appreciate that minor deviations from the layout of components as illustrated in the drawings will not detract from the proper functioning of the disclosed embodiments of the present invention. </p><p id="p0107" num="0107">DETAILED DESCRIPTION OF THE INVENTION </p><p id="p0108" num="0108"> [0066] Embodiments of the present invention comprise three-dimensional imaging devices and methods. Elements of the invention are illustrated in concise outline form in the drawings, showing only those specific details that are necessary to the understanding of the embodiments of the present invention, but so as not to clutter the disclosure with excessive detail that will be obvious to those of ordinary skill in the art in light of the present description. </p><p id="p0109" num="0109">[0067] In this patent specification, adjectives such as first and second, left and right, front and back, top and bottom, etc., are used solely to define one element or method step from another element or method step without necessarily requiring a specific relative position or sequence that is described by the adjectives. Words such as "comprises" or "includes" are not used to define an exclusive set of elements or method steps. Rather, such words merely define a minimum set of elements or method steps included in a particular embodiment of the present invention. </p><p id="p0110" num="0110">[0068] According to one aspect the invention seeks to provide a system for generating a three-dimensional model of an object, the system including: a portable hand-held imaging device including: a housing; a plurality of sensors attached to the housing; at least one electronic processing device coupled to the plurality of sensors, wherein the electronic processing device: determines from the plurality of sensors and whilst the imaging device is 
<!-- EPO <DP n="14"/>-->
 in at least two different poses relative to the object: range data indicative of a range of the object; thermal infrared data indicative of a thermal infrared image of the object; generates a three-dimensional model based at least partially upon the range data from the at least two different poses; and associates model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object. </p><p id="p0111" num="0111">[0069] According to one aspect, the invention resides in a system for generating a three- dimensional model of an object, the system including: a portable imaging device including: a housing; a range sensor attached to the housing; a visible light image sensor attached to the housing; and an thermal infrared image sensor attached to the housing, wherein the range sensor, the visible light image sensor and the thermal infrared image sensor have overlapping fields of view; at least one processor coupled to the range sensor, the visible light image sensor and the thermal infrared image sensor of the portable imaging device; and a memory, coupled to the at least one processor, including instruction code executable by the processor for: receiving range data from the range sensor, image data from the visible light image sensor, and thermal infrared data from the thermal infrared sensor; generating a three- dimensional model based upon the range data captured from several positions; and associating data from the image data and the thermal infrared data from at least two positions with the three dimensional model. </p><p id="p0112" num="0112">[0070] Advantages of some embodiments of the present invention can include an ability to generate a holistic and geometrically accurate three-dimensional model of an object including thermal infrared and optionally visible image data, without requiring complex and/or expensive hardware. Such three-dimensional models can be acquired at different times and be aligned and compared to detect any spatio-temporal changes. </p><p id="p0113" num="0113">[0071] The present invention seeks to provide improved accuracy of thermal infrared data, particularly where reflective objects are present, or other factors are present that would otherwise cause artefacts or errors in the thermal infrared data. The accuracy of thermal infrared data can be improved since thermal infrared data can be captured from several angles 
<!-- EPO <DP n="15"/>-->
 and distances, and only thermal infrared data with little or no thermal infrared reflection or noise can be selected. </p><p id="p0114" num="0114">[0072] The present invention can also enable rapid three-dimensional visualization of thermal infrared information, which is particularly advantageous, for example, in the context of fire management and response, as well as within an electrical, mechanical or structural building inspection. </p><p id="p0115" num="0115">[0073] The three-dimensional model provides an opportunity for a user to investigate and detect thermal infrared anomalies of an element or object, in a context of its surroundings. Such three-dimensional models can be utilized by inspectors for monitoring non-visible thermal infrared irregularities of buildings, or by medical practitioners to identify thermal infrared anomalies in a patient's body. </p><p id="p0116" num="0116">[0074] According to certain embodiments, image and thermal infrared data can be viewed together on a single model, which enables more accurate localisation of problems. For example, in an electrical maintenance context, the present invention enables a user to determine which of many electrical connections is heating up abnormally due to a faulty connection. The image data enables the user to view a text or colour-based label, whereas the thermal infrared data can be used to detect the heat flows caused by the fault. Similarly, colour-coded pipes and wires can be identified by the image data. </p><p id="p0117" num="0117">[0075] Further, embodiments of the present invention are able to operate in darkness, which is advantageous when explicit night-time analysis is called for by applications such as building energy auditing. Additionally, patients may prefer to be scanned in darkness rather than a bright room. Thus, it will be appreciated that the collection of visible image information is not necessarily required, depending on the circumstances in which the system is to be used. </p><p id="p0118" num="0118">[0076] Finally, embodiments of the present invention do not necessarily require an external tracking device in order to accurately estimate a pose of the device, and can be implemented using simple, off-the-shelf components, although optionally external tracking can be used to 
<!-- EPO <DP n="16"/>-->
 improve pose determination, for example in the event that high resolution thermal infrared mapping is required, for example in medical applications. </p><p id="p0119" num="0119">[0077] FIG. 1 illustrates a system 100 for generating a model of an object, according to an embodiment of the present invention. The system 100 can be used for various purposes such as energy-efficiency monitoring, non-destructive structural, mechanical and electrical assessment, construction monitoring, fire detection, by first responders when analysing an emergency scene, and in non-invasive medical diagnosis. </p><p id="p0120" num="0120">[0078] The system 100 includes a portable imaging device 105, for capturing images of the object from various, advantageously overlapping perspectives. </p><p id="p0121" num="0121">[0079] The portable imaging device 105 includes a range sensor 1 10, a visible light image sensor 115 and an thermal infrared image sensor 120. The range sensor 110, the visible light image sensor 1 15 and the thermal infrared sensor 120 are fixedly attached to a housing of the portable imaging device 105 in a spaced relationship, and are directed in a common direction. Accordingly, in use, the range sensor 110, the visible light image sensor 115 and the thermal infrared sensor 120 capture substantially the same view of the object; however, as discussed further below hard synchronisation of the sensors 1 10, 1 15, 120 is not required. Additionally, whilst three discrete physical sensors are shown, this is not essential and alternatively, two or more sensors can be used. For example, a range and thermal infrared sensor only may be used if visible image sensing is not required. Alternatively, in the event that dual purpose sensors are used, three sensors may not be required, so for example in the event that the range sensor is also capable of sensing a visible image, for example if stereoscopic range sensing is used, then a separate range and visible sensor may not be required. </p><p id="p0122" num="0122">[0080] The visible light image sensor 115 determines colour data, such as RGB data, indicative of a visible image of the object and is typically capable of detecting light that is visible to the human visual system, e.g. with wavelengths from around 390nm to 700nm, and in particular a visible image. The thermal infrared sensor 120 is designed to determine thermal infrared data indicative of a thermal infrared image of the object and in particular to 
<!-- EPO <DP n="17"/>-->
 detect thermal infrared radiation beyond the visible spectrum, such as wavelengths between 8μπι and 14μπι. Thermal infrared radiation in this portion of the electromagnetic spectrum is emitted from objects based on their temperatures. The thermal infrared sensor 120 can capture this thermal infrared radiation and estimate a surface temperature of the objects based thereon. The range sensor 110 can determine range data indicative of a range of the object, and can be one of a variety of commercially available electromagnetic or ultrasonic proximity sensors, which are well known in the art. The visible light image sensor 1 15 can be a standard camera, as are well known in the art. An example of the thermal infrared sensor 120 is the Thermoteknix Miricle 307K camera of Thermoteknix Systems Ltd., Cambridge, United Kingdom. However, other types of thermal infrared sensors can be used. </p><p id="p0123" num="0123">[0081] The housing of the portable imaging device 105 further includes a trigger 125 for activating capture of data from the portable imaging device 105. Typically, in use, the user presses the trigger 125 to activate capture of data, and presses the trigger 125 again to deactivate capture of data. Alternatively, the user holds down the trigger 125 to activate the capture of data, and releases the trigger 125 to deactivate the capture of data. </p><p id="p0124" num="0124">[0082] Either way, data can be captured while moving the portable imaging device 105 around the object, thus capturing a sequence of images of the object from different angles and/or positions. However, as will be understood by the skilled addressee, the portable imaging device 105 can be used to capture discrete thermal infrared and/or visible light images of the object from different angles by manually pressing the trigger 125 at , different points. </p><p id="p0125" num="0125">[0083] The portable imaging device 105 is connected to a computing apparatus 130, via a signal cable 135. However, as will be readily understood by the skilled addressee, the signal cable 135 can be replaced by wireless communication between the portable imaging device 105 and the computing apparatus 130. Examples of wireless communication techniques include wireless local area networks (WLANs) according to Institute of Electrical and Electronics Engineers (IEEE) 802.1 1 standards, short-wavelength radio transmissions such as 
<!-- EPO <DP n="18"/>-->
 Bluetooth and near-field communication based upon radio-frequency identification (RFID) standards. </p><p id="p0126" num="0126">[0084] The computing apparatus 130, described in further detail below, includes at least one processor (not shown) and a memory (not shown). The processor is coupled to the range sensor 1 10, the visible light image sensor 115 and the thermal infrared sensor 120 by the signal cable 135 and a data interface (not shown). </p><p id="p0127" num="0127">[0085] The memory includes instructions for receiving a plurality of range images from the range sensor 1 10, a plurality of images of the visible light image sensor 1 15, and a plurality of thermal infrared images from the thermal infrared sensor 120. Upon receipt of the images, a three-dimension model is generated based upon the plurality of range images. According to certain embodiments, the thermal infrared images, from the thermal infrared sensor 120 and/or the images from the visible light image sensor 115 can be additionally used to generate and/or refine the three-dimensional model. </p><p id="p0128" num="0128">[0086] After generating the three dimensional model, typically using the range data, data from the plurality of images of the visible light image sensor 1 15 and the plurality of thermal infrared images from the thermal infrared sensor 120 are associated with the three- dimensional model. This can, for example, comprise associating image and thermal infrared data to all or some points or parts of the three-dimensional model. Typically this involves creating model data, with the model data being based on the thermal infrared data and optionall colour data for at least two different poses of the hand-held device, as will be described in more detail below. </p><p id="p0129" num="0129">[0087] The range sensor 110, the visible light image sensor 1 15 and the thermal infrared sensor 120 generally require geometrical and temporal calibration with respect to each other. </p><p id="p0130" num="0130">[0088] The points of view of each of the sensors 110, 1 15, 120 of the portable imaging device 105 are different since they are geometrically separated. Geometric calibration is thus performed to determine relative positions and orientations between the sensors 110, 115, 120, as well as internal parameters of the sensors 110, 115, 120, such as focal length. Additionally, 
<!-- EPO <DP n="19"/>-->
 a relative timing between the range sensor 1 10, the visible light image sensor 1 15 and the thermal infrared sensor 120 also can be determined, if required, for example in the event that the sensors do not use a common clock signal when capturing data. </p><p id="p0131" num="0131">[0089] Relative poses (i.e. positions and orientations) of the sensors 1 10, 1 15, 120 are thus determined. This may be performed at a time of manufacture, or periodically by the user if there is a risk of relative movement among the sensors 110, 1 15, 120. </p><p id="p0132" num="0132">[0090] Any of several range-image geometric calibration techniques known in the art can be used. For example, a range-image geometric calibration algorithm typically requires external artificial targets to be observed simultaneously from the range and imaging sensors. A checkerboard pattern is a common external target. Examples of such range-image geometric calibration techniques are described in Q. Zhang and R. Pless, "Extrinsic calibration of a camera and laser range finder (improves camera calibration)," in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, vol. 3, 2004, pp. 2301-2306, and R. Unnikrishnan and M. Hebert, "Fast extrinsic calibration of a laser rangefinder to a camera," Robotics Institute, Carnegie Mellon University, Tech. Rep. CMU- RI-TR-05-09, 2005. </p><p id="p0133" num="0133">[0091] Alternatively, a set of natural scene feature correspondences are extracted from the thermal infrared data, visible image data and the range data respectively. Once the correspondences are known, the rigid transformation between the two reference frames can be determined using an iterative non-linear optimization minimizing a distance between corresponding pairs. Finally, transformations having six degrees of freedom are generated, transforming the thermal infrared data, range data and visible image data to a common coordinate system, thus providing the relative poses of the sensors 110, 1 15, 120. </p><p id="p0134" num="0134">[0092] During use, a relative pose of each of the sensors 110, 115, 120 can be determined as the portable imaging device 105 is moved around an object. As data from the sensors 110, 1 15, 120 is not necessarily captured in a synchronised manner, range data captured using the range sensor 1 10 may correspond to a slightly different pose than thermal infrared data captured from the thermal infrared sensor 120. A pose is therefore generated from the range 
<!-- EPO <DP n="20"/>-->
 sensor data, which is then used as a basis for estimating a pose corresponding to the thermal infrared data and visible image data. </p><p id="p0135" num="0135">[0093] Specifically, four neighbouring range frames are selected, two immediately preceding the thermal infrared or visible image data in question, and two immediately following. A cubic spline is fitted to four poses corresponding to the four neighbouring range frames, representing the three-dimensional path of movement of the portable imaging device 105 throughout this interval. The pose of the portable imaging device 105 corresponding to the time the thermal infrared or visible image data was captured is estimated using a spline, which is then transformed to a pose of the relevant sensor 1 15, 120 using the relative positions of the sensors 110, 115, 120. </p><p id="p0136" num="0136">[0094] Therefore, some parts of the object during scanning may be occluded for the sensors 1 10, 115, 120. According to certain embodiments, range points from the three-dimensional model are first projected into image planes. Then, a k-nearest neighbours (KNN) search is utilized to cluster neighbouring projected points based on their distance to the portable imaging device and a size of the cluster. Points that have a large distance to the centre of the cluster are then discarded as outliers, and potentially occluded points. </p><p id="p0137" num="0137">[0095] Depending on the field of view of each sensor 110, 1 15, 120, some points might be outside of the field of other sensors 1 10, 1 15, 120 and therefore may not be able to have an assigned temperature or colour. </p><p id="p0138" num="0138">[0096] Capture of data from the range sensor 1 10, the visible light image sensor 1 15 and the thermal infrared sensor 120 does not need to be synchronised. Instead, respective poses of images from the visible light image sensor 1 15 and thermal infrared images from the thermal infrared sensor 120 can be determined by analysing a relative timing of the images compared with the range images, for which a capture time is known, together with a pre-determined knowledge of the relative poses of the sensors 1 10, 1 15, 120. Relative pose is generated by interpolating between poses based on an estimate of timing skew between sensors 110, 115, 
<!-- EPO <DP n="21"/>-->
 [0097] Weighted averaging raycasting then can be used to project data from the images to the three-dimensional model, using knowledge of the geometry of the sensors 1 10, 115, 120 of the portable imaging device 105. This process can comprise colouring voxels of the three- dimensional model, as well as assigning temperature values to each voxel from multiple views. </p><p id="p0139" num="0139">[0098] Accuracy of thermal infrared data is improved as the thermal infrared sensor 120 is moved across an object's surface to obtain multiple overlapping readings. Anomalies caused by reflections from windows or shiny materials, and local environmental factors such as humidity thus can be compensated for. </p><p id="p0140" num="0140">[0099] According to certain embodiments, an uncoloured three-dimensional voxel occupancy map is first created using the range data. The voxel occupancy map is then converted to a dense three-dimensional model using the marching cubes algorithm, which is later simplified using a quadric-based edge collapse algorithm, which algorithms are well known in the art. Weighted raycasting is then performed for each valid frame from both the thermal infrared sensor 120 and the visible light image sensor 1 15, in order to assign visible image and thermal infrared data to each vertex in the three-dimensional model. </p><p id="p0141" num="0141">[00100] According to certain embodiments, thermal infrared data is selectively added to the three-dimensional model, based upon an angle of incidence, a distance between the thermal infrared sensor 120 and the surface, environmental sensor data and/or other factors influencing a confidence of the thermal infrared data when captured. Furthermore, a geometric position of pixels in the camera can be a factor influencing confidence of the thermal infrared data, where pixels located towards a centre of the camera have a higher confidence than pixels located towards the corners of the camera. Thus pixels located towards the centre of the camera can be given a higher weight when raycasting. </p><p id="p0142" num="0142">[00101] By capturing thermal infrared data from several positions and selectively choosing which thermal infrared data is added to the three-dimensional model, greater thermal infrared accuracy can be provided. 
<!-- EPO <DP n="22"/>-->
 [00102] The three dimensional model then can be rendered on a screen 140 of the computing device 130, including details from the plurality of images of the visible light image sensor 1 15 and the plurality of thermal infrared images from the thermal infrared sensor 120. As discussed in further detail below, image data from the plurality of images of the visible light image sensor 115 can be rendered on the three-dimensional model to provide a lifelike or realistic appearance, while data from the plurality of thermal infrared images from the thermal infrared sensor 120 can be, for example, overlayed over part of the three- dimensional model to provide thermal infrared information. </p><p id="p0143" num="0143">[00103] The three-dimensional model produced by the system 100 can be used to generate synthetic views at any angle or scale, which may help a user, such as a medical practitioner, to interpret the results more effectively. </p><p id="p0144" num="0144">[00104] After the data has been captured and a three-dimensional model has been generated, it is displayed to the user. According to certain embodiments, the three- dimensional model is displayed to the user in real time as it is being generated and/or refined, enabling the user to stop capturing data when sufficient data is received. </p><p id="p0145" num="0145">[00105] The three-dimensional model is advantageously presented using thermal infrared highlighting, wherein visible image data and thermal infrared data is incorporated into an output image. However, as will be understood by the skilled addressee, any suitable method of presenting the three-dimensional model together with the visible image and thermal infrared data can be provided. </p><p id="p0146" num="0146">[00106] Thermal infrared highlighting involves applying a weighting scheme wherein weights are provided to thermal infrared and visible image data. In its simplest form, the weights are either 0 or 1, wherein either thermal infrared data or visible image data is displayed on a single portion of the output image, however other weights can also be used. </p><p id="p0147" num="0147">[00107] According to certain embodiments, thermal infrared highlighting is used to illustrate thermal infrared extremes. Thus visible image data is rendered in the output image where the thermal infrared data is near average, and thermal infrared data is rendered in the 
<!-- EPO <DP n="23"/>-->
 output image where the thermal infrared data is extreme, such as outside given standard deviations above or below a mean value. This serves the purpose of highlighting the "hottest" and/or the "coldest" surfaces with strong colours that make them highly salient to a human viewer, whilst still retaining colour and texture data for the majority of the scene. </p><p id="p0148" num="0148">[00108] In the medical field, the use of visible image data together with thermal infrared data may enhance identification of anomalies by presenting skin irregularities to the user. In the energy monitoring environment, it enables inspectors to know which thermal infrared data relates to what equipment, through images of the equipment's tags and surface colour, which is especially useful when the user wants to use generated images for offline tasks such as, for example, creating energy loss reports requiring an equipment's serial number. </p><p id="p0149" num="0149">[00109] Thermal infrared highlighting can also be applied to emphasize thermal infrared data in a particular range, rather than just the extremes. This is further illustrated below in FIG. 4. </p><p id="p0150" num="0150">[00110] According to certain embodiments, a first three-dimensional model is generated at a first point of time, and a second three-dimensional model is generated at a second point of time. The first and second three-dimensional models are aligned and compared. This can include an overlay of the three-dimensional models, a difference map generated for the visible image and/or thermal infrared data, or a side-by-side presentation of the models. This enables a user to detect thermal infrared changes in an object over time. </p><p id="p0151" num="0151">[00111] FIG. 2a illustrates a perspective view of the portable imaging device 105. FIG. 2b illustrates a side view of the portable imaging device 105. </p><p id="p0152" num="0152">[00112] As discussed above, the portable imaging device 105 includes the range sensor 1 10, the visible light image sensor 1 15 and the thermal infrared sensor 120. The range sensor 1 10 is positioned on a housing above and between the visible light image sensor 115 and the thermal infrared sensor 120. However, as will be readily understood by the skilled addressee, 
<!-- EPO <DP n="24"/>-->
 the range sensor 1 10, the visible light image sensor 1 15 and the thermal infrared sensor 120 can be placed in any suitable arrangement. </p><p id="p0153" num="0153">[00113] The housing of the portable imaging device 105 includes a handle portion 205, in which the trigger 125 is incorporated, and an elongate barrel-like portion 210, on the end of which the range sensor 1 10, the visible light image sensor 1 15 and the thermal infrared sensor 120 are located. This enables ergonomic, handheld operation of the portable imaging device 105 when moving it around the object, and gives the portable imaging device 105 a pistol-like shape, which makes it particularly easy to use without specific training. </p><p id="p0154" num="0154">[00114] FIG. 3a illustrates a perspective view of a portable imaging device 300, according to an alternative embodiment of the present invention. FIG. 3b illustrates a side view of the portable imaging device 300. </p><p id="p0155" num="0155">[00115] The portable imaging device 300 is similar to the portable imaging device 105 of FIG. 1. The portable imaging device 300 further includes a display screen 305 for displaying a captured image, for example an image received from the visible light image sensor 115. </p><p id="p0156" num="0156">[00116] The display screen 305 enables a user to more accurately target areas of the object, and/or see which areas of the object are currently being scanned. The display screen 305 can comprise a Liquid Crystal Display (LCD), for example, however various other types of display are also suitable. </p><p id="p0157" num="0157">[00117] According to certain embodiments, capture of thermal infrared data is activated by the trigger 125, whereas visible image and range data is continuously captured. This can be used where thermal infrared glare and reflection is a problem, as the user can select poses having minimal glare in real time by viewing the thermal infrared data on the display screen 305. </p><p id="p0158" num="0158">[00118] According to alternative embodiments (not shown), a system for generating a model of the object is housed in a portable imaging device similar to the portable imaging 
<!-- EPO <DP n="25"/>-->
 device 300. In such a case, data can be processed and a three-dimensional model can be generated by the portable imaging device. The three dimensional model, with overlaid visible image and thermal infrared data, can then be displayed on a screen of the portable imaging device. This enables the portable imaging device to be a standalone entity. </p><p id="p0159" num="0159">[0119] According to some embodiments of the present invention, an optional orientation or pose sensor (not shown) is provided for measuring the orientation and/or pose of the portable imaging device. The orientation or pose sensor can be of any suitable form, and can include one or more sensors, such as an inertial measurement unit (IMU), attached to the portable imaging device 105, 300. Such an orientation sensor can assist processing data from the sensors 110, 115, 120. </p><p id="p0160" num="0160">[0120] According to further embodiments, the system 100 or the portable imaging device 300 comprises an environmental sensor (not shown). The environmental sensor can estimate and compensate for any environmental impacts on measurements using the thermal infrared sensor 120. Examples of environments sensors include a carbon dioxide sensor, a humidity sensor and/or a temperature sensor. </p><p id="p0161" num="0161">[0121] According to further embodiments, the system 100 or the portable imaging device 300 includes an audio sensor (not shown). The audio sensor enables the user to verbally annotate the model of the object with semantic information such as condition of the objects, add time and date of the inspection or provide more information about an object under examination which are not detectable or visible by either thermal infrared or visible light image sensors. The annotation can then be extracted from recorded voice to text and overlayed on the generated 3D model, for example by storing the text as meta-data associated with the model. The audio sensor can also be used to provide voice commands to the system 100, for example to allow voice actuation of sensing, or the like. </p><p id="p0162" num="0162">[0122] According to yet further embodiments of the present invention, the portable imaging device 105, 300 can be mounted on a mobile platform for fine control motions (such as a mobile robot and/or arm or endoscopes) and/or can be tele-operated and/or pre-programmed and/or autonomously moved to. various poses around the object to complete the model. 
<!-- EPO <DP n="26"/>-->
 [0123] The three dimensional model can be generated based upon an assumption that the object does not move during the scan and does not deform. However, human subjects may move slightly during a scan period which may cause artifacts in the 3D model. Accordingly, deformable model tracking can be employed to jointly estimate a time- varying, dynamic shape of the object and its motion. This information can be combined with the surface reconstruction to provide a more robust 3D model of the object, and in particular of human and/or animal body parts. </p><p id="p0163" num="0163">[0124] FIG. 4 schematically illustrates a system 400 for generating a model of an object, according to an embodiment of the present invention. The system 400 is similar to the system 100, but with the addition of further sensors, as discussed below. </p><p id="p0164" num="0164">[0125] The system 400 includes a range sensor 405, an image sensor 410 and an thermal infrared sensor 415 similar to the range sensor 1 10, visible light image sensor 115 and thermal infrared image sensor 120 of FIG. 1, coupled to a processor 420. </p><p id="p0165" num="0165">[0126] The system further includes a microphone 425, for capturing voice from a user, and a temperature sensor 430, for measuring a temperature of the environment in which the system 400 is operating. </p><p id="p0166" num="0166">[0127] The temperature sensor 430 is an example of an environmental sensor, and other environmental sensors can be included in the system 400, including, for example, a carbon dioxide sensor. </p><p id="p0167" num="0167">[0128] The system 400 further includes a memory 435, coupled to the processor 420. The memory 435 includes instruction code executable by the processor 420 for generating a three-dimensional model based upon the range data captured from several positions and associating data from the visible image data and the thermal infrared data, from at least two positions, with the three dimensional model. Thermal infrared data is associated with the three dimensional model based upon an angle of incidence of the thermal infrared data, a distance between the camera and the object during capture, and data from the temperature sensor 430. 
<!-- EPO <DP n="27"/>-->
 [0129] Accordingly, the system 400 is able to select thermal infrared data having high confidence values, such as thermal infrared data having a low angle of incidence that is recorded close to the object and away from high temperature areas. Thus thermal infrared reflection and errors in the thermal infrared data caused by local environmental impact can be avoided in the model. </p><p id="p0168" num="0168">[0130] As discussed earlier, the user is able to verbally annotate the model of the object with semantic information or provide more information about an object under examination, and such information is able to be captured by the microphone 425. </p><p id="p0169" num="0169"> I. </p><p id="p0170" num="0170"> [0131] FIG. 5 illustrates a screenshot 500 of a display of the system 100, illustrating a rendering of a three-dimensional model 505 with visible image data 510 and thermal infrared data 515. The three-dimensional model 505 corresponds to an upper body of a person, and the person's upper back is viewed in the screenshot 500. </p><p id="p0171" num="0171">[0132] The visible image data 510 includes skin tone 520, hair colour 525 and other visual identifiers of the person. For example, a tattoo (not shown) or scarring (not shown) can be present in the visible image data 510. </p><p id="p0172" num="0172">[0133] The thermal infrared data 515 comprises thermal infrared data, corresponding to a temperature measured on the skin of the person. The thermal infrared data 515 can be colour coded, for example where purple corresponds to approximately 33.0 degrees Celsius, blue corresponds to approximately 33.2 degrees Celsius, green corresponds to approximately 33.5 degrees Celsius, yellow corresponds to approximately 33.7 degrees Celsius and red corresponds to approximately 34 degrees Celsius. </p><p id="p0173" num="0173">[0134] In the example provided in FIG. 5, the thermal infrared data 515, is rendered onto the three-dimensional model when a temperature of the skin is between 33.0 and 34.0 degrees Celsius. For all temperatures below this range, visible image data 510, including skin tone 520, is displayed. Accordingly, one or more threshold values can be used to determine if thermal infrared data is to be displayed or not. 
<!-- EPO <DP n="28"/>-->
 [0135] According to certain embodiments, a user of the system 100 can adjust the temperature range upon which thermal infrared data is overlaid. In one scenario, the user can choose to select to render thermal infrared data over the entire three-dimensional model. In other scenario, a user can choose to render a very narrow range of thermal infrared data corresponding to peak temperatures of the object. </p><p id="p0174" num="0174">[0136] As an example, it is considered that the person rendered in the screenshot 500 lias chronic pain in the right-hand shoulder. Thus an asymmetry in the skin temperature can clearly be recognized, in particular in an upper right portion 530 corresponding to the right- hand shoulder. </p><p id="p0175" num="0175">[0137] Variations in surface temperature on the skin are low. Nevertheless, an asymmetry in the heat distribution is apparent, particularly in upper right portion 530, suggesting that there is more blood flowing to the affected (right) shoulder. </p><p id="p0176" num="0176">[0138] FIG. 6 diagrammatically illustrates the computing apparatus 130, according to an embodiment of the present invention. </p><p id="p0177" num="0177">[0139] The computing apparatus 130 includes a central processor 602, a system memory 604 and a system bus 606 that couples various system components, including coupling the system memory 604 to the central processor 602. The system bus 606 may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. The structure of system memory 604 is well known to those skilled in the art and may include a basic input/output system (BIOS) stored in a read only memory (ROM) and one or more program modules such as operating systems, application programs and program data stored in random access memory (RAM). </p><p id="p0178" num="0178">[0140] The computing apparatus 130 may also include a variety of interface units and drives for reading and writing data. In particular, the computing apparatus 130 includes a hard disk interface 608 and a removable memory interface 610, respectively coupling a hard disk drive 612 and a removable memory drive 614 to the system bus 606. Examples of removable memory drives 614 include magnetic disk drives and optical disk drives. The drives and their 
<!-- EPO <DP n="29"/>-->
 associated computer-readable media, such as a Digital Versatile Disc (DVD) 616 provide non-volatile storage of computer readable instructions, data structures, program modules and other data for the computer system 600. A single hard disk drive 612 and a single removable memory drive 614 are shown for illustration purposes only and with the understanding that the computing apparatus 130 may include several similar drives. Furthermore, the computing apparatus 130 may include drives for interfacing with other types of computer readable media. </p><p id="p0179" num="0179">[0141] The computing apparatus 130 may include additional interfaces for connecting devices to the system bus 606. FIG. 6 shows a universal serial bus (USB) interface 618 which may be used to couple a device to the system bus 606. For example, an IEEE 1394 interface 620 may be used to couple additional devices to the computing apparatus 130. </p><p id="p0180" num="0180">[0142] The computing apparatus 130 can operate in a networked environment using logical connections to one or more remote computers or other devices, such as a server, a router, a network personal computer, a peer device or other common network node, a wireless telephone or wireless personal digital assistant. The computing apparatus 130 includes a network interface 622 that couples the system bus 606 to a local area network (LAN) 624. Networking environments are commonplace in offices, enterprise-wide computer networks and home computer systems. </p><p id="p0181" num="0181">[0143] A wide area network (WAN), such as the Internet, can also be accessed by the computing apparatus 130, for example via a modem unit connected to a serial port interface 626 or via the LAN 624. </p><p id="p0182" num="0182">[0144] It will be appreciated that the network connections shown and described are exemplary and other ways of establishing a communications link between computers can be used. The existence of any of various well-known protocols, such as TCP/IP, Frame Relay, Ethernet, FTP, HTTP and the like, is presumed, and the computing apparatus 130 can be operated in a client-server configuration to permit a user to retrieve web pages from a web- based server. Furthermore, any of various conventional web browsers can be used to display and manipulate data on web pages. 
<!-- EPO <DP n="30"/>-->
 [0145] The operation of the computing apparatus 130 can be controlled by a variety of different program modules. Examples of program modules are routines, programs, objects, components, and data structures that perform particular tasks or implement particular abstract data types. The present invention may also be practiced with other computer system configurations, including hand-held devices, multiprocessor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, personal digital assistants and the like. Furthermore, the invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote memory storage devices. </p><p id="p0183" num="0183">[0146] Accordingly, it will be appreciated that the computing apparatus 130 may be formed from any suitable processing system, such as a suitably programmed computer system, PC, web server, network server, or the like. In one particular example, the processing systems 201 are standard processing system, which executes software applications stored on non- volatile (e.g., hard disk) storage, although this is not essential. However, it will also be understood that the processing system could be or could include any electronic processing device such as a microprocessor, microchip processor, logic gate configuration, firmware optionally associated with implementing logic such as an FPGA (Field Programmable Gate Array), or any other electronic device, system or arrangement. The term electronic processing device is therefore intended to cover any suitable processing or computing apparatus, and is not intended to be limiting. </p><p id="p0184" num="0184">[0147] It will also be appreciated that whilst a separate computer system 130 is shown in the above examples, this is not essential and alternatively some or all of the functionality of the computer system 130, could be built into the handset. For example, the hand-held device could include a processor for generating a three dimensional model, with this being transferred wirelessly to a separate display device, such as a tablet computer, mobile phone, or the like. In a further example, the sensors could be integrated into a portable device including sensors, such as a mobile phone, or similar that includes range and thermal sensors. It will be further appreciated that in such a situation, in the event that processing 
<!-- EPO <DP n="31"/>-->
 requirements exceed capabilities of the device, the data could be transferred wirelessly to another device for processing, with the model being returned for display in substantially realtime. </p><p id="p0185" num="0185">[0148] FIG. 7 illustrates a method 700 of generating a three-dimensional model of an object, according to an embodiment of the present invention. </p><p id="p0186" num="0186">[0149] In step 705, range data, image data and thermal infrared data is received from a range sensor, an image sensor and an thermal infrared sensor. The range sensor, the image sensor and the thermal infrared image sensor have overlapping fields of view, and can thus capture data from the object simultaneously. The range data, image data and thermal infrared data each correspond to data from at least two different positions, and typically will include data from several different positions. </p><p id="p0187" num="0187">[0150] In step 710, a three-dimensional model is generated based upon the range data. The three dimensional model is generated based upon the range data from several positions and geometries. </p><p id="p0188" num="0188">[0151] In step 715, data from the image data and the thermal infrared data is associated with the three dimensional model. The data is captured from at least two different positions of the image sensor and the thermal infrared sensor, thus providing more than a single view of image and thermal infrared data. The different positions comprise arbitrary positions, wherein at least a part of the object is visible in a first position of the at least two different positions, and a second part is visible in a second position of the at least two positions. Thus there is no need to follow a strict pattern, or calibrate camera positions. </p><p id="p0189" num="0189">[0152] In step 720, which is an optional step, the method further comprises generating an output image corresponding to a view of the three dimensional model. The output image includes at least part of the associated image data and at least part of the associated thermal infrared data superimposed on the model. One part of the output image can contain thermal infrared data, whereas another part contains image data, for example. Alternatively, image 
<!-- EPO <DP n="32"/>-->
 W 201 </p><p id="p0190" num="0190">- 30 - data and thermal infrared data can be combined on a same part of the output image, as discussed above. </p><p id="p0191" num="0191">[0153] Thus, the model can incorporate model data derived from the thermal infrared data and optionally the visible image data allowing a thermal infrared and/or colour model of the object to be provided. Furthermore, the thermal infrared and colour model can be combined in a fused model, so that the fused model highlights specific attributes of interest. For example, the fused model could display colour over a majority of the model, with thermal infrared representations being shown only in the event that temperatures fall outside a predetermined range. Thus parameters for the model can be defined including a normal or expected temperature range, with indications of the temperature being shown in parts of the model where the temperature falls outside the normal or expected range. </p><p id="p0192" num="0192">[0154] In order to generate the model, the computer system, and in particular the electronic processing device selects an object surface part, such as a vertex, or the like, and then identifies images of the selected object surface part whilst the imaging device is in at least two different poses relative to the object. The images will include thermal infrared and/or visible images, allowing temperature or colour to be added to the model. The computer system then identifies at least one image pixel corresponding to the selected object surface part for each identified image and uses the identified image pixels to determine the model data. Thus, by using pixels from multiple different images, this can help in mitigating the effect of reflections, leading to more reliable temperature and/or colour representation. </p><p id="p0193" num="0193">[0155] Typically, the electronic processing device associates the data with a voxel in the three dimensional model corresponding to the selected object surface part. The electronic processing device determines the model data using a weighted sum of data associated with the identified pixels of images from different poses, allowing reflections from the surface of the object to be accounted for. In one example, the electronic processing device achieves this by using a confidence score associated with the identified pixels in different images. The confidence score can be based on factors including at least one of: a velocity of a sensor when the image was captured; a position of the one or more pixels in a field of view of the 
<!-- EPO <DP n="33"/>-->
 sensor; an angle between a ray and an object surface, the ray extending from the object surface part to the sensor; and, the range when the image was captured. </p><p id="p0194" num="0194">[0156] Thus, the above described system can generate large-scale 3D models with accurate surface temperature information. This is achieved as the result of two main processes. First, the trajectory of the sensor in 3D space is simultaneously estimated along with the 3D structure of the object or environment being explored. This is done by employing a SLAM (Simultaneous Localization And Mapping) solution that can function in real-time. Second, a raycasting method is applied to accurately assign temperature and colour estimates to the 3D model from multiple views. This method implements a multi-variable weighting scheme to achieve more accurate estimates than previous methods. </p><p id="p0195" num="0195">[0157] Visualization for the output of the system comes in two forms; online/real-time visualization for verification of the completeness and integrity of the model as data is being captured, and offline visualization for in-depth analysis and the optimization of display properties. This visualization capability includes the conversion of the model into one of a number of alternative multimodal representations that allow temperature and visible- spectrum information to be viewed simultaneously. </p><p id="p0196" num="0196">[0158] Offline processing post-capture can be advantageous as the relaxed restrictions on processing time allow more precise and thorough calculations to be performed, resulting in improvements in spatial resolution and accuracy relative to the real-time model. However, it will be appreciated that this will depend on the circumstances in which the system is to be used. </p><p id="p0197" num="0197">[0159] To generate accurate 3D models which can be visualized in real-time, a SLAM solution is used, which utilizes a video stream of range images from the range sensor to continuously update and optimize a 3D voxel model. The algorithm also generates an accurate trajectory estimate of the device throughout the sequence, with 6 degrees of freedom, namely translation about X, Y and Z positions, and rotation about all three axes. In this example, a GPU (Graphics Processing Unit) can be heavily utilized in order to perform this process quickly so that it can keep up with the frame rate of the range sensor. 
<!-- EPO <DP n="34"/>-->
 [0160] The first range image received from the range sensor is processed to initialize a voxel occupancy map stored on the GPU. As further range images are received, they are incrementally registered to this 3D model using the Iterative Closest Point (ICP) algorithm. This process also leads to an estimate of the relative six degrees of freedom pose of the camera for the new frame, and this pose is utilized to further optimize the voxel occupancy map. </p><p id="p0198" num="0198">[0161] Thermal infrared data is calibrated so that each pixel of the thermal infrared image contains an estimate of the temperature of the surface rather than a unitless digital value. </p><p id="p0199" num="0199">[0162] Following this, temperature estimates can be accurately assigned to a 3D model using thermal infrared-thermal infrared image data. In one example, this process considers a variety of factors that influence the reliability of temperature measurements, as well as utilizing a large number of images, in order to improve the estimation of temperature for surfaces in the 3D model. Furthermore, many of the advantages of the method apply equally to the visible modality so the technique is also used to improve the raycasting of colours on the model. </p><p id="p0200" num="0200">[0163] Given the estimate of the pose of the range sensor for all frames, which is an output of the aforementioned SLAM algorithm, estimates of the pose of the colour and thermal infrared camera for all their respective frames can be made. The accuracy of the estimates depend on the effectiveness of both the geometric and temporal calibration procedures. </p><p id="p0201" num="0201">[0164] For a single image, the estimated pose of the camera relative to the 3D model and the camera instrinsic parameters can be utilized to perform raycasting. This refers to the process through which pixels in the image are associated with vertices in the 3D model. </p><p id="p0202" num="0202">[0165] Given that the video sequences have many frames, most vertices will have many images and therefore many rays (pixels) associated with them. A naive approach would simply use a single pixel value to assign an estimate to each vertex. A more sophisticated approach may average the pixel values that are associated with each vertex, which is likely to reduce noise and achieve a slight improvement in accuracy. However, this assumes that each ray (the association between a pixel and a vertex) is equally valid. 
<!-- EPO <DP n="35"/>-->
 [0166] For the preferred approach however, temperatures (and colours) are assigned based on a weighted sum of pixel values. The overall weighting for a ray W<sub>ray</sub> (corresponding to an image pixel) is based on a number of factors that reflect how reliable that ray is for estimating the value for the vertex. </p><p id="p0203" num="0203">[0167] In one example, the following factors can be considered for assigning a confidence level to a ray (for both thermal infrared or visible imaging modalities unless otherwise stated), including the velocity of the camera, the position of the pixel in the field of view, the angle between the ray and the surface normal of the vertex, the distance of the vertex from the camera, the validity of the sensor or sensor calibration over the operating range, as well as external factors, such as the external ambient temperature. </p><p id="p0204" num="0204">[0168] For example, for the velocity of the sensor is high, for example if the hand-held imaging device is moving faster (either in translation or rotation), motion blur in the resulting image would decrease the accuracy of the ray. Accordingly, for higher velocities, the pixel is given a lower confidence level. Regarding the position of the pixel, a lower confidence is given to rays associated with pixels located far from the camera centre, based on the assumption that points closer to the center have less distortion, and less noise. For the angle between the ray and the normal of the vertex, rays that are closer to parallel to the surface are likely to be less accurate due to the fact that slight errors in pose estimation have a larger effect at these angles, and hence these are given a lower confidence score. In considering the distance of the object surface from the sensor, a higher confidence is assigned to surfaces close to the camera, up to a certain distance. Sensor calibration is taken based on a calibration model, if required. Finally, for external factors, these could depend on readings from sensors. So for example, a lower confidence might be assigned to pixels that are close to an ambient temperature, as these may be harder to measure more accurately. </p><p id="p0205" num="0205">[0169] However, it will be appreciated that any suitable combination of factors could be used. </p><p id="p0206" num="0206">[0170] In any event, once an overall confidence score has been determined for each this allows the values for pixels across multiple images to be combined based on the confidence 
<!-- EPO <DP n="36"/>-->
 scores, thereby providing an overall value (either temperature or colour) that can be associated with part of the object surface, such as a vertex. In this manner, a temperature and/or colour profile can be established for the entire surface of the object, whilst allowing reflections or the like to be accounted for so that the three dimensional model of the object represents the true temperature or colour of the object. </p><p id="p0207" num="0207">[0171] In summary, advantages of some embodiments of the present invention include an ability to generate a holistic and geometrically accurate three-dimensional model of an object including thermal infrared and image data, without requiring complex and/or expensive hardware. </p><p id="p0208" num="0208">[0172] The present invention enables rapid three-dimensional visualization of thermal infrared information, which is particularly advantageous in the context of fire management and response, as well as within an electrical or structural building inspection. </p><p id="p0209" num="0209">[0173] The three-dimensional model provides an opportunity for a user to investigate and detect anomalies of an element or object, in a context of its surroundings. Such three- dimensional models can be utilized by inspectors for monitoring non-visible thermal infrared irregularities of buildings, or by medical practitioners to identify thermal infrared abnormalities in a patient. </p><p id="p0210" num="0210">[0174] According to certain embodiments, image and thermal infrared data can be viewed together, on a single model, which enables more accurate localisation of problems. For example, in an electrical maintenance context, the present invention enables a user to determine which of many electrical connections is heating up abnormally due to a faulty connection. The image data enables the user to view a text or colour-based label, whereas the thermal infrared data can be used to detect the heat caused by the fault. </p><p id="p0211" num="0211">[0175] Also, the present invention is able to operate in darkness, which is advantageous when explicit night-time analysis is called for by applications such as building energy auditing. Additionally, patients may prefer to be scanned in darkness rather than in a bright room. 
<!-- EPO <DP n="37"/>-->
 [0176] Finally, the present invention does not require an external tracking device in order to accurately estimate a pose of the device, and can be implemented using simple, off-the-shelf components. </p><p id="p0212" num="0212">[0177] The above description focuses primarily on applications in the medical and energy auditing areas, however the present invention is useful in various other applications. These include application in the manufacturing sector, where thermal infrared profiles of moulds can be created to determine mould wear, cracks, and improper cooling, and the agriculture sector, where, for example, the quality of fruit, vegetables and grains can be determined. In particular, early detection of fungal infection in cereal grains can be performed as thermal infrared data can be integrated with optical image data to detect different fungal species caused by colour differences. </p><p id="p0213" num="0213">[0178] For example the system can be used in the medical field. This can include performing health inspections in working sites, monitoring effects such as vibration white fingers (circulatory/nerve damage due to use of certain tools and machinery), improving the decompression time and experience of divers, inspecting blood flow in the skin to test physiology and sympathetic nervous system responses, thermographic imaging to scan for fevers at airports and other transport hubs, as well as search and rescue situations. The system can be further used for diagnosing or validating treatment for inflammatory arthritis, Osteoarthritis, Tennis elbow, Peripheral circulation, melanoma, other cancers (breast), validation of criotherapy and diabetes. </p><p id="p0214" num="0214">[0179] Additionally, diagnosis of skin conditions and disorders, musculoskeletal diseases well as diseases that may leave a thermal signature, such as cancer. This can include thermography to detect breast cancer. Monitoring of these conditions requires obtaining multiple readings during periodic consultations, which requires fixed systems to take readings from the exact space locations every time. A portable 3D system may reduce this need. A portable technology may take diagnosis and medical services outside hospitals (remote locations). 
<!-- EPO <DP n="38"/>-->
 [0180] In the agriculture field, the system can be used in quality evaluation of fruits and vegetables, measurement of maturity, size and number of fruits on trees, wheat class identification, quantification of non-uniform heating of cereal grains and oil seeds during cooking and microwave treatment, predicting water stress in crops and planning irrigation scheduling, bruise detection in fruits and vegetables, disease and pathogen detection in plants. There is also the potential for using thermal imaging system for early detection of fungal infection in cereal grains, integrating thermal imaging with optical imaging to take advantage of colour differences caused by different fungal species </p><p id="p0215" num="0215">[0181] The system can be used in construction for diagnostics of building envelopes both after completing the building and during the operation period, determination of heat losses in facades, detection of defects of interpanel and expansion seams, monitoring of curing and/or drying of materials, or the like. </p><p id="p0216" num="0216">[0182] The system can be used by fire services, for example in field operations ranging from fire attack or detection, search/rescue, hot spot detection, overhaul activities, detecting the location of hazardous materials, including the detection of gases. </p><p id="p0217" num="0217">[0183] The system has industrial applications, such as monitoring electrical installations/equipment inspection (loose fuse connections, overload voltage in circuits), mechanical equipment inspection (overheated components, gas and liquid leakages), factory building diagnostics (moisture and water damages on floors and walls), process monitoring, such as curing, drying and thermal treatments, monitoring energy losses in industrial environments for cost saving purposes (missing insulations faulty HVAC systems), inspection of tank levels, lines, smoke pipes, gas flues and boilers for safety and operational efficiency purposes, detection of welding flaws in the polyethylene pipes (usage of polyethylene pipes which are being used more and more every day, including gas pipelines). </p><p id="p0218" num="0218">[0184] In manufacturing the system can be used for production of turboshaft engines for aircraft (use of 3D technology for creating images of components), creating thermal profiles of molds in industries such as plastics, glass, metals, and ceramics, to determine mold, identifying wear, cracks, and improper cooling, improving mold design or the like. This can 
<!-- EPO <DP n="39"/>-->
 include monitoring lines covered with very high levels of insulation, or shiny aluminium or stainless steel sheet metal coverings, which is currently difficult due to the high thermal reflectivity/low emissivity of the surfaces encountered, which can be obviated using the current system. </p><p id="p0219" num="0219">[0185] In domestic environments, the system can be used for monitoring energy losses for cost saving purposes (building and equipment inspection), identifying destructive pests (insects, rodents, termites). </p><p id="p0220" num="0220">[0186] The system can be used for security and surveillance, such as perimeter protection, access control, night vision monitoring systems, or the like, as well as providing enhanced vision systems for cars (image from pedestrians, cyclists, animals and other roadside objects). </p><p id="p0221" num="0221">[0187] It will also be appreciated that the system could be implemented with additional features, such as enhanced image/signal processing for better detail, video analytics for automated detection, threat assessment and response, high resolution for longer range performance and better image quality, higher-performance light sources and improved optics. </p><p id="p0222" num="0222">[0188] It will also be appreciated that the above described uses are not intended to be limiting and that the apparatus can be used in a wide range of activities and circumstances. </p><p id="p0223" num="0223">[0189] The above description of various embodiments of the present invention is provided for purposes of description to one of ordinary skill in the related art. It is not intended to be exhaustive or to limit the invention to a single disclosed embodiment. As mentioned above, numerous alternatives and variations to the present invention will be apparent to those skilled in the art of the above teaching. Accordingly, while some alternative embodiments have been discussed specifically, other embodiments will be apparent or relatively easily developed by those of ordinary skill in the art. Accordingly, this patent specification is intended to embrace all alternatives, modifications and variations of the present invention that have been discussed herein, and other embodiments that fall within the spirit and scope of the above described invention. 
</p></description><claims mxw-id="PCLM70078944" ref-ucid="WO-2014127431-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="40"/>--> THE CLAIMS DEFINING THE INVENTION ARE: </claim-statement><claim id="clm-0001" num="1"><claim-text> 1. A system for generating a three-dimensional model of an object, the system including: </claim-text><claim-text> a portable hand-neld imaging device including: </claim-text><claim-text> a housing; </claim-text><claim-text> a plurality of sensors attached to the housing; </claim-text><claim-text> at least one electronic processing device coupled to the plurality of sensors, wherein the electronic processing device: </claim-text><claim-text> determines from the plurality of sensors and whilst the imaging device is in at least two different poses relative to the object: </claim-text><claim-text> range data indicative of a range of the object; </claim-text><claim-text> thermal infrared data indicative of a thermal infrared image of the object; </claim-text><claim-text> generates a three-dimensional model based at least partially upon the range data from the at least two different poses; and </claim-text><claim-text> associates model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object. </claim-text></claim><claim id="clm-0002" num="2"><claim-text> 2. The system of claim 1, wherein the electronic processing device: </claim-text><claim-text> determines colour data indicative of a visible image of the object; and </claim-text><claim-text> associates model data derived from the visible image data from the at least two different poses with the three dimensional model to thereby provide a three dimensional colour model of the object. </claim-text></claim><claim id="clm-0003" num="3"><claim-text> 3. The system of claim 2, wherein the electronic processing device generates a three dimensional fused thermal infrared and colour model of the object. </claim-text></claim><claim id="clm-0004" num="4"><claim-text> 4. The system of claim 2 or claim 3, wherein the system includes: </claim-text><claim-text> a range sensor attached to the housing that senses a range of the object: </claim-text><claim-text> a visible light image sensor attached to the housing that senses a visible image of the object; and 
<!-- EPO <DP n="41"/>-->
 a thermal infrared image sensor attached to the housing that senses a thermal infrared image of the object. </claim-text></claim><claim id="clm-0005" num="5"><claim-text> 5. The system of any one of the claims 1 to 4, wherein the electronic processing device; determines a relative pose of the hand-held device; and, </claim-text><claim-text> uses the relative pose to generate the three dimensional model. </claim-text></claim><claim id="clm-0006" num="6"><claim-text> 6. The system of claim 5, wherein the electronic processing device determines the relative pose of the hand-held device using at least one of range data, image data and thermal infrared data from at least some of the different poses. </claim-text></claim><claim id="clm-0007" num="7"><claim-text> 7. The system of claim 6, wherein the electronic processing device: </claim-text><claim-text> estimates a timing skew between the range sensor and at least one of a thermal infrared image sensor and a visible light image sensor; and </claim-text><claim-text> estimates a pose associated with the image data and/or the thermal infrared data based upon the skew. </claim-text></claim><claim id="clm-0008" num="8"><claim-text> 8. The system of claim 7, wherein the electronic processing device: </claim-text><claim-text> determines a device pose using at least one of the range sensor and a pose sensor; determines a path of movement of the hand-held device; </claim-text><claim-text> determines a visible light image sensor pose and a thermal infrared image sensor pose based on the device pose and the path of movement. </claim-text></claim><claim id="clm-0009" num="9"><claim-text> 9. The system of any one of the claims 5 to 8, wherein the system includes a pose sensor and wherein the electronic processing device determines the relative pose using pose data <sup>*</sup> from the pose sensor. </claim-text></claim><claim id="clm-0010" num="10"><claim-text> 10. The system of claim 9, wherein the pose sensor is at least one of an orientation sensor and an inertial measurement unit. </claim-text></claim><claim id="clm-0011" num="11"><claim-text> 1 1. The system of any one of the claims 1 to 10, wherein the electronic processing device: selects an object surface part; </claim-text><claim-text> identifies images of the selected object surface part whilst the imaging device is in at least two different poses relative to the object; </claim-text><claim-text> identifies at least one image pixel corresponding to the selected object surface part for each identified image; and, </claim-text><claim-text> uses the identified image pixels to determine the model data. 
<!-- EPO <DP n="42"/>-->
</claim-text></claim><claim id="clm-0012" num="12"><claim-text>12. The system of claim 1 1 , wherein for a thermal infrared image the model data includes model thermal infrared data indicative of a temperature and for a visible image the model data includes model colour data indicative of a colour. </claim-text></claim><claim id="clm-0013" num="13"><claim-text> 13. The system of claim 1 1 or claim 12, wherein the electronic processing device associates the data with a voxel in the three dimensional model corresponding to the selected object surface part. </claim-text></claim><claim id="clm-0014" num="14"><claim-text> 14. The system of any one of the claims 11 to 13, wherein the selected object surface part is a vertex. </claim-text></claim><claim id="clm-0015" num="15"><claim-text> 15. The system of any one of the claims 1 1 to 14, wherein the electronic processing device determines the model data using a weighted sum of data associated with the identified pixels. </claim-text></claim><claim id="clm-0016" num="16"><claim-text> 16. The system of claim 15, wherein the electronic processing device determines the weighted sum using a confidence score associated with the identified pixels. </claim-text></claim><claim id="clm-0017" num="17"><claim-text> 17. The system of claim 16, wherein the electronic processing device determines the confidence score based on factors including at least one of: </claim-text><claim-text> a velocity of a sensor when the image was captured; </claim-text><claim-text> a position of the one or more pixels in a field of view of the sensor; </claim-text><claim-text> an angle between a ray and an object surface, the ray extending from the object surface part to the sensor; and, </claim-text><claim-text> the range when the image was captured. </claim-text></claim><claim id="clm-0018" num="18"><claim-text> 18. The system of claim 17, wherein, for a thermal infrared image, the electronic processing device determines the. confidence score based on a temperature value associated with the identified pixels. </claim-text></claim><claim id="clm-0019" num="19"><claim-text> 19. A method of generating a three-dimensional model of an object, the method including, in an electronic processing device: </claim-text><claim-text> determining from the plurality of sensors and whilst an imaging device is in at least two different poses relative to^the object: </claim-text><claim-text> range data indicative of a range of the object; </claim-text><claim-text> thermal infrared data indicative of a thermal infrared image of the object; 
<!-- EPO <DP n="43"/>-->
 generating a three-dimensional model based at least partially upon the range data from the at least two different poses; and </claim-text><claim-text> associating model data derived from the thermal infrared data from the at least two different poses with the three dimensional model to thereby provide a three dimensional thermal infrared model of the object. </claim-text></claim><claim id="clm-0020" num="20"><claim-text> 20. A system for generating a three-dimensional model of an object, the system including: </claim-text><claim-text> a portable imaging device including: </claim-text><claim-text> a housing; </claim-text><claim-text> a range sensor attached to the housing; </claim-text><claim-text> a visible light image sensor attached to the housing; and </claim-text><claim-text> an thermal infrared image sensor attached to the housing, wherein the range sensor, the visible light image sensor and the thermal infrared image sensor have overlapping fields of view; </claim-text><claim-text> at least one processor coupled to the range sensor, the visible light image sensor and the thermal infrared image sensor of the portable imaging device; and </claim-text><claim-text> a memory, coupled to the at least one processor, including instruction code executable by the processor for: </claim-text><claim-text> receiving range data from the range sensor, image data from the visible light image sensor, and thermal infrared data from the thermal infrared image sensor; </claim-text><claim-text> generating a three-dimensional model based upon the range data captured from several positions; and </claim-text><claim-text> associating data from the image data and the thermal infrared data, from at least two positions, with the three dimensional model. </claim-text></claim><claim id="clm-0021" num="21"><claim-text> 21. The system of claim 20, wherein associating the image data and the thermal infrared data with the three-dimensional model comprises estimating a timing skew between the range sensor and at least one of the visible light image sensor and the thermal infrared image sensor, and estimating a pose associated with the image data and/or the thermal infrared data based upon the skew. 
<!-- EPO <DP n="44"/>-->
 - 42 - </claim-text></claim><claim id="clm-0022" num="22"><claim-text>22. The system of claim 20 or claim 21, wherein the thermal infrared data is associated with the three-dimensional model based at least partly upon an angle of incidence. </claim-text></claim><claim id="clm-0023" num="23"><claim-text> 23. The system of any one of the claims 20 to 22, wherein the portable imaging device further comprises an orientation sensor. </claim-text></claim><claim id="clm-0024" num="24"><claim-text> 24. The system of any one of the claims 20 to 23, wherein the system further comprises at least one environmental sensor, wherein the thermal infrared data is associated with the three- dimensional model based at least partly upon data from the at least one environmental sensor.</claim-text></claim><claim id="clm-0025" num="25"><claim-text>25. The system of claim 24, wherein the at least one environmental sensor comprises a carbon dioxide sensor, a humidity sensor and/or a temperature sensor. </claim-text></claim><claim id="clm-0026" num="26"><claim-text> 26. The system of any one of the claims 20 to 25, wherein the system further comprises an audio sensor for recording an operator's voice during operation of the system. </claim-text></claim><claim id="clm-0027" num="27"><claim-text> 27. The system of any one of the claims 20 to 26, wherein the housing of the portable imaging device comprises an elongate portion, wherein the range sensor, visible light image sensor and thermal infrared image sensor are attached to an end of the elongate portion. </claim-text></claim><claim id="clm-0028" num="28"><claim-text> 28. The system of any one of the claims 20 to 27, wherein the housing of the portable imaging device comprises a trigger for activating at least one of the range sensor, the visible light image sensor and the thermal infrared image sensor. </claim-text></claim><claim id="clm-0029" num="29"><claim-text> 29. The system of any one of the claims 20 to 28, wherein the at least one processor is coupled to the range sensor, visible light image sensor and thermal infrared image sensor of the portable imaging device at least in part by a wireless data communication link. </claim-text></claim><claim id="clm-0030" num="30"><claim-text> 30. The system of any one of the claims 20 to 29, wherein the thermal infrared image sensor comprises a thermal infrared sensor. </claim-text></claim><claim id="clm-0031" num="31"><claim-text> 31. The system of any one of the claims 20 to 30, wherein the memory further includes instruction code executable by the processor for: </claim-text><claim-text> generating an output image corresponding to a view of the three-dimensional model, the output image including at least part of the associated image data superimposed on the model and at least part of the associated thermal infrared data superimposed on the model.</claim-text></claim><claim id="clm-0032" num="32"><claim-text>32. The system of any one of the claims 20 to 31, wherein the output image is generated by rendering at least part of the associated visible light image data on a first portion of the 
<!-- EPO <DP n="45"/>-->
 model and at least part of the associated thermal infrared data on a second portion of the model. </claim-text></claim><claim id="clm-0033" num="33"><claim-text> 33. The system of any one of the claims 20 to 32, wherein the three-dimensional model corresponds to at least a part of a person's body and the associated image data includes skin tones of the person. </claim-text></claim><claim id="clm-0034" num="34"><claim-text> 34. The system of any one of the claims 20 to 33, wherein the memory further includes instruction code executable by the processor for refining the three-dimension model based upon the image data and/or the thermal infrared data. </claim-text></claim><claim id="clm-0035" num="35"><claim-text> 35. The system of any one of the claims 20 to 34, wherein the a portable imaging device further includes a display, for displaying data captured from one or more of the range sensor, visible light image sensor and thermal infrared image sensor, in real time as it is captured.</claim-text></claim><claim id="clm-0036" num="36"><claim-text>36. A method of generating a three-dimensional model of an object, the method including: </claim-text><claim-text> receiving, from a range sensor, a visible light image sensor and an thermal infrared image sensor, range data, image data and thermal infrared data, wherein the range sensor, the visible light image sensor and the thermal infrared image sensor have overlapping fields of view, and wherein the range data, image data and thermal infrared data each correspond to data from at least two different positions of the range sensor, visible light image sensor and thermal infrared image sensor; </claim-text><claim-text> generating a three-dimensional model based upon the range data; and </claim-text><claim-text> associating data from the image data and the thermal infrared data, corresponding to the at least two different positions, with the three dimensional model. </claim-text></claim><claim id="clm-0037" num="37"><claim-text> 37. The method of claim 36, wherein the at least two different positions comprise arbitrary positions, wherein at least a part of an object visible in a first position of the at least two different positions is visible in a second position of the at least two positions. </claim-text></claim><claim id="clm-0038" num="38"><claim-text> 38. The method of claim 37, wherein the arbitrary positions- comprise overlapping views caused by movement of the range sensor, the visible light image sensor and the thermal infrared image sensor across the object. </claim-text></claim><claim id="clm-0039" num="39"><claim-text> 39. The method of any one of the claims 36 to 38, further comprising generating an output image corresponding to a view of the three dimensional model, the output image 
<!-- EPO <DP n="46"/>-->
 including at least part of the associated image data superimposed on the model and at least part of the associated thermal infrared data superimposed on the model. 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
