<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959454-A1" country="EP" doc-number="2959454" kind="A1" date="20151230" family-id="48917235" file-reference-id="311199" date-produced="20180825" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160453207" ucid="EP-2959454-A1"><document-id><country>EP</country><doc-number>2959454</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-13824663-A" is-representative="NO"><document-id mxw-id="PAPP193869382" load-source="docdb" format="epo"><country>EP</country><doc-number>13824663</doc-number><kind>A</kind><date>20131210</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193869383" load-source="patent-office" format="original"><country>EP</country><doc-number>13824663.2</doc-number><date>20131210</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162033968" ucid="IL-2013051020-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>IL</country><doc-number>2013051020</doc-number><kind>W</kind><date>20131210</date></document-id></priority-claim><priority-claim mxw-id="PPC162031176" ucid="IL-22489613-A" load-source="docdb"><document-id format="epo"><country>IL</country><doc-number>22489613</doc-number><kind>A</kind><date>20130225</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1988524075" load-source="docdb">G06T   7/00        20060101AFI20140909BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1988526961" load-source="docdb">G06T   7/40        20060101ALI20140909BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1824856234" load-source="docdb" scheme="CPC">G06T   5/50        20130101 FI20170408BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1858707868" load-source="docdb" scheme="CPC">G06T   7/194       20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1861375302" load-source="docdb" scheme="CPC">G06T   7/11        20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1861376969" load-source="docdb" scheme="CPC">G06T   7/44        20170101 LI20170102BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984697026" load-source="docdb" scheme="CPC">G06T2207/20104     20130101 LA20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700104" load-source="docdb" scheme="CPC">G06T2207/30232     20130101 LA20160108BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700334" load-source="docdb" scheme="CPC">G06T2207/20224     20130101 LA20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984700572" load-source="docdb" scheme="CPC">G06T2207/10016     20130101 LA20160108BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984705164" load-source="docdb" scheme="CPC">G06K   9/46        20130101 LI20160107BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165551087" lang="DE" load-source="patent-office">VERFAHREN, SYSTEM UND SOFTWAREMODUL ZUR VORDERGRUNDEXTRAKTION</invention-title><invention-title mxw-id="PT165551088" lang="EN" load-source="patent-office">METHOD, SYSTEM AND SOFTWARE MODULE FOR FOREGROUND EXTRACTION</invention-title><invention-title mxw-id="PT165551089" lang="FR" load-source="patent-office">PROCÉDÉ, SYSTÈME ET MODULE LOGICIEL D'EXTRACTION D'AVANT-PLAN</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103317353" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>AGENT VIDEO INTELLIGENCE LTD</last-name><address><country>IL</country></address></addressbook></applicant><applicant mxw-id="PPAR1103343934" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>AGENT VIDEO INTELLIGENCE LTD</last-name></addressbook></applicant><applicant mxw-id="PPAR1101644886" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Agent Video Intelligence Ltd</last-name><iid>100976135</iid><address><street>13 HaMelacha Afek Industrial Park</street><city>ROSHA HA'AYIN 48091</city><country>IL</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103317626" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ASHANI ZVIKA</last-name><address><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103325049" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>ASHANI, Zvika</last-name></addressbook></inventor><inventor mxw-id="PPAR1101649265" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>ASHANI, Zvika</last-name><address><street>69 Hadekel St.</street><city>5590000 Ganey Tikva</city><country>IL</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101653459" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Harrison IP Limited</last-name><iid>101296927</iid><address><street>Box Tree House Northminster Business Park Northfield Lane</street><city>York YO26 6QU</city><country>GB</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="IL-2013051020-W"><document-id><country>IL</country><doc-number>2013051020</doc-number><kind>W</kind><date>20131210</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014128688-A1"><document-id><country>WO</country><doc-number>2014128688</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660722675" load-source="docdb">AL</country><country mxw-id="DS660742038" load-source="docdb">AT</country><country mxw-id="DS660722677" load-source="docdb">BE</country><country mxw-id="DS660636815" load-source="docdb">BG</country><country mxw-id="DS660792103" load-source="docdb">CH</country><country mxw-id="DS660716688" load-source="docdb">CY</country><country mxw-id="DS660742083" load-source="docdb">CZ</country><country mxw-id="DS660712918" load-source="docdb">DE</country><country mxw-id="DS660716689" load-source="docdb">DK</country><country mxw-id="DS660716690" load-source="docdb">EE</country><country mxw-id="DS660634910" load-source="docdb">ES</country><country mxw-id="DS660636816" load-source="docdb">FI</country><country mxw-id="DS660636817" load-source="docdb">FR</country><country mxw-id="DS660722678" load-source="docdb">GB</country><country mxw-id="DS660716695" load-source="docdb">GR</country><country mxw-id="DS660722699" load-source="docdb">HR</country><country mxw-id="DS660742084" load-source="docdb">HU</country><country mxw-id="DS660792104" load-source="docdb">IE</country><country mxw-id="DS660722700" load-source="docdb">IS</country><country mxw-id="DS660636818" load-source="docdb">IT</country><country mxw-id="DS660716696" load-source="docdb">LI</country><country mxw-id="DS660712935" load-source="docdb">LT</country><country mxw-id="DS660742085" load-source="docdb">LU</country><country mxw-id="DS660712936" load-source="docdb">LV</country><country mxw-id="DS660712937" load-source="docdb">MC</country><country mxw-id="DS660633738" load-source="docdb">MK</country><country mxw-id="DS660633743" load-source="docdb">MT</country><country mxw-id="DS660634915" load-source="docdb">NL</country><country mxw-id="DS660636831" load-source="docdb">NO</country><country mxw-id="DS660634916" load-source="docdb">PL</country><country mxw-id="DS660792105" load-source="docdb">PT</country><country mxw-id="DS660634917" load-source="docdb">RO</country><country mxw-id="DS660792106" load-source="docdb">RS</country><country mxw-id="DS660634918" load-source="docdb">SE</country><country mxw-id="DS660792107" load-source="docdb">SI</country><country mxw-id="DS660633744" load-source="docdb">SK</country><country mxw-id="DS660633745" load-source="docdb">SM</country><country mxw-id="DS660716697" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139078864" ref-ucid="WO-2014128688-A1" lang="EN" load-source="patent-office"><p num="0000">A method is provided, suitable for use in extraction of foreground objects from image stream. The method comprises: providing input image data of a region of interest, providing background model of said region of interest, and utilizing said background model for processing each image of the input image data. The processing comprises: determining a background gradient map for pixels in said background model and an image gradient map for pixels in the image; defining a predetermined number of one or more segments in said image and corresponding one or more segments in the background model; determining, for each image segment, an edge density factor is a first relation between the image and background gradient maps for said segment; and calculating foreground detection threshold based on said certain relation, thereby enabling use of said foreground detection threshold for classifying each pixel in the segment as being a foreground or background pixel.</p></abstract><abstract mxw-id="PA139545419" ref-ucid="WO-2014128688-A1" lang="EN" source="national office" load-source="docdb"><p>A method is provided, suitable for use in extraction of foreground objects from image stream. The method comprises: providing input image data of a region of interest, providing background model of said region of interest, and utilizing said background model for processing each image of the input image data. The processing comprises: determining a background gradient map for pixels in said background model and an image gradient map for pixels in the image; defining a predetermined number of one or more segments in said image and corresponding one or more segments in the background model; determining, for each image segment, an edge density factor is a first relation between the image and background gradient maps for said segment; and calculating foreground detection threshold based on said certain relation, thereby enabling use of said foreground detection threshold for classifying each pixel in the segment as being a foreground or background pixel.</p></abstract><abstract mxw-id="PA139078865" ref-ucid="WO-2014128688-A1" lang="FR" load-source="patent-office"><p num="0000">L'invention concerne un procédé apte à être utilisé dans l'extraction d'objets d'avant-plan à partir d'un flux d'images. Le procédé comporte les étapes consistant à: communiquer des données d'images d'entrée d'une région d'intérêt, à mettre en place un modèle d'arrière-plan de ladite région d'intérêt et à utiliser ledit modèle d'arrière-plan pour traiter chaque image des données d'images d'entrée. Le traitement comporte les étapes consistant à: déterminer une carte de gradient d'arrière-plan pour des pixels dudit modèle d'arrière-plan et une carte de gradient d'image pour des pixels de l'image; définir un nombre prédéterminé d'un ou plusieurs segments de ladite image et d'un ou plusieurs segments correspondants du modèle d'arrière-plan; déterminer, pour chaque segment d'image, un facteur de densité de contours dans une première relation entre les cartes de gradient d'image et d'arrière-plan pour ledit segment; et calculer un seuil de détection d'avant-plan basé sur la relation en question, permettant ainsi l'utilisation dudit seuil de détection d'avant-plan pour classifier chaque pixel du segment comme étant un pixel d'avant-plan ou d'arrière-plan.</p></abstract><abstract mxw-id="PA139545420" ref-ucid="WO-2014128688-A1" lang="FR" source="national office" load-source="docdb"><p>L'invention concerne un procédé apte à être utilisé dans l'extraction d'objets d'avant-plan à partir d'un flux d'images. Le procédé comporte les étapes consistant à: communiquer des données d'images d'entrée d'une région d'intérêt, à mettre en place un modèle d'arrière-plan de ladite région d'intérêt et à utiliser ledit modèle d'arrière-plan pour traiter chaque image des données d'images d'entrée. Le traitement comporte les étapes consistant à: déterminer une carte de gradient d'arrière-plan pour des pixels dudit modèle d'arrière-plan et une carte de gradient d'image pour des pixels de l'image; définir un nombre prédéterminé d'un ou plusieurs segments de ladite image et d'un ou plusieurs segments correspondants du modèle d'arrière-plan; déterminer, pour chaque segment d'image, un facteur de densité de contours dans une première relation entre les cartes de gradient d'image et d'arrière-plan pour ledit segment; et calculer un seuil de détection d'avant-plan basé sur la relation en question, permettant ainsi l'utilisation dudit seuil de détection d'avant-plan pour classifier chaque pixel du segment comme étant un pixel d'avant-plan ou d'arrière-plan.</p></abstract><description mxw-id="PDES78477732" ref-ucid="WO-2014128688-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001"> METHOD, SYSTEM AND SOFTWARE MODULE FOR FOREGROUND EXTRACTION </p><p id="p0002" num="0002">TECHNOLOGICAL FIELD </p><p id="p0003" num="0003">The present invention is generally in the field of image processing techniques and relates to a system and method for processing an image stream, which is particularly useful for computerized vision and automated video surveillance. BACKGROUND </p><p id="p0004" num="0004"> Foreground extraction is one of the basic building blocks of computerized vision and automatic video surveillance systems. Generally, most object detection techniques are based, at their early stages, on identifying which of the image's pixels are associated with the background and which pixels are to be further processed as being associated with foreground objects. </p><p id="p0005" num="0005"> Typical techniques for foreground extraction from an image or image stream are based on background subtraction. To this end, a background model is typically utilized; the model is then subtracted from a currently processed frame/image to identify differences between the model and the image. More specifically, the background model is subtracted from the frame being processed, and for each pixel the so-determined difference is analyzed with respect to a specific threshold. For example, if the subtracted pixel value exceeds the threshold, the pixel is considered as foreground, otherwise the pixel is assumed as background related. </p><p id="p0006" num="0006"> For example, US 8,285,046 describes techniques for a computer vision engine to update both a background model and thresholds used to classify pixels as depicting scene foreground or background in response to detecting that a sudden illumination changes has occurred in a sequence of video frames. The threshold values may be used to specify how much a given pixel may differ from corresponding values in the background model before being classified as depicting foreground. When a sudden illumination change is detected, the values for pixels affected by sudden illumination 
<!-- EPO <DP n="3"/>-->
 change may be used to update the value in the background image to reflect the value for that pixel following the sudden illumination change as well as update the threshold for classifying that pixel as depicting foreground/background in subsequent frames of video. GENERAL DESCRIPTION </p><p id="p0007" num="0007"> The present invention provides a novel technique for identifying and extracting pixels and/or regions of an image associated with foreground objects over regions/pixels of the image associated with a background. The technique of the invention may be implemented as a software product, installable (e.g. by downloading from the Internet) in a storage utility of a computer device or any other computer readable media. </p><p id="p0008" num="0008"> Generally, the technique of the present invention deals with subtraction of a background model from the image data of a currently processed frame/image. In other words, the invention concerns the novel thresholding technique for background extraction or determination of a foreground detection threshold. For each pixel of an image being processed, a difference between the pixel value and that of the corresponding pixel in the background model (previously defined/received) is calculated. The calculated difference is then compared to a threshold determined in accordance with the technique of the invention. If the difference exceeds the threshold, the pixel is considered as associated with a foreground object. According to the invention, the threshold, generally termed herein "foreground detection threshold', is determined for different blocks/segments of the image in accordance with a calculated probability that the segment may contain foreground objects. The foreground detection threshold may thus be the same or different for different blocks/segments of the same image, e.g. it may be spatially different within the image, as well as may dynamically change from frame to frame. </p><p id="p0009" num="0009"> Thus, the present invention provides an accurate threshold determination for different segments of the image/frame. To this end, the calculated threshold is based on a comparison of image gradients between the image and the corresponding background model. Any segment of the image or the background model that contains significant variation in the gradient(s) is assumed to have a higher probability of containing foreground objects. If a certain segment of the image displays a relatively low 
<!-- EPO <DP n="4"/>-->
 difference in gradients from the background model, it is assumed to have a lower probability of containing foreground objects. </p><p id="p0010" num="0010"> Thus, the present invention provides a novel technique for the extraction of foreground objects, or identifying pixels associated with such objects in an image. The technique involves: providing image data and an associated background model, processing said image data to identify image segments having higher probability of containing foreground objects by calculating a gradient map of the background model and the image data, and determining a relationship between the gradient maps in different segments of the image, and using this relationship to determine a corresponding foreground detection threshold for the specific image segment. Then, the threshold is applied to each pixel in the respective image segment to identify pixels associated with foreground objects, and the same is performed for each image segment. </p><p id="p0011" num="0011"> In some embodiments of the invention, the foreground extraction technique also considers variations in illumination intensity of the region of interest to adjust the foreground detection threshold. Since thermal and electronic noise often cause variations in values measured by any imager/camera, and such noise/variations are typically greater when the pixel value is larger, the foreground detection threshold is higher for brighter segments of the frame and lower for darker segments thereof. </p><p id="p0012" num="0012"> Thus, according to one broad aspect of the present invention, there is provided a method for use in extraction of foreground objects in an image stream. The method comprising: providing input image data of a region of interest; providing a background model of said region of interest; and utilizing said background model and processing each image in said input image data. Said processing comprising: determining a background gradient map for pixels in said background model and an image gradient map for pixels in the image; defining a predetermined number of one or more segments in said image and corresponding one or more segments in the background model; for each image segment, determining an edge density factor being a first relation between the image and background gradient maps for said segment, and calculating a foreground detection threshold based on said certain relation, thereby enabling use of said foreground detection threshold for classifying each pixel in the segment as being a foreground or background pixel. According to some embodiments of the invention, determining said edge density factor comprises estimating a relative probability that said segment includes foreground objects. 
<!-- EPO <DP n="5"/>-->
 According to some embodiment of the present invention, the method comprises determining a second relation between values of the image pixels and values of corresponding pixels of said background model, using the corresponding foreground detection threshold to classify the pixels in said image segment as foreground or background related pixels in each image segment, such that if a second relation between values of said pixels is higher than said foreground detection threshold, the pixel is classified as being associated with a foreground object. </p><p id="p0013" num="0013"> Calculating of said foreground detection threshold may comprise determining an illumination intensity factor being indicative of illumination intensity of said region of interest, said foreground detection threshold being determined in accordance with said intensity factor and the edge density factor. Additionally determining of said illumination intensity factor may comprise determining said illumination intensity factor in at least one segment of said of said region of interest. </p><p id="p0014" num="0014"> According to some embodiments, said defining a predetermined number of one or more segments in said image may comprise receiving data indicative of pixel to meter ratios in said image data, determining one or more distance segments in said image data and defining said predetermined number of one or more segments such that pixels of said one or more segments correspond to one of said one or more distance segments. Additionally, according to some embodiments of the invention the background model may be updated in accordance with pixels of said image classified as associated with the background. </p><p id="p0015" num="0015"> According to one other broad aspect the present invention provides a method for use in extracting foreground objects from an image stream, the method comprising: receiving data indicative of said image stream corresponding to a region of interest, said data indicative of said image stream comprising image data of one or more images; receiving a data about a background model of said region of interest; calculating edge maps of said background model and of image data of at least one image from said image stream; segmenting said region of interest to at least one segment and for each pixel in said at least one segment calculating a difference between edge map of said background model and of said image data to generate an edge difference map; and calculating a threshold map for use in detection of foreground pixels in said image data, the threshold map comprising foreground detection threshold values for said at least one segment of the region of interest based on said edge difference map. 
<!-- EPO <DP n="6"/>-->
 According to some embodiments of the invention the method comprises calculating a difference between pixels of the background model and of said image data. If the difference in pixel values exceeds a corresponding foreground detection threshold of said threshold map the pixel is classified as associated with a foreground object. </p><p id="p0016" num="0016"> According to one other broad aspect, the present invention provides a system for use in extraction of foreground objects in an image stream, the system comprising: data input utility for receiving input image data of a region of interest, and data comprising a corresponding background model of said region of interest; and a data processing and analyzing utility for utilizing said background model and processing each image in said input image data. The data processing and analyzing utility being configured and operable for carrying out the following: calculating a background gradient map for pixels in said background model and an image gradient map for pixels in the image; utilizing said background and image gradient maps and generating an edge relation map and calculating a foreground detection threshold for at least one segment of the region of interest in accordance with values of said relation map in said segment, calculating a relation between pixel values of the background model and of said image data in said at least one segment, and if said relation exceeds said foreground detection threshold classifying the corresponding pixel as associated with foreground of said image data and generating output data indicative of foreground related pixels in said image data. </p><p id="p0017" num="0017"> The data processing and analyzing utility may comprise an illumination estimator utility configured and operable for determining an illumination intensity factor being indicative of illumination intensity of said region of interest. The data processing and analyzing utility may be configured and operable for utilizing said illumination intensity factor for calculating of said foreground detection threshold, said foreground detection threshold being determined in accordance with said intensity factor and the edge density factor. The illumination estimator utility may determine said illumination intensity factor in at least one segment of said of said region of interest. </p><p id="p0018" num="0018"> According to some embodiments of the present invention, said data processing and analyzing utility may be configured and operable for receiving data indicative of pixel to meter ratios in said image data, determining one or more distance segments in said image data and defining a predetermined number of one or more segments such that pixels of said one or more segments correspond to one of said one or more distance segments. The data processing and analyzing utility may also be configured and 
<!-- EPO <DP n="7"/>-->
 operable for providing data indicative of pixels of said image classified as associated with the background for updating said background model in accordance with said data. </p><p id="p0019" num="0019"> According to yet another broad aspect of the present invention, there is provided a software product for use in analyzing an image stream. The product comprising: a foreground extraction module configured and operable to receive input data comprising image data and a corresponding background model and to process said image data using said background model. The foreground extraction module comprises: an edge detection module configured to calculate a background gradient map for said background model and an image gradient map for said image data; a regional threshold calculation module configured to receive said background and image gradient maps and generate an edge relation map and calculate a foreground detection threshold for at least one segment of the region of interest in accordance with values of said relation map in said segment, for use in detection of foreground related pixels in said image data. </p><p id="p0020" num="0020"> The software product may comprise a comparator module configured to calculate a relation between pixel values of the background model and of said image data in said at least one segment, and if said relation exceeds said foreground detection threshold classifying the corresponding pixel as associated with foreground of said image data. The foreground extraction module may comprise an illumination estimator module configured to determine an illumination intensity factor in accordance with illumination intensity of at least one of said background model and said image data, said regional threshold calculation module is configured to receive said background intensity factor and to adjust said foreground detection threshold accordingly. </p><p id="p0021" num="0021"> According to some embodiments, the software product may comprise background updating module configured for receiving data about pixels of an image being associated with the background and for updating the background model. </p><p id="p0022" num="0022"> Additionally, according to some embodiments, the foreground extraction module may be configured to receive data indicative of pixel to meter ratio in said image data. The regional threshold calculation module may be configured to receive said data indicative of pixel to meter ratios, determine accordingly one or more distance segments in said image data to thereby calculate said foreground detection threshold for said at least one segment of the region of interest in accordance with pixel to meter ratios is said one or more distance segments. 
<!-- EPO <DP n="8"/>-->
 BRIEF DESCRIPTION OF THE DRAWINGS </p><p id="p0023" num="0023"> To better understand the subject matter disclosed herein, and to exemplify how it may be carried out in practice, embodiments will now be described, by way of non- limiting example only, with reference to the accompanying drawings: </p><p id="p0024" num="0024"> Fig. 1 illustrates schematically an image processing system suitable for utilizing the technique of the present invention; </p><p id="p0025" num="0025"> Fig. 2 illustrates schematically a foreground detection module according to the present invention; </p><p id="p0026" num="0026"> Figs. 3A-3B respectively exemplify calculation of edge density Factor and illumination intensity factor for use in selection of the foreground detection threshold according to the present invention; </p><p id="p0027" num="0027"> Figs. 4A-4G exemplify foreground subtraction process utilizing the technique of the present invention, Fig 4A shows a background model of a region of interest; Fig 4B shows a background gradient map; Fig 4C shows a current image data to be processed; Fig 4D shows image gradient maps; Figs 4E-4F show a difference between the gradient maps and division of the difference map to segments; and Fig 4G shows the resulting foreground pixels identified by the technique of the present invention; </p><p id="p0028" num="0028"> Fig. 5 illustrates segmentation of an image frame in accordance with distance of elements from the camera according to some embodiments of the invention; </p><p id="p0029" num="0029"> Figs. 6A-6D exemplify variations of the edge density factor and illumination factor in accordance with distances of objects from the camera. </p><p id="p0030" num="0030">DETAILED DESCRIPTION OF EMBODIMENTS </p><p id="p0031" num="0031"> Various image processing techniques, in particular video processing techniques, rely on distinguishing between the foreground and background objects in an image. The present invention provides a novel and efficient technique for identifying pixels (or regions) in image data, which are associated with foreground objects over the background of the image. Such a technique would typically be implemented as a software module, configured to be executed by any image processing utility, specifically useful for processing video data. </p><p id="p0032" num="0032"> Fig. 1 illustrates an image processing system 200 of the present invention, configured as a video analysis system. As shown, the system 200 is associated (capable 
<!-- EPO <DP n="9"/>-->
 of communicating with) with an image stream source 210, which may include one or more camera unit(s) and/or any other source configured to provide image data 50 (e.g. a storage unit containing images). The image data 50 may be transmitted to the video analysis system 200 directly and/or through any type of data network 220. The system 200 thus typically includes one or more dedicated input ports (not specifically shown). The image data 50 includes data of one or more images, preferably in some sequential order, of a region of interest. </p><p id="p0033" num="0033"> The video analysis system 200 may typically be a computer system in the meaning that it is configured as a data processing utility which is preprogrammed to perform certain image processing, and possibly other data processing, and is connected to data input and output utilities (which are not specifically shown) and a memory utility (at least temporal memory) 240. Generally, the system 200 includes a foreground extractor (foreground extraction module) 100 which receives and processes data indicative of an input image to identify image data (pixels) 50 associated with foreground in said image, and distinguish between foreground related pixels and background related pixels. Additionally, the system 200 is associated with a background model generator 230 for defining a background model to be used in the extraction of foreground pixels from the image data 50. It should be understood that the background model generator 230 may be a module of the data processing utility 200 or may be installed in an external computer to which the data processing utility 200 is connectable (e.g. via a communication network). The system 200 may also include a pixel size calculator 280 configured to calculate a pixel to meter (P2M) ratio defining distance of various objects or regions of the frame from a camera. </p><p id="p0034" num="0034"> The system 200 may be configured for the entire image processing and may thus include a foreground object detection module 250 and an object tracking module 260 configured and operable to identify, from the pixels associated with foreground of the image, foreground blobs and specific objects located in the region of interest. It should be noted that, generally, either one or both of the foreground object detection and object tracking modules may be installed in external computers/utilities connectable to the data processing utility 200. </p><p id="p0035" num="0035"> Thus, generally, the data processing utility 200 includes at least the foreground extraction module 100 which is configured for receiving input image data 50 and background model data 60 and generating output data indicative of foreground 
<!-- EPO <DP n="10"/>-->
 associated pixels in the input image data. The foreground extraction module 100 is typically a software module configured for execution by a computer system to analyze input image data 50 utilizing a certain background model 60 to identify pixels in the image data associated with foreground elements/objects in the image data 50. This background model 60 is in turn determined based on the input image data. The principles of generation of the background model are generally known and do not form part of the invention, and therefore are not specifically described here. </p><p id="p0036" num="0036"> As will be described more specifically further below, the foreground extraction module 100 operates to determine threshold value/values to be used to classify a predetermined relation between the difference pixel values in the image data 50 and corresponding pixels in the background model 60, for each pixel in the image, as corresponding to that of the foreground or background pixel. </p><p id="p0037" num="0037"> It should be noted that pixels classified by the foreground extraction module 100 as foreground related pixels are not necessarily those of the foreground object itself. After the foreground extraction module 100 identifies such foreground related pixels in the image data 50, these pixels undergo further processing by the foreground object detection and tracking modules 250 and 260, which applies to said pixels processing algorithms including aggregation to blobs that correspond to objects, detection and identification of such blobs/objects and object tracking. Additionally, some foreground blobs, or isolated foreground pixels, might be neglected in the processing and detection of foreground objects for various reasons, such as being smaller than a blob's size threshold. </p><p id="p0038" num="0038"> Fig. 2 illustrates, by way of a block diagram, an example of the foreground extraction module 100 of the present invention. As noted, the foreground extraction module 100 is configured to receive image data 50 indicative of one or more images of a region of interest, and corresponding background model data 60, a certain model describing the background of said region of interest. The foreground extraction module 100 is configured for determining, for each image segment, its probability for including foreground data, and selecting an appropriate foreground detection threshold for the segment in accordance with the respective calculated probability, and then using the foreground detection threshold for identifying foreground related pixels in said image segment. More specifically, the foreground extraction module 100 includes an edge detection utility 120, a regional threshold calculation module 140, and a comparator 
<!-- EPO <DP n="11"/>-->
 module 160. The edge detection utility 120 is configured to calculate gradient maps, 125 and 126, of the input image data 50 and the background model 60 respectively. The regional threshold calculation module 140 is configured to utilize the gradient maps 125 and 126 to calculate appropriate foreground detection threshold values for various segments of the image of the region of interest. The comparator module 160 is configured to calculate a relationship between the image data pixels' values 50 and the corresponding pixels of the background model 60, and compare said relationship to the calculated foreground detection threshold. </p><p id="p0039" num="0039"> In some embodiments, the foreground extraction module 100 may also include an illumination estimator module 180 (shown in dashed lines as its provision is optional) configured to determine one or more parameters associated with light intensity in the image data 50 and/or the background model 60. The light intensity parameters may then be utilized to fine tune the foreground detection threshold accordingly. Additionally, in some embodiments, the regional threshold calculation module 140 is configured to receive pixel size data 70, e.g. provided by the pixel size calculator shown in Fig. 1. The regional threshold calculation module 140 may be configured to utilize a pixel to meter ratios from the pixel size data 70 for selection of the foreground detection threshold as will be described in more details below. </p><p id="p0040" num="0040"> As indicated above, various techniques for determining the background model are generally known in the art, and thus need not be described here in detail, except to note that the background model may typically be in the form of image data corresponding to image captured without any foreground objects in the scene. In addition, the background model may be updated over time, in accordance with determination of background regions during operation of the foreground extraction module, and in accordance with variations detected in the background, such as illumination variations, camera movement and/or movement of non-foreground objects. </p><p id="p0041" num="0041"> As noted, the foreground extraction module 100 receives input data including data indicative of an image stream 50; and data of a current background model 60; and is operable to analyze this data to generate corresponding output data, including data on the existence of pixel(s) associated with foreground 70 in the image stream. The foreground extraction module 100 may also output data about the background which data may be used in a feedback loop (see Fig. 1) for updating the background model, which in turn is used for processing further images. 
<!-- EPO <DP n="12"/>-->
 As mentioned above, the edge detection module 120 processes the input image data 50 and generates corresponding edge/gradient map 125 of the input image data. As for the gradient map 126 of the background model 60, it may generally be a part of background model data received by foreground extraction module 100. Typically, the edge detection module 120 is capable of generating such gradient map 126 as well. Typically, the edge detection module 120 may utilize any known technique for calculating the gradient map(s). The gradient maps 125 and 126 are then transmitted (by the edge detection module 120, or generally by the edge detection module 120 and the background generator) to the regional threshold calculation module 140 which uses these maps and calculates a foreground detection threshold, by comparing between the gradients of the two maps. The foreground detection threshold is determined in accordance with a relationship between the gradient maps, 125 and 126, such that if the gradient maps have significant differences in a segment of the image, the foreground detection threshold is selected to be of a relatively lower value to thereby provide more sensitive foreground detection; while in segments where the gradient maps are relatively similar, the threshold is selected to be of a relatively higher value allowing less sensitive foreground detection in those segments. It should be noted that the image segments may be of any size starting from a single -pixel segment up to the entire image; the selected foreground detection threshold may be the same or different for at least some segments of the image. </p><p id="p0042" num="0042"> The calculated foreground detection threshold is then used to classify each pixel in the image (using the foreground detection threshold of the respective segment) as being the foreground pixel or background one. This is carried out by the comparator module 160, which may be a part of the foreground extractor, or may generally, as exemplified in the figure, be a separate module (e.g. in a separate computer) in communication with the foreground extractor 100. The comparator module 160 calculates a difference function between the pixel value in the image data 50 and in the background model 60, and analyzes this difference function with respect to the foreground detection threshold. </p><p id="p0043" num="0043"> Such a difference function may for example be in the form:</p><p id="p0044" num="0044"><img id="imgf000012_0001" he="6" wi="26" file="imgf000012_0001.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 (equation 1) where PC, is the value of pixel in the image data and PB<sub>{</sub> is the value of the corresponding pixel in the background model, and D<sup>7</sup>, is the difference function for 
<!-- EPO <DP n="13"/>-->
 pixel in segment j of the image. For example, the technique of the present invention may utilized the difference function in the form</p><p id="p0045" num="0045"><img id="imgf000013_0001" he="5" wi="23" file="imgf000013_0001.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 (equation 2) thus simplifying the calculations. It should be noted that for simplicity, the difference function of equation 2 is used herein below as a non-limiting example. </p><p id="p0046" num="0046"> The comparator module 160 compares the relation (function) D of the difference to the calculated foreground detection threshold for the corresponding segment j: if the relation exceeds the threshold, pixel is considered associated as foreground pixel (i.e. may represent a part of a foreground object). </p><p id="p0047" num="0047"> As indicated, the foreground detection threshold is determined dynamically, for each image 50 and each background model 60, according to the corresponding gradient maps. The present invention's technique utilizes gradient maps of the processed image data, and the background model for comparing differences in segments of the frame/image, providing indications of segments having relatively high probability to contain foreground objects. More specifically, the regional threshold calculator module 140 analyzes the frame in the image data and decides about a number (and possibly a size) of segments therein, being any number from 1 (the entire frame) to the number of pixels in the image (e.g. single -pixel segment). Thus, in some embodiments, the regional threshold calculator module 140 performs segmentation of the frame in the image data to an array of blocks/segments; and for each segment calculates a relationship between the gradient maps, to thereby generate a relations' map (typically a binary or gray level map) for each pixel in each block. </p><p id="p0048" num="0048"> The relationship between the gradient maps may be constituted by a difference between the maps. If the difference exceeds a certain (typically predetermined) second threshold, allowing some variations in the background maps (e.g. variations caused by noise), the corresponding pixels of the relation map are assigned with an edge value "i"; if the difference is lower than the second threshold, the corresponding pixels are assigned with an edge value "0". For each segment/block of the frame, the regional threshold calculator module 140 calculates the ratio between the number of such edge pixels (assigned value "1" or "0") in the segment and the entire number of pixels in said segment. This ratio defines an edge density factor which does not necessarily measure the density of edges but is rather utilized to simplify calculations of the foreground detection threshold. The edge density factor as a function of the number of edge pixels 
<!-- EPO <DP n="14"/>-->
 is exemplified in Fig. 3A for a 32 pixels' segment. The foreground detection threshold is in a predetermined relation (function) with the edge density factor, for example as illustrated in Fig 3A the foreground detection threshold may be selected as the edge density factor or a linear function thereof, or in some embodiments as the edge density factor multiplied by an illumination factor as will be described below. </p><p id="p0049" num="0049"> For example, as shown in Fig. 3A, the foreground detection threshold may be selected in accordance with the edge density factor and the corresponding segment's size. Generally, for a segment of the size NxM, where N is the number of pixels in a row and M is the number of pixels in a column, the edge density factor may vary linearly from maximal value (EDF<sub>max</sub>) to minimal value (EDF<sub>min</sub>) while the edge pixel count varies from minimal value of NxM/2 to maximal value of NxM, if the edge pixel count is lower than the minimal value the edge density factor remains in it maximal value, i.e. the edge density factor may follow the following equation </p><p id="p0050" num="0050">EDF = (equation 3) 
<img id="imgf000014_0001" he="20" wi="94" file="imgf000014_0001.tif" img-format="tif" img-content="drawing" orientation="portrait" inline="no"/>
 </p><p id="p0051" num="0051">where EDF is the edge density factor, EDF<sub>max</sub> and EDF<sub>min</sub> are the maximal and minimal acceptable values (e.g. EDF<sub>min</sub>=6, EDF<sub>max</sub>=50), N and M are the number of pixels in row/column and EPC is the edge pixel count. Typical values for EDF<sub>max</sub> may be between 20 to 50 and typical values for EDF<sub>min</sub> may be between 4 to 8. It should be noted that the values of EDF<sub>min</sub> and EDF<sub>max</sub> may vary in accordance with design of the system and intended use of the technique. It should also be noted that other functional dependency of the edge density factor may be used, which may or may not be linear with respect to the edge pixel count. </p><p id="p0052" num="0052"> In some embodiments, the foreground detection threshold is selected from a list of possible threshold values corresponding to the calculated edge density factors for the edge pixel values, which may be stored in the storage unit (240 in Fig. 1), or received by the system 200 via a network connection. </p><p id="p0053" num="0053"> Turning back to Fig. 2, according to some embodiments, the present invention's technique also utilizes an intensity factor of the region of interest for appropriate selection of the foreground detection threshold. The intensity factor represents the average intensity of a frame segment calculated based on the image data and/or the 
<!-- EPO <DP n="15"/>-->
 background model, thus used to correct the foreground detection threshold to vary the sensitivity of foreground detection in accordance with illumination and noise amplitude of the processed segment. This technique enables efficient foreground detection in both bright and relatively dark regions of the frame. To this end, the foreground extraction module 100 includes an illumination estimator 180 configured to receive input data of the background model and/or the image data. In the specific but not limiting example of Fig. 2, the illumination estimator 180 receives the background model 50 only as the input data. The illumination estimator 180 is configured to process the received data to identify/estimate the illumination intensity factor of the received image/model, e.g. in accordance with the brightness parameter of the image. To this end, in general, pixel values corresponding to the primary colors, RGB, may be used, as well other pixel values (e.g. brightness or gray level for black-white (BW) imaging). The illumination factor for a certain segment depends on the average pixel values in the segment. This is exemplified in Fig. 3B. This is associated with the following: The pixel values include variations caused by noise associated with the scene itself and/or the camera sensor. The noise amplitude in a certain pixel is typically higher for pixels with higher values. The illumination estimator 180 identifies illumination intensity in different segments/blocks of the frame and calculates the intensity factor for each segment. </p><p id="p0054" num="0054"> The regional threshold calculating module 140 utilizes the intensity factor, together with the edge density factor(s) for the specific segment, and selects the corresponding foreground detection threshold for said segment of the frame. As indicated above, the foreground detection threshold may be selected to be equal to the edge density factor or a linear function thereof, e.g. edge density factor multiplied by the illumination factor. </p><p id="p0055" num="0055"> Thus, the regional threshold calculating module 140 utilizes the edge density factor, and may also utilizes the illumination factor, to appropriately select the foreground detection threshold for each segment of the frame of the region of interest under processing. For example, the module 140 utilizes the edge density factor itself, or this factor multiplied by the illumination factor (as the case may be), to select a certain factor which is such that the higher the edge density factor (or its product with the illumination factor), the higher is the foreground detection sensitivity, and thus the lower is the selected foreground detection threshold. 
<!-- EPO <DP n="16"/>-->
 Reference is made to Figs. 4A-4G exemplifying the technique of the present invention as utilized for detection of foreground objects in a region of interest. Fig. 4A illustrates a background model of the region; Fig. 4B shows a gradient map generated from the background model of Fig. 4A. The background model with or without the background gradient map may be generated by a background model generator, being a part of the processing system or not. Additionally, the background gradient map may be calculated by the edge detection nodule of the foreground extraction module of the present invention. Fig. 4C shows image data of the region of interest acquired by any suitable camera unit. Typically such camera unit is positioned at the region of interest and transmits image data to be processed. It should be noted that the background model of Fig. 4A may typically be calculated based on one or more images which were also acquired by the same camera unit. Fig. 4D shows an image gradient map and Figs. 4E- 4F show a binary difference map. The binary difference map assigns the value "1" to pixels where the difference between the image and background gradients exceeds a corresponding second threshold. To this end, the difference between the gradient maps of Fig. 4B and 4D is calculated by the regional threshold calculating module and each pixel is assigned the value "i" if the difference exceed the second threshold or "0" if the difference is lower than the second threshold. In this non-limiting example the second threshold is selected to be 8. In Fig. 4F the difference gradient map divided into 16 segments for separate calculation of the corresponding foreground detection threshold. Fig. 4G shows the detected foreground pixels. </p><p id="p0056" num="0056"> The regional threshold calculation module utilizes a segmented difference map as shown in Fig. 4F to calculate an appropriate foreground detection threshold as described above. The regional threshold calculation module counts the number of "white" pixels in each segment of the difference map, i.e. the pixels corresponding the gradient difference between the image and the background model, and determined an edge density factor for each segment. According to some embodiments, the foreground detection threshold is selected to be equal to the edge density factor, or a linear function thereof. In the present example the foreground detection threshold values were selected in the range of 16-48 in accordance with edge density factor in each segment and in accordance with equation 3 above where and EDF<sub>max</sub>=24. The selected foreground detection threshold is then used in each segment to identify which pixels correspond to foreground of the image and which are background pixels as shown in 
<!-- EPO <DP n="17"/>-->
 Fig. 4G showing the foreground pixels in white. As shown in the present example the difference map describing differences in gradients between the background model and the image may typically be a binary map, however it should be understood that a gray- level map or complete difference map may also be used, and preferred at times. A gray- level difference map may be used to provide weights to pixels describing greater difference in the image gradient relative to the background. </p><p id="p0057" num="0057"> As indicated above, the technique of the present invention may also utilize pixel size data (pixel to meter ratio data) for selection of the foreground detection threshold. Reference is made to Fig. 5 illustrating an image of a region of interest and exemplifying variations in the pixel to meter ratio. The pixel size data, typically in the form of a ratio between pixels and the actual area these pixels represent in the region of interest, is often used in image processing systems to increase accuracy and efficiency of object detection and recognition. As shown above, with reference to Fig. 1, the video analysis system 200 may include a pixel size calculation unit 280 or may be connectable to an external size calculation unit. Such pixel size calculation unit may utilize raw or processed image data (e.g. a corresponding background model) to determine for each pixel in the image data (or in the background model) a size in the actual region of interest. The pixel size data may be calculated for individual pixels but is usually calculated for groups of pixels representing a segment (region) of the scene to provide more meaningful data. As shown in the example of Fig. 5, the image data is divided into two distance related segments including part 1 and part 2 of the image frame. In this non-limiting example the distance segments selection is chosen by selection of a pixel to meter threshold, e.g. in this example the parts of the frame corresponding to more than 100 pixels to meter are assigned to distance segment 1 and part of the frame where there are less than 100 pixels per meter are assigned to distance segment 2. </p><p id="p0058" num="0058"> The regional threshold calculation module (140 in Fig. 2) may utilize this division of the frame to select appropriate foreground detection threshold values in accordance with the pixel size data (pixel to meter ratio). Typically the pixel size data is used to determine variations of the edge density factor and the illumination intensity factor which determined the final foreground detection threshold. This is exemplified in Figs. 6A-6D showing edge density and illumination intensity factors variations with respect to distance segments 1 and 2 shown in Fig. 5. As shown in the figures, the values of EDF<sub>min</sub>, EDF<sub>max</sub> and the values of the illumination factor may be selected for 
<!-- EPO <DP n="18"/>-->
 different distances in accordance with desired focus of the region of interest. In the present example, illustrated in Fig. 5, the system may be aimed at identifying objects which are relatively distant from the camera, and are located in distance segment 2. The edge density and illumination factors are therefore selected to be lower for distance segment 2 relative to distance segment 1 in order to provide higher sensitivity. </p><p id="p0059" num="0059"> Thus the present invention provides a novel technique for use in foreground extraction processes. The technique utilizes background and image gradient maps for appropriate calculation of a foreground detection threshold which may later be used to classify pixels of the image as associated with the foreground or background. Those skilled in the art will readily appreciate that various modifications and changes can be applied to the embodiments of the invention as hereinbefore described without departing from its scope defined in and by the appended claims. 
</p></description><claims mxw-id="PCLM70078013" ref-ucid="WO-2014128688-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="19"/>-->CLAIMS: </claim-statement><claim id="clm-0001" num="1"><claim-text> 1. A method for use in extraction of foreground objects in an image stream, the method comprising: </claim-text><claim-text> providing input image data of a region of interest; </claim-text><claim-text> providing a background model of said region of interest; </claim-text><claim-text> utilizing said background model and processing each image in said input image data, said processing comprising: </claim-text><claim-text> determining a background gradient map for pixels in said background model and an image gradient map for pixels in the image; </claim-text><claim-text> defining a predetermined number of one or more segments in said image and corresponding one or more segments in the background model; </claim-text><claim-text> for each image segment, determining an edge density factor being a first relation between the image and background gradient maps for said segment, and calculating a foreground detection threshold based on said certain relation, thereby enabling use of said foreground detection threshold for classifying each pixel in the segment as being a foreground or background pixel. </claim-text></claim><claim id="clm-0002" num="2"><claim-text> 2. The method of Claim 1, wherein determining said edge density factor comprises estimating a relative probability that said segment includes foreground objects. </claim-text></claim><claim id="clm-0003" num="3"><claim-text> 3. The method of Claim 1 or 2, comprising determining a second relation between values of the image pixels and values of corresponding pixels of said background model, in each image segment using the corresponding foreground detection threshold to classify the pixels in said image segment as foreground or background related pixels, such that if a second relation between values of said pixels is higher than said foreground detection threshold, the pixel is classified as being associated with a foreground object. </claim-text></claim><claim id="clm-0004" num="4"><claim-text> 4. The method of any one of Claims 1 to 3, wherein said calculating of said foreground detection threshold comprising determining an illumination intensity factor being indicative of illumination intensity of said region of interest, said foreground detection threshold being determined in accordance with said intensity factor and the edge density factor. 
<!-- EPO <DP n="20"/>-->
</claim-text></claim><claim id="clm-0005" num="5"><claim-text>5. The method of Claim 4, wherein said determining of said illumination intensity factor comprises determining said illumination intensity factor in at least one segment of said of said region of interest. </claim-text></claim><claim id="clm-0006" num="6"><claim-text> 6. The method of any one of claims 1 to 5, wherein said defining a predetermined number of one or more segments in said image comprising receiving data indicative of pixel to meter ratios in said image data, determining one or more distance segments in said image data and defining said predetermined number of one or more segments such that pixels of said one or more segments correspond to one of said one or more distance segments. </claim-text></claim><claim id="clm-0007" num="7"><claim-text>7. The method of any one of claims 1 to 6, comprising updating said background model in accordance with pixels of said image classified as associated with the background. </claim-text></claim><claim id="clm-0008" num="8"><claim-text> 8. A method for use in extracting foreground objects from an image stream, the method comprising: </claim-text><claim-text> o receiving data indicative of said image stream corresponding to a region of interest, said data indicative of said image stream comprising image data of one or more images; </claim-text><claim-text> o receiving a data about a background model of said region of interest; </claim-text><claim-text> o calculating edge maps of said background model and of image data of at least one image from said image stream; </claim-text><claim-text> o segmenting said region of interest to at least one segment and for each pixel in said at least one segment calculating a difference between edge map of said background model and of said image data to generate an edge difference map; </claim-text><claim-text> o calculating a threshold map for use in detection of foreground pixels in said image data, the threshold map comprising foreground detection threshold values for said at least one segment of the region of interest based on said edge difference map. </claim-text></claim><claim id="clm-0009" num="9"><claim-text> 9. The method of Claim 8, comprising calculating a difference between pixels of the background model and of said image data, if a difference in pixel values exceeds a corresponding foreground detection threshold of said threshold map the pixel is classified as associated with a foreground object. 
<!-- EPO <DP n="21"/>-->
</claim-text></claim><claim id="clm-0010" num="10"><claim-text>10. A system for use in extraction of foreground objects in an image stream, the system comprising: </claim-text><claim-text> data input utility for receiving input image data of a region of interest, and data comprising a corresponding background model of said region of interest; and </claim-text><claim-text> a data processing and analyzing utility for utilizing said background model and processing each image in said input image data, said data processing and analyzing utility being configured and operable for carrying out the following: </claim-text><claim-text> calculating a background gradient map for pixels in said background model and an image gradient map for pixels in the image; </claim-text><claim-text> utilizing said background and image gradient maps and generating an edge relation map and calculating a foreground detection threshold for at least one segment of the region of interest in accordance with values of said relation map in said segment, </claim-text><claim-text> calculating a relation between pixel values of the background model and of said image data in said at least one segment, and if said relation exceeds said foreground detection threshold classifying the corresponding pixel as associated with foreground of said image data and generating output data indicative of foreground related pixels in said image data. </claim-text></claim><claim id="clm-0011" num="11"><claim-text> 11. The system of Claim 10, wherein said data processing and analyzing utility comprises an illumination estimator utility configured and operable for determining an illumination intensity factor being indicative of illumination intensity of said region of interest, said data processing and analyzing utility being configured and operable for utilizing said illumination intensity factor for calculating of said foreground detection threshold, said foreground detection threshold being determined in accordance with said intensity factor and the edge density factor. </claim-text></claim><claim id="clm-0012" num="12"><claim-text> 12. The system of Claim 11 , wherein said illumination estimator utility determines said illumination intensity factor in at least one segment of said of said region of interest. </claim-text></claim><claim id="clm-0013" num="13"><claim-text> 13. The system of any one of claims 10 to 12, wherein said data processing and analyzing utility is configured and operable for receiving data indicative of pixel to meter ratios in said image data, determining one or more distance segments in said image data and defining a predetermined number of one or more segments such that 
<!-- EPO <DP n="22"/>-->
 pixels of said one or more segments correspond to one of said one or more distance segments. </claim-text></claim><claim id="clm-0014" num="14"><claim-text> 14. The system of any one of claims 10 to 13, wherein said data processing and analyzing utility is configured and operable for providing data indicative of pixels of</claim-text><claim-text>5 said image classified as associated with the background for updating said background model in accordance with said data. </claim-text></claim><claim id="clm-0015" num="15"><claim-text> 15. A software product for use in analyzing an image stream, the product comprising: a foreground extraction module configured and operable to receive input data comprising image data and a corresponding background model and to process said</claim-text><claim-text>10 image data using said background model; said foreground extraction module comprising: </claim-text><claim-text> an edge detection module configured to calculate a background gradient map for said background model and an image gradient map for said image data, </claim-text><claim-text> a regional threshold calculation module configured to receive said background 15 and image gradient maps and generate an edge relation map and calculate a foreground detection threshold for at least one segment of the region of interest in accordance with values of said relation map in said segment, for use in detection of foreground related pixels in said image data. </claim-text></claim><claim id="clm-0016" num="16"><claim-text> 16. The software product of Claim 15, comprising a comparator module configured 20 to calculate a relation between pixel values of the background model and of said image data in said at least one segment, and if said relation exceeds said foreground detection threshold classifying the corresponding pixel as associated with foreground of said image data. </claim-text></claim><claim id="clm-0017" num="17"><claim-text> 17. The software product of Claims 15 or 16, wherein the foreground extraction 25 module comprising an illumination estimator module configured to determine an illumination intensity factor in accordance with illumination intensity of at least one of said background model and said image data, said regional threshold calculation module is configured to receive said background intensity factor and to adjust said foreground detection threshold accordingly. </claim-text></claim><claim id="clm-0018" num="18"><claim-text>30 18. The software product of any one of Claims 15 to 17, comprising a background updating module configured for receiving data about pixels of an image being associated with the background and for updating the background model. 
<!-- EPO <DP n="23"/>-->
</claim-text></claim><claim id="clm-0019" num="19"><claim-text>19. The software module of any one of claims 15 to 18, wherein said foreground extraction module is configured to receive data indicative of pixel to meter ratio in said image data, said regional threshold calculation module is configured to receive said data indicative of pixel to meter ratios, determine accordingly one or more distance segments in said image data to thereby calculate said foreground detection threshold for said at least one segment of the region of interest in accordance with pixel to meter ratios is said one or more distance segments. 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
