<?xml version="1.0" encoding="UTF-8"?>
<patent-document ucid="EP-2959712-A1" country="EP" doc-number="2959712" kind="A1" date="20151230" family-id="51390580" file-reference-id="315735" date-produced="20180826" status="corrected" lang="EN"><bibliographic-data><publication-reference fvid="160452389" ucid="EP-2959712-A1"><document-id><country>EP</country><doc-number>2959712</doc-number><kind>A1</kind><date>20151230</date><lang>EN</lang></document-id></publication-reference><application-reference ucid="EP-14754230-A" is-representative="NO"><document-id mxw-id="PAPP193867746" load-source="patent-office" format="original"><country>EP</country><doc-number>14754230.2</doc-number><date>20140218</date><lang>EN</lang></document-id><document-id mxw-id="PAPP193867747" load-source="docdb" format="epo"><country>EP</country><doc-number>14754230</doc-number><kind>A</kind><date>20140218</date><lang>EN</lang></document-id></application-reference><priority-claims><priority-claim mxw-id="PPC162034575" ucid="IL-2014050173-W" linkage-type="W" load-source="docdb"><document-id format="epo"><country>IL</country><doc-number>2014050173</doc-number><kind>W</kind><date>20140218</date></document-id></priority-claim><priority-claim mxw-id="PPC162036300" ucid="US-201361766132-P" load-source="docdb"><document-id format="epo"><country>US</country><doc-number>201361766132</doc-number><kind>P</kind><date>20130219</date></document-id></priority-claim></priority-claims><technical-data><classifications-ipcr><classification-ipcr mxw-id="PCL-1889089222" load-source="docdb">H04W   4/02        20090101ALI20161005BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1889090203" load-source="docdb">H04W  24/00        20090101AFI20161005BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1889093162" load-source="docdb">H04W  84/18        20090101ALI20161005BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1889093471" load-source="docdb">H04M   1/725       20060101ALI20161005BHEP        </classification-ipcr><classification-ipcr mxw-id="PCL-1889094505" load-source="docdb">H04W   4/22        20090101ALI20161005BHEP        </classification-ipcr></classifications-ipcr><classifications-cpc><classification-cpc mxw-id="PCL-1687166178" load-source="docdb" scheme="CPC">H04W   4/90        20180201 LI20180219BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1687166179" load-source="docdb" scheme="CPC">H04W   4/029       20180201 FI20180219BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984699136" load-source="docdb" scheme="CPC">H04W  84/18        20130101 LA20160107BHEP        </classification-cpc><classification-cpc mxw-id="PCL-1984701437" load-source="docdb" scheme="CPC">H04M   1/72569     20130101 LI20160107BHEP        </classification-cpc></classifications-cpc><invention-title mxw-id="PT165548633" lang="DE" load-source="patent-office">VERFAHREN UND SYSTEM ZUR IDENTIFIZIERUNG VON AUSNAHMEN BEI MENSCHLICHEM VERHALTEN</invention-title><invention-title mxw-id="PT165548634" lang="EN" load-source="patent-office">METHOD AND SYSTEM FOR IDENTIFYING EXCEPTIONS OF PEOPLE BEHAVIOR</invention-title><invention-title mxw-id="PT165548635" lang="FR" load-source="patent-office">PROCÉDÉ ET SYSTÈME D'IDENTIFICATION D'EXCEPTIONS DE COMPORTEMENTS D'INDIVIDUS</invention-title></technical-data><parties><applicants><applicant mxw-id="PPAR1103305351" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>ANGEL SENSE LTD</last-name><address><country>IL</country></address></addressbook></applicant><applicant mxw-id="PPAR1103343606" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>ANGEL SENSE LTD.</last-name></addressbook></applicant><applicant mxw-id="PPAR1101644435" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Angel Sense Ltd.</last-name><iid>101476527</iid><address><street>Mevo Etrog 1</street><city>5640589 Yehud</city><country>IL</country></address></addressbook></applicant></applicants><inventors><inventor mxw-id="PPAR1103343254" load-source="docdb" sequence="1" format="epo"><addressbook><last-name>SOMER DORON</last-name><address><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103309024" load-source="docdb" sequence="1" format="intermediate"><addressbook><last-name>SOMER, Doron</last-name></addressbook></inventor><inventor mxw-id="PPAR1101647321" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>SOMER, Doron</last-name><address><street>20A Habroshim Street</street><city>6092000 Kadima</city><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103306678" load-source="docdb" sequence="2" format="epo"><addressbook><last-name>BEN-AZAR NERY</last-name><address><country>IL</country></address></addressbook></inventor><inventor mxw-id="PPAR1103310392" load-source="docdb" sequence="2" format="intermediate"><addressbook><last-name>BEN-AZAR, NERY</last-name></addressbook></inventor><inventor mxw-id="PPAR1101646386" load-source="patent-office" sequence="2" format="original"><addressbook><last-name>BEN-AZAR, NERY</last-name><address><street>Mevo Etorg 1</street><city>5640589 Yehud</city><country>IL</country></address></addressbook></inventor></inventors><agents><agent mxw-id="PPAR1101649827" load-source="patent-office" sequence="1" format="original"><addressbook><last-name>Balder IP Law, S.L.</last-name><iid>101507025</iid><address><street>Paseo de la Castellana 93 5a planta</street><city>28046 Madrid</city><country>ES</country></address></addressbook></agent></agents></parties><international-convention-data><pct-or-regional-filing-data ucid="IL-2014050173-W"><document-id><country>IL</country><doc-number>2014050173</doc-number><kind>W</kind><date>20140218</date><lang>EN</lang></document-id></pct-or-regional-filing-data><pct-or-regional-publishing-data ucid="WO-2014128698-A1"><document-id><country>WO</country><doc-number>2014128698</doc-number><kind>A1</kind><date>20140828</date><lang>EN</lang></document-id></pct-or-regional-publishing-data><designated-states><ep-contracting-states><country mxw-id="DS660623445" load-source="docdb">AL</country><country mxw-id="DS660719498" load-source="docdb">AT</country><country mxw-id="DS660623446" load-source="docdb">BE</country><country mxw-id="DS660621649" load-source="docdb">BG</country><country mxw-id="DS660788044" load-source="docdb">CH</country><country mxw-id="DS660701386" load-source="docdb">CY</country><country mxw-id="DS660701407" load-source="docdb">CZ</country><country mxw-id="DS660622968" load-source="docdb">DE</country><country mxw-id="DS660623451" load-source="docdb">DK</country><country mxw-id="DS660623452" load-source="docdb">EE</country><country mxw-id="DS660706877" load-source="docdb">ES</country><country mxw-id="DS660621650" load-source="docdb">FI</country><country mxw-id="DS660621659" load-source="docdb">FR</country><country mxw-id="DS660622969" load-source="docdb">GB</country><country mxw-id="DS660623453" load-source="docdb">GR</country><country mxw-id="DS660622970" load-source="docdb">HR</country><country mxw-id="DS660701408" load-source="docdb">HU</country><country mxw-id="DS660788045" load-source="docdb">IE</country><country mxw-id="DS660623454" load-source="docdb">IS</country><country mxw-id="DS660621660" load-source="docdb">IT</country><country mxw-id="DS660623471" load-source="docdb">LI</country><country mxw-id="DS660622979" load-source="docdb">LT</country><country mxw-id="DS660719511" load-source="docdb">LU</country><country mxw-id="DS660621661" load-source="docdb">LV</country><country mxw-id="DS660622980" load-source="docdb">MC</country><country mxw-id="DS660719512" load-source="docdb">MK</country><country mxw-id="DS660719513" load-source="docdb">MT</country><country mxw-id="DS660706878" load-source="docdb">NL</country><country mxw-id="DS660700914" load-source="docdb">NO</country><country mxw-id="DS660706883" load-source="docdb">PL</country><country mxw-id="DS660621662" load-source="docdb">PT</country><country mxw-id="DS660701409" load-source="docdb">RO</country><country mxw-id="DS660621675" load-source="docdb">RS</country><country mxw-id="DS660706884" load-source="docdb">SE</country><country mxw-id="DS660788046" load-source="docdb">SI</country><country mxw-id="DS660700923" load-source="docdb">SK</country><country mxw-id="DS660700924" load-source="docdb">SM</country><country mxw-id="DS660719514" load-source="docdb">TR</country></ep-contracting-states><ep-extended-states><ep-extended-state-data><country>BA</country></ep-extended-state-data><ep-extended-state-data><country>ME</country></ep-extended-state-data></ep-extended-states></designated-states></international-convention-data><office-specific-data><eptags><ep-no-a-document-published>*</ep-no-a-document-published></eptags></office-specific-data></bibliographic-data><abstract mxw-id="PA139077576" ref-ucid="WO-2014128698-A1" lang="EN" load-source="patent-office"><p num="0000">The present invention discloses a method and system for identifying exceptions, of a person/child behavior through scheduled behavior, using a personal communication device having at least one sensor attached to person/child body which provide location data and motion data, The method comprising the steps of: sampling measurements data for each sensor of the personal communication device, identifying atomic pattern of sampled measurements, said atomic pattern representing basic behavior of the person/child, identify location in which person has spent time using a clustering algorithm, create activity segmentation characterized by location, schedule, caregiver, or content derived from sensors measurements and atomic patterns and analyzing characteristics changes or combination thereof of activities in sequenced/complex activities in comparison to baseline, to identify exception which indicate of at least one of the following: changes at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule.</p></abstract><abstract mxw-id="PA139544312" ref-ucid="WO-2014128698-A1" lang="EN" source="national office" load-source="docdb"><p>The present invention discloses a method and system for identifying exceptions, of a person/child behavior through scheduled behavior, using a personal communication device having at least one sensor attached to person/child body which provide location data and motion data, The method comprising the steps of: sampling measurements data for each sensor of the personal communication device, identifying atomic pattern of sampled measurements, said atomic pattern representing basic behavior of the person/child, identify location in which person has spent time using a clustering algorithm, create activity segmentation characterized by location, schedule, caregiver, or content derived from sensors measurements and atomic patterns and analyzing characteristics changes or combination thereof of activities in sequenced/complex activities in comparison to baseline, to identify exception which indicate of at least one of the following: changes at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule.</p></abstract><abstract mxw-id="PA139077577" ref-ucid="WO-2014128698-A1" lang="FR" load-source="patent-office"><p num="0000">L'invention concerne un procédé et un système d'identification d'exceptions de comportements de personne/enfant, en fonction d'un comportement programmé, au moyen d'un dispositif de communication personnel pourvu d'au moins un capteur fixé sur le corps d'une personne/enfant et qui fournit des données de localisation et des données de mouvement. Le procédé selon l'invention consiste : à échantillonner des données de mesures pour chaque capteur du dispositif de communication personnel ; à identifier un modèle atomique des mesures échantillonnées, ledit modèle représentant un comportement de base de la personne/enfant ; à identifier un lieu où la personne a passé du temps, au moyen d'un algorithme de regroupement ; à créer une segmentation d'activités caractérisée par un lieu, un programme, un soignant ou un contenu issu des mesures de capteurs et des modèles atomiques ; et à analyser des changements de caractéristiques ou des combinaisons de changements d'activités séquentielles/complexes par rapport à une référence, de sorte à identifier une exception qui indique au moins un des éléments suivants : changement de niveau d'activité, changement de niveau d'éveil émotionnel, lieu inconnu ou lieu inattendu par rapport à un programme.</p></abstract><abstract mxw-id="PA139544313" ref-ucid="WO-2014128698-A1" lang="FR" source="national office" load-source="docdb"><p>L'invention concerne un procédé et un système d'identification d'exceptions de comportements de personne/enfant, en fonction d'un comportement programmé, au moyen d'un dispositif de communication personnel pourvu d'au moins un capteur fixé sur le corps d'une personne/enfant et qui fournit des données de localisation et des données de mouvement. Le procédé selon l'invention consiste : à échantillonner des données de mesures pour chaque capteur du dispositif de communication personnel ; à identifier un modèle atomique des mesures échantillonnées, ledit modèle représentant un comportement de base de la personne/enfant ; à identifier un lieu où la personne a passé du temps, au moyen d'un algorithme de regroupement ; à créer une segmentation d'activités caractérisée par un lieu, un programme, un soignant ou un contenu issu des mesures de capteurs et des modèles atomiques ; et à analyser des changements de caractéristiques ou des combinaisons de changements d'activités séquentielles/complexes par rapport à une référence, de sorte à identifier une exception qui indique au moins un des éléments suivants : changement de niveau d'activité, changement de niveau d'éveil émotionnel, lieu inconnu ou lieu inattendu par rapport à un programme.</p></abstract><description mxw-id="PDES78477423" ref-ucid="WO-2014128698-A1" lang="EN" load-source="patent-office"><!-- EPO <DP n="2"/>--><p id="p0001" num="0001"> METHOD AND SYSTEM FOR IDENTIFYING EXCEPTIONS OF </p><p id="p0002" num="0002"> PEOPLE BEHAVIOR </p><p id="p0003" num="0003">FIELD OF THE INVENTION </p><p id="p0004" num="0004"> [0001] The present invention relates generally to the field of systems and methods for tracking, analyzing, and identifying exceptions in subject behavior and more particularly, to systems and methods for tracking, analyzing, and identifying exceptions in subject behavior through scheduled location based behavior. </p><p id="p0005" num="0005">BACKGROUND OF THE INVENTION </p><p id="p0006" num="0006"> People with disabilities and in particular, those with communication and cognitive disabilities may experience great difficulties to express their feelings and emotions. The professional literature indicates that people with autism or other communication and cognitive disabilities are vulnerable and subjected to mistreatment and higher rates of violence and abuse (as compared to their peers without a disability). Hence, there is a need to ensure their personal safety and to develop tools that will allow their family members and service providers to have a better sense of their well-being. </p><p id="p0007" num="0007">SUMMARY OF THE INVENTION </p><p id="p0008" num="0008"> The present invention provides a method for identifying exceptions, of a person behavior through scheduled behavior, using a personal communication device associated with least one sensor attached to a person body or garments which provides at least location data and/or motion data. The method comprising the steps of: sampling measurements data for each sensor associated with the person communication device, identifying atomic pattern of sampled measurements, said atomic pattern representing basic behavior of the person, identify locations in which the person has spent time using a clustering algorithm, create activity segmentation characterized by location, schedule, activity type, or content derived from sensors measurements and atomic patterns and analyzing characteristics changes or combination thereof of activities in sequenced/complex activities in comparison to given baseline, to identify exception which indicate of at least one of the following: changes at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule. 
<!-- EPO <DP n="3"/>-->
 According to some embodiments of the present invention the method further comprising the step of creating visualization presentation of the scheduled location based on activity segmentation. </p><p id="p0009" num="0009"> According to some embodiments of the present invention the method further comprising the steps of receiving sensor measurements received from communication device associated with a caregiver or person/child currently located in vicinity of the person/child and identifying correlations/association between different sensors measurements. </p><p id="p0010" num="0010"> According to some embodiments of the present invention the method further comprising the steps of receiving data from environmental sensors </p><p id="p0011" num="0011"> According to some embodiments of the present invention the method further comprising the steps of receiving location based data from web based or external services. </p><p id="p0012" num="0012"> According to some embodiments of the present invention the method further comprising the step of creating a hierarchical representation of activity sequences of time period with the same context. </p><p id="p0013" num="0013"> According to some embodiments of the present invention the method further comprising the step of using schedule information provided by the guardian or other external sources to refine segmentation. </p><p id="p0014" num="0014"> According to some embodiments of the present invention the method comprising the step of receiving information from guardian associating a specific pattern or a specific time interval with a specific emotion and applying algorithms to learn the typical behavior of the person representing each emotion using the information provided by the guardian. </p><p id="p0015" num="0015"> According to some embodiments of the present invention the method the further comprising the step of using predefined domain specific rules for determining a reporting mechanism for sending an alert of behavior exceptions based on at least one of the following: severity of exceptions, classifications determined in the supervised learning stage and user preferences. </p><p id="p0016" num="0016"> According to some embodiments of the present invention the method further comprising the step of providing the guardian with communication interface to perform inquiries based on exceptions. </p><p id="p0017" num="0017"> According to some embodiments of the present invention the activities segmentation include classifying transits state between locations. 
<!-- EPO <DP n="4"/>-->
 The present invention provides a system for identifying exceptions, of a person behavior through scheduled behavior. The system comprised of : a personal communication device having at least one sensor attached to person body which provide location data and/or motion data , said communication associated with a sensor processing module for sampling measurements data for each sensor of the personal communication device and identifying atomic pattern of sampled measurements, said atomic pattern representing basic behavior of the person and a sever including segmentation modules for identifying location in which the person has spent time using a clustering algorithm, creating activity segmentation characterized by location, schedule, caregiver, or content derived from sensors measurements and atomic patterns and Exception Identification module for analyzing characteristics changes or combination thereof of activities in sequenced/complex activities in comparison to given baseline, to identify exception which indicate of at least one of the following: changes at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule. </p><p id="p0018" num="0018"> According to some embodiments of the present invention the system further comprising visualization presentation of the scheduled location based on activity segmentation. According to some embodiments of the present invention the segmentation module further receives sensor measurements received from communication device associated with a caregiver or person currently located in vicinity of the person and identifies correlations/association between different sensors measurements. </p><p id="p0019" num="0019">According to some embodiments of the present invention the segmentation modules further receives data from environmental sensors </p><p id="p0020" num="0020"> According to some embodiments of the present invention the segmentation module further receives location based data from web based services. </p><p id="p0021" num="0021"> According to some embodiments of the present invention the segmentation module further creates a hierarchical representation of activity sequences of time period with the same context. </p><p id="p0022" num="0022"> According to some embodiments of the present invention the segmentation module further uses schedule information provided by the guardian or other external sources to refine segmentation. </p><p id="p0023" num="0023"> According to some embodiments of the present invention the Exception Identification module further receives information from guardian associating a specific pattern or a 
<!-- EPO <DP n="5"/>-->
 specific time interval with a specific emotion and applying algorithms to learn the typical behavior of the person representing each emotion using the information provided by the guardian. </p><p id="p0024" num="0024"> According to some embodiments of the present invention the Exception Identification module further uses predefined domain specific rules for determining a reporting mechanism for sending an alert of behavior exceptions based on at least one of the following: severity of exceptions, classifications determined in the supervised learning stage and user preferences. </p><p id="p0025" num="0025"> According to some embodiments of the present invention the system further comprising communication interface for the guardian to perform inquiries based on exceptions. According to some embodiments of the present invention the segmentation module include classifying transits state between locations. </p><p id="p0026" num="0026">BRIEF DESCRIPTIONS OF THE DRAWINGS </p><p id="p0027" num="0027"> [0002] The subject matter regarded as the invention will become more clearly understood in light of the ensuing description of embodiments herein, given by way of example and for purposes of illustrative discussion of the present invention only, with reference to the accompanying drawings, wherein </p><p id="p0028" num="0028"> Fig. 1 is a block diagram, schematically illustrating a system for tracking, analyzing, and identifying exceptions in subject behavior, according to some embodiments of the invention; </p><p id="p0029" num="0029"> Fig. 2 is a flowchart, schematically illustrating a method for sensors processing, according to some embodiments of the invention; </p><p id="p0030" num="0030"> Fig. 3 is a flowchart, schematically illustrating a method of Power Management</p><p id="p0031" num="0031">, according to some embodiments of the invention; </p><p id="p0032" num="0032"> Fig. 4 is a flowchart, schematically illustrating a method for Day Segmentation by Location, according to some embodiments of the invention; </p><p id="p0033" num="0033"> Fig. 5 is a flowchart, schematically illustrating a method for Day Segmentation by Activities, according to some embodiments of the invention; </p><p id="p0034" num="0034"> Fig. 6 is a flowchart, schematically illustrating a method for Supervised Learning of Behaviors Classification by Emotion, according to some embodiments of the invention; </p><p id="p0035" num="0035"> Fig. 7 is a flowchart, schematically illustrating a method for Exception </p><p id="p0036" num="0036"> Identification and Reporting, according to some embodiments of the invention; 
<!-- EPO <DP n="6"/>-->
 Fig. 8 is a flowchart, schematically illustrating a Person Application processing, according to some embodiments of the invention; and </p><p id="p0037" num="0037"> Fig. 9 is a flowchart, schematically illustrating Caregiver Application processing, according to some embodiments of the invention. </p><p id="p0038" num="0038">DETAILED DESCRIPTIONS OF SOME EMBODIMENTS OF THE INVENTION </p><p id="p0039" num="0039"> [0003] While the description below contains many specifications, these should not be construed as limitations on the scope of the invention, but rather as exemplifications of the preferred embodiments. Those skilled in the art will envision other possible variations that are within its scope. Accordingly, the scope of the invention should be determined not by the embodiment illustrated, but by the appended claims and their legal equivalents. </p><p id="p0040" num="0040"> [0004] An embodiment is an example or implementation of the inventions. The various appearances of "one embodiment," "an embodiment" or "some embodiments" do not necessarily all refer to the same embodiments. Although various features of the invention may be described in the context of a single embodiment, the features may also be provided separately or in any suitable combination. Conversely, although the invention may be described herein in the context of separate embodiments for clarity, the invention may also be implemented in a single embodiment. </p><p id="p0041" num="0041"> [0005] Reference in the specification to "one embodiment", "an embodiment", "some embodiments" or "other embodiments" means that a particular feature, structure, or characteristic described in connection with the embodiments is included in at least one embodiments, but not necessarily all embodiments, of the inventions. It is understood that the phraseology and terminology employed herein is not to be construed as limiting and are for descriptive purpose only. </p><p id="p0042" num="0042"> [0006] The principles and uses of the teachings of the present invention may be better understood with reference to the accompanying description, figures and examples. It is to be understood that the details set forth herein do not construe a limitation to an application of the invention. Furthermore, it is to be understood that the invention can be carried out or practiced in various ways and that the invention can be implemented in embodiments other than the ones outlined in the description below. </p><p id="p0043" num="0043">[0007] It is to be understood that the terms "including", "comprising", "consisting" and grammatical variants thereof do not preclude the addition of one or more components, 
<!-- EPO <DP n="7"/>-->
 features, steps, or integers or groups thereof and that the terms are to be construed as specifying components, features, steps or integers. The phrase "consisting essentially of", and grammatical variants thereof, when used herein is not to be construed as excluding additional components, steps, features, integers or groups thereof but rather that the additional features, integers, steps, components or groups thereof do not materially alter the basic and novel characteristics of the claimed composition, device or method. </p><p id="p0044" num="0044"> [0008] If the specification or claims refer to "an additional" element, that does not preclude there being more than one of the additional element. It is to be understood that where the claims or specification refer to "a" or "an" element, such reference is not be construed that there is only one of that element. It is to be understood that where the specification states that a component, feature, structure, or characteristic "may", "might", "can" or "could" be included, that particular component, feature, structure, or characteristic is not required to be included. </p><p id="p0045" num="0045"> [0009] Where applicable, although state diagrams, flow diagrams or both may be used to describe embodiments, the invention is not limited to those diagrams or to the corresponding descriptions. For example, flow need not move through each illustrated box or state, or in exactly the same order as illustrated and described. </p><p id="p0046" num="0046">[0010] Methods of the present invention may be implemented by performing or completing manually, automatically, or a combination thereof, selected steps or tasks. The term "method" refers to manners, means, techniques and procedures for accomplishing a given task including, but not limited to, those manners, means, techniques and procedures either known to, or readily developed from known manners, means, techniques and procedures by practitioners of the art to which the invention belongs. The descriptions, examples, methods and materials presented in the claims and the specification are not to be construed as limiting but rather as illustrative only. </p><p id="p0047" num="0047">[0011] Meanings of technical and scientific terms used herein are to be commonly understood as by one of ordinary skill in the art to which the invention belongs, unless otherwise defined. The present invention can be implemented in the testing or practice with methods and materials equivalent or similar to those described herein. </p><p id="p0048" num="0048">[0012] Any publications, including patents, patent applications and articles, referenced or mentioned in this specification are herein incorporated in their entirety into the specification, to the same extent as if each individual publication was specifically and individually indicated to be incorporated herein. In addition, citation or identification of 
<!-- EPO <DP n="8"/>-->
 any reference in the description of some embodiments of the invention shall not be construed as an admission that such reference is available as prior art to the present invention. </p><p id="p0049" num="0049"> [0013] Fig. 1 is a block diagram, schematically illustrating a system for tracking, analyzing, and identifying exceptions in subject behavior, according to some embodiments of the invention. The system according to some embodiments of the present invention is comprised of at least one a Personal Tracking Device 10 for measuring behavior data of a Person, a server 50 for analyzing measured data and identifying exceptional behavior based on transmitted measured data, and Person database 80 for maintaining history of measured data and analyzed data. The system may be further be associated with one of the following information sources: Caregiver web based Application 20 for receiving location data and reports, Guardian web based Application 30 , environmental Sensors 40 enabling to measure environment condition in vicinity of the Person, such as temperature or monitoring the environment using camera or microphone, location-based Web Services 60 for providing real-time data at the current position of the Person and/or personal application enabling to track in realtime the person behavior and provide feedback. The Personal Tracking Device 10 basic structure includes at least one of the following: location sensor such as GPS, motion sensor such an accelerator, and or microphone. Optionally the Personal Tracking Device 10 includes sensor processing module 100 for sampling, compressing, filtering and/or power management module 110. The server 50 may comprise: Sensor Processing 510 for analyzing atomic behavior pattern , Day Segmentation by Location 520 and Day Segmentation by Activities 530 for analyzing and identifying activities and their characteristics, Supervised Learning of Behaviors Classification by Emotion 540 enabling to classify behaviors using guardian or caregivers feedback and Exception Identification and Reporting module 550 for identifying exceptional behavior based on identified activities and their characteristics by comparing to predefined base line. The power management and sensor processing modules may be processed at the tracking device or at the server or in both locations. </p><p id="p0050" num="0050"> [0014] According to some embodiments of the present invention the server may include management module for automatically controlling the personal tracking device sensors and modes of operation: such as listen-in mode or to 'ring' mode or to 'bidirectional call' mode or out of deep sleep mode and optionally defining settings of the 
<!-- EPO <DP n="9"/>-->
 device such as authorized numbers for listen-in or automatically setup geo-fences parameters. </p><p id="p0051" num="0051"> [0015] Fig. 2 is a flowchart, schematically illustrating a method for sensors processing, according to some embodiments of the invention. The method includes: at least one of the following operations: Smart activation of sensors based on data from other sensors, (E.g. activate microphone when measuring extreme forces in the accelerometer 1020), Collecting data from different sensors 1030, Data sampling, compression, filtering noise reduction and filtering 1040 and or identification of atomic patterns such as location change descriptions, motion patterns and keywords. </p><p id="p0052" num="0052"> [0016] The sensor of the tracking device may include at least one of the following: GPS, G-Sensor, Gyro, P-Sensor, Microphone, Camera, Heart Beat Monitor, blood pressure and GSR (Galvanic Skin Sensor). </p><p id="p0053" num="0053"> [0017] The compression and filter processes may include one of the following actions: </p><p id="p0054" num="0054"> [0018] For GPS sensor, accumulating of readings and sending measurements at changing frequencies based on activity, eliminating redundant GPS locations (same location) and incremental representation of the GPS readings (send an initial full GPS location and then only the differences), optionally transmitting only meaningful atomic motion patterns and not the raw data. </p><p id="p0055" num="0055"> [0019] Acceleration sensor: sampling of specific actual movements such as steps (pedometer) hand flapping, head banging, stretching skin etc. as well as some overall summary characteristics of movement (e.g. an indication of level of activity per minute). </p><p id="p0056" num="0056"> [0020] Microphone eliminating data below a certain volume and eliminating data where no human sound was detected, identifying keywords </p><p id="p0057" num="0057">[0021] Noise reduction may include, implementing noise reduction such as, GPS spikes elimination, smoothing and noise reduction can be done over the raw </p><p id="p0058" num="0058">acceleration samples (imply unreasonable speed). </p><p id="p0059" num="0059"> [0022] For Arousal sensors: sending only significant readings representing arousal over a certain level and/or sending summary data. E.g. averages readings over a number of seconds, heartbeat variability over a number of seconds, etc </p><p id="p0060" num="0060">[0023] Identification of atomic patterns is performed by analyzing real time basic behavior pattern, comparing to known pattern templates of classified action such as walking, hand clapping etc. 
<!-- EPO <DP n="10"/>-->
 [0024] Fig. 3 is a flowchart, schematically illustrating a method of Power Management, according to some embodiments of the invention. This method may include one of following steps: Identifying indoor location using GPS, CELL ID information as well as location information of network communication elements such as Wi-Fi or Bluetooth transmitters, to reduce GPS data transmission frequency 2020 and/or Set a dynamic geo-fence at the device, based on the learnt locations in the server to immediately identify exiting the location. </p><p id="p0061" num="0061"> [0025] The system according to some embodiments of the present invention uses learning algorithms to detect information regarding locations in which the person/child spends time in. The location information may include parameters such as the boundaries of the location for defining a geo-fence alert. </p><p id="p0062" num="0062"> [0026] Optionally the system of the present invention uses supervised data from the guardian or caregiver for providing meaningful names/tags to identified locations. </p><p id="p0063" num="0063">[0027] Once identifying a child/person, entering a location which is identified by name, the server automatically uses the history data aggregated by the learning algorithm related to this location for dynamically setting a geo-fence range which can be used at the personal device to generate alerts. The personal device may send GPS data once identifying the person/child is out of the geo-fence range. </p><p id="p0064" num="0064"> [0028] The geo-fence range may be dependent on multiple parameters, including the aggregated history of GPS data or other communication element (such as Bluetooth) accumulated by the learning algorithm and the history of GPS errors. </p><p id="p0065" num="0065">[0029] Fig. 4 is a flowchart, schematically illustrating a method for Day Segmentation by Location, according to some embodiments of the invention. This method may include one of following steps: Receiving real-time location sensor data of multiple sensors associated with a person carrying personal tracking device and caregiver including: GPS, acceleration, gyro , WIFI, Bluetooth, cellular network, orientation sensor 5210, analyzing location data across different sources to reduce location error and improve its accuracy 5220, Identify location in which the Person has spent time using a clustering algorithm 5230, (the location can be identified by type such as school ,park, home and/or by specific names of places) , Classifying transits state between locations such drives, walks, etc., and create an hierarchical representation of location data by context, e.g. grouping transits and stops to the entire drive from home to school. 
<!-- EPO <DP n="11"/>-->
 [0030] The analysis of location data include checking caregiver location and location of other Persons that are expected to be in same location. The analysis may applied by using techniques such as Kalman Filter. </p><p id="p0066" num="0066"> [0031] According to some embodiments of the present invention, the location can be refined by associating the location data to the schedule of the Persons based on given schedule by the guardian, other external sources or learned scheduled. </p><p id="p0067" num="0067">[0032] Fig. 5 is a flowchart, schematically illustrating a method for Day Segmentation by Activities, according to some embodiments of the invention. This method may include one of following steps: Identifying correlations from different sensors of person/child, guardian caregiver and/or environmental sensors 5310, Receiving, identified atomic patterns and activities characteristics in real-time 5320, Creating activity segmentation, where an activity is identified by analyzing schedule, location, and/or caregiver and/or content of activity 5330 and/or Creating a hierarchical representation of activity sequences. </p><p id="p0068" num="0068"> [0033] According to some embodiments of the present invention the segmentation is achieved by analyzing sampled raw measurement data and atomic patterns. </p><p id="p0069" num="0069">[0034] The hierarchical representation may include a sequence of activities of time period with the same context, for example the entire school stay that is composed out of the different activities in school or the entire ride from home to school that is composed of the stops and transits between them. </p><p id="p0070" num="0070"> [0035] Fig. 6 is a flowchart, schematically illustrating a method for Supervised Learning of Behaviors Classification by Emotion, according to some embodiments of the invention. This method includes at least one of the following steps: Receiving information from guardian associating a specific pattern or a specific time interval with a specific emotion 5410 and/or Applying algorithms to learn the typical behavior of the person/child representing each emotion using the information provided by the parent. The received information may include, indicating that a head banging identifies stress, or that the entire last 5 minutes represent stressful behavior. </p><p id="p0071" num="0071">[0036] Fig. 7 is a flowchart, schematically illustrating a method for Exception Identification and Reporting, according to some embodiments of the invention. The method includes at least one of the following steps: Analyzing characteristics changes and/or irregular activities in sequence/complex activities in all level of activities hierarchies in comparison to (predefined rules/templates) baseline 5510, using predefined domain specific rules for determining reporting mechanism 5520, 
<!-- EPO <DP n="12"/>-->
 and allow guardian to perform inquiries based on exceptions 5530, e.g. opening microphone or camera remotely for verifying person/child's status/reaction and optionally communicate with the person/child. </p><p id="p0072" num="0072"> [0037] The characteristics analysis enable to identify exceptions such as hanges at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule (such as Suspected Abuse in Transportation Event) longer transit routes, different transit routes. The exception analysis may be performed using time series analysis techniques. </p><p id="p0073" num="0073"> [0038] The reporting mechanism may include at least one of the following: daily report, weekly report, real-time notification using email/SMS/Notifications. </p><p id="p0074" num="0074">[0039] The predefined domain specific rules may be based on at least one of the following: severity of exceptions, classifications determined in the supervised learning stage and user preferences. </p><p id="p0075" num="0075"> [0040] Fig. 8 is a flowchart, schematically illustrating a Person Application processing, according to some embodiments of the invention. The person application may perform at one of the following: Generate a visual representation of the day scheduled segments by location or by activities, using pictures of locations or activities retrieved from web databases as well as pictures taken by caregiver or guardian 710, enable guardian to collect feedback from the person and or potentially add person /child's reaction classification by parent into the activity analysis. </p><p id="p0076" num="0076">[0041] Fig. 9 is a flowchart, schematically illustrating Caregiver Application processing, according to some embodiments of the invention. The Caregiver application may perform the following: Allow caregivers to report on the person activities (e.g. eating, medical taking, physical activities) input classifications of Person state (e.g. stress level) and/or Analyze and correlate caregivers classifications to activities to identify e.g. stressful activities. 
</p></description><claims mxw-id="PCLM70077694" ref-ucid="WO-2014128698-A1" lang="EN" load-source="patent-office"><claim-statement><!-- EPO <DP n="13"/>-->CLAIMS: </claim-statement><claim id="clm-0001" num="1"><claim-text>1. A method for identifying exceptions , of a person behavior and/or location </claim-text><claim-text> through scheduled behavior , using a personal communication device associated with least one sensor, which provide at least location data and/or motion data, attached to a person body or garments , said method comprising the steps of: sampling measurements data for each sensor associated with the person's communication device; </claim-text><claim-text> identifying atomic pattern of sampled measurements, said atomic pattern representing basic behavior of the person; </claim-text><claim-text> identify locations in which the person has spent time using a clustering algorithm; </claim-text><claim-text> create activity segmentation characterized by location, schedule, activity type, or content derived from sensors measurements and atomic patterns; analyzing characteristics changes or combination thereof of activities in sequenced/complex activities in comparison to given baseline, to identify exception which indicate of at least one of the following: changes at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule. </claim-text></claim><claim id="clm-0002" num="2"><claim-text> 2. The method of claim 1 further comprising the step of creating visualization presentation of the scheduled location based on activity segmentation. </claim-text></claim><claim id="clm-0003" num="3"><claim-text> 3. The method of claim 1 further comprising the steps of receiving sensor measurements received from communication device associated with a caregiver or person currently located in vicinity of the person and identifying correlations/association between different sensors measurements. </claim-text></claim><claim id="clm-0004" num="4"><claim-text> 4. The method of claim 1 further comprising the steps of receiving data from environmental sensors </claim-text></claim><claim id="clm-0005" num="5"><claim-text> 5. The method of claim 1 further comprising the steps of receiving location based data from web based or external services. </claim-text></claim><claim id="clm-0006" num="6"><claim-text> 6. The method of claim 1 further comprising the step of creating a hierarchical representation of activity sequences of time period with the same context. </claim-text></claim><claim id="clm-0007" num="7"><claim-text> 7. The method of claim 1 further comprising the step of using schedule information provided by the guardian or other external sources to refine segmentation. 
<!-- EPO <DP n="14"/>-->
</claim-text></claim><claim id="clm-0008" num="8"><claim-text>8. The method of claim 1 further comprising the step of receiving information from guardian associating a specific pattern or a specific time interval with a specific emotion and applying algorithms to learn the typical behavior of the person representing each emotion using the information provided by the guardian. </claim-text></claim><claim id="clm-0009" num="9"><claim-text> 9. The method of claim 1 further comprising the step of using predefined domain specific rules for determining a reporting mechanism for sending an alert of behavior exceptions based on at least one of the following: severity of exceptions, classifications determined in the supervised learning stage and user preferences. </claim-text></claim><claim id="clm-0010" num="10"><claim-text> 10. The method of claim 1 further comprising the step of providing the guardian with communication interface to perform inquiries based on exceptions. </claim-text></claim><claim id="clm-0011" num="11"><claim-text> 11. The method pf claim 1 wherein the activities segmentation includes classifying transits state between locations. </claim-text></claim><claim id="clm-0012" num="12"><claim-text> 12. The method of claim 1 wherein the sensor is at least one of: GPS, temperature, accelerometer, heartbeat, blood pressure, GSR, WIFI, Bluetooth. </claim-text></claim><claim id="clm-0013" num="13"><claim-text> 13. A system for identifying exceptions , of a person behavior and/or location through scheduled behavior , said system comprised of </claim-text><claim-text> a personal communication device associated with least one sensor attached to a person body or garments which provides at least location data and/or motion data said communication device is associated with a sensor processing module for sampling measurements data for each sensor of the personal communication device and identifying atomic pattern of sampled measurements, said atomic pattern represents basic behavior of the person; a sever including segmentation modules for identifying location in which the person has spent time using a clustering algorithm, creating activity segmentation characterized by location, schedule, caregiver, or content derived from sensors measurements and atomic patterns and Exception Identification module for analyzing characteristics changes or combination thereof of activities in sequenced/complex activities in comparison to given baseline, to identify exception which indicate of at least one of the following: changes at activity level, changes at emotional arousal level, unknown locations or unexpected locations based on schedule. 
<!-- EPO <DP n="15"/>-->
</claim-text></claim><claim id="clm-0014" num="14"><claim-text>14. The system of claim 12 further comprising visualization presentation of the scheduled location based on activity segmentation. </claim-text></claim><claim id="clm-0015" num="15"><claim-text> 15. The system of claim 12 wherein segmentation module further receives sensor measurements received from communication device associated with a caregiver or person currently located in vicinity of the person and identifies correlations/association between different sensors measurements. </claim-text></claim><claim id="clm-0016" num="16"><claim-text> 16. The system of claim 12 wherein the segmentation modules further receives data from environmental sensors </claim-text></claim><claim id="clm-0017" num="17"><claim-text> 17. The system of claim 12 wherein segmentation module further receives location based data from web based services. </claim-text></claim><claim id="clm-0018" num="18"><claim-text> 18. The system of claim 12 wherein segmentation module further creates a hierarchical representation of activity sequences of time period with the same context. </claim-text></claim><claim id="clm-0019" num="19"><claim-text> 19. The system of claim 12 wherein segmentation module further uses schedule information provided by the guardian or other external sources to refine segmentation. </claim-text></claim><claim id="clm-0020" num="20"><claim-text> 20. The system of claim 12 wherein Exception Identification module further receives information from guardian associating a specific pattern or a specific time interval with a specific emotion and applying algorithms to learn the typical behavior of the person representing each emotion using the information provided by the guardian. </claim-text></claim><claim id="clm-0021" num="21"><claim-text> 21. The system of claim 12 further wherein Exception Identification module further uses predefined domain specific rules for determining a reporting mechanism for sending an alert of behavior exceptions based on at least one of the following: severity of exceptions, classifications determined in the supervised learning stage and user preferences. </claim-text></claim><claim id="clm-0022" num="22"><claim-text> 22. The system of claim 12 further comprising communication interface for the guardian to perform inquiries based on exceptions. </claim-text></claim><claim id="clm-0023" num="23"><claim-text> 23. The system pf claim 12 wherein segmentation module includes classifying transits state between locations. </claim-text></claim><claim id="clm-0024" num="24"><claim-text> 24. The system of claim 12 wherein the sensor is at least one of: GPS, temperature, accelerometer, heartbeat, blood pressure, GSR, WIFI, Bluetooth. 
</claim-text></claim></claims><copyright>User acknowledges that Fairview Research LLC and its third party providers retain all right, title and interest in and to this xml under applicable copyright laws.  User acquires no ownership rights to this xml including but not limited to its format.  User hereby accepts the terms and conditions of the Licence Agreement</copyright></patent-document>
